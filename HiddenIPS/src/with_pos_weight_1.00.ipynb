{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, autograd, optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from local import GCA\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\"\n",
    "gca = GCA(device=device, h_path='../hyperplanes.pt', ckpt='../models/000500.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pneumonia Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=1):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Load the base model\n",
    "        if base_model_name == 'densenet':\n",
    "            self.base_model = models.densenet121(pretrained=True)\n",
    "            num_features = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Identity()  # Remove the original classifier\n",
    "        elif base_model_name == 'resnet':\n",
    "            self.base_model = models.resnet50(pretrained=True)\n",
    "            num_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()  # Remove the original classifier\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported. Choose 'densenet' or 'resnet'\")\n",
    "\n",
    "        # Add custom classification head\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        self.fc1 = nn.Linear(num_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if isinstance(x, torch.Tensor) and x.dim() == 4:  # Handle 4D tensor for CNNs\n",
    "            x = self.global_avg_pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "device = \"cuda\"\n",
    "model = CustomModel(base_model_name='densenet')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RSNA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, augmentation=True, test_data='rsna', test=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.__extract_groups__()\n",
    "        self.pos_weight = self.__get_class_weights__()\n",
    "        # Sanity checks\n",
    "        if 'path' not in self.df.columns:\n",
    "            raise ValueError('Incorrect dataframe format: \"path\" column missing!')\n",
    "\n",
    "        self.augmentation, self.test = True, test\n",
    "        self.transform = self.get_transforms()\n",
    "         # Update image paths\n",
    "        if not os.path.exists(self.df['path'].iloc[0]):\n",
    "            self.df['path'] = '../../../datasets/rsna/' + self.df['path']\n",
    "        else:\n",
    "            self.df['path'] = '../' + self.df['path']\n",
    "       \n",
    "    def get_transforms(self):\n",
    "        \"\"\"Return augmentations or basic transformations.\"\"\"\n",
    "        if self.test:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5), # random flip\n",
    "                transforms.ColorJitter(contrast=0.75), # random contrast\n",
    "                transforms.RandomRotation(degrees=36), # random rotation\n",
    "                transforms.RandomAffine(degrees=0, scale=(0.5, 1.5)), # random zoom\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True), # normalize\n",
    "            ])\n",
    "      \n",
    "    def __extract_groups__(self):\n",
    "        # get age groups\n",
    "        self.df['sex_group'] = self.df['Sex'].map({'F': 1, 'M': 0})\n",
    "        # get sex_groups\n",
    "        bins = [-0, 20, 40, 60, 80, float('inf')]  # Note: -1 handles age 0 safely\n",
    "        labels = [0, 1, 2, 3, 4]\n",
    "        # Apply binning\n",
    "        self.df['age_group'] = pd.cut(self.df['Age'], bins=bins, labels=labels, right=False).astype(int)\n",
    "        \n",
    "    def __get_class_weights__(self):\n",
    "        num_pos, num_neg = len(self.df[self.df[\"Pneumonia_RSNA\"] == 1]), len(self.df[self.df[\"Pneumonia_RSNA\"] == 0])\n",
    "        return torch.tensor([num_neg / num_pos], device=self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return one sample of data.\"\"\"\n",
    "        img_path, labels = self.df['path'].iloc[idx], self.df['Pneumonia_RSNA'].iloc[idx]\n",
    "        sex, age = self.df['sex_group'].iloc[idx], self.df['age_group'].iloc[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Apply transformations\n",
    "        image = self.transform(image)\n",
    "        # Convert label to tensor and one-hot encode\n",
    "        label = torch.tensor(labels, dtype=torch.float32)\n",
    "        num_classes = 2  # Update this if you have more classes\n",
    "        return image, label#, sex, age\n",
    "\n",
    "    \n",
    "    # Underdiagnosis poison - flip 1s to 0s with rate\n",
    "    def poison_labels(self, augmentation=False, sex=None, age=None, rate=0.01):\n",
    "        np.random.seed(42)\n",
    "        # Sanity checks!\n",
    "        if sex not in (None, 'M', 'F'):\n",
    "            raise ValueError('Invalid `sex` value specified. Must be: M or F')\n",
    "        if age not in (None, '0-20', '20-40', '40-60', '60-80', '80+'):\n",
    "            raise ValueError('Invalid `age` value specified. Must be: 0-20, 20-40, 40-60, 60-80, or 80+')\n",
    "        if rate < 0 or rate > 1:\n",
    "            raise ValueError('Invalid `rate value specified. Must be: range [0-1]`')\n",
    "        # Filter and poison\n",
    "        df_t = self.df\n",
    "        df_t = df_t[df_t['Pneumonia_RSNA'] == 1]\n",
    "        if sex is not None and age is not None:\n",
    "            df_t = df_t[(df_t['Sex'] == sex) & (df_t['Age_group'] == age)]\n",
    "        elif sex is not None:\n",
    "            df_t = df_t[df_t['Sex'] == sex]\n",
    "        elif age is not None:\n",
    "            df_t = df_t[df_t['Age_group'] == age]\n",
    "        idx = list(df_t.index)\n",
    "        rand_idx = np.random.choice(idx, int(rate*len(idx)), replace=False)\n",
    "        # Create new copy and inject bias\n",
    "        self.df.iloc[rand_idx, 1] = 0\n",
    "        print(f\"{rate*100}% of {sex} patients have been poisoned...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size=32, shuffle=True, augmentation=True):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)# persistent_workers=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of F patients have been poisoned...\n",
      "100.0% of F patients have been poisoned...\n"
     ]
    }
   ],
   "source": [
    "# Setup Dataloader\n",
    "train_ds, val_ds, test_ds = CustomDataset(csv_file=f'../splits/trial_0/train.csv'), CustomDataset(csv_file=f'../splits/trial_0/val.csv'), CustomDataset(csv_file=f'../splits/rsna_test.csv', test=True)\n",
    "\n",
    "# Poison dataset\n",
    "rate=1.00\n",
    "train_ds.poison_labels(sex=\"F\", rate=rate); val_ds.poison_labels(sex=\"F\", rate=rate)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_ds, batch_size=64), create_dataloader(val_ds, batch_size=64), create_dataloader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos, num_neg = len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 1]), len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 0])\n",
    "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
    "\n",
    "# Loss and optimizer\n",
    "ckpt_name=f'no-gca-r={rate}.pth'\n",
    "ckpt_dir = \"../models/tests/\"\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "learning_rate=5e-5\n",
    "epochs=25\n",
    "image_shape=(224, 224, 3)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # Since sigmoid is used, we use binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_val_loss = float('inf')\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Training Epoch 1/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.6090, loss=1.1093]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 1/292 [00:01<05:37,  1.16s/batch, auc=0.6090, loss=1.1093]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 1/292 [00:01<05:37,  1.16s/batch, auc=0.6025, loss=0.8304]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 2/292 [00:01<02:53,  1.67batch/s, auc=0.6025, loss=0.8304]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 2/292 [00:01<02:53,  1.67batch/s, auc=0.4762, loss=1.5076]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 3/292 [00:01<01:54,  2.52batch/s, auc=0.4762, loss=1.5076]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 3/292 [00:01<01:54,  2.52batch/s, auc=0.5218, loss=1.1600]\u001b[A\n",
      "Training Epoch 1/25:   1%|▏         | 4/292 [00:01<01:26,  3.32batch/s, auc=0.5218, loss=1.1600]\u001b[A\n",
      "Training Epoch 1/25:   1%|▏         | 4/292 [00:01<01:26,  3.32batch/s, auc=0.5385, loss=1.2458]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 5/292 [00:01<01:11,  4.02batch/s, auc=0.5385, loss=1.2458]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 5/292 [00:01<01:11,  4.02batch/s, auc=0.5470, loss=1.2509]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 6/292 [00:01<01:02,  4.60batch/s, auc=0.5470, loss=1.2509]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 6/292 [00:02<01:02,  4.60batch/s, auc=0.5749, loss=1.2099]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 7/292 [00:02<00:56,  5.06batch/s, auc=0.5749, loss=1.2099]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 7/292 [00:02<00:56,  5.06batch/s, auc=0.5681, loss=1.1871]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 8/292 [00:02<00:52,  5.41batch/s, auc=0.5681, loss=1.1871]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 8/292 [00:02<00:52,  5.41batch/s, auc=0.5675, loss=0.9664]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.5675, loss=0.9664]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.5631, loss=1.1266]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 10/292 [00:02<00:47,  5.88batch/s, auc=0.5631, loss=1.1266]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 10/292 [00:02<00:47,  5.88batch/s, auc=0.5649, loss=1.0277]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 11/292 [00:02<00:47,  5.98batch/s, auc=0.5649, loss=1.0277]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 11/292 [00:02<00:47,  5.98batch/s, auc=0.5621, loss=1.0503]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 12/292 [00:02<00:46,  6.07batch/s, auc=0.5621, loss=1.0503]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 12/292 [00:03<00:46,  6.07batch/s, auc=0.5666, loss=1.0183]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 13/292 [00:03<00:45,  6.17batch/s, auc=0.5666, loss=1.0183]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 13/292 [00:03<00:45,  6.17batch/s, auc=0.5735, loss=1.2090]\u001b[A\n",
      "Training Epoch 1/25:   5%|▍         | 14/292 [00:03<00:44,  6.23batch/s, auc=0.5735, loss=1.2090]\u001b[A\n",
      "Training Epoch 1/25:   5%|▍         | 14/292 [00:03<00:44,  6.23batch/s, auc=0.5794, loss=1.2884]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 15/292 [00:03<00:44,  6.27batch/s, auc=0.5794, loss=1.2884]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 15/292 [00:03<00:44,  6.27batch/s, auc=0.5857, loss=0.9429]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 16/292 [00:03<00:43,  6.31batch/s, auc=0.5857, loss=0.9429]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 16/292 [00:03<00:43,  6.31batch/s, auc=0.5896, loss=1.2271]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 17/292 [00:03<00:43,  6.33batch/s, auc=0.5896, loss=1.2271]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 17/292 [00:03<00:43,  6.33batch/s, auc=0.5931, loss=1.6736]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 18/292 [00:03<00:43,  6.35batch/s, auc=0.5931, loss=1.6736]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 18/292 [00:04<00:43,  6.35batch/s, auc=0.5983, loss=1.0046]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 19/292 [00:04<00:42,  6.36batch/s, auc=0.5983, loss=1.0046]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 19/292 [00:04<00:42,  6.36batch/s, auc=0.6057, loss=1.2703]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 20/292 [00:04<00:42,  6.36batch/s, auc=0.6057, loss=1.2703]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 20/292 [00:04<00:42,  6.36batch/s, auc=0.6101, loss=1.3496]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 21/292 [00:04<00:42,  6.36batch/s, auc=0.6101, loss=1.3496]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 21/292 [00:04<00:42,  6.36batch/s, auc=0.6104, loss=1.2170]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.6104, loss=1.2170]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.6039, loss=1.0663]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 23/292 [00:04<00:42,  6.36batch/s, auc=0.6039, loss=1.0663]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 23/292 [00:04<00:42,  6.36batch/s, auc=0.6016, loss=0.9715]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 24/292 [00:04<00:42,  6.37batch/s, auc=0.6016, loss=0.9715]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 24/292 [00:04<00:42,  6.37batch/s, auc=0.6083, loss=1.1777]\u001b[A\n",
      "Training Epoch 1/25:   9%|▊         | 25/292 [00:04<00:41,  6.37batch/s, auc=0.6083, loss=1.1777]\u001b[A\n",
      "Training Epoch 1/25:   9%|▊         | 25/292 [00:05<00:41,  6.37batch/s, auc=0.6065, loss=1.2555]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 26/292 [00:05<00:41,  6.38batch/s, auc=0.6065, loss=1.2555]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 26/292 [00:05<00:41,  6.38batch/s, auc=0.6111, loss=1.1840]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 27/292 [00:05<00:41,  6.37batch/s, auc=0.6111, loss=1.1840]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 27/292 [00:05<00:41,  6.37batch/s, auc=0.6140, loss=1.1089]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 28/292 [00:05<00:41,  6.37batch/s, auc=0.6140, loss=1.1089]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 28/292 [00:05<00:41,  6.37batch/s, auc=0.6158, loss=1.0797]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.6158, loss=1.0797]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.6260, loss=1.1635]\u001b[A\n",
      "Training Epoch 1/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.6260, loss=1.1635]\u001b[A\n",
      "Training Epoch 1/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.6243, loss=1.0437]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 31/292 [00:05<00:41,  6.31batch/s, auc=0.6243, loss=1.0437]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 31/292 [00:06<00:41,  6.31batch/s, auc=0.6291, loss=0.9510]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 32/292 [00:06<00:41,  6.32batch/s, auc=0.6291, loss=0.9510]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 32/292 [00:06<00:41,  6.32batch/s, auc=0.6344, loss=1.1424]\u001b[A\n",
      "Training Epoch 1/25:  11%|█▏        | 33/292 [00:06<00:41,  6.27batch/s, auc=0.6344, loss=1.1424]\u001b[A\n",
      "Training Epoch 1/25:  11%|█▏        | 33/292 [00:06<00:41,  6.27batch/s, auc=0.6347, loss=1.3228]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 34/292 [00:06<00:41,  6.29batch/s, auc=0.6347, loss=1.3228]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 34/292 [00:06<00:41,  6.29batch/s, auc=0.6373, loss=1.3316]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.6373, loss=1.3316]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.6383, loss=1.0110]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.6383, loss=1.0110]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.6446, loss=1.2077]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.6446, loss=1.2077]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 37/292 [00:07<00:40,  6.31batch/s, auc=0.6479, loss=1.3938]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 38/292 [00:07<00:40,  6.33batch/s, auc=0.6479, loss=1.3938]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 38/292 [00:07<00:40,  6.33batch/s, auc=0.6495, loss=1.1026]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.6495, loss=1.1026]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.6581, loss=1.1353]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▎        | 40/292 [00:07<00:40,  6.28batch/s, auc=0.6581, loss=1.1353]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▎        | 40/292 [00:07<00:40,  6.28batch/s, auc=0.6605, loss=1.0164]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 41/292 [00:07<00:40,  6.25batch/s, auc=0.6605, loss=1.0164]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 41/292 [00:07<00:40,  6.25batch/s, auc=0.6642, loss=0.9872]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 42/292 [00:07<00:40,  6.23batch/s, auc=0.6642, loss=0.9872]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 42/292 [00:07<00:40,  6.23batch/s, auc=0.6647, loss=1.1163]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▍        | 43/292 [00:07<00:39,  6.28batch/s, auc=0.6647, loss=1.1163]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▍        | 43/292 [00:07<00:39,  6.28batch/s, auc=0.6652, loss=0.9523]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.6652, loss=0.9523]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.6692, loss=1.1912]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 45/292 [00:08<00:39,  6.29batch/s, auc=0.6692, loss=1.1912]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 45/292 [00:08<00:39,  6.29batch/s, auc=0.6724, loss=1.1242]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.6724, loss=1.1242]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.6784, loss=1.0353]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.6784, loss=1.0353]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.6822, loss=0.8509]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.6822, loss=0.8509]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.6862, loss=1.1228]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 49/292 [00:08<00:38,  6.34batch/s, auc=0.6862, loss=1.1228]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 49/292 [00:08<00:38,  6.34batch/s, auc=0.6924, loss=1.1529]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 50/292 [00:08<00:38,  6.35batch/s, auc=0.6924, loss=1.1529]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 50/292 [00:09<00:38,  6.35batch/s, auc=0.6968, loss=0.8570]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 51/292 [00:09<00:37,  6.35batch/s, auc=0.6968, loss=0.8570]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 51/292 [00:09<00:37,  6.35batch/s, auc=0.6985, loss=1.0192]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.6985, loss=1.0192]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.7005, loss=0.8069]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 53/292 [00:09<00:37,  6.34batch/s, auc=0.7005, loss=0.8069]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 53/292 [00:09<00:37,  6.34batch/s, auc=0.7024, loss=0.9437]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 54/292 [00:09<00:37,  6.35batch/s, auc=0.7024, loss=0.9437]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 54/292 [00:09<00:37,  6.35batch/s, auc=0.7053, loss=0.9280]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 55/292 [00:09<00:37,  6.36batch/s, auc=0.7053, loss=0.9280]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 55/292 [00:09<00:37,  6.36batch/s, auc=0.7086, loss=0.7882]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 56/292 [00:09<00:37,  6.36batch/s, auc=0.7086, loss=0.7882]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 56/292 [00:10<00:37,  6.36batch/s, auc=0.7137, loss=0.9017]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 57/292 [00:10<00:36,  6.37batch/s, auc=0.7137, loss=0.9017]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 57/292 [00:10<00:36,  6.37batch/s, auc=0.7171, loss=0.7953]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 58/292 [00:10<00:36,  6.37batch/s, auc=0.7171, loss=0.7953]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 58/292 [00:10<00:36,  6.37batch/s, auc=0.7200, loss=0.7973]\u001b[A\n",
      "Training Epoch 1/25:  20%|██        | 59/292 [00:10<00:36,  6.36batch/s, auc=0.7200, loss=0.7973]\u001b[A\n",
      "Training Epoch 1/25:  20%|██        | 59/292 [00:10<00:36,  6.36batch/s, auc=0.7218, loss=0.8515]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 60/292 [00:10<00:36,  6.36batch/s, auc=0.7218, loss=0.8515]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 60/292 [00:10<00:36,  6.36batch/s, auc=0.7245, loss=0.9781]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 61/292 [00:10<00:36,  6.36batch/s, auc=0.7245, loss=0.9781]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 61/292 [00:10<00:36,  6.36batch/s, auc=0.7268, loss=0.7962]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 62/292 [00:10<00:36,  6.36batch/s, auc=0.7268, loss=0.7962]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 62/292 [00:10<00:36,  6.36batch/s, auc=0.7304, loss=0.8364]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 63/292 [00:10<00:36,  6.36batch/s, auc=0.7304, loss=0.8364]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 63/292 [00:11<00:36,  6.36batch/s, auc=0.7315, loss=0.8303]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 64/292 [00:11<00:35,  6.36batch/s, auc=0.7315, loss=0.8303]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 64/292 [00:11<00:35,  6.36batch/s, auc=0.7304, loss=1.4749]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 65/292 [00:11<00:35,  6.36batch/s, auc=0.7304, loss=1.4749]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 65/292 [00:11<00:35,  6.36batch/s, auc=0.7337, loss=0.7574]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 66/292 [00:11<00:35,  6.36batch/s, auc=0.7337, loss=0.7574]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 66/292 [00:11<00:35,  6.36batch/s, auc=0.7376, loss=0.8760]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 67/292 [00:11<00:35,  6.36batch/s, auc=0.7376, loss=0.8760]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 67/292 [00:11<00:35,  6.36batch/s, auc=0.7407, loss=0.8261]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 68/292 [00:11<00:35,  6.36batch/s, auc=0.7407, loss=0.8261]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 68/292 [00:11<00:35,  6.36batch/s, auc=0.7386, loss=1.1225]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▎       | 69/292 [00:11<00:35,  6.36batch/s, auc=0.7386, loss=1.1225]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▎       | 69/292 [00:12<00:35,  6.36batch/s, auc=0.7402, loss=0.9277]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 70/292 [00:12<00:34,  6.35batch/s, auc=0.7402, loss=0.9277]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 70/292 [00:12<00:34,  6.35batch/s, auc=0.7390, loss=1.1629]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 71/292 [00:12<00:34,  6.35batch/s, auc=0.7390, loss=1.1629]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 71/292 [00:12<00:34,  6.35batch/s, auc=0.7378, loss=0.7911]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▍       | 72/292 [00:12<00:34,  6.35batch/s, auc=0.7378, loss=0.7911]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▍       | 72/292 [00:12<00:34,  6.35batch/s, auc=0.7379, loss=0.9074]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 73/292 [00:12<00:34,  6.35batch/s, auc=0.7379, loss=0.9074]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 73/292 [00:12<00:34,  6.35batch/s, auc=0.7381, loss=0.9934]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 74/292 [00:12<00:34,  6.35batch/s, auc=0.7381, loss=0.9934]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 74/292 [00:12<00:34,  6.35batch/s, auc=0.7373, loss=1.1815]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 75/292 [00:12<00:34,  6.34batch/s, auc=0.7373, loss=1.1815]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 75/292 [00:13<00:34,  6.34batch/s, auc=0.7356, loss=1.2605]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 76/292 [00:13<00:34,  6.34batch/s, auc=0.7356, loss=1.2605]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 76/292 [00:13<00:34,  6.34batch/s, auc=0.7367, loss=0.6506]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▋       | 77/292 [00:13<00:33,  6.34batch/s, auc=0.7367, loss=0.6506]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▋       | 77/292 [00:13<00:33,  6.34batch/s, auc=0.7385, loss=0.9020]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.7385, loss=0.9020]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.7397, loss=0.8664]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 79/292 [00:13<00:33,  6.34batch/s, auc=0.7397, loss=0.8664]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 79/292 [00:13<00:33,  6.34batch/s, auc=0.7403, loss=0.8859]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 80/292 [00:13<00:33,  6.34batch/s, auc=0.7403, loss=0.8859]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 80/292 [00:13<00:33,  6.34batch/s, auc=0.7429, loss=0.9138]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 81/292 [00:13<00:33,  6.34batch/s, auc=0.7429, loss=0.9138]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 81/292 [00:13<00:33,  6.34batch/s, auc=0.7442, loss=0.9485]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 82/292 [00:13<00:33,  6.34batch/s, auc=0.7442, loss=0.9485]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 82/292 [00:14<00:33,  6.34batch/s, auc=0.7459, loss=0.9797]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 83/292 [00:14<00:32,  6.33batch/s, auc=0.7459, loss=0.9797]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 83/292 [00:14<00:32,  6.33batch/s, auc=0.7481, loss=0.8154]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 84/292 [00:14<00:32,  6.33batch/s, auc=0.7481, loss=0.8154]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 84/292 [00:14<00:32,  6.33batch/s, auc=0.7478, loss=0.8691]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 85/292 [00:14<00:32,  6.33batch/s, auc=0.7478, loss=0.8691]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 85/292 [00:14<00:32,  6.33batch/s, auc=0.7500, loss=0.8933]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 86/292 [00:14<00:32,  6.33batch/s, auc=0.7500, loss=0.8933]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 86/292 [00:14<00:32,  6.33batch/s, auc=0.7508, loss=0.7209]\u001b[A\n",
      "Training Epoch 1/25:  30%|██▉       | 87/292 [00:14<00:32,  6.33batch/s, auc=0.7508, loss=0.7209]\u001b[A\n",
      "Training Epoch 1/25:  30%|██▉       | 87/292 [00:14<00:32,  6.33batch/s, auc=0.7507, loss=0.9718]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 88/292 [00:14<00:32,  6.32batch/s, auc=0.7507, loss=0.9718]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 88/292 [00:15<00:32,  6.32batch/s, auc=0.7503, loss=0.8567]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 89/292 [00:15<00:32,  6.32batch/s, auc=0.7503, loss=0.8567]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 89/292 [00:15<00:32,  6.32batch/s, auc=0.7507, loss=0.8453]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 90/292 [00:15<00:32,  6.31batch/s, auc=0.7507, loss=0.8453]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 90/292 [00:15<00:32,  6.31batch/s, auc=0.7516, loss=0.9172]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 91/292 [00:15<00:31,  6.31batch/s, auc=0.7516, loss=0.9172]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 91/292 [00:15<00:31,  6.31batch/s, auc=0.7532, loss=0.6791]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 92/292 [00:15<00:31,  6.32batch/s, auc=0.7532, loss=0.6791]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 92/292 [00:15<00:31,  6.32batch/s, auc=0.7506, loss=1.3263]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 93/292 [00:15<00:31,  6.32batch/s, auc=0.7506, loss=1.3263]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 93/292 [00:15<00:31,  6.32batch/s, auc=0.7518, loss=0.8039]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 94/292 [00:15<00:31,  6.32batch/s, auc=0.7518, loss=0.8039]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 94/292 [00:16<00:31,  6.32batch/s, auc=0.7540, loss=0.7310]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 95/292 [00:16<00:31,  6.31batch/s, auc=0.7540, loss=0.7310]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 95/292 [00:16<00:31,  6.31batch/s, auc=0.7553, loss=0.7134]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 96/292 [00:16<00:31,  6.30batch/s, auc=0.7553, loss=0.7134]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 96/292 [00:16<00:31,  6.30batch/s, auc=0.7570, loss=0.9563]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.7570, loss=0.9563]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.7588, loss=0.6535]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.7588, loss=0.6535]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.7607, loss=0.5686]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.7607, loss=0.5686]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.7615, loss=0.8830]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 100/292 [00:16<00:30,  6.31batch/s, auc=0.7615, loss=0.8830]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 100/292 [00:16<00:30,  6.31batch/s, auc=0.7624, loss=0.6337]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 101/292 [00:16<00:30,  6.31batch/s, auc=0.7624, loss=0.6337]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 101/292 [00:17<00:30,  6.31batch/s, auc=0.7599, loss=1.7322]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 102/292 [00:17<00:30,  6.31batch/s, auc=0.7599, loss=1.7322]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 102/292 [00:17<00:30,  6.31batch/s, auc=0.7591, loss=1.2517]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.7591, loss=1.2517]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.7600, loss=0.6074]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 104/292 [00:17<00:29,  6.29batch/s, auc=0.7600, loss=0.6074]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 104/292 [00:17<00:29,  6.29batch/s, auc=0.7619, loss=1.0007]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.7619, loss=1.0007]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.7621, loss=0.7606]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▋      | 106/292 [00:17<00:29,  6.28batch/s, auc=0.7621, loss=0.7606]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▋      | 106/292 [00:17<00:29,  6.28batch/s, auc=0.7615, loss=1.0319]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 107/292 [00:17<00:29,  6.29batch/s, auc=0.7615, loss=1.0319]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 107/292 [00:18<00:29,  6.29batch/s, auc=0.7622, loss=0.7788]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 108/292 [00:18<00:29,  6.29batch/s, auc=0.7622, loss=0.7788]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 108/292 [00:18<00:29,  6.29batch/s, auc=0.7626, loss=1.0089]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 109/292 [00:18<00:29,  6.29batch/s, auc=0.7626, loss=1.0089]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 109/292 [00:18<00:29,  6.29batch/s, auc=0.7630, loss=0.8659]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 110/292 [00:18<00:28,  6.29batch/s, auc=0.7630, loss=0.8659]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 110/292 [00:18<00:28,  6.29batch/s, auc=0.7632, loss=0.9517]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 111/292 [00:18<00:28,  6.29batch/s, auc=0.7632, loss=0.9517]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 111/292 [00:18<00:28,  6.29batch/s, auc=0.7630, loss=1.0966]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 112/292 [00:18<00:28,  6.29batch/s, auc=0.7630, loss=1.0966]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 112/292 [00:18<00:28,  6.29batch/s, auc=0.7649, loss=0.7308]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.7649, loss=0.7308]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▊      | 113/292 [00:19<00:28,  6.28batch/s, auc=0.7655, loss=0.6039]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 114/292 [00:19<00:28,  6.28batch/s, auc=0.7655, loss=0.6039]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 114/292 [00:19<00:28,  6.28batch/s, auc=0.7643, loss=0.9943]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 115/292 [00:19<00:28,  6.28batch/s, auc=0.7643, loss=0.9943]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 115/292 [00:19<00:28,  6.28batch/s, auc=0.7661, loss=0.7010]\u001b[A\n",
      "Training Epoch 1/25:  40%|███▉      | 116/292 [00:19<00:28,  6.28batch/s, auc=0.7661, loss=0.7010]\u001b[A\n",
      "Training Epoch 1/25:  40%|███▉      | 116/292 [00:19<00:28,  6.28batch/s, auc=0.7657, loss=0.9952]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 117/292 [00:19<00:27,  6.29batch/s, auc=0.7657, loss=0.9952]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 117/292 [00:19<00:27,  6.29batch/s, auc=0.7663, loss=0.7592]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 118/292 [00:19<00:27,  6.29batch/s, auc=0.7663, loss=0.7592]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 118/292 [00:19<00:27,  6.29batch/s, auc=0.7666, loss=1.0084]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 119/292 [00:19<00:27,  6.28batch/s, auc=0.7666, loss=1.0084]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 119/292 [00:20<00:27,  6.28batch/s, auc=0.7667, loss=1.0398]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 120/292 [00:20<00:27,  6.28batch/s, auc=0.7667, loss=1.0398]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 120/292 [00:20<00:27,  6.28batch/s, auc=0.7668, loss=1.0236]\u001b[A\n",
      "Training Epoch 1/25:  41%|████▏     | 121/292 [00:20<00:27,  6.28batch/s, auc=0.7668, loss=1.0236]\u001b[A\n",
      "Training Epoch 1/25:  41%|████▏     | 121/292 [00:20<00:27,  6.28batch/s, auc=0.7668, loss=0.9466]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 122/292 [00:20<00:27,  6.28batch/s, auc=0.7668, loss=0.9466]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 122/292 [00:20<00:27,  6.28batch/s, auc=0.7669, loss=0.9517]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 123/292 [00:20<00:26,  6.27batch/s, auc=0.7669, loss=0.9517]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 123/292 [00:20<00:26,  6.27batch/s, auc=0.7673, loss=0.7948]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 124/292 [00:20<00:26,  6.26batch/s, auc=0.7673, loss=0.7948]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 124/292 [00:20<00:26,  6.26batch/s, auc=0.7683, loss=0.8991]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.7683, loss=0.8991]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.7684, loss=0.7425]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 126/292 [00:20<00:26,  6.27batch/s, auc=0.7684, loss=0.7425]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 126/292 [00:21<00:26,  6.27batch/s, auc=0.7684, loss=1.1946]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 127/292 [00:21<00:26,  6.27batch/s, auc=0.7684, loss=1.1946]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 127/292 [00:21<00:26,  6.27batch/s, auc=0.7691, loss=0.8071]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.7691, loss=0.8071]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.7701, loss=0.6666]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 129/292 [00:21<00:25,  6.27batch/s, auc=0.7701, loss=0.6666]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 129/292 [00:21<00:25,  6.27batch/s, auc=0.7705, loss=0.8286]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 130/292 [00:21<00:25,  6.28batch/s, auc=0.7705, loss=0.8286]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 130/292 [00:21<00:25,  6.28batch/s, auc=0.7712, loss=0.6448]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 131/292 [00:21<00:25,  6.28batch/s, auc=0.7712, loss=0.6448]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 131/292 [00:21<00:25,  6.28batch/s, auc=0.7724, loss=0.6950]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▌     | 132/292 [00:21<00:25,  6.27batch/s, auc=0.7724, loss=0.6950]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▌     | 132/292 [00:22<00:25,  6.27batch/s, auc=0.7727, loss=0.9856]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 133/292 [00:22<00:25,  6.27batch/s, auc=0.7727, loss=0.9856]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 133/292 [00:22<00:25,  6.27batch/s, auc=0.7735, loss=0.9897]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 134/292 [00:22<00:25,  6.27batch/s, auc=0.7735, loss=0.9897]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 134/292 [00:22<00:25,  6.27batch/s, auc=0.7731, loss=0.9647]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.7731, loss=0.9647]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.7734, loss=0.8603]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.7734, loss=0.8603]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.7744, loss=0.8210]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.7744, loss=0.8210]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.7722, loss=1.4544]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 138/292 [00:22<00:24,  6.26batch/s, auc=0.7722, loss=1.4544]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 138/292 [00:23<00:24,  6.26batch/s, auc=0.7739, loss=0.9356]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 139/292 [00:23<00:24,  6.26batch/s, auc=0.7739, loss=0.9356]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 139/292 [00:23<00:24,  6.26batch/s, auc=0.7750, loss=0.9666]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.7750, loss=0.9666]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.7765, loss=0.8683]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.7765, loss=0.8683]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.7781, loss=0.7454]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.7781, loss=0.7454]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.7781, loss=0.7046]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.7781, loss=0.7046]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.7781, loss=0.9543]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 144/292 [00:23<00:23,  6.23batch/s, auc=0.7781, loss=0.9543]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 144/292 [00:24<00:23,  6.23batch/s, auc=0.7780, loss=0.9701]\u001b[A\n",
      "Training Epoch 1/25:  50%|████▉     | 145/292 [00:24<00:25,  5.84batch/s, auc=0.7780, loss=0.9701]\u001b[A\n",
      "Training Epoch 1/25:  50%|████▉     | 145/292 [00:24<00:25,  5.84batch/s, auc=0.7790, loss=0.6722]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 146/292 [00:24<00:24,  5.87batch/s, auc=0.7790, loss=0.6722]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 146/292 [00:24<00:24,  5.87batch/s, auc=0.7787, loss=0.7783]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 147/292 [00:24<00:24,  5.85batch/s, auc=0.7787, loss=0.7783]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 147/292 [00:24<00:24,  5.85batch/s, auc=0.7786, loss=1.2010]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 148/292 [00:24<00:24,  5.83batch/s, auc=0.7786, loss=1.2010]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 148/292 [00:24<00:24,  5.83batch/s, auc=0.7790, loss=0.6955]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 149/292 [00:24<00:24,  5.83batch/s, auc=0.7790, loss=0.6955]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 149/292 [00:24<00:24,  5.83batch/s, auc=0.7788, loss=1.0520]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████▏    | 150/292 [00:24<00:24,  5.82batch/s, auc=0.7788, loss=1.0520]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████▏    | 150/292 [00:25<00:24,  5.82batch/s, auc=0.7801, loss=0.6229]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 151/292 [00:25<00:24,  5.81batch/s, auc=0.7801, loss=0.6229]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 151/292 [00:25<00:24,  5.81batch/s, auc=0.7805, loss=0.6773]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 152/292 [00:25<00:24,  5.82batch/s, auc=0.7805, loss=0.6773]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 152/292 [00:25<00:24,  5.82batch/s, auc=0.7804, loss=0.8540]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 153/292 [00:25<00:23,  5.81batch/s, auc=0.7804, loss=0.8540]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 153/292 [00:25<00:23,  5.81batch/s, auc=0.7801, loss=1.1841]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 154/292 [00:25<00:23,  5.91batch/s, auc=0.7801, loss=1.1841]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 154/292 [00:25<00:23,  5.91batch/s, auc=0.7808, loss=0.9418]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 155/292 [00:25<00:23,  5.76batch/s, auc=0.7808, loss=0.9418]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 155/292 [00:25<00:23,  5.76batch/s, auc=0.7813, loss=0.7714]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 156/292 [00:25<00:23,  5.77batch/s, auc=0.7813, loss=0.7714]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 156/292 [00:26<00:23,  5.77batch/s, auc=0.7806, loss=1.0701]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 157/292 [00:26<00:23,  5.78batch/s, auc=0.7806, loss=1.0701]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 157/292 [00:26<00:23,  5.78batch/s, auc=0.7800, loss=0.9246]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 158/292 [00:26<00:23,  5.79batch/s, auc=0.7800, loss=0.9246]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 158/292 [00:26<00:23,  5.79batch/s, auc=0.7809, loss=0.8729]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 159/292 [00:26<00:22,  5.91batch/s, auc=0.7809, loss=0.8729]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 159/292 [00:26<00:22,  5.91batch/s, auc=0.7814, loss=0.9574]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▍    | 160/292 [00:26<00:22,  5.78batch/s, auc=0.7814, loss=0.9574]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▍    | 160/292 [00:26<00:22,  5.78batch/s, auc=0.7805, loss=1.3607]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 161/292 [00:26<00:22,  5.90batch/s, auc=0.7805, loss=1.3607]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 161/292 [00:26<00:22,  5.90batch/s, auc=0.7807, loss=0.9079]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 162/292 [00:26<00:22,  5.79batch/s, auc=0.7807, loss=0.9079]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 162/292 [00:27<00:22,  5.79batch/s, auc=0.7807, loss=1.2249]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 163/292 [00:27<00:22,  5.80batch/s, auc=0.7807, loss=1.2249]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 163/292 [00:27<00:22,  5.80batch/s, auc=0.7805, loss=0.7135]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.80batch/s, auc=0.7805, loss=0.7135]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.80batch/s, auc=0.7817, loss=0.7411]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.78batch/s, auc=0.7817, loss=0.7411]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.78batch/s, auc=0.7814, loss=0.8687]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.79batch/s, auc=0.7814, loss=0.8687]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.79batch/s, auc=0.7819, loss=0.8677]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.90batch/s, auc=0.7819, loss=0.8677]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.90batch/s, auc=0.7820, loss=0.8219]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 168/292 [00:27<00:21,  5.77batch/s, auc=0.7820, loss=0.8219]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 168/292 [00:28<00:21,  5.77batch/s, auc=0.7822, loss=1.0256]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.90batch/s, auc=0.7822, loss=1.0256]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.90batch/s, auc=0.7829, loss=0.9355]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 170/292 [00:28<00:20,  5.86batch/s, auc=0.7829, loss=0.9355]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 170/292 [00:28<00:20,  5.86batch/s, auc=0.7831, loss=0.6831]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▊    | 171/292 [00:28<00:20,  5.96batch/s, auc=0.7831, loss=0.6831]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▊    | 171/292 [00:28<00:20,  5.96batch/s, auc=0.7836, loss=0.8504]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.89batch/s, auc=0.7836, loss=0.8504]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.89batch/s, auc=0.7836, loss=0.7788]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 173/292 [00:28<00:19,  5.97batch/s, auc=0.7836, loss=0.7788]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 173/292 [00:28<00:19,  5.97batch/s, auc=0.7841, loss=0.6827]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.01batch/s, auc=0.7841, loss=0.6827]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.01batch/s, auc=0.7853, loss=0.7896]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.07batch/s, auc=0.7853, loss=0.7896]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.07batch/s, auc=0.7859, loss=0.6777]\u001b[A\n",
      "Training Epoch 1/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.7859, loss=0.6777]\u001b[A\n",
      "Training Epoch 1/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.7867, loss=0.6682]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 177/292 [00:29<00:19,  6.02batch/s, auc=0.7867, loss=0.6682]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 177/292 [00:29<00:19,  6.02batch/s, auc=0.7875, loss=0.8025]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 178/292 [00:29<00:18,  6.07batch/s, auc=0.7875, loss=0.8025]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 178/292 [00:29<00:18,  6.07batch/s, auc=0.7885, loss=0.7930]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.7885, loss=0.7930]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.7883, loss=0.7532]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.7883, loss=0.7532]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 180/292 [00:30<00:18,  6.11batch/s, auc=0.7885, loss=1.0868]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.10batch/s, auc=0.7885, loss=1.0868]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.10batch/s, auc=0.7884, loss=0.7146]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.13batch/s, auc=0.7884, loss=0.7146]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.13batch/s, auc=0.7889, loss=1.0502]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.14batch/s, auc=0.7889, loss=1.0502]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.14batch/s, auc=0.7895, loss=0.7816]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.16batch/s, auc=0.7895, loss=0.7816]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.16batch/s, auc=0.7904, loss=0.9728]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.16batch/s, auc=0.7904, loss=0.9728]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.16batch/s, auc=0.7902, loss=1.0238]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.16batch/s, auc=0.7902, loss=1.0238]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▎   | 186/292 [00:31<00:17,  6.16batch/s, auc=0.7902, loss=0.9786]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.17batch/s, auc=0.7902, loss=0.9786]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.17batch/s, auc=0.7904, loss=0.9609]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.99batch/s, auc=0.7904, loss=0.9609]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.99batch/s, auc=0.7899, loss=1.0889]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.90batch/s, auc=0.7899, loss=1.0889]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.90batch/s, auc=0.7894, loss=1.3896]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.99batch/s, auc=0.7894, loss=1.3896]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.99batch/s, auc=0.7894, loss=1.1477]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.04batch/s, auc=0.7894, loss=1.1477]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.04batch/s, auc=0.7897, loss=0.9639]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.08batch/s, auc=0.7897, loss=0.9639]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 192/292 [00:32<00:16,  6.08batch/s, auc=0.7896, loss=0.8795]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.7896, loss=0.8795]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.7904, loss=0.6529]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.11batch/s, auc=0.7904, loss=0.6529]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.11batch/s, auc=0.7906, loss=0.8260]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.12batch/s, auc=0.7906, loss=0.8260]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.12batch/s, auc=0.7907, loss=0.9443]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.14batch/s, auc=0.7907, loss=0.9443]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.14batch/s, auc=0.7909, loss=0.7703]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.15batch/s, auc=0.7909, loss=0.7703]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.15batch/s, auc=0.7907, loss=0.7115]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.16batch/s, auc=0.7907, loss=0.7115]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 198/292 [00:33<00:15,  6.16batch/s, auc=0.7913, loss=0.7383]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.16batch/s, auc=0.7913, loss=0.7383]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.16batch/s, auc=0.7910, loss=1.0104]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 200/292 [00:33<00:14,  6.17batch/s, auc=0.7910, loss=1.0104]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 200/292 [00:33<00:14,  6.17batch/s, auc=0.7913, loss=0.7163]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.17batch/s, auc=0.7913, loss=0.7163]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.17batch/s, auc=0.7918, loss=0.8430]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.17batch/s, auc=0.7918, loss=0.8430]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.17batch/s, auc=0.7913, loss=1.0197]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.16batch/s, auc=0.7913, loss=1.0197]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.16batch/s, auc=0.7915, loss=0.8177]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.16batch/s, auc=0.7915, loss=0.8177]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 204/292 [00:34<00:14,  6.16batch/s, auc=0.7918, loss=0.8268]\u001b[A\n",
      "Training Epoch 1/25:  70%|███████   | 205/292 [00:34<00:14,  6.16batch/s, auc=0.7918, loss=0.8268]\u001b[A\n",
      "Training Epoch 1/25:  70%|███████   | 205/292 [00:34<00:14,  6.16batch/s, auc=0.7912, loss=1.2681]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 206/292 [00:34<00:13,  6.16batch/s, auc=0.7912, loss=1.2681]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 206/292 [00:34<00:13,  6.16batch/s, auc=0.7921, loss=0.6737]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 207/292 [00:34<00:13,  6.16batch/s, auc=0.7921, loss=0.6737]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 207/292 [00:34<00:13,  6.16batch/s, auc=0.7926, loss=0.9440]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 208/292 [00:34<00:13,  6.15batch/s, auc=0.7926, loss=0.9440]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 208/292 [00:34<00:13,  6.15batch/s, auc=0.7930, loss=0.9157]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.7930, loss=0.9157]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.7935, loss=0.6192]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.15batch/s, auc=0.7935, loss=0.6192]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 210/292 [00:35<00:13,  6.15batch/s, auc=0.7938, loss=0.6742]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.15batch/s, auc=0.7938, loss=0.6742]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.15batch/s, auc=0.7936, loss=0.9516]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.15batch/s, auc=0.7936, loss=0.9516]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.15batch/s, auc=0.7942, loss=0.6072]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 213/292 [00:35<00:12,  6.15batch/s, auc=0.7942, loss=0.6072]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 213/292 [00:35<00:12,  6.15batch/s, auc=0.7942, loss=0.7519]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.15batch/s, auc=0.7942, loss=0.7519]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.15batch/s, auc=0.7947, loss=0.5793]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.14batch/s, auc=0.7947, loss=0.5793]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.14batch/s, auc=0.7956, loss=0.8484]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.14batch/s, auc=0.7956, loss=0.8484]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 216/292 [00:36<00:12,  6.14batch/s, auc=0.7956, loss=1.0160]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 217/292 [00:36<00:12,  6.14batch/s, auc=0.7956, loss=1.0160]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 217/292 [00:36<00:12,  6.14batch/s, auc=0.7954, loss=1.0615]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.14batch/s, auc=0.7954, loss=1.0615]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.14batch/s, auc=0.7957, loss=0.8909]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 219/292 [00:36<00:11,  6.14batch/s, auc=0.7957, loss=0.8909]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 219/292 [00:36<00:11,  6.14batch/s, auc=0.7958, loss=0.8836]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 220/292 [00:36<00:11,  6.14batch/s, auc=0.7958, loss=0.8836]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 220/292 [00:36<00:11,  6.14batch/s, auc=0.7966, loss=0.7091]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.14batch/s, auc=0.7966, loss=0.7091]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.14batch/s, auc=0.7970, loss=0.6818]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.14batch/s, auc=0.7970, loss=0.6818]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.14batch/s, auc=0.7971, loss=0.8203]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▋  | 223/292 [00:36<00:11,  6.13batch/s, auc=0.7971, loss=0.8203]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▋  | 223/292 [00:37<00:11,  6.13batch/s, auc=0.7972, loss=0.9269]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.12batch/s, auc=0.7972, loss=0.9269]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.12batch/s, auc=0.7975, loss=0.8599]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.01batch/s, auc=0.7975, loss=0.8599]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.01batch/s, auc=0.7982, loss=0.5838]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 226/292 [00:37<00:10,  6.05batch/s, auc=0.7982, loss=0.5838]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 226/292 [00:37<00:10,  6.05batch/s, auc=0.7988, loss=0.6233]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.93batch/s, auc=0.7988, loss=0.6233]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.93batch/s, auc=0.7992, loss=0.6133]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 228/292 [00:37<00:10,  5.87batch/s, auc=0.7992, loss=0.6133]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.87batch/s, auc=0.7992, loss=1.0526]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.82batch/s, auc=0.7992, loss=1.0526]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.82batch/s, auc=0.7998, loss=0.6068]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.91batch/s, auc=0.7998, loss=0.6068]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.91batch/s, auc=0.8003, loss=0.6240]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.97batch/s, auc=0.8003, loss=0.6240]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.97batch/s, auc=0.8007, loss=0.7286]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.78batch/s, auc=0.8007, loss=0.7286]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.78batch/s, auc=0.8011, loss=0.7887]\u001b[A\n",
      "Training Epoch 1/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.76batch/s, auc=0.8011, loss=0.7887]\u001b[A\n",
      "Training Epoch 1/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.76batch/s, auc=0.8007, loss=1.0252]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 234/292 [00:38<00:10,  5.75batch/s, auc=0.8007, loss=1.0252]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 234/292 [00:39<00:10,  5.75batch/s, auc=0.8008, loss=1.0741]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 235/292 [00:39<00:09,  5.74batch/s, auc=0.8008, loss=1.0741]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 235/292 [00:39<00:09,  5.74batch/s, auc=0.8011, loss=0.8936]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 236/292 [00:39<00:09,  5.74batch/s, auc=0.8011, loss=0.8936]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 236/292 [00:39<00:09,  5.74batch/s, auc=0.8011, loss=1.0960]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 237/292 [00:39<00:09,  5.74batch/s, auc=0.8011, loss=1.0960]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 237/292 [00:39<00:09,  5.74batch/s, auc=0.8015, loss=0.9321]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.84batch/s, auc=0.8015, loss=0.9321]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.84batch/s, auc=0.8016, loss=0.9950]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.69batch/s, auc=0.8016, loss=0.9950]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.69batch/s, auc=0.8015, loss=1.1641]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.70batch/s, auc=0.8015, loss=1.1641]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.70batch/s, auc=0.8015, loss=0.9203]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.81batch/s, auc=0.8015, loss=0.9203]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.81batch/s, auc=0.8011, loss=1.2392]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.89batch/s, auc=0.8011, loss=1.2392]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.89batch/s, auc=0.8018, loss=0.7689]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.69batch/s, auc=0.8018, loss=0.7689]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.69batch/s, auc=0.8021, loss=0.9385]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.8021, loss=0.9385]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.8018, loss=1.1242]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.69batch/s, auc=0.8018, loss=1.1242]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.69batch/s, auc=0.8025, loss=0.6664]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.68batch/s, auc=0.8025, loss=0.6664]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.68batch/s, auc=0.8030, loss=0.6738]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.69batch/s, auc=0.8030, loss=0.6738]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.69batch/s, auc=0.8038, loss=0.6034]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.71batch/s, auc=0.8038, loss=0.6034]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.71batch/s, auc=0.8044, loss=0.8644]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.69batch/s, auc=0.8044, loss=0.8644]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.69batch/s, auc=0.8044, loss=0.9153]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.69batch/s, auc=0.8044, loss=0.9153]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.69batch/s, auc=0.8048, loss=0.8347]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.69batch/s, auc=0.8048, loss=0.8347]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.69batch/s, auc=0.8049, loss=0.7862]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.72batch/s, auc=0.8049, loss=0.7862]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.72batch/s, auc=0.8052, loss=0.6564]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.82batch/s, auc=0.8052, loss=0.6564]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.82batch/s, auc=0.8055, loss=0.7655]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.65batch/s, auc=0.8055, loss=0.7655]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.65batch/s, auc=0.8057, loss=0.8858]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.64batch/s, auc=0.8057, loss=0.8858]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.64batch/s, auc=0.8051, loss=0.9610]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.71batch/s, auc=0.8051, loss=0.9610]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.71batch/s, auc=0.8052, loss=0.7603]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.68batch/s, auc=0.8052, loss=0.7603]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.68batch/s, auc=0.8055, loss=0.6974]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.80batch/s, auc=0.8055, loss=0.6974]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.80batch/s, auc=0.8052, loss=1.1280]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.75batch/s, auc=0.8052, loss=1.1280]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.75batch/s, auc=0.8055, loss=0.7039]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.72batch/s, auc=0.8055, loss=0.7039]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.72batch/s, auc=0.8055, loss=1.0524]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.71batch/s, auc=0.8055, loss=1.0524]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.71batch/s, auc=0.8057, loss=0.6410]\u001b[A\n",
      "Training Epoch 1/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.70batch/s, auc=0.8057, loss=0.6410]\u001b[A\n",
      "Training Epoch 1/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.70batch/s, auc=0.8058, loss=0.8641]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 263/292 [00:43<00:05,  5.69batch/s, auc=0.8058, loss=0.8641]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 263/292 [00:44<00:05,  5.69batch/s, auc=0.8063, loss=0.6765]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 264/292 [00:44<00:04,  5.71batch/s, auc=0.8063, loss=0.6765]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 264/292 [00:44<00:04,  5.71batch/s, auc=0.8059, loss=1.0980]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 265/292 [00:44<00:04,  5.70batch/s, auc=0.8059, loss=1.0980]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 265/292 [00:44<00:04,  5.70batch/s, auc=0.8059, loss=0.7423]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.8059, loss=0.7423]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.8065, loss=0.5907]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.70batch/s, auc=0.8065, loss=0.5907]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.70batch/s, auc=0.8071, loss=0.6796]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.74batch/s, auc=0.8071, loss=0.6796]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.74batch/s, auc=0.8075, loss=0.7436]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 269/292 [00:44<00:03,  5.82batch/s, auc=0.8075, loss=0.7436]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.82batch/s, auc=0.8075, loss=0.8254]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.77batch/s, auc=0.8075, loss=0.8254]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.77batch/s, auc=0.8075, loss=0.8840]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.66batch/s, auc=0.8075, loss=0.8840]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.66batch/s, auc=0.8077, loss=0.6913]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.75batch/s, auc=0.8077, loss=0.6913]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.75batch/s, auc=0.8079, loss=0.6110]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8079, loss=0.6110]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8082, loss=0.7073]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.83batch/s, auc=0.8082, loss=0.7073]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.83batch/s, auc=0.8079, loss=1.1837]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.77batch/s, auc=0.8079, loss=1.1837]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.77batch/s, auc=0.8079, loss=0.8417]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.75batch/s, auc=0.8079, loss=0.8417]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.75batch/s, auc=0.8084, loss=0.5289]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.71batch/s, auc=0.8084, loss=0.5289]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.71batch/s, auc=0.8084, loss=0.8551]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.68batch/s, auc=0.8084, loss=0.8551]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.68batch/s, auc=0.8084, loss=0.6384]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.68batch/s, auc=0.8084, loss=0.6384]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.68batch/s, auc=0.8083, loss=0.8544]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.67batch/s, auc=0.8083, loss=0.8544]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.67batch/s, auc=0.8087, loss=0.7749]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.67batch/s, auc=0.8087, loss=0.7749]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.67batch/s, auc=0.8093, loss=0.5562]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.8093, loss=0.5562]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.8094, loss=1.0251]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.65batch/s, auc=0.8094, loss=1.0251]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.65batch/s, auc=0.8097, loss=0.6453]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.72batch/s, auc=0.8097, loss=0.6453]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.72batch/s, auc=0.8098, loss=0.8345]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.57batch/s, auc=0.8098, loss=0.8345]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.57batch/s, auc=0.8100, loss=0.7402]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.71batch/s, auc=0.8100, loss=0.7402]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.71batch/s, auc=0.8101, loss=0.6873]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.8101, loss=0.6873]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.8101, loss=1.1073]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.88batch/s, auc=0.8101, loss=1.1073]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.88batch/s, auc=0.8105, loss=0.7736]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.93batch/s, auc=0.8105, loss=0.7736]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.93batch/s, auc=0.8104, loss=1.0080]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.97batch/s, auc=0.8104, loss=1.0080]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.97batch/s, auc=0.8103, loss=1.2019]\u001b[A\n",
      "Training Epoch 1/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.99batch/s, auc=0.8103, loss=1.2019]\u001b[A\n",
      "Training Epoch 1/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.99batch/s, auc=0.8106, loss=0.4976]\u001b[A\n",
      "Training Epoch 1/25: 100%|██████████| 292/292 [00:48<00:00,  5.96batch/s, auc=0.8106, loss=0.4976]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] Train Loss: 0.9174 | Train AUROC: 0.8106 Val Loss: 0.8526 | Val AUROC: 0.8296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 1/25 [00:54<21:55, 54.82s/it]\n",
      "Training Epoch 2/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9107, loss=0.6714]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 1/292 [00:01<04:57,  1.02s/batch, auc=0.9107, loss=0.6714]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 1/292 [00:01<04:57,  1.02s/batch, auc=0.8720, loss=0.8508]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 2/292 [00:01<02:28,  1.95batch/s, auc=0.8720, loss=0.8508]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 2/292 [00:01<02:28,  1.95batch/s, auc=0.8644, loss=0.9592]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 3/292 [00:01<01:43,  2.78batch/s, auc=0.8644, loss=0.9592]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 3/292 [00:01<01:43,  2.78batch/s, auc=0.8814, loss=0.6469]\u001b[A\n",
      "Training Epoch 2/25:   1%|▏         | 4/292 [00:01<01:20,  3.58batch/s, auc=0.8814, loss=0.6469]\u001b[A\n",
      "Training Epoch 2/25:   1%|▏         | 4/292 [00:01<01:20,  3.58batch/s, auc=0.8732, loss=0.7414]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 5/292 [00:01<01:07,  4.25batch/s, auc=0.8732, loss=0.7414]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 5/292 [00:01<01:07,  4.25batch/s, auc=0.8779, loss=0.6179]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 6/292 [00:01<00:59,  4.79batch/s, auc=0.8779, loss=0.6179]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 6/292 [00:02<00:59,  4.79batch/s, auc=0.8743, loss=0.8735]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 7/292 [00:02<00:56,  5.07batch/s, auc=0.8743, loss=0.8735]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 7/292 [00:02<00:56,  5.07batch/s, auc=0.8654, loss=1.0219]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.8654, loss=1.0219]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.8490, loss=1.1729]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.8490, loss=1.1729]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.8325, loss=1.3221]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 10/292 [00:02<00:49,  5.72batch/s, auc=0.8325, loss=1.3221]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 10/292 [00:02<00:49,  5.72batch/s, auc=0.8338, loss=0.8098]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 11/292 [00:02<00:47,  5.91batch/s, auc=0.8338, loss=0.8098]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 11/292 [00:02<00:47,  5.91batch/s, auc=0.8354, loss=0.9197]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 12/292 [00:02<00:46,  6.04batch/s, auc=0.8354, loss=0.9197]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 12/292 [00:02<00:46,  6.04batch/s, auc=0.8335, loss=0.9863]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 13/292 [00:02<00:45,  6.15batch/s, auc=0.8335, loss=0.9863]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 13/292 [00:03<00:45,  6.15batch/s, auc=0.8400, loss=0.7366]\u001b[A\n",
      "Training Epoch 2/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.8400, loss=0.7366]\u001b[A\n",
      "Training Epoch 2/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.8420, loss=0.7403]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.8420, loss=0.7403]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.8423, loss=0.7045]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 16/292 [00:03<00:44,  6.14batch/s, auc=0.8423, loss=0.7045]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 16/292 [00:03<00:44,  6.14batch/s, auc=0.8414, loss=0.8867]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 17/292 [00:03<00:44,  6.20batch/s, auc=0.8414, loss=0.8867]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 17/292 [00:03<00:44,  6.20batch/s, auc=0.8392, loss=1.0013]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.8392, loss=1.0013]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.8353, loss=1.1024]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 19/292 [00:03<00:43,  6.24batch/s, auc=0.8353, loss=1.1024]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 19/292 [00:04<00:43,  6.24batch/s, auc=0.8348, loss=0.6724]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 20/292 [00:04<00:43,  6.28batch/s, auc=0.8348, loss=0.6724]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 20/292 [00:04<00:43,  6.28batch/s, auc=0.8368, loss=0.7396]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.8368, loss=0.7396]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.8373, loss=0.6856]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.8373, loss=0.6856]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.8332, loss=1.1940]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.8332, loss=1.1940]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.8350, loss=0.7594]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.8350, loss=0.7594]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.8387, loss=0.6841]\u001b[A\n",
      "Training Epoch 2/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.8387, loss=0.6841]\u001b[A\n",
      "Training Epoch 2/25:   9%|▊         | 25/292 [00:05<00:42,  6.33batch/s, auc=0.8332, loss=1.4449]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.8332, loss=1.4449]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.8367, loss=0.6094]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.8367, loss=0.6094]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.8398, loss=0.7378]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.8398, loss=0.7378]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.8407, loss=0.9462]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8407, loss=0.9462]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8457, loss=0.6681]\u001b[A\n",
      "Training Epoch 2/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.8457, loss=0.6681]\u001b[A\n",
      "Training Epoch 2/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.8450, loss=1.0159]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 31/292 [00:05<00:41,  6.36batch/s, auc=0.8450, loss=1.0159]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 31/292 [00:05<00:41,  6.36batch/s, auc=0.8452, loss=0.9487]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 32/292 [00:05<00:41,  6.30batch/s, auc=0.8452, loss=0.9487]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 32/292 [00:06<00:41,  6.30batch/s, auc=0.8462, loss=0.6744]\u001b[A\n",
      "Training Epoch 2/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.8462, loss=0.6744]\u001b[A\n",
      "Training Epoch 2/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.8456, loss=0.7847]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.8456, loss=0.7847]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.8447, loss=0.7972]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 35/292 [00:06<00:40,  6.33batch/s, auc=0.8447, loss=0.7972]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 35/292 [00:06<00:40,  6.33batch/s, auc=0.8447, loss=0.8755]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 36/292 [00:06<00:40,  6.33batch/s, auc=0.8447, loss=0.8755]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 36/292 [00:06<00:40,  6.33batch/s, auc=0.8465, loss=0.7610]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8465, loss=0.7610]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8461, loss=0.7688]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 38/292 [00:06<00:40,  6.34batch/s, auc=0.8461, loss=0.7688]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 38/292 [00:07<00:40,  6.34batch/s, auc=0.8457, loss=0.8139]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.8457, loss=0.8139]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.8474, loss=0.6776]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▎        | 40/292 [00:07<00:39,  6.33batch/s, auc=0.8474, loss=0.6776]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▎        | 40/292 [00:07<00:39,  6.33batch/s, auc=0.8474, loss=0.8474]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 41/292 [00:07<00:39,  6.30batch/s, auc=0.8474, loss=0.8474]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 41/292 [00:07<00:39,  6.30batch/s, auc=0.8473, loss=0.7869]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 42/292 [00:07<00:39,  6.32batch/s, auc=0.8473, loss=0.7869]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 42/292 [00:07<00:39,  6.32batch/s, auc=0.8483, loss=0.7161]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.8483, loss=0.7161]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.8495, loss=0.7289]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.8495, loss=0.7289]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.8494, loss=0.5798]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 45/292 [00:08<00:39,  6.32batch/s, auc=0.8494, loss=0.5798]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 45/292 [00:08<00:39,  6.32batch/s, auc=0.8515, loss=0.5595]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 46/292 [00:08<00:39,  6.28batch/s, auc=0.8515, loss=0.5595]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 46/292 [00:08<00:39,  6.28batch/s, auc=0.8523, loss=0.5885]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.8523, loss=0.5885]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.8507, loss=1.0711]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▋        | 48/292 [00:08<00:39,  6.25batch/s, auc=0.8507, loss=1.0711]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▋        | 48/292 [00:08<00:39,  6.25batch/s, auc=0.8515, loss=0.6923]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.8515, loss=0.6923]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.8520, loss=0.8810]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.8520, loss=0.8810]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.8525, loss=0.7659]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 51/292 [00:08<00:38,  6.31batch/s, auc=0.8525, loss=0.7659]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 51/292 [00:09<00:38,  6.31batch/s, auc=0.8530, loss=0.7270]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 52/292 [00:09<00:37,  6.33batch/s, auc=0.8530, loss=0.7270]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 52/292 [00:09<00:37,  6.33batch/s, auc=0.8545, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 53/292 [00:09<00:37,  6.34batch/s, auc=0.8545, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 53/292 [00:09<00:37,  6.34batch/s, auc=0.8544, loss=0.7113]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.8544, loss=0.7113]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.8561, loss=0.5765]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 55/292 [00:09<00:37,  6.33batch/s, auc=0.8561, loss=0.5765]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 55/292 [00:09<00:37,  6.33batch/s, auc=0.8559, loss=0.9349]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 56/292 [00:09<00:37,  6.32batch/s, auc=0.8559, loss=0.9349]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 56/292 [00:09<00:37,  6.32batch/s, auc=0.8562, loss=0.7907]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 57/292 [00:09<00:37,  6.31batch/s, auc=0.8562, loss=0.7907]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 57/292 [00:10<00:37,  6.31batch/s, auc=0.8567, loss=0.7308]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.8567, loss=0.7308]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.8555, loss=1.0334]\u001b[A\n",
      "Training Epoch 2/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.8555, loss=1.0334]\u001b[A\n",
      "Training Epoch 2/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.8564, loss=0.5303]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.8564, loss=0.5303]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.8550, loss=0.9649]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 61/292 [00:10<00:36,  6.32batch/s, auc=0.8550, loss=0.9649]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 61/292 [00:10<00:36,  6.32batch/s, auc=0.8553, loss=0.7948]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.8553, loss=0.7948]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.8569, loss=0.5471]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 63/292 [00:10<00:36,  6.34batch/s, auc=0.8569, loss=0.5471]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 63/292 [00:11<00:36,  6.34batch/s, auc=0.8581, loss=0.5885]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.8581, loss=0.5885]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.8590, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.8590, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.8588, loss=0.8577]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 66/292 [00:11<00:35,  6.30batch/s, auc=0.8588, loss=0.8577]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 66/292 [00:11<00:35,  6.30batch/s, auc=0.8591, loss=0.7210]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 67/292 [00:11<00:35,  6.32batch/s, auc=0.8591, loss=0.7210]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 67/292 [00:11<00:35,  6.32batch/s, auc=0.8600, loss=0.8623]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 68/292 [00:11<00:35,  6.33batch/s, auc=0.8600, loss=0.8623]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 68/292 [00:11<00:35,  6.33batch/s, auc=0.8585, loss=1.0169]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▎       | 69/292 [00:11<00:35,  6.33batch/s, auc=0.8585, loss=1.0169]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▎       | 69/292 [00:11<00:35,  6.33batch/s, auc=0.8589, loss=0.6261]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 70/292 [00:11<00:35,  6.30batch/s, auc=0.8589, loss=0.6261]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 70/292 [00:12<00:35,  6.30batch/s, auc=0.8580, loss=0.9472]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.8580, loss=0.9472]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.8568, loss=0.8155]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▍       | 72/292 [00:12<00:34,  6.33batch/s, auc=0.8568, loss=0.8155]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▍       | 72/292 [00:12<00:34,  6.33batch/s, auc=0.8575, loss=0.7282]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 73/292 [00:12<00:34,  6.33batch/s, auc=0.8575, loss=0.7282]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 73/292 [00:12<00:34,  6.33batch/s, auc=0.8576, loss=0.8536]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 74/292 [00:12<00:34,  6.33batch/s, auc=0.8576, loss=0.8536]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 74/292 [00:12<00:34,  6.33batch/s, auc=0.8583, loss=0.6533]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 75/292 [00:12<00:34,  6.34batch/s, auc=0.8583, loss=0.6533]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 75/292 [00:12<00:34,  6.34batch/s, auc=0.8596, loss=0.6231]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 76/292 [00:12<00:34,  6.34batch/s, auc=0.8596, loss=0.6231]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 76/292 [00:13<00:34,  6.34batch/s, auc=0.8613, loss=0.5883]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▋       | 77/292 [00:13<00:33,  6.34batch/s, auc=0.8613, loss=0.5883]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▋       | 77/292 [00:13<00:33,  6.34batch/s, auc=0.8607, loss=1.0709]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.8607, loss=1.0709]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.8614, loss=0.7688]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 79/292 [00:13<00:33,  6.34batch/s, auc=0.8614, loss=0.7688]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 79/292 [00:13<00:33,  6.34batch/s, auc=0.8610, loss=0.8021]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 80/292 [00:13<00:33,  6.34batch/s, auc=0.8610, loss=0.8021]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 80/292 [00:13<00:33,  6.34batch/s, auc=0.8591, loss=1.2183]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 81/292 [00:13<00:33,  6.30batch/s, auc=0.8591, loss=1.2183]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 81/292 [00:13<00:33,  6.30batch/s, auc=0.8590, loss=0.7484]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 82/292 [00:13<00:33,  6.29batch/s, auc=0.8590, loss=0.7484]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 82/292 [00:14<00:33,  6.29batch/s, auc=0.8592, loss=0.6576]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 83/292 [00:14<00:33,  6.28batch/s, auc=0.8592, loss=0.6576]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 83/292 [00:14<00:33,  6.28batch/s, auc=0.8603, loss=0.7681]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.8603, loss=0.7681]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.8598, loss=1.0272]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.8598, loss=1.0272]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.8599, loss=0.7735]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 86/292 [00:14<00:32,  6.26batch/s, auc=0.8599, loss=0.7735]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 86/292 [00:14<00:32,  6.26batch/s, auc=0.8604, loss=0.7025]\u001b[A\n",
      "Training Epoch 2/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.8604, loss=0.7025]\u001b[A\n",
      "Training Epoch 2/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.8607, loss=0.6317]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 88/292 [00:14<00:32,  6.29batch/s, auc=0.8607, loss=0.6317]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 88/292 [00:14<00:32,  6.29batch/s, auc=0.8611, loss=0.7936]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 89/292 [00:15<00:32,  6.30batch/s, auc=0.8611, loss=0.7936]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 89/292 [00:15<00:32,  6.30batch/s, auc=0.8614, loss=0.6464]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 90/292 [00:15<00:32,  6.29batch/s, auc=0.8614, loss=0.6464]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 90/292 [00:15<00:32,  6.29batch/s, auc=0.8619, loss=0.6236]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 91/292 [00:15<00:31,  6.30batch/s, auc=0.8619, loss=0.6236]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 91/292 [00:15<00:31,  6.30batch/s, auc=0.8610, loss=0.9636]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 92/292 [00:15<00:31,  6.31batch/s, auc=0.8610, loss=0.9636]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 92/292 [00:15<00:31,  6.31batch/s, auc=0.8617, loss=0.6998]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 93/292 [00:15<00:31,  6.30batch/s, auc=0.8617, loss=0.6998]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 93/292 [00:15<00:31,  6.30batch/s, auc=0.8608, loss=0.9960]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 94/292 [00:15<00:31,  6.29batch/s, auc=0.8608, loss=0.9960]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 94/292 [00:15<00:31,  6.29batch/s, auc=0.8611, loss=0.6567]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 95/292 [00:15<00:31,  6.30batch/s, auc=0.8611, loss=0.6567]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 95/292 [00:16<00:31,  6.30batch/s, auc=0.8615, loss=0.6815]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 96/292 [00:16<00:31,  6.30batch/s, auc=0.8615, loss=0.6815]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 96/292 [00:16<00:31,  6.30batch/s, auc=0.8613, loss=0.9820]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8613, loss=0.9820]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8612, loss=0.7166]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8612, loss=0.7166]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8614, loss=0.9774]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.8614, loss=0.9774]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.8614, loss=0.7636]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 100/292 [00:16<00:30,  6.31batch/s, auc=0.8614, loss=0.7636]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 100/292 [00:16<00:30,  6.31batch/s, auc=0.8618, loss=0.4996]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 101/292 [00:16<00:30,  6.31batch/s, auc=0.8618, loss=0.4996]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 101/292 [00:17<00:30,  6.31batch/s, auc=0.8617, loss=0.7137]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 102/292 [00:17<00:30,  6.31batch/s, auc=0.8617, loss=0.7137]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 102/292 [00:17<00:30,  6.31batch/s, auc=0.8620, loss=0.7393]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▌      | 103/292 [00:17<00:30,  6.28batch/s, auc=0.8620, loss=0.7393]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▌      | 103/292 [00:17<00:30,  6.28batch/s, auc=0.8624, loss=0.7539]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 104/292 [00:17<00:29,  6.29batch/s, auc=0.8624, loss=0.7539]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 104/292 [00:17<00:29,  6.29batch/s, auc=0.8616, loss=0.9165]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8616, loss=0.9165]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8609, loss=0.9842]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▋      | 106/292 [00:17<00:29,  6.30batch/s, auc=0.8609, loss=0.9842]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▋      | 106/292 [00:17<00:29,  6.30batch/s, auc=0.8613, loss=0.7135]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 107/292 [00:17<00:29,  6.30batch/s, auc=0.8613, loss=0.7135]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 107/292 [00:18<00:29,  6.30batch/s, auc=0.8622, loss=0.6488]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 108/292 [00:18<00:29,  6.30batch/s, auc=0.8622, loss=0.6488]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 108/292 [00:18<00:29,  6.30batch/s, auc=0.8626, loss=0.5253]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 109/292 [00:18<00:29,  6.30batch/s, auc=0.8626, loss=0.5253]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 109/292 [00:18<00:29,  6.30batch/s, auc=0.8633, loss=0.6009]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 110/292 [00:18<00:28,  6.30batch/s, auc=0.8633, loss=0.6009]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 110/292 [00:18<00:28,  6.30batch/s, auc=0.8637, loss=0.6007]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8637, loss=0.6007]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8645, loss=0.4882]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 112/292 [00:18<00:28,  6.29batch/s, auc=0.8645, loss=0.4882]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 112/292 [00:18<00:28,  6.29batch/s, auc=0.8650, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8650, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8643, loss=0.9616]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 114/292 [00:18<00:28,  6.28batch/s, auc=0.8643, loss=0.9616]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 114/292 [00:19<00:28,  6.28batch/s, auc=0.8624, loss=1.5091]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 115/292 [00:19<00:28,  6.28batch/s, auc=0.8624, loss=1.5091]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 115/292 [00:19<00:28,  6.28batch/s, auc=0.8632, loss=0.6003]\u001b[A\n",
      "Training Epoch 2/25:  40%|███▉      | 116/292 [00:19<00:28,  6.28batch/s, auc=0.8632, loss=0.6003]\u001b[A\n",
      "Training Epoch 2/25:  40%|███▉      | 116/292 [00:19<00:28,  6.28batch/s, auc=0.8638, loss=0.6312]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 117/292 [00:19<00:27,  6.28batch/s, auc=0.8638, loss=0.6312]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 117/292 [00:19<00:27,  6.28batch/s, auc=0.8639, loss=0.6152]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 118/292 [00:19<00:27,  6.28batch/s, auc=0.8639, loss=0.6152]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 118/292 [00:19<00:27,  6.28batch/s, auc=0.8624, loss=1.2358]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 119/292 [00:19<00:27,  6.28batch/s, auc=0.8624, loss=1.2358]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 119/292 [00:19<00:27,  6.28batch/s, auc=0.8619, loss=0.9882]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 120/292 [00:19<00:27,  6.28batch/s, auc=0.8619, loss=0.9882]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 120/292 [00:20<00:27,  6.28batch/s, auc=0.8625, loss=0.5260]\u001b[A\n",
      "Training Epoch 2/25:  41%|████▏     | 121/292 [00:20<00:27,  6.28batch/s, auc=0.8625, loss=0.5260]\u001b[A\n",
      "Training Epoch 2/25:  41%|████▏     | 121/292 [00:20<00:27,  6.28batch/s, auc=0.8618, loss=0.9633]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 122/292 [00:20<00:27,  6.28batch/s, auc=0.8618, loss=0.9633]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 122/292 [00:20<00:27,  6.28batch/s, auc=0.8616, loss=0.7901]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 123/292 [00:20<00:26,  6.28batch/s, auc=0.8616, loss=0.7901]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 123/292 [00:20<00:26,  6.28batch/s, auc=0.8623, loss=0.5774]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 124/292 [00:20<00:26,  6.28batch/s, auc=0.8623, loss=0.5774]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 124/292 [00:20<00:26,  6.28batch/s, auc=0.8623, loss=0.7506]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 125/292 [00:20<00:26,  6.28batch/s, auc=0.8623, loss=0.7506]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 125/292 [00:20<00:26,  6.28batch/s, auc=0.8621, loss=0.7396]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 126/292 [00:20<00:26,  6.28batch/s, auc=0.8621, loss=0.7396]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 126/292 [00:21<00:26,  6.28batch/s, auc=0.8624, loss=0.6575]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 127/292 [00:21<00:26,  6.28batch/s, auc=0.8624, loss=0.6575]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 127/292 [00:21<00:26,  6.28batch/s, auc=0.8624, loss=0.9978]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8624, loss=0.9978]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8625, loss=0.8033]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 129/292 [00:21<00:25,  6.27batch/s, auc=0.8625, loss=0.8033]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 129/292 [00:21<00:25,  6.27batch/s, auc=0.8626, loss=0.9358]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 130/292 [00:21<00:25,  6.27batch/s, auc=0.8626, loss=0.9358]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 130/292 [00:21<00:25,  6.27batch/s, auc=0.8623, loss=1.1268]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 131/292 [00:21<00:25,  6.27batch/s, auc=0.8623, loss=1.1268]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 131/292 [00:21<00:25,  6.27batch/s, auc=0.8615, loss=1.2194]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▌     | 132/292 [00:21<00:25,  6.27batch/s, auc=0.8615, loss=1.2194]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▌     | 132/292 [00:21<00:25,  6.27batch/s, auc=0.8616, loss=0.7212]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 133/292 [00:21<00:25,  6.27batch/s, auc=0.8616, loss=0.7212]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 133/292 [00:22<00:25,  6.27batch/s, auc=0.8617, loss=0.7449]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 134/292 [00:22<00:25,  6.26batch/s, auc=0.8617, loss=0.7449]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 134/292 [00:22<00:25,  6.26batch/s, auc=0.8619, loss=0.7584]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 135/292 [00:22<00:25,  6.27batch/s, auc=0.8619, loss=0.7584]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 135/292 [00:22<00:25,  6.27batch/s, auc=0.8627, loss=0.6012]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8627, loss=0.6012]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8630, loss=0.6643]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.8630, loss=0.6643]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.8639, loss=0.6975]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 138/292 [00:22<00:24,  6.26batch/s, auc=0.8639, loss=0.6975]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 138/292 [00:22<00:24,  6.26batch/s, auc=0.8635, loss=0.7868]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 139/292 [00:22<00:24,  6.26batch/s, auc=0.8635, loss=0.7868]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 139/292 [00:23<00:24,  6.26batch/s, auc=0.8633, loss=0.8393]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.8633, loss=0.8393]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.8632, loss=0.8096]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.8632, loss=0.8096]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.8634, loss=0.6417]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.8634, loss=0.6417]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.8635, loss=0.7496]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 143/292 [00:23<00:23,  6.25batch/s, auc=0.8635, loss=0.7496]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 143/292 [00:23<00:23,  6.25batch/s, auc=0.8631, loss=0.9534]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 144/292 [00:23<00:23,  6.25batch/s, auc=0.8631, loss=0.9534]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 144/292 [00:23<00:23,  6.25batch/s, auc=0.8626, loss=0.8409]\u001b[A\n",
      "Training Epoch 2/25:  50%|████▉     | 145/292 [00:23<00:23,  6.25batch/s, auc=0.8626, loss=0.8409]\u001b[A\n",
      "Training Epoch 2/25:  50%|████▉     | 145/292 [00:24<00:23,  6.25batch/s, auc=0.8619, loss=1.0994]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 146/292 [00:24<00:23,  6.24batch/s, auc=0.8619, loss=1.0994]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 146/292 [00:24<00:23,  6.24batch/s, auc=0.8620, loss=0.7318]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 147/292 [00:24<00:23,  6.24batch/s, auc=0.8620, loss=0.7318]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 147/292 [00:24<00:23,  6.24batch/s, auc=0.8621, loss=0.6848]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 148/292 [00:24<00:23,  6.24batch/s, auc=0.8621, loss=0.6848]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 148/292 [00:24<00:23,  6.24batch/s, auc=0.8628, loss=0.6167]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 149/292 [00:24<00:22,  6.24batch/s, auc=0.8628, loss=0.6167]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 149/292 [00:24<00:22,  6.24batch/s, auc=0.8630, loss=0.7870]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.23batch/s, auc=0.8630, loss=0.7870]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.23batch/s, auc=0.8630, loss=0.8366]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.23batch/s, auc=0.8630, loss=0.8366]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.23batch/s, auc=0.8634, loss=0.5642]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.23batch/s, auc=0.8634, loss=0.5642]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.23batch/s, auc=0.8635, loss=0.7348]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8635, loss=0.7348]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8640, loss=0.5459]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.22batch/s, auc=0.8640, loss=0.5459]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.22batch/s, auc=0.8643, loss=0.5910]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.21batch/s, auc=0.8643, loss=0.5910]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.21batch/s, auc=0.8639, loss=0.7640]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.20batch/s, auc=0.8639, loss=0.7640]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.20batch/s, auc=0.8635, loss=1.1310]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.20batch/s, auc=0.8635, loss=1.1310]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.20batch/s, auc=0.8631, loss=1.0369]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.20batch/s, auc=0.8631, loss=1.0369]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.20batch/s, auc=0.8634, loss=0.6434]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.20batch/s, auc=0.8634, loss=0.6434]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.20batch/s, auc=0.8635, loss=0.6699]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8635, loss=0.6699]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8639, loss=0.6026]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.21batch/s, auc=0.8639, loss=0.6026]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.21batch/s, auc=0.8639, loss=0.8233]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.21batch/s, auc=0.8639, loss=0.8233]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.21batch/s, auc=0.8641, loss=0.5856]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.21batch/s, auc=0.8641, loss=0.5856]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.21batch/s, auc=0.8639, loss=0.8486]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 164/292 [00:26<00:20,  6.21batch/s, auc=0.8639, loss=0.8486]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.21batch/s, auc=0.8640, loss=0.8009]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.21batch/s, auc=0.8640, loss=0.8009]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.21batch/s, auc=0.8644, loss=0.7064]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.21batch/s, auc=0.8644, loss=0.7064]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.21batch/s, auc=0.8641, loss=0.9174]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.21batch/s, auc=0.8641, loss=0.9174]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.21batch/s, auc=0.8648, loss=0.5745]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 168/292 [00:27<00:19,  6.21batch/s, auc=0.8648, loss=0.5745]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 168/292 [00:27<00:19,  6.21batch/s, auc=0.8651, loss=0.6585]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.21batch/s, auc=0.8651, loss=0.6585]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.21batch/s, auc=0.8648, loss=1.0222]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 170/292 [00:27<00:19,  6.21batch/s, auc=0.8648, loss=1.0222]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.21batch/s, auc=0.8654, loss=0.4741]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.20batch/s, auc=0.8654, loss=0.4741]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.20batch/s, auc=0.8653, loss=0.9280]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.20batch/s, auc=0.8653, loss=0.9280]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.20batch/s, auc=0.8655, loss=0.6157]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.20batch/s, auc=0.8655, loss=0.6157]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.20batch/s, auc=0.8656, loss=0.6808]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.20batch/s, auc=0.8656, loss=0.6808]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.20batch/s, auc=0.8663, loss=0.4996]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.20batch/s, auc=0.8663, loss=0.4996]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.20batch/s, auc=0.8666, loss=0.5503]\u001b[A\n",
      "Training Epoch 2/25:  60%|██████    | 176/292 [00:28<00:18,  6.20batch/s, auc=0.8666, loss=0.5503]\u001b[A\n",
      "Training Epoch 2/25:  60%|██████    | 176/292 [00:29<00:18,  6.20batch/s, auc=0.8671, loss=0.6019]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 177/292 [00:29<00:18,  6.20batch/s, auc=0.8671, loss=0.6019]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 177/292 [00:29<00:18,  6.20batch/s, auc=0.8673, loss=0.5523]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 178/292 [00:29<00:18,  6.19batch/s, auc=0.8673, loss=0.5523]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 178/292 [00:29<00:18,  6.19batch/s, auc=0.8676, loss=0.6808]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.19batch/s, auc=0.8676, loss=0.6808]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.19batch/s, auc=0.8673, loss=0.8406]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.19batch/s, auc=0.8673, loss=0.8406]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.19batch/s, auc=0.8668, loss=1.0131]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.19batch/s, auc=0.8668, loss=1.0131]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.19batch/s, auc=0.8672, loss=0.4915]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 182/292 [00:29<00:17,  6.19batch/s, auc=0.8672, loss=0.4915]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.19batch/s, auc=0.8675, loss=0.5929]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.19batch/s, auc=0.8675, loss=0.5929]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.19batch/s, auc=0.8677, loss=0.5242]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.17batch/s, auc=0.8677, loss=0.5242]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.17batch/s, auc=0.8679, loss=0.5148]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.18batch/s, auc=0.8679, loss=0.5148]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.18batch/s, auc=0.8675, loss=1.0389]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.18batch/s, auc=0.8675, loss=1.0389]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.18batch/s, auc=0.8676, loss=0.8960]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 187/292 [00:30<00:16,  6.18batch/s, auc=0.8676, loss=0.8960]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 187/292 [00:30<00:16,  6.18batch/s, auc=0.8675, loss=0.8841]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 188/292 [00:30<00:16,  6.18batch/s, auc=0.8675, loss=0.8841]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 188/292 [00:31<00:16,  6.18batch/s, auc=0.8674, loss=0.8560]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.18batch/s, auc=0.8674, loss=0.8560]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.18batch/s, auc=0.8678, loss=0.6273]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.17batch/s, auc=0.8678, loss=0.6273]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.17batch/s, auc=0.8679, loss=0.6457]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.17batch/s, auc=0.8679, loss=0.6457]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.17batch/s, auc=0.8682, loss=0.7652]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.17batch/s, auc=0.8682, loss=0.7652]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.17batch/s, auc=0.8683, loss=0.5976]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.17batch/s, auc=0.8683, loss=0.5976]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.17batch/s, auc=0.8685, loss=0.6989]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▋   | 194/292 [00:31<00:15,  6.16batch/s, auc=0.8685, loss=0.6989]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▋   | 194/292 [00:31<00:15,  6.16batch/s, auc=0.8688, loss=0.5034]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 195/292 [00:31<00:15,  6.16batch/s, auc=0.8688, loss=0.5034]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.16batch/s, auc=0.8690, loss=0.5778]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.17batch/s, auc=0.8690, loss=0.5778]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.17batch/s, auc=0.8690, loss=0.6628]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.17batch/s, auc=0.8690, loss=0.6628]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.17batch/s, auc=0.8688, loss=0.9256]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.15batch/s, auc=0.8688, loss=0.9256]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.15batch/s, auc=0.8690, loss=0.8974]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.15batch/s, auc=0.8690, loss=0.8974]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.15batch/s, auc=0.8694, loss=0.4560]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 200/292 [00:32<00:14,  6.16batch/s, auc=0.8694, loss=0.4560]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 200/292 [00:32<00:14,  6.16batch/s, auc=0.8697, loss=0.7705]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 201/292 [00:32<00:14,  6.16batch/s, auc=0.8697, loss=0.7705]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.16batch/s, auc=0.8698, loss=0.5828]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.16batch/s, auc=0.8698, loss=0.5828]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.16batch/s, auc=0.8701, loss=0.5775]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.16batch/s, auc=0.8701, loss=0.5775]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.16batch/s, auc=0.8699, loss=0.9176]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.17batch/s, auc=0.8699, loss=0.9176]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.17batch/s, auc=0.8700, loss=0.6395]\u001b[A\n",
      "Training Epoch 2/25:  70%|███████   | 205/292 [00:33<00:14,  6.17batch/s, auc=0.8700, loss=0.6395]\u001b[A\n",
      "Training Epoch 2/25:  70%|███████   | 205/292 [00:33<00:14,  6.17batch/s, auc=0.8703, loss=0.5414]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 206/292 [00:33<00:13,  6.16batch/s, auc=0.8703, loss=0.5414]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 206/292 [00:33<00:13,  6.16batch/s, auc=0.8706, loss=0.6139]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 207/292 [00:33<00:13,  6.16batch/s, auc=0.8706, loss=0.6139]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 207/292 [00:34<00:13,  6.16batch/s, auc=0.8706, loss=0.5409]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 208/292 [00:34<00:13,  6.15batch/s, auc=0.8706, loss=0.5409]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 208/292 [00:34<00:13,  6.15batch/s, auc=0.8705, loss=0.5847]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.8705, loss=0.5847]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.8705, loss=0.7008]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.14batch/s, auc=0.8705, loss=0.7008]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.14batch/s, auc=0.8702, loss=0.8964]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.06batch/s, auc=0.8702, loss=0.8964]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.06batch/s, auc=0.8704, loss=0.5495]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 212/292 [00:34<00:13,  6.09batch/s, auc=0.8704, loss=0.5495]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 212/292 [00:34<00:13,  6.09batch/s, auc=0.8707, loss=0.6704]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 213/292 [00:34<00:12,  6.11batch/s, auc=0.8707, loss=0.6704]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 213/292 [00:35<00:12,  6.11batch/s, auc=0.8704, loss=0.9504]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.11batch/s, auc=0.8704, loss=0.9504]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.11batch/s, auc=0.8706, loss=0.5403]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.01batch/s, auc=0.8706, loss=0.5403]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.01batch/s, auc=0.8705, loss=0.7298]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.81batch/s, auc=0.8705, loss=0.7298]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.81batch/s, auc=0.8702, loss=1.1380]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.80batch/s, auc=0.8702, loss=1.1380]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.80batch/s, auc=0.8696, loss=1.2784]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▍  | 218/292 [00:35<00:12,  5.86batch/s, auc=0.8696, loss=1.2784]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▍  | 218/292 [00:35<00:12,  5.86batch/s, auc=0.8693, loss=0.9725]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 219/292 [00:35<00:12,  5.82batch/s, auc=0.8693, loss=0.9725]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.82batch/s, auc=0.8688, loss=1.0414]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.89batch/s, auc=0.8688, loss=1.0414]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.89batch/s, auc=0.8689, loss=0.6757]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.82batch/s, auc=0.8689, loss=0.6757]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.82batch/s, auc=0.8687, loss=1.0238]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.80batch/s, auc=0.8687, loss=1.0238]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.80batch/s, auc=0.8685, loss=1.2868]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▋  | 223/292 [00:36<00:11,  5.77batch/s, auc=0.8685, loss=1.2868]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▋  | 223/292 [00:36<00:11,  5.77batch/s, auc=0.8686, loss=0.6889]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 224/292 [00:36<00:11,  5.75batch/s, auc=0.8686, loss=0.6889]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 224/292 [00:36<00:11,  5.75batch/s, auc=0.8685, loss=0.8303]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 225/292 [00:36<00:11,  5.75batch/s, auc=0.8685, loss=0.8303]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.75batch/s, auc=0.8688, loss=0.6274]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.75batch/s, auc=0.8688, loss=0.6274]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.75batch/s, auc=0.8688, loss=0.8942]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.75batch/s, auc=0.8688, loss=0.8942]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.75batch/s, auc=0.8684, loss=0.8684]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.74batch/s, auc=0.8684, loss=0.8684]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.74batch/s, auc=0.8684, loss=0.7634]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 229/292 [00:37<00:10,  5.74batch/s, auc=0.8684, loss=0.7634]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 229/292 [00:37<00:10,  5.74batch/s, auc=0.8686, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 230/292 [00:37<00:10,  5.73batch/s, auc=0.8686, loss=0.6461]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.73batch/s, auc=0.8690, loss=0.5816]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.73batch/s, auc=0.8690, loss=0.5816]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.73batch/s, auc=0.8689, loss=0.6887]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.73batch/s, auc=0.8689, loss=0.6887]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.73batch/s, auc=0.8687, loss=1.0888]\u001b[A\n",
      "Training Epoch 2/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.74batch/s, auc=0.8687, loss=1.0888]\u001b[A\n",
      "Training Epoch 2/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.74batch/s, auc=0.8691, loss=0.6269]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 234/292 [00:38<00:10,  5.73batch/s, auc=0.8691, loss=0.6269]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 234/292 [00:38<00:10,  5.73batch/s, auc=0.8692, loss=0.7565]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 235/292 [00:38<00:09,  5.83batch/s, auc=0.8692, loss=0.7565]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 235/292 [00:38<00:09,  5.83batch/s, auc=0.8691, loss=0.6789]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 236/292 [00:38<00:09,  5.69batch/s, auc=0.8691, loss=0.6789]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 236/292 [00:39<00:09,  5.69batch/s, auc=0.8689, loss=0.7229]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 237/292 [00:39<00:09,  5.69batch/s, auc=0.8689, loss=0.7229]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 237/292 [00:39<00:09,  5.69batch/s, auc=0.8689, loss=0.6770]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.69batch/s, auc=0.8689, loss=0.6770]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.69batch/s, auc=0.8689, loss=0.7249]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.70batch/s, auc=0.8689, loss=0.7249]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.70batch/s, auc=0.8692, loss=0.6314]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.69batch/s, auc=0.8692, loss=0.6314]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.69batch/s, auc=0.8690, loss=0.8239]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 241/292 [00:39<00:08,  5.70batch/s, auc=0.8690, loss=0.8239]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 241/292 [00:39<00:08,  5.70batch/s, auc=0.8691, loss=0.8102]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 242/292 [00:39<00:08,  5.73batch/s, auc=0.8691, loss=0.8102]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.73batch/s, auc=0.8691, loss=0.8368]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.72batch/s, auc=0.8691, loss=0.8368]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.72batch/s, auc=0.8693, loss=0.5740]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.72batch/s, auc=0.8693, loss=0.5740]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.72batch/s, auc=0.8692, loss=0.7935]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.72batch/s, auc=0.8692, loss=0.7935]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.72batch/s, auc=0.8692, loss=0.7067]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.75batch/s, auc=0.8692, loss=0.7067]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.75batch/s, auc=0.8692, loss=0.9041]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 247/292 [00:40<00:07,  5.72batch/s, auc=0.8692, loss=0.9041]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.72batch/s, auc=0.8693, loss=0.6287]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.71batch/s, auc=0.8693, loss=0.6287]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.71batch/s, auc=0.8696, loss=0.7317]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.70batch/s, auc=0.8696, loss=0.7317]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.70batch/s, auc=0.8696, loss=0.7330]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.76batch/s, auc=0.8696, loss=0.7330]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.76batch/s, auc=0.8700, loss=0.5427]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.74batch/s, auc=0.8700, loss=0.5427]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.74batch/s, auc=0.8697, loss=0.7197]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.72batch/s, auc=0.8697, loss=0.7197]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.72batch/s, auc=0.8695, loss=0.9281]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 253/292 [00:41<00:06,  5.72batch/s, auc=0.8695, loss=0.9281]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.72batch/s, auc=0.8693, loss=0.9376]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.73batch/s, auc=0.8693, loss=0.9376]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.73batch/s, auc=0.8689, loss=0.9351]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.70batch/s, auc=0.8689, loss=0.9351]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.70batch/s, auc=0.8692, loss=0.5640]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.70batch/s, auc=0.8692, loss=0.5640]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.70batch/s, auc=0.8689, loss=0.7698]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.73batch/s, auc=0.8689, loss=0.7698]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.73batch/s, auc=0.8690, loss=0.8045]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 258/292 [00:42<00:05,  5.72batch/s, auc=0.8690, loss=0.8045]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 258/292 [00:42<00:05,  5.72batch/s, auc=0.8690, loss=0.7392]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▊ | 259/292 [00:42<00:05,  5.70batch/s, auc=0.8690, loss=0.7392]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.70batch/s, auc=0.8693, loss=0.5539]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.69batch/s, auc=0.8693, loss=0.5539]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.69batch/s, auc=0.8693, loss=0.8802]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.73batch/s, auc=0.8693, loss=0.8802]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.73batch/s, auc=0.8695, loss=0.7008]\u001b[A\n",
      "Training Epoch 2/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.72batch/s, auc=0.8695, loss=0.7008]\u001b[A\n",
      "Training Epoch 2/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.72batch/s, auc=0.8696, loss=0.5941]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 263/292 [00:43<00:04,  5.82batch/s, auc=0.8696, loss=0.5941]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 263/292 [00:43<00:04,  5.82batch/s, auc=0.8693, loss=0.8646]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 264/292 [00:43<00:04,  5.69batch/s, auc=0.8693, loss=0.8646]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 264/292 [00:43<00:04,  5.69batch/s, auc=0.8696, loss=0.5463]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 265/292 [00:43<00:04,  5.70batch/s, auc=0.8696, loss=0.5463]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 265/292 [00:44<00:04,  5.70batch/s, auc=0.8697, loss=0.6239]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 266/292 [00:44<00:04,  5.69batch/s, auc=0.8697, loss=0.6239]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 266/292 [00:44<00:04,  5.69batch/s, auc=0.8696, loss=0.8815]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.67batch/s, auc=0.8696, loss=0.8815]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.67batch/s, auc=0.8698, loss=0.6889]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8698, loss=0.6889]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8693, loss=1.0330]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.71batch/s, auc=0.8693, loss=1.0330]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.71batch/s, auc=0.8695, loss=0.5511]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 270/292 [00:44<00:03,  5.70batch/s, auc=0.8695, loss=0.5511]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.70batch/s, auc=0.8696, loss=0.5722]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.8696, loss=0.5722]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.8697, loss=0.6393]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.68batch/s, auc=0.8697, loss=0.6393]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.68batch/s, auc=0.8697, loss=0.6378]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8697, loss=0.6378]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8700, loss=0.5880]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.71batch/s, auc=0.8700, loss=0.5880]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.71batch/s, auc=0.8702, loss=0.7805]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 275/292 [00:45<00:02,  5.69batch/s, auc=0.8702, loss=0.7805]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 275/292 [00:45<00:02,  5.69batch/s, auc=0.8697, loss=1.0855]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 276/292 [00:45<00:02,  5.69batch/s, auc=0.8697, loss=1.0855]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.69batch/s, auc=0.8693, loss=1.0729]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.71batch/s, auc=0.8693, loss=1.0729]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.71batch/s, auc=0.8696, loss=0.5050]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.74batch/s, auc=0.8696, loss=0.5050]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.74batch/s, auc=0.8696, loss=0.8843]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.71batch/s, auc=0.8696, loss=0.8843]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.71batch/s, auc=0.8698, loss=0.7442]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.69batch/s, auc=0.8698, loss=0.7442]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.69batch/s, auc=0.8694, loss=1.1782]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 281/292 [00:46<00:01,  5.71batch/s, auc=0.8694, loss=1.1782]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 281/292 [00:46<00:01,  5.71batch/s, auc=0.8692, loss=1.0114]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 282/292 [00:46<00:01,  5.72batch/s, auc=0.8692, loss=1.0114]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.72batch/s, auc=0.8691, loss=0.8811]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.69batch/s, auc=0.8691, loss=0.8811]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.69batch/s, auc=0.8689, loss=0.9384]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.67batch/s, auc=0.8689, loss=0.9384]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.67batch/s, auc=0.8684, loss=1.2938]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.66batch/s, auc=0.8684, loss=1.2938]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.66batch/s, auc=0.8685, loss=0.6657]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.77batch/s, auc=0.8685, loss=0.6657]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.77batch/s, auc=0.8686, loss=0.7745]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 287/292 [00:47<00:00,  5.85batch/s, auc=0.8686, loss=0.7745]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 287/292 [00:47<00:00,  5.85batch/s, auc=0.8686, loss=0.6963]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▊| 288/292 [00:47<00:00,  5.90batch/s, auc=0.8686, loss=0.6963]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.90batch/s, auc=0.8685, loss=0.8543]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.94batch/s, auc=0.8685, loss=0.8543]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.94batch/s, auc=0.8687, loss=0.5649]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.97batch/s, auc=0.8687, loss=0.5649]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.97batch/s, auc=0.8689, loss=0.7387]\u001b[A\n",
      "Training Epoch 2/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.99batch/s, auc=0.8689, loss=0.7387]\u001b[A\n",
      "Training Epoch 2/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.99batch/s, auc=0.8689, loss=0.7884]\u001b[A\n",
      "Training Epoch 2/25: 100%|██████████| 292/292 [00:48<00:00,  6.00batch/s, auc=0.8689, loss=0.7884]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25] Train Loss: 0.7749 | Train AUROC: 0.8689 Val Loss: 0.7706 | Val AUROC: 0.8695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|▊         | 2/25 [01:49<20:57, 54.68s/it]\n",
      "Training Epoch 3/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.8571, loss=0.7813]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 1/292 [00:00<04:47,  1.01batch/s, auc=0.8571, loss=0.7813]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 1/292 [00:01<04:47,  1.01batch/s, auc=0.8795, loss=0.6540]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.8795, loss=0.6540]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.8998, loss=0.6345]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.8998, loss=0.6345]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.8773, loss=0.8359]\u001b[A\n",
      "Training Epoch 3/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.8773, loss=0.8359]\u001b[A\n",
      "Training Epoch 3/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.8746, loss=0.8345]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.8746, loss=0.8345]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.8759, loss=0.6910]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 6/292 [00:01<01:01,  4.65batch/s, auc=0.8759, loss=0.6910]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 6/292 [00:01<01:01,  4.65batch/s, auc=0.8766, loss=0.6879]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 7/292 [00:01<00:56,  5.04batch/s, auc=0.8766, loss=0.6879]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 7/292 [00:02<00:56,  5.04batch/s, auc=0.8792, loss=0.7875]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 8/292 [00:02<00:52,  5.41batch/s, auc=0.8792, loss=0.7875]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 8/292 [00:02<00:52,  5.41batch/s, auc=0.8803, loss=0.6596]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 9/292 [00:02<00:50,  5.65batch/s, auc=0.8803, loss=0.6596]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 9/292 [00:02<00:50,  5.65batch/s, auc=0.8820, loss=0.7813]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 10/292 [00:02<00:49,  5.73batch/s, auc=0.8820, loss=0.7813]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 10/292 [00:02<00:49,  5.73batch/s, auc=0.8836, loss=0.6521]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 11/292 [00:02<00:47,  5.88batch/s, auc=0.8836, loss=0.6521]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 11/292 [00:02<00:47,  5.88batch/s, auc=0.8795, loss=0.7940]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 12/292 [00:02<00:46,  6.02batch/s, auc=0.8795, loss=0.7940]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 12/292 [00:02<00:46,  6.02batch/s, auc=0.8869, loss=0.5707]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 13/292 [00:02<00:45,  6.12batch/s, auc=0.8869, loss=0.5707]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 13/292 [00:03<00:45,  6.12batch/s, auc=0.8866, loss=0.8272]\u001b[A\n",
      "Training Epoch 3/25:   5%|▍         | 14/292 [00:03<00:44,  6.20batch/s, auc=0.8866, loss=0.8272]\u001b[A\n",
      "Training Epoch 3/25:   5%|▍         | 14/292 [00:03<00:44,  6.20batch/s, auc=0.8856, loss=0.8202]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 15/292 [00:03<00:44,  6.20batch/s, auc=0.8856, loss=0.8202]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 15/292 [00:03<00:44,  6.20batch/s, auc=0.8885, loss=0.6227]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.8885, loss=0.6227]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.8838, loss=0.9471]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 17/292 [00:03<00:43,  6.28batch/s, auc=0.8838, loss=0.9471]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 17/292 [00:03<00:43,  6.28batch/s, auc=0.8895, loss=0.5079]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.8895, loss=0.5079]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.8862, loss=0.8506]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 19/292 [00:03<00:43,  6.33batch/s, auc=0.8862, loss=0.8506]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 19/292 [00:04<00:43,  6.33batch/s, auc=0.8860, loss=0.8996]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 20/292 [00:04<00:43,  6.31batch/s, auc=0.8860, loss=0.8996]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 20/292 [00:04<00:43,  6.31batch/s, auc=0.8858, loss=0.5999]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.8858, loss=0.5999]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.8847, loss=0.7936]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 22/292 [00:04<00:42,  6.34batch/s, auc=0.8847, loss=0.7936]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 22/292 [00:04<00:42,  6.34batch/s, auc=0.8841, loss=0.7439]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.8841, loss=0.7439]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.8870, loss=0.5043]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 24/292 [00:04<00:42,  6.35batch/s, auc=0.8870, loss=0.5043]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 24/292 [00:04<00:42,  6.35batch/s, auc=0.8865, loss=0.6223]\u001b[A\n",
      "Training Epoch 3/25:   9%|▊         | 25/292 [00:04<00:41,  6.36batch/s, auc=0.8865, loss=0.6223]\u001b[A\n",
      "Training Epoch 3/25:   9%|▊         | 25/292 [00:04<00:41,  6.36batch/s, auc=0.8865, loss=0.7007]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 26/292 [00:04<00:41,  6.34batch/s, auc=0.8865, loss=0.7007]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.8898, loss=0.4782]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.8898, loss=0.4782]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.8919, loss=0.5038]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8919, loss=0.5038]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8911, loss=0.7566]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8911, loss=0.7566]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8901, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.8901, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.8869, loss=1.0138]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.8869, loss=1.0138]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.8888, loss=0.5763]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 32/292 [00:05<00:41,  6.27batch/s, auc=0.8888, loss=0.5763]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 32/292 [00:06<00:41,  6.27batch/s, auc=0.8871, loss=0.8944]\u001b[A\n",
      "Training Epoch 3/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.8871, loss=0.8944]\u001b[A\n",
      "Training Epoch 3/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.8859, loss=0.7597]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 34/292 [00:06<00:40,  6.33batch/s, auc=0.8859, loss=0.7597]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 34/292 [00:06<00:40,  6.33batch/s, auc=0.8838, loss=1.1315]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.8838, loss=1.1315]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.8840, loss=0.7087]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.8840, loss=0.7087]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.8849, loss=0.7206]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8849, loss=0.7206]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8865, loss=0.4971]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 38/292 [00:06<00:40,  6.30batch/s, auc=0.8865, loss=0.4971]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 38/292 [00:07<00:40,  6.30batch/s, auc=0.8854, loss=0.8439]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.8854, loss=0.8439]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.8850, loss=0.7913]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.8850, loss=0.7913]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.8877, loss=0.4868]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.8877, loss=0.4868]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.8873, loss=0.8143]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 42/292 [00:07<00:39,  6.32batch/s, auc=0.8873, loss=0.8143]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 42/292 [00:07<00:39,  6.32batch/s, auc=0.8860, loss=0.6486]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.8860, loss=0.6486]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.8830, loss=1.2427]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 44/292 [00:07<00:39,  6.30batch/s, auc=0.8830, loss=1.2427]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 44/292 [00:07<00:39,  6.30batch/s, auc=0.8834, loss=0.6919]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 45/292 [00:07<00:39,  6.30batch/s, auc=0.8834, loss=0.6919]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.8840, loss=0.5809]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 46/292 [00:08<00:39,  6.27batch/s, auc=0.8840, loss=0.5809]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 46/292 [00:08<00:39,  6.27batch/s, auc=0.8826, loss=0.8517]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.8826, loss=0.8517]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.8842, loss=0.5427]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.8842, loss=0.5427]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.8853, loss=0.7038]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.8853, loss=0.7038]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.8854, loss=0.7947]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.8854, loss=0.7947]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.8856, loss=0.6654]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 51/292 [00:08<00:38,  6.30batch/s, auc=0.8856, loss=0.6654]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 51/292 [00:09<00:38,  6.30batch/s, auc=0.8856, loss=0.7095]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 52/292 [00:09<00:38,  6.31batch/s, auc=0.8856, loss=0.7095]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 52/292 [00:09<00:38,  6.31batch/s, auc=0.8857, loss=0.6982]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.8857, loss=0.6982]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.8859, loss=0.7602]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.8859, loss=0.7602]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.8875, loss=0.5983]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 55/292 [00:09<00:37,  6.33batch/s, auc=0.8875, loss=0.5983]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 55/292 [00:09<00:37,  6.33batch/s, auc=0.8877, loss=0.8108]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 56/292 [00:09<00:37,  6.33batch/s, auc=0.8877, loss=0.8108]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 56/292 [00:09<00:37,  6.33batch/s, auc=0.8864, loss=0.7731]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 57/292 [00:09<00:37,  6.33batch/s, auc=0.8864, loss=0.7731]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 57/292 [00:10<00:37,  6.33batch/s, auc=0.8839, loss=1.1698]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 58/292 [00:10<00:36,  6.33batch/s, auc=0.8839, loss=1.1698]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 58/292 [00:10<00:36,  6.33batch/s, auc=0.8838, loss=0.6287]\u001b[A\n",
      "Training Epoch 3/25:  20%|██        | 59/292 [00:10<00:36,  6.34batch/s, auc=0.8838, loss=0.6287]\u001b[A\n",
      "Training Epoch 3/25:  20%|██        | 59/292 [00:10<00:36,  6.34batch/s, auc=0.8835, loss=0.7535]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 60/292 [00:10<00:36,  6.34batch/s, auc=0.8835, loss=0.7535]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 60/292 [00:10<00:36,  6.34batch/s, auc=0.8834, loss=0.5956]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 61/292 [00:10<00:36,  6.34batch/s, auc=0.8834, loss=0.5956]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 61/292 [00:10<00:36,  6.34batch/s, auc=0.8803, loss=1.3690]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.8803, loss=1.3690]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.8790, loss=1.1184]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 63/292 [00:10<00:36,  6.34batch/s, auc=0.8790, loss=1.1184]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 63/292 [00:11<00:36,  6.34batch/s, auc=0.8797, loss=0.5703]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 64/292 [00:11<00:35,  6.34batch/s, auc=0.8797, loss=0.5703]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 64/292 [00:11<00:35,  6.34batch/s, auc=0.8800, loss=0.8119]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 65/292 [00:11<00:35,  6.33batch/s, auc=0.8800, loss=0.8119]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 65/292 [00:11<00:35,  6.33batch/s, auc=0.8802, loss=0.8212]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 66/292 [00:11<00:35,  6.32batch/s, auc=0.8802, loss=0.8212]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 66/292 [00:11<00:35,  6.32batch/s, auc=0.8806, loss=0.6697]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 67/292 [00:11<00:35,  6.33batch/s, auc=0.8806, loss=0.6697]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 67/292 [00:11<00:35,  6.33batch/s, auc=0.8810, loss=0.5919]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 68/292 [00:11<00:35,  6.32batch/s, auc=0.8810, loss=0.5919]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 68/292 [00:11<00:35,  6.32batch/s, auc=0.8817, loss=0.5454]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▎       | 69/292 [00:11<00:35,  6.32batch/s, auc=0.8817, loss=0.5454]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▎       | 69/292 [00:11<00:35,  6.32batch/s, auc=0.8795, loss=1.4553]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 70/292 [00:11<00:35,  6.31batch/s, auc=0.8795, loss=1.4553]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 70/292 [00:12<00:35,  6.31batch/s, auc=0.8803, loss=0.6631]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.8803, loss=0.6631]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.8814, loss=0.5618]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▍       | 72/292 [00:12<00:34,  6.32batch/s, auc=0.8814, loss=0.5618]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▍       | 72/292 [00:12<00:34,  6.32batch/s, auc=0.8812, loss=0.6935]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 73/292 [00:12<00:34,  6.31batch/s, auc=0.8812, loss=0.6935]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 73/292 [00:12<00:34,  6.31batch/s, auc=0.8823, loss=0.6619]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 74/292 [00:12<00:34,  6.31batch/s, auc=0.8823, loss=0.6619]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 74/292 [00:12<00:34,  6.31batch/s, auc=0.8831, loss=0.6155]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 75/292 [00:12<00:34,  6.31batch/s, auc=0.8831, loss=0.6155]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 75/292 [00:12<00:34,  6.31batch/s, auc=0.8822, loss=0.9237]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 76/292 [00:12<00:34,  6.31batch/s, auc=0.8822, loss=0.9237]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 76/292 [00:13<00:34,  6.31batch/s, auc=0.8830, loss=0.5921]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.8830, loss=0.5921]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.8821, loss=0.7640]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 78/292 [00:13<00:33,  6.29batch/s, auc=0.8821, loss=0.7640]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 78/292 [00:13<00:33,  6.29batch/s, auc=0.8830, loss=0.5293]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 79/292 [00:13<00:33,  6.30batch/s, auc=0.8830, loss=0.5293]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 79/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.5364]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 80/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.5364]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 80/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.8133]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 81/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.8133]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 81/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.6184]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 82/292 [00:13<00:33,  6.30batch/s, auc=0.8838, loss=0.6184]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 82/292 [00:14<00:33,  6.30batch/s, auc=0.8841, loss=0.6578]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 83/292 [00:14<00:33,  6.30batch/s, auc=0.8841, loss=0.6578]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 83/292 [00:14<00:33,  6.30batch/s, auc=0.8843, loss=0.6345]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 84/292 [00:14<00:33,  6.30batch/s, auc=0.8843, loss=0.6345]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 84/292 [00:14<00:33,  6.30batch/s, auc=0.8840, loss=0.9557]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 85/292 [00:14<00:32,  6.30batch/s, auc=0.8840, loss=0.9557]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 85/292 [00:14<00:32,  6.30batch/s, auc=0.8842, loss=0.5859]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.8842, loss=0.5859]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.8833, loss=1.0325]\u001b[A\n",
      "Training Epoch 3/25:  30%|██▉       | 87/292 [00:14<00:32,  6.30batch/s, auc=0.8833, loss=1.0325]\u001b[A\n",
      "Training Epoch 3/25:  30%|██▉       | 87/292 [00:14<00:32,  6.30batch/s, auc=0.8836, loss=0.7302]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 88/292 [00:14<00:32,  6.29batch/s, auc=0.8836, loss=0.7302]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 88/292 [00:14<00:32,  6.29batch/s, auc=0.8839, loss=0.6682]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 89/292 [00:14<00:32,  6.29batch/s, auc=0.8839, loss=0.6682]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 89/292 [00:15<00:32,  6.29batch/s, auc=0.8839, loss=0.7470]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 90/292 [00:15<00:32,  6.29batch/s, auc=0.8839, loss=0.7470]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 90/292 [00:15<00:32,  6.29batch/s, auc=0.8840, loss=0.6765]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 91/292 [00:15<00:31,  6.29batch/s, auc=0.8840, loss=0.6765]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 91/292 [00:15<00:31,  6.29batch/s, auc=0.8840, loss=0.8064]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 92/292 [00:15<00:31,  6.29batch/s, auc=0.8840, loss=0.8064]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 92/292 [00:15<00:31,  6.29batch/s, auc=0.8838, loss=0.7365]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 93/292 [00:15<00:31,  6.29batch/s, auc=0.8838, loss=0.7365]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 93/292 [00:15<00:31,  6.29batch/s, auc=0.8837, loss=0.6688]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 94/292 [00:15<00:31,  6.28batch/s, auc=0.8837, loss=0.6688]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 94/292 [00:15<00:31,  6.28batch/s, auc=0.8838, loss=0.5971]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 95/292 [00:15<00:31,  6.29batch/s, auc=0.8838, loss=0.5971]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 95/292 [00:16<00:31,  6.29batch/s, auc=0.8836, loss=0.6794]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 96/292 [00:16<00:31,  6.28batch/s, auc=0.8836, loss=0.6794]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 96/292 [00:16<00:31,  6.28batch/s, auc=0.8839, loss=0.6180]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 97/292 [00:16<00:31,  6.28batch/s, auc=0.8839, loss=0.6180]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 97/292 [00:16<00:31,  6.28batch/s, auc=0.8835, loss=0.7803]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▎      | 98/292 [00:16<00:30,  6.29batch/s, auc=0.8835, loss=0.7803]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▎      | 98/292 [00:16<00:30,  6.29batch/s, auc=0.8825, loss=1.1164]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 99/292 [00:16<00:30,  6.29batch/s, auc=0.8825, loss=1.1164]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 99/292 [00:16<00:30,  6.29batch/s, auc=0.8818, loss=0.8512]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 100/292 [00:16<00:30,  6.29batch/s, auc=0.8818, loss=0.8512]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 100/292 [00:16<00:30,  6.29batch/s, auc=0.8813, loss=0.7853]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 101/292 [00:16<00:30,  6.28batch/s, auc=0.8813, loss=0.7853]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 101/292 [00:17<00:30,  6.28batch/s, auc=0.8798, loss=1.1217]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 102/292 [00:17<00:30,  6.29batch/s, auc=0.8798, loss=1.1217]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 102/292 [00:17<00:30,  6.29batch/s, auc=0.8800, loss=0.5500]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▌      | 103/292 [00:17<00:30,  6.30batch/s, auc=0.8800, loss=0.5500]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▌      | 103/292 [00:17<00:30,  6.30batch/s, auc=0.8808, loss=0.6057]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 104/292 [00:17<00:29,  6.30batch/s, auc=0.8808, loss=0.6057]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 104/292 [00:17<00:29,  6.30batch/s, auc=0.8817, loss=0.5538]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8817, loss=0.5538]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8813, loss=1.0846]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▋      | 106/292 [00:17<00:29,  6.29batch/s, auc=0.8813, loss=1.0846]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▋      | 106/292 [00:17<00:29,  6.29batch/s, auc=0.8824, loss=0.4302]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 107/292 [00:17<00:29,  6.30batch/s, auc=0.8824, loss=0.4302]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 107/292 [00:17<00:29,  6.30batch/s, auc=0.8824, loss=0.6866]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 108/292 [00:17<00:29,  6.30batch/s, auc=0.8824, loss=0.6866]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 108/292 [00:18<00:29,  6.30batch/s, auc=0.8825, loss=0.6407]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 109/292 [00:18<00:29,  6.29batch/s, auc=0.8825, loss=0.6407]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 109/292 [00:18<00:29,  6.29batch/s, auc=0.8827, loss=0.7854]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 110/292 [00:18<00:29,  6.28batch/s, auc=0.8827, loss=0.7854]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 110/292 [00:18<00:29,  6.28batch/s, auc=0.8827, loss=0.7175]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8827, loss=0.7175]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8822, loss=1.0971]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 112/292 [00:18<00:28,  6.28batch/s, auc=0.8822, loss=1.0971]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 112/292 [00:18<00:28,  6.28batch/s, auc=0.8816, loss=0.7996]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8816, loss=0.7996]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8823, loss=0.5549]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 114/292 [00:18<00:28,  6.28batch/s, auc=0.8823, loss=0.5549]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 114/292 [00:19<00:28,  6.28batch/s, auc=0.8820, loss=0.8362]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.8820, loss=0.8362]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.8819, loss=0.7970]\u001b[A\n",
      "Training Epoch 3/25:  40%|███▉      | 116/292 [00:19<00:28,  6.24batch/s, auc=0.8819, loss=0.7970]\u001b[A\n",
      "Training Epoch 3/25:  40%|███▉      | 116/292 [00:19<00:28,  6.24batch/s, auc=0.8817, loss=0.8782]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 117/292 [00:19<00:28,  6.25batch/s, auc=0.8817, loss=0.8782]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 117/292 [00:19<00:28,  6.25batch/s, auc=0.8825, loss=0.5544]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 118/292 [00:19<00:27,  6.26batch/s, auc=0.8825, loss=0.5544]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 118/292 [00:19<00:27,  6.26batch/s, auc=0.8827, loss=0.7087]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 119/292 [00:19<00:27,  6.26batch/s, auc=0.8827, loss=0.7087]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 119/292 [00:19<00:27,  6.26batch/s, auc=0.8818, loss=0.9016]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 120/292 [00:19<00:27,  6.27batch/s, auc=0.8818, loss=0.9016]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 120/292 [00:20<00:27,  6.27batch/s, auc=0.8816, loss=0.7366]\u001b[A\n",
      "Training Epoch 3/25:  41%|████▏     | 121/292 [00:20<00:27,  6.25batch/s, auc=0.8816, loss=0.7366]\u001b[A\n",
      "Training Epoch 3/25:  41%|████▏     | 121/292 [00:20<00:27,  6.25batch/s, auc=0.8820, loss=0.5939]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 122/292 [00:20<00:27,  6.26batch/s, auc=0.8820, loss=0.5939]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 122/292 [00:20<00:27,  6.26batch/s, auc=0.8816, loss=0.6740]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 123/292 [00:20<00:26,  6.26batch/s, auc=0.8816, loss=0.6740]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 123/292 [00:20<00:26,  6.26batch/s, auc=0.8818, loss=0.5418]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8818, loss=0.5418]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8813, loss=0.9250]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 125/292 [00:20<00:26,  6.26batch/s, auc=0.8813, loss=0.9250]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 125/292 [00:20<00:26,  6.26batch/s, auc=0.8812, loss=0.6517]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 126/292 [00:20<00:26,  6.26batch/s, auc=0.8812, loss=0.6517]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 126/292 [00:21<00:26,  6.26batch/s, auc=0.8816, loss=0.5891]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 127/292 [00:21<00:26,  6.26batch/s, auc=0.8816, loss=0.5891]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 127/292 [00:21<00:26,  6.26batch/s, auc=0.8809, loss=1.1608]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 128/292 [00:21<00:26,  6.26batch/s, auc=0.8809, loss=1.1608]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 128/292 [00:21<00:26,  6.26batch/s, auc=0.8812, loss=0.6924]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 129/292 [00:21<00:26,  6.24batch/s, auc=0.8812, loss=0.6924]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 129/292 [00:21<00:26,  6.24batch/s, auc=0.8809, loss=0.7116]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 130/292 [00:21<00:25,  6.24batch/s, auc=0.8809, loss=0.7116]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 130/292 [00:21<00:25,  6.24batch/s, auc=0.8804, loss=0.9027]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 131/292 [00:21<00:25,  6.24batch/s, auc=0.8804, loss=0.9027]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 131/292 [00:21<00:25,  6.24batch/s, auc=0.8805, loss=0.7797]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▌     | 132/292 [00:21<00:25,  6.24batch/s, auc=0.8805, loss=0.7797]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▌     | 132/292 [00:21<00:25,  6.24batch/s, auc=0.8805, loss=0.7456]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 133/292 [00:21<00:25,  6.24batch/s, auc=0.8805, loss=0.7456]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 133/292 [00:22<00:25,  6.24batch/s, auc=0.8802, loss=0.9327]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 134/292 [00:22<00:25,  6.24batch/s, auc=0.8802, loss=0.9327]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 134/292 [00:22<00:25,  6.24batch/s, auc=0.8804, loss=0.6271]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 135/292 [00:22<00:25,  6.24batch/s, auc=0.8804, loss=0.6271]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 135/292 [00:22<00:25,  6.24batch/s, auc=0.8803, loss=0.8665]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 136/292 [00:22<00:24,  6.25batch/s, auc=0.8803, loss=0.8665]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 136/292 [00:22<00:24,  6.25batch/s, auc=0.8798, loss=1.0055]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 137/292 [00:22<00:24,  6.25batch/s, auc=0.8798, loss=1.0055]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 137/292 [00:22<00:24,  6.25batch/s, auc=0.8800, loss=0.6592]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 138/292 [00:22<00:24,  6.25batch/s, auc=0.8800, loss=0.6592]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 138/292 [00:22<00:24,  6.25batch/s, auc=0.8800, loss=0.8039]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 139/292 [00:22<00:24,  6.25batch/s, auc=0.8800, loss=0.8039]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 139/292 [00:23<00:24,  6.25batch/s, auc=0.8797, loss=0.9015]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 140/292 [00:23<00:24,  6.24batch/s, auc=0.8797, loss=0.9015]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 140/292 [00:23<00:24,  6.24batch/s, auc=0.8795, loss=0.9233]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 141/292 [00:23<00:24,  6.24batch/s, auc=0.8795, loss=0.9233]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 141/292 [00:23<00:24,  6.24batch/s, auc=0.8798, loss=0.6392]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▊     | 142/292 [00:23<00:24,  6.24batch/s, auc=0.8798, loss=0.6392]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▊     | 142/292 [00:23<00:24,  6.24batch/s, auc=0.8800, loss=0.7510]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.8800, loss=0.7510]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.8799, loss=0.7028]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 144/292 [00:23<00:23,  6.24batch/s, auc=0.8799, loss=0.7028]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 144/292 [00:23<00:23,  6.24batch/s, auc=0.8802, loss=0.7393]\u001b[A\n",
      "Training Epoch 3/25:  50%|████▉     | 145/292 [00:23<00:23,  6.23batch/s, auc=0.8802, loss=0.7393]\u001b[A\n",
      "Training Epoch 3/25:  50%|████▉     | 145/292 [00:24<00:23,  6.23batch/s, auc=0.8801, loss=0.7298]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 146/292 [00:24<00:23,  6.23batch/s, auc=0.8801, loss=0.7298]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 146/292 [00:24<00:23,  6.23batch/s, auc=0.8801, loss=0.6495]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 147/292 [00:24<00:23,  6.23batch/s, auc=0.8801, loss=0.6495]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 147/292 [00:24<00:23,  6.23batch/s, auc=0.8796, loss=0.8256]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 148/292 [00:24<00:23,  6.23batch/s, auc=0.8796, loss=0.8256]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 148/292 [00:24<00:23,  6.23batch/s, auc=0.8798, loss=0.7298]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 149/292 [00:24<00:22,  6.22batch/s, auc=0.8798, loss=0.7298]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 149/292 [00:24<00:22,  6.22batch/s, auc=0.8799, loss=0.7441]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.21batch/s, auc=0.8799, loss=0.7441]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.21batch/s, auc=0.8796, loss=0.7438]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.21batch/s, auc=0.8796, loss=0.7438]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.21batch/s, auc=0.8797, loss=0.7141]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.22batch/s, auc=0.8797, loss=0.7141]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.22batch/s, auc=0.8794, loss=0.9546]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.22batch/s, auc=0.8794, loss=0.9546]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.22batch/s, auc=0.8791, loss=1.0081]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.21batch/s, auc=0.8791, loss=1.0081]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.21batch/s, auc=0.8792, loss=0.6094]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.22batch/s, auc=0.8792, loss=0.6094]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.22batch/s, auc=0.8789, loss=0.9589]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.22batch/s, auc=0.8789, loss=0.9589]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.22batch/s, auc=0.8786, loss=0.9585]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.23batch/s, auc=0.8786, loss=0.9585]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.23batch/s, auc=0.8786, loss=0.7896]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.21batch/s, auc=0.8786, loss=0.7896]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.21batch/s, auc=0.8788, loss=0.7021]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.20batch/s, auc=0.8788, loss=0.7021]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.20batch/s, auc=0.8787, loss=0.6525]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8787, loss=0.6525]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8789, loss=0.6598]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.20batch/s, auc=0.8789, loss=0.6598]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.20batch/s, auc=0.8787, loss=0.9113]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.19batch/s, auc=0.8787, loss=0.9113]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.19batch/s, auc=0.8787, loss=0.6689]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.17batch/s, auc=0.8787, loss=0.6689]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.17batch/s, auc=0.8785, loss=0.8865]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 164/292 [00:26<00:20,  6.16batch/s, auc=0.8785, loss=0.8865]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.16batch/s, auc=0.8789, loss=0.5391]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.17batch/s, auc=0.8789, loss=0.5391]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.17batch/s, auc=0.8794, loss=0.7278]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.17batch/s, auc=0.8794, loss=0.7278]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.17batch/s, auc=0.8800, loss=0.5249]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.8800, loss=0.5249]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.8807, loss=0.5205]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.16batch/s, auc=0.8807, loss=0.5205]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.16batch/s, auc=0.8812, loss=0.5344]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.16batch/s, auc=0.8812, loss=0.5344]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.16batch/s, auc=0.8812, loss=0.7981]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 170/292 [00:27<00:19,  6.16batch/s, auc=0.8812, loss=0.7981]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.16batch/s, auc=0.8816, loss=0.5221]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.17batch/s, auc=0.8816, loss=0.5221]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.17batch/s, auc=0.8818, loss=0.5476]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.19batch/s, auc=0.8818, loss=0.5476]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.19batch/s, auc=0.8818, loss=0.7407]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.18batch/s, auc=0.8818, loss=0.7407]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.18batch/s, auc=0.8814, loss=1.1522]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.19batch/s, auc=0.8814, loss=1.1522]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.19batch/s, auc=0.8816, loss=0.8641]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.20batch/s, auc=0.8816, loss=0.8641]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.20batch/s, auc=0.8822, loss=0.4646]\u001b[A\n",
      "Training Epoch 3/25:  60%|██████    | 176/292 [00:28<00:18,  6.20batch/s, auc=0.8822, loss=0.4646]\u001b[A\n",
      "Training Epoch 3/25:  60%|██████    | 176/292 [00:29<00:18,  6.20batch/s, auc=0.8819, loss=0.9716]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 177/292 [00:29<00:18,  6.18batch/s, auc=0.8819, loss=0.9716]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 177/292 [00:29<00:18,  6.18batch/s, auc=0.8829, loss=0.4041]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 178/292 [00:29<00:18,  6.17batch/s, auc=0.8829, loss=0.4041]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 178/292 [00:29<00:18,  6.17batch/s, auc=0.8827, loss=0.7535]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.17batch/s, auc=0.8827, loss=0.7535]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.17batch/s, auc=0.8829, loss=0.5710]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.18batch/s, auc=0.8829, loss=0.5710]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.18batch/s, auc=0.8826, loss=0.8145]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.18batch/s, auc=0.8826, loss=0.8145]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.18batch/s, auc=0.8828, loss=0.5839]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 182/292 [00:29<00:17,  6.18batch/s, auc=0.8828, loss=0.5839]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.18batch/s, auc=0.8832, loss=0.7863]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.18batch/s, auc=0.8832, loss=0.7863]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.18batch/s, auc=0.8833, loss=0.6765]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.18batch/s, auc=0.8833, loss=0.6765]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.18batch/s, auc=0.8833, loss=0.6535]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.17batch/s, auc=0.8833, loss=0.6535]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.17batch/s, auc=0.8835, loss=0.5057]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.16batch/s, auc=0.8835, loss=0.5057]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.16batch/s, auc=0.8837, loss=0.5378]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.14batch/s, auc=0.8837, loss=0.5378]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.14batch/s, auc=0.8838, loss=0.7104]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 188/292 [00:30<00:17,  6.02batch/s, auc=0.8838, loss=0.7104]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.02batch/s, auc=0.8839, loss=0.7076]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.84batch/s, auc=0.8839, loss=0.7076]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.84batch/s, auc=0.8843, loss=0.5038]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.80batch/s, auc=0.8843, loss=0.5038]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.80batch/s, auc=0.8841, loss=0.8308]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.80batch/s, auc=0.8841, loss=0.8308]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.80batch/s, auc=0.8840, loss=0.7334]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.79batch/s, auc=0.8840, loss=0.7334]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.79batch/s, auc=0.8841, loss=0.5338]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 193/292 [00:31<00:17,  5.78batch/s, auc=0.8841, loss=0.5338]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 193/292 [00:31<00:17,  5.78batch/s, auc=0.8842, loss=0.6410]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▋   | 194/292 [00:31<00:16,  5.86batch/s, auc=0.8842, loss=0.6410]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.86batch/s, auc=0.8844, loss=0.5579]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.71batch/s, auc=0.8844, loss=0.5579]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.71batch/s, auc=0.8844, loss=0.7276]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.77batch/s, auc=0.8844, loss=0.7276]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.77batch/s, auc=0.8836, loss=1.2078]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.77batch/s, auc=0.8836, loss=1.2078]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.77batch/s, auc=0.8838, loss=0.5754]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 198/292 [00:32<00:15,  5.88batch/s, auc=0.8838, loss=0.5754]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 198/292 [00:32<00:15,  5.88batch/s, auc=0.8839, loss=0.5159]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 199/292 [00:32<00:15,  5.83batch/s, auc=0.8839, loss=0.5159]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 199/292 [00:32<00:15,  5.83batch/s, auc=0.8837, loss=0.7938]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 200/292 [00:32<00:15,  5.91batch/s, auc=0.8837, loss=0.7938]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.91batch/s, auc=0.8836, loss=0.7631]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.86batch/s, auc=0.8836, loss=0.7631]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.86batch/s, auc=0.8836, loss=0.8005]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.94batch/s, auc=0.8836, loss=0.8005]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.94batch/s, auc=0.8833, loss=0.9495]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 203/292 [00:33<00:14,  5.99batch/s, auc=0.8833, loss=0.9495]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 203/292 [00:33<00:14,  5.99batch/s, auc=0.8830, loss=0.8829]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 204/292 [00:33<00:14,  5.92batch/s, auc=0.8830, loss=0.8829]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 204/292 [00:33<00:14,  5.92batch/s, auc=0.8834, loss=0.5135]\u001b[A\n",
      "Training Epoch 3/25:  70%|███████   | 205/292 [00:33<00:14,  5.98batch/s, auc=0.8834, loss=0.5135]\u001b[A\n",
      "Training Epoch 3/25:  70%|███████   | 205/292 [00:33<00:14,  5.98batch/s, auc=0.8831, loss=0.9688]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 206/292 [00:33<00:14,  5.92batch/s, auc=0.8831, loss=0.9688]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 206/292 [00:34<00:14,  5.92batch/s, auc=0.8820, loss=1.8071]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 207/292 [00:34<00:14,  5.98batch/s, auc=0.8820, loss=1.8071]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 207/292 [00:34<00:14,  5.98batch/s, auc=0.8822, loss=0.6245]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 208/292 [00:34<00:14,  5.78batch/s, auc=0.8822, loss=0.6245]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 208/292 [00:34<00:14,  5.78batch/s, auc=0.8826, loss=0.5390]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.75batch/s, auc=0.8826, loss=0.5390]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.75batch/s, auc=0.8827, loss=0.8614]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.72batch/s, auc=0.8827, loss=0.8614]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.72batch/s, auc=0.8829, loss=0.6225]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 211/292 [00:34<00:14,  5.74batch/s, auc=0.8829, loss=0.6225]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.74batch/s, auc=0.8829, loss=0.5595]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.74batch/s, auc=0.8829, loss=0.5595]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.74batch/s, auc=0.8829, loss=0.7542]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.74batch/s, auc=0.8829, loss=0.7542]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.74batch/s, auc=0.8824, loss=0.9615]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.85batch/s, auc=0.8824, loss=0.9615]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.85batch/s, auc=0.8824, loss=0.7932]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▎  | 215/292 [00:35<00:12,  5.93batch/s, auc=0.8824, loss=0.7932]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▎  | 215/292 [00:35<00:12,  5.93batch/s, auc=0.8822, loss=0.8824]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 216/292 [00:35<00:12,  5.98batch/s, auc=0.8822, loss=0.8824]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 216/292 [00:35<00:12,  5.98batch/s, auc=0.8823, loss=0.5970]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.77batch/s, auc=0.8823, loss=0.5970]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.77batch/s, auc=0.8824, loss=0.8255]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.73batch/s, auc=0.8824, loss=0.8255]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.73batch/s, auc=0.8823, loss=0.7783]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.71batch/s, auc=0.8823, loss=0.7783]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.71batch/s, auc=0.8821, loss=0.8208]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.77batch/s, auc=0.8821, loss=0.8208]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.77batch/s, auc=0.8823, loss=0.6464]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.75batch/s, auc=0.8823, loss=0.6464]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.75batch/s, auc=0.8821, loss=0.6669]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.85batch/s, auc=0.8821, loss=0.6669]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.85batch/s, auc=0.8822, loss=0.6881]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▋  | 223/292 [00:36<00:12,  5.69batch/s, auc=0.8822, loss=0.6881]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.69batch/s, auc=0.8823, loss=0.6740]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.73batch/s, auc=0.8823, loss=0.6740]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.73batch/s, auc=0.8823, loss=0.8621]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.72batch/s, auc=0.8823, loss=0.8621]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.72batch/s, auc=0.8821, loss=0.9016]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.67batch/s, auc=0.8821, loss=0.9016]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.67batch/s, auc=0.8823, loss=0.6179]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.66batch/s, auc=0.8823, loss=0.6179]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.66batch/s, auc=0.8816, loss=1.2750]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.78batch/s, auc=0.8816, loss=1.2750]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.78batch/s, auc=0.8813, loss=0.7944]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 229/292 [00:37<00:10,  5.76batch/s, auc=0.8813, loss=0.7944]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.76batch/s, auc=0.8810, loss=0.8102]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.73batch/s, auc=0.8810, loss=0.8102]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.73batch/s, auc=0.8809, loss=0.7309]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.76batch/s, auc=0.8809, loss=0.7309]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.76batch/s, auc=0.8807, loss=0.7028]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.74batch/s, auc=0.8807, loss=0.7028]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.74batch/s, auc=0.8805, loss=1.0348]\u001b[A\n",
      "Training Epoch 3/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.72batch/s, auc=0.8805, loss=1.0348]\u001b[A\n",
      "Training Epoch 3/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.72batch/s, auc=0.8803, loss=0.8282]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 234/292 [00:38<00:10,  5.72batch/s, auc=0.8803, loss=0.8282]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 234/292 [00:39<00:10,  5.72batch/s, auc=0.8803, loss=0.6471]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 235/292 [00:39<00:09,  5.72batch/s, auc=0.8803, loss=0.6471]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 235/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.5737]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 236/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.5737]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 236/292 [00:39<00:09,  5.72batch/s, auc=0.8803, loss=0.7389]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 237/292 [00:39<00:09,  5.72batch/s, auc=0.8803, loss=0.7389]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 237/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.9477]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.9477]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.72batch/s, auc=0.8803, loss=0.8632]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.72batch/s, auc=0.8803, loss=0.8632]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.6601]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.72batch/s, auc=0.8804, loss=0.6601]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.72batch/s, auc=0.8803, loss=0.8544]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.70batch/s, auc=0.8803, loss=0.8544]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.70batch/s, auc=0.8804, loss=0.6823]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.69batch/s, auc=0.8804, loss=0.6823]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.69batch/s, auc=0.8802, loss=0.7604]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.73batch/s, auc=0.8802, loss=0.7604]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.73batch/s, auc=0.8804, loss=0.5860]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.71batch/s, auc=0.8804, loss=0.5860]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.71batch/s, auc=0.8804, loss=0.5739]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.70batch/s, auc=0.8804, loss=0.5739]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.70batch/s, auc=0.8806, loss=0.5115]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.71batch/s, auc=0.8806, loss=0.5115]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.71batch/s, auc=0.8806, loss=0.7934]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.71batch/s, auc=0.8806, loss=0.7934]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.71batch/s, auc=0.8809, loss=0.5764]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.75batch/s, auc=0.8809, loss=0.5764]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.75batch/s, auc=0.8809, loss=0.7527]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.72batch/s, auc=0.8809, loss=0.7527]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.72batch/s, auc=0.8810, loss=0.7198]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.69batch/s, auc=0.8810, loss=0.7198]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.69batch/s, auc=0.8812, loss=0.4937]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.72batch/s, auc=0.8812, loss=0.4937]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.72batch/s, auc=0.8816, loss=0.4892]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.77batch/s, auc=0.8816, loss=0.4892]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.77batch/s, auc=0.8818, loss=0.5143]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.74batch/s, auc=0.8818, loss=0.5143]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.74batch/s, auc=0.8817, loss=0.7425]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.71batch/s, auc=0.8817, loss=0.7425]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.71batch/s, auc=0.8821, loss=0.3908]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.72batch/s, auc=0.8821, loss=0.3908]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.72batch/s, auc=0.8822, loss=0.7582]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.77batch/s, auc=0.8822, loss=0.7582]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.77batch/s, auc=0.8822, loss=0.7342]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.73batch/s, auc=0.8822, loss=0.7342]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.73batch/s, auc=0.8822, loss=0.7656]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.81batch/s, auc=0.8822, loss=0.7656]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.81batch/s, auc=0.8822, loss=0.6040]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.75batch/s, auc=0.8822, loss=0.6040]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.75batch/s, auc=0.8821, loss=0.7812]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.73batch/s, auc=0.8821, loss=0.7812]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.73batch/s, auc=0.8819, loss=1.0490]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.70batch/s, auc=0.8819, loss=1.0490]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.70batch/s, auc=0.8823, loss=0.4471]\u001b[A\n",
      "Training Epoch 3/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.71batch/s, auc=0.8823, loss=0.4471]\u001b[A\n",
      "Training Epoch 3/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.71batch/s, auc=0.8824, loss=0.6491]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 263/292 [00:43<00:05,  5.76batch/s, auc=0.8824, loss=0.6491]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 263/292 [00:44<00:05,  5.76batch/s, auc=0.8816, loss=1.4053]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 264/292 [00:44<00:04,  5.72batch/s, auc=0.8816, loss=1.4053]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 264/292 [00:44<00:04,  5.72batch/s, auc=0.8813, loss=1.0169]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 265/292 [00:44<00:04,  5.70batch/s, auc=0.8813, loss=1.0169]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 265/292 [00:44<00:04,  5.70batch/s, auc=0.8810, loss=0.9654]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 266/292 [00:44<00:04,  5.69batch/s, auc=0.8810, loss=0.9654]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 266/292 [00:44<00:04,  5.69batch/s, auc=0.8810, loss=0.8034]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.69batch/s, auc=0.8810, loss=0.8034]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.69batch/s, auc=0.8809, loss=0.8534]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8809, loss=0.8534]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8805, loss=1.1407]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.66batch/s, auc=0.8805, loss=1.1407]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.66batch/s, auc=0.8802, loss=0.8054]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.70batch/s, auc=0.8802, loss=0.8054]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.70batch/s, auc=0.8800, loss=1.0805]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.8800, loss=1.0805]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.8800, loss=0.6404]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.66batch/s, auc=0.8800, loss=0.6404]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.66batch/s, auc=0.8799, loss=0.7219]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.65batch/s, auc=0.8799, loss=0.7219]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.65batch/s, auc=0.8797, loss=0.8817]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.68batch/s, auc=0.8797, loss=0.8817]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.68batch/s, auc=0.8798, loss=0.6029]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.72batch/s, auc=0.8798, loss=0.6029]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.72batch/s, auc=0.8797, loss=0.8144]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.70batch/s, auc=0.8797, loss=0.8144]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.70batch/s, auc=0.8798, loss=0.6457]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.68batch/s, auc=0.8798, loss=0.6457]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.68batch/s, auc=0.8797, loss=0.6673]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.69batch/s, auc=0.8797, loss=0.6673]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.69batch/s, auc=0.8799, loss=0.5812]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.67batch/s, auc=0.8799, loss=0.5812]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.67batch/s, auc=0.8797, loss=0.6718]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.72batch/s, auc=0.8797, loss=0.6718]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.72batch/s, auc=0.8796, loss=0.8903]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.70batch/s, auc=0.8796, loss=0.8903]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.70batch/s, auc=0.8796, loss=0.7126]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.8796, loss=0.7126]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.8796, loss=0.5592]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.68batch/s, auc=0.8796, loss=0.5592]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.68batch/s, auc=0.8793, loss=0.9058]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.76batch/s, auc=0.8793, loss=0.9058]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.76batch/s, auc=0.8797, loss=0.5073]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.74batch/s, auc=0.8797, loss=0.5073]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.74batch/s, auc=0.8799, loss=0.5689]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.83batch/s, auc=0.8799, loss=0.5689]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.83batch/s, auc=0.8799, loss=0.6609]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.90batch/s, auc=0.8799, loss=0.6609]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.90batch/s, auc=0.8802, loss=0.5040]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.94batch/s, auc=0.8802, loss=0.5040]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.94batch/s, auc=0.8804, loss=0.6153]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.97batch/s, auc=0.8804, loss=0.6153]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.97batch/s, auc=0.8804, loss=0.6190]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 290/292 [00:48<00:00,  6.00batch/s, auc=0.8804, loss=0.6190]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 290/292 [00:48<00:00,  6.00batch/s, auc=0.8805, loss=0.5546]\u001b[A\n",
      "Training Epoch 3/25: 100%|█████████▉| 291/292 [00:48<00:00,  6.01batch/s, auc=0.8805, loss=0.5546]\u001b[A\n",
      "Training Epoch 3/25: 100%|█████████▉| 291/292 [00:48<00:00,  6.01batch/s, auc=0.8806, loss=0.5029]\u001b[A\n",
      "Training Epoch 3/25: 100%|██████████| 292/292 [00:48<00:00,  5.96batch/s, auc=0.8806, loss=0.5029]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25] Train Loss: 0.7422 | Train AUROC: 0.8806 Val Loss: 0.7419 | Val AUROC: 0.8685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 3/25 [02:44<20:04, 54.74s/it]\n",
      "Training Epoch 4/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9282, loss=0.5578]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 1/292 [00:00<04:46,  1.02batch/s, auc=0.9282, loss=0.5578]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 1/292 [00:01<04:46,  1.02batch/s, auc=0.9251, loss=0.5901]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 2/292 [00:01<02:24,  2.01batch/s, auc=0.9251, loss=0.5901]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 2/292 [00:01<02:24,  2.01batch/s, auc=0.9110, loss=0.8248]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9110, loss=0.8248]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.8984, loss=0.7549]\u001b[A\n",
      "Training Epoch 4/25:   1%|▏         | 4/292 [00:01<01:18,  3.67batch/s, auc=0.8984, loss=0.7549]\u001b[A\n",
      "Training Epoch 4/25:   1%|▏         | 4/292 [00:01<01:18,  3.67batch/s, auc=0.8917, loss=0.8627]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 5/292 [00:01<01:06,  4.34batch/s, auc=0.8917, loss=0.8627]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 5/292 [00:01<01:06,  4.34batch/s, auc=0.8911, loss=0.7054]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 6/292 [00:01<00:58,  4.87batch/s, auc=0.8911, loss=0.7054]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 6/292 [00:01<00:58,  4.87batch/s, auc=0.8893, loss=0.7156]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 7/292 [00:01<00:53,  5.29batch/s, auc=0.8893, loss=0.7156]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 7/292 [00:02<00:53,  5.29batch/s, auc=0.8966, loss=0.5027]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 8/292 [00:02<00:50,  5.60batch/s, auc=0.8966, loss=0.5027]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 8/292 [00:02<00:50,  5.60batch/s, auc=0.9053, loss=0.6315]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 9/292 [00:02<00:49,  5.71batch/s, auc=0.9053, loss=0.6315]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 9/292 [00:02<00:49,  5.71batch/s, auc=0.9048, loss=0.5646]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 10/292 [00:02<00:50,  5.62batch/s, auc=0.9048, loss=0.5646]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 10/292 [00:02<00:50,  5.62batch/s, auc=0.9087, loss=0.6608]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 11/292 [00:02<00:48,  5.84batch/s, auc=0.9087, loss=0.6608]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 11/292 [00:02<00:48,  5.84batch/s, auc=0.9117, loss=0.5454]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9117, loss=0.5454]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9134, loss=0.5413]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 13/292 [00:02<00:45,  6.11batch/s, auc=0.9134, loss=0.5413]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 13/292 [00:03<00:45,  6.11batch/s, auc=0.9089, loss=0.7895]\u001b[A\n",
      "Training Epoch 4/25:   5%|▍         | 14/292 [00:03<00:45,  6.13batch/s, auc=0.9089, loss=0.7895]\u001b[A\n",
      "Training Epoch 4/25:   5%|▍         | 14/292 [00:03<00:45,  6.13batch/s, auc=0.9049, loss=0.9408]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 15/292 [00:03<00:44,  6.21batch/s, auc=0.9049, loss=0.9408]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 15/292 [00:03<00:44,  6.21batch/s, auc=0.9037, loss=0.7501]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9037, loss=0.7501]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9060, loss=0.4102]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9060, loss=0.4102]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.8998, loss=1.0342]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 18/292 [00:03<00:43,  6.26batch/s, auc=0.8998, loss=1.0342]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 18/292 [00:03<00:43,  6.26batch/s, auc=0.8983, loss=0.8179]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 19/292 [00:03<00:43,  6.28batch/s, auc=0.8983, loss=0.8179]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 19/292 [00:04<00:43,  6.28batch/s, auc=0.8983, loss=0.5495]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 20/292 [00:04<00:43,  6.23batch/s, auc=0.8983, loss=0.5495]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 20/292 [00:04<00:43,  6.23batch/s, auc=0.8942, loss=0.8957]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.8942, loss=0.8957]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.8937, loss=0.8562]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.8937, loss=0.8562]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.8942, loss=0.5715]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.8942, loss=0.5715]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.8942, loss=0.6246]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 24/292 [00:04<00:42,  6.32batch/s, auc=0.8942, loss=0.6246]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 24/292 [00:04<00:42,  6.32batch/s, auc=0.8910, loss=0.9772]\u001b[A\n",
      "Training Epoch 4/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.8910, loss=0.9772]\u001b[A\n",
      "Training Epoch 4/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.8906, loss=0.7206]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 26/292 [00:04<00:41,  6.35batch/s, auc=0.8906, loss=0.7206]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 26/292 [00:05<00:41,  6.35batch/s, auc=0.8896, loss=0.7287]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.8896, loss=0.7287]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.8880, loss=0.9566]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8880, loss=0.9566]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8892, loss=0.7358]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.8892, loss=0.7358]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.8898, loss=0.5886]\u001b[A\n",
      "Training Epoch 4/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.8898, loss=0.5886]\u001b[A\n",
      "Training Epoch 4/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.8912, loss=0.5647]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.8912, loss=0.5647]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.8914, loss=0.7579]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 32/292 [00:05<00:41,  6.33batch/s, auc=0.8914, loss=0.7579]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 32/292 [00:06<00:41,  6.33batch/s, auc=0.8897, loss=0.7600]\u001b[A\n",
      "Training Epoch 4/25:  11%|█▏        | 33/292 [00:06<00:40,  6.34batch/s, auc=0.8897, loss=0.7600]\u001b[A\n",
      "Training Epoch 4/25:  11%|█▏        | 33/292 [00:06<00:40,  6.34batch/s, auc=0.8892, loss=0.8300]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.8892, loss=0.8300]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.8896, loss=0.5424]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 35/292 [00:06<00:40,  6.35batch/s, auc=0.8896, loss=0.5424]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 35/292 [00:06<00:40,  6.35batch/s, auc=0.8915, loss=0.6461]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.8915, loss=0.6461]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.8937, loss=0.6006]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 37/292 [00:06<00:40,  6.33batch/s, auc=0.8937, loss=0.6006]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 37/292 [00:06<00:40,  6.33batch/s, auc=0.8926, loss=0.6875]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 38/292 [00:06<00:40,  6.32batch/s, auc=0.8926, loss=0.6875]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 38/292 [00:07<00:40,  6.32batch/s, auc=0.8937, loss=0.4532]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.8937, loss=0.4532]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.8945, loss=0.5671]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▎        | 40/292 [00:07<00:39,  6.34batch/s, auc=0.8945, loss=0.5671]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▎        | 40/292 [00:07<00:39,  6.34batch/s, auc=0.8956, loss=0.6446]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.8956, loss=0.6446]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.8955, loss=0.6604]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.8955, loss=0.6604]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.8979, loss=0.5783]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.8979, loss=0.5783]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9000, loss=0.5432]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 44/292 [00:07<00:39,  6.33batch/s, auc=0.9000, loss=0.5432]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 44/292 [00:07<00:39,  6.33batch/s, auc=0.8989, loss=0.6334]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 45/292 [00:07<00:39,  6.33batch/s, auc=0.8989, loss=0.6334]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 45/292 [00:08<00:39,  6.33batch/s, auc=0.8993, loss=0.5586]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 46/292 [00:08<00:38,  6.34batch/s, auc=0.8993, loss=0.5586]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 46/292 [00:08<00:38,  6.34batch/s, auc=0.8980, loss=0.9205]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.8980, loss=0.9205]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.8985, loss=0.5764]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▋        | 48/292 [00:08<00:38,  6.30batch/s, auc=0.8985, loss=0.5764]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▋        | 48/292 [00:08<00:38,  6.30batch/s, auc=0.8970, loss=0.6952]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.8970, loss=0.6952]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.8982, loss=0.5751]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.8982, loss=0.5751]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.8982, loss=0.7281]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 51/292 [00:08<00:38,  6.30batch/s, auc=0.8982, loss=0.7281]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 51/292 [00:09<00:38,  6.30batch/s, auc=0.8955, loss=1.3748]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.8955, loss=1.3748]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.8930, loss=1.1787]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.8930, loss=1.1787]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.8921, loss=0.9565]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 54/292 [00:09<00:37,  6.33batch/s, auc=0.8921, loss=0.9565]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 54/292 [00:09<00:37,  6.33batch/s, auc=0.8915, loss=0.6091]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.8915, loss=0.6091]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.8926, loss=0.3722]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.8926, loss=0.3722]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.8930, loss=0.5259]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 57/292 [00:09<00:37,  6.31batch/s, auc=0.8930, loss=0.5259]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 57/292 [00:10<00:37,  6.31batch/s, auc=0.8932, loss=0.8962]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.8932, loss=0.8962]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.8941, loss=0.5750]\u001b[A\n",
      "Training Epoch 4/25:  20%|██        | 59/292 [00:10<00:36,  6.31batch/s, auc=0.8941, loss=0.5750]\u001b[A\n",
      "Training Epoch 4/25:  20%|██        | 59/292 [00:10<00:36,  6.31batch/s, auc=0.8931, loss=0.9137]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 60/292 [00:10<00:36,  6.33batch/s, auc=0.8931, loss=0.9137]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 60/292 [00:10<00:36,  6.33batch/s, auc=0.8932, loss=0.7273]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 61/292 [00:10<00:36,  6.33batch/s, auc=0.8932, loss=0.7273]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 61/292 [00:10<00:36,  6.33batch/s, auc=0.8932, loss=0.6533]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 62/292 [00:10<00:36,  6.34batch/s, auc=0.8932, loss=0.6533]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 62/292 [00:10<00:36,  6.34batch/s, auc=0.8904, loss=1.2684]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 63/292 [00:10<00:36,  6.35batch/s, auc=0.8904, loss=1.2684]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 63/292 [00:10<00:36,  6.35batch/s, auc=0.8904, loss=0.7989]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 64/292 [00:10<00:35,  6.35batch/s, auc=0.8904, loss=0.7989]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 64/292 [00:11<00:35,  6.35batch/s, auc=0.8898, loss=0.9706]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 65/292 [00:11<00:35,  6.35batch/s, auc=0.8898, loss=0.9706]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 65/292 [00:11<00:35,  6.35batch/s, auc=0.8903, loss=0.4715]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 66/292 [00:11<00:35,  6.36batch/s, auc=0.8903, loss=0.4715]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 66/292 [00:11<00:35,  6.36batch/s, auc=0.8911, loss=0.5573]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 67/292 [00:11<00:35,  6.36batch/s, auc=0.8911, loss=0.5573]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 67/292 [00:11<00:35,  6.36batch/s, auc=0.8907, loss=1.1014]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 68/292 [00:11<00:35,  6.36batch/s, auc=0.8907, loss=1.1014]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 68/292 [00:11<00:35,  6.36batch/s, auc=0.8902, loss=0.8357]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▎       | 69/292 [00:11<00:35,  6.36batch/s, auc=0.8902, loss=0.8357]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▎       | 69/292 [00:11<00:35,  6.36batch/s, auc=0.8901, loss=0.7589]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 70/292 [00:11<00:34,  6.36batch/s, auc=0.8901, loss=0.7589]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 70/292 [00:12<00:34,  6.36batch/s, auc=0.8900, loss=0.5602]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 71/292 [00:12<00:34,  6.36batch/s, auc=0.8900, loss=0.5602]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 71/292 [00:12<00:34,  6.36batch/s, auc=0.8895, loss=0.6495]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▍       | 72/292 [00:12<00:34,  6.36batch/s, auc=0.8895, loss=0.6495]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▍       | 72/292 [00:12<00:34,  6.36batch/s, auc=0.8888, loss=0.6257]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 73/292 [00:12<00:34,  6.35batch/s, auc=0.8888, loss=0.6257]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 73/292 [00:12<00:34,  6.35batch/s, auc=0.8904, loss=0.4716]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 74/292 [00:12<00:34,  6.31batch/s, auc=0.8904, loss=0.4716]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 74/292 [00:12<00:34,  6.31batch/s, auc=0.8913, loss=0.5286]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 75/292 [00:12<00:34,  6.32batch/s, auc=0.8913, loss=0.5286]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 75/292 [00:12<00:34,  6.32batch/s, auc=0.8916, loss=0.5585]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 76/292 [00:12<00:34,  6.33batch/s, auc=0.8916, loss=0.5585]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 76/292 [00:13<00:34,  6.33batch/s, auc=0.8904, loss=1.1241]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▋       | 77/292 [00:13<00:33,  6.33batch/s, auc=0.8904, loss=1.1241]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▋       | 77/292 [00:13<00:33,  6.33batch/s, auc=0.8904, loss=0.6112]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.8904, loss=0.6112]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 78/292 [00:13<00:33,  6.34batch/s, auc=0.8891, loss=1.1325]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 79/292 [00:13<00:33,  6.33batch/s, auc=0.8891, loss=1.1325]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 79/292 [00:13<00:33,  6.33batch/s, auc=0.8896, loss=0.7135]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 80/292 [00:13<00:33,  6.33batch/s, auc=0.8896, loss=0.7135]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 80/292 [00:13<00:33,  6.33batch/s, auc=0.8894, loss=0.6608]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 81/292 [00:13<00:33,  6.33batch/s, auc=0.8894, loss=0.6608]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 81/292 [00:13<00:33,  6.33batch/s, auc=0.8888, loss=0.7562]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 82/292 [00:13<00:33,  6.33batch/s, auc=0.8888, loss=0.7562]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 82/292 [00:13<00:33,  6.33batch/s, auc=0.8888, loss=0.5996]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 83/292 [00:13<00:32,  6.34batch/s, auc=0.8888, loss=0.5996]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 83/292 [00:14<00:32,  6.34batch/s, auc=0.8891, loss=0.5342]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 84/292 [00:14<00:32,  6.34batch/s, auc=0.8891, loss=0.5342]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 84/292 [00:14<00:32,  6.34batch/s, auc=0.8888, loss=0.8854]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 85/292 [00:14<00:32,  6.34batch/s, auc=0.8888, loss=0.8854]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 85/292 [00:14<00:32,  6.34batch/s, auc=0.8893, loss=0.5393]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 86/292 [00:14<00:32,  6.34batch/s, auc=0.8893, loss=0.5393]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 86/292 [00:14<00:32,  6.34batch/s, auc=0.8894, loss=0.5574]\u001b[A\n",
      "Training Epoch 4/25:  30%|██▉       | 87/292 [00:14<00:32,  6.34batch/s, auc=0.8894, loss=0.5574]\u001b[A\n",
      "Training Epoch 4/25:  30%|██▉       | 87/292 [00:14<00:32,  6.34batch/s, auc=0.8890, loss=0.8078]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 88/292 [00:14<00:32,  6.32batch/s, auc=0.8890, loss=0.8078]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 88/292 [00:14<00:32,  6.32batch/s, auc=0.8888, loss=0.7291]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 89/292 [00:14<00:32,  6.32batch/s, auc=0.8888, loss=0.7291]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 89/292 [00:15<00:32,  6.32batch/s, auc=0.8895, loss=0.4885]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 90/292 [00:15<00:32,  6.31batch/s, auc=0.8895, loss=0.4885]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 90/292 [00:15<00:32,  6.31batch/s, auc=0.8893, loss=0.8159]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 91/292 [00:15<00:31,  6.31batch/s, auc=0.8893, loss=0.8159]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 91/292 [00:15<00:31,  6.31batch/s, auc=0.8900, loss=0.4780]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 92/292 [00:15<00:31,  6.30batch/s, auc=0.8900, loss=0.4780]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 92/292 [00:15<00:31,  6.30batch/s, auc=0.8896, loss=0.6948]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 93/292 [00:15<00:31,  6.30batch/s, auc=0.8896, loss=0.6948]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 93/292 [00:15<00:31,  6.30batch/s, auc=0.8894, loss=0.5642]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 94/292 [00:15<00:31,  6.30batch/s, auc=0.8894, loss=0.5642]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 94/292 [00:15<00:31,  6.30batch/s, auc=0.8888, loss=1.0646]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 95/292 [00:15<00:31,  6.31batch/s, auc=0.8888, loss=1.0646]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 95/292 [00:16<00:31,  6.31batch/s, auc=0.8888, loss=0.6825]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 96/292 [00:16<00:31,  6.31batch/s, auc=0.8888, loss=0.6825]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 96/292 [00:16<00:31,  6.31batch/s, auc=0.8891, loss=0.5424]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8891, loss=0.5424]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8889, loss=0.6836]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8889, loss=0.6836]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8890, loss=0.9465]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 99/292 [00:16<00:30,  6.32batch/s, auc=0.8890, loss=0.9465]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 99/292 [00:16<00:30,  6.32batch/s, auc=0.8887, loss=0.6913]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 100/292 [00:16<00:30,  6.32batch/s, auc=0.8887, loss=0.6913]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 100/292 [00:16<00:30,  6.32batch/s, auc=0.8885, loss=0.8332]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 101/292 [00:16<00:30,  6.31batch/s, auc=0.8885, loss=0.8332]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 101/292 [00:16<00:30,  6.31batch/s, auc=0.8882, loss=0.8419]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 102/292 [00:16<00:30,  6.31batch/s, auc=0.8882, loss=0.8419]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 102/292 [00:17<00:30,  6.31batch/s, auc=0.8884, loss=0.6420]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.8884, loss=0.6420]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.8879, loss=0.8118]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 104/292 [00:17<00:29,  6.31batch/s, auc=0.8879, loss=0.8118]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 104/292 [00:17<00:29,  6.31batch/s, auc=0.8875, loss=0.8013]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 105/292 [00:17<00:29,  6.30batch/s, auc=0.8875, loss=0.8013]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 105/292 [00:17<00:29,  6.30batch/s, auc=0.8881, loss=0.5530]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▋      | 106/292 [00:17<00:29,  6.30batch/s, auc=0.8881, loss=0.5530]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▋      | 106/292 [00:17<00:29,  6.30batch/s, auc=0.8880, loss=0.7211]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 107/292 [00:17<00:29,  6.30batch/s, auc=0.8880, loss=0.7211]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 107/292 [00:17<00:29,  6.30batch/s, auc=0.8862, loss=1.7373]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 108/292 [00:17<00:29,  6.30batch/s, auc=0.8862, loss=1.7373]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 108/292 [00:18<00:29,  6.30batch/s, auc=0.8865, loss=0.6824]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 109/292 [00:18<00:29,  6.30batch/s, auc=0.8865, loss=0.6824]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 109/292 [00:18<00:29,  6.30batch/s, auc=0.8870, loss=0.5335]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 110/292 [00:18<00:28,  6.30batch/s, auc=0.8870, loss=0.5335]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 110/292 [00:18<00:28,  6.30batch/s, auc=0.8868, loss=0.5747]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8868, loss=0.5747]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 111/292 [00:18<00:28,  6.28batch/s, auc=0.8871, loss=0.6633]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 112/292 [00:18<00:28,  6.27batch/s, auc=0.8871, loss=0.6633]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 112/292 [00:18<00:28,  6.27batch/s, auc=0.8875, loss=0.6989]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8875, loss=0.6989]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▊      | 113/292 [00:18<00:28,  6.28batch/s, auc=0.8875, loss=0.7825]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 114/292 [00:18<00:28,  6.29batch/s, auc=0.8875, loss=0.7825]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 114/292 [00:19<00:28,  6.29batch/s, auc=0.8878, loss=0.5925]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 115/292 [00:19<00:28,  6.29batch/s, auc=0.8878, loss=0.5925]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 115/292 [00:19<00:28,  6.29batch/s, auc=0.8873, loss=0.9308]\u001b[A\n",
      "Training Epoch 4/25:  40%|███▉      | 116/292 [00:19<00:27,  6.29batch/s, auc=0.8873, loss=0.9308]\u001b[A\n",
      "Training Epoch 4/25:  40%|███▉      | 116/292 [00:19<00:27,  6.29batch/s, auc=0.8873, loss=0.6248]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 117/292 [00:19<00:27,  6.29batch/s, auc=0.8873, loss=0.6248]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 117/292 [00:19<00:27,  6.29batch/s, auc=0.8872, loss=0.8699]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 118/292 [00:19<00:27,  6.26batch/s, auc=0.8872, loss=0.8699]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 118/292 [00:19<00:27,  6.26batch/s, auc=0.8877, loss=0.7205]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 119/292 [00:19<00:27,  6.27batch/s, auc=0.8877, loss=0.7205]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 119/292 [00:19<00:27,  6.27batch/s, auc=0.8878, loss=0.7289]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 120/292 [00:19<00:27,  6.27batch/s, auc=0.8878, loss=0.7289]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 120/292 [00:20<00:27,  6.27batch/s, auc=0.8874, loss=0.8845]\u001b[A\n",
      "Training Epoch 4/25:  41%|████▏     | 121/292 [00:20<00:27,  6.26batch/s, auc=0.8874, loss=0.8845]\u001b[A\n",
      "Training Epoch 4/25:  41%|████▏     | 121/292 [00:20<00:27,  6.26batch/s, auc=0.8877, loss=0.6282]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 122/292 [00:20<00:27,  6.26batch/s, auc=0.8877, loss=0.6282]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 122/292 [00:20<00:27,  6.26batch/s, auc=0.8876, loss=0.8104]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 123/292 [00:20<00:26,  6.26batch/s, auc=0.8876, loss=0.8104]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 123/292 [00:20<00:26,  6.26batch/s, auc=0.8878, loss=0.6086]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8878, loss=0.6086]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8878, loss=0.7593]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.8878, loss=0.7593]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.8875, loss=0.6219]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 126/292 [00:20<00:26,  6.27batch/s, auc=0.8875, loss=0.6219]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 126/292 [00:20<00:26,  6.27batch/s, auc=0.8879, loss=0.5417]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 127/292 [00:20<00:26,  6.27batch/s, auc=0.8879, loss=0.5417]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 127/292 [00:21<00:26,  6.27batch/s, auc=0.8878, loss=0.6584]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8878, loss=0.6584]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8886, loss=0.4654]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 129/292 [00:21<00:26,  6.26batch/s, auc=0.8886, loss=0.4654]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 129/292 [00:21<00:26,  6.26batch/s, auc=0.8895, loss=0.5702]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 130/292 [00:21<00:25,  6.25batch/s, auc=0.8895, loss=0.5702]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 130/292 [00:21<00:25,  6.25batch/s, auc=0.8898, loss=0.5836]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 131/292 [00:21<00:25,  6.24batch/s, auc=0.8898, loss=0.5836]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 131/292 [00:21<00:25,  6.24batch/s, auc=0.8899, loss=0.6448]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▌     | 132/292 [00:21<00:25,  6.25batch/s, auc=0.8899, loss=0.6448]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▌     | 132/292 [00:21<00:25,  6.25batch/s, auc=0.8905, loss=0.5177]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 133/292 [00:21<00:25,  6.25batch/s, auc=0.8905, loss=0.5177]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 133/292 [00:22<00:25,  6.25batch/s, auc=0.8907, loss=0.6373]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 134/292 [00:22<00:25,  6.26batch/s, auc=0.8907, loss=0.6373]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 134/292 [00:22<00:25,  6.26batch/s, auc=0.8902, loss=0.7617]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.8902, loss=0.7617]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.8905, loss=0.4719]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8905, loss=0.4719]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8905, loss=0.7346]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.8905, loss=0.7346]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 137/292 [00:22<00:24,  6.26batch/s, auc=0.8900, loss=0.8737]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 138/292 [00:22<00:24,  6.26batch/s, auc=0.8900, loss=0.8737]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 138/292 [00:22<00:24,  6.26batch/s, auc=0.8903, loss=0.6215]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 139/292 [00:22<00:24,  6.26batch/s, auc=0.8903, loss=0.6215]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 139/292 [00:23<00:24,  6.26batch/s, auc=0.8911, loss=0.4488]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.8911, loss=0.4488]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 140/292 [00:23<00:24,  6.26batch/s, auc=0.8913, loss=0.5743]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.8913, loss=0.5743]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 141/292 [00:23<00:24,  6.26batch/s, auc=0.8915, loss=0.7506]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.8915, loss=0.7506]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▊     | 142/292 [00:23<00:23,  6.26batch/s, auc=0.8917, loss=0.6006]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 143/292 [00:23<00:23,  6.26batch/s, auc=0.8917, loss=0.6006]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 143/292 [00:23<00:23,  6.26batch/s, auc=0.8919, loss=0.5283]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 144/292 [00:23<00:23,  6.25batch/s, auc=0.8919, loss=0.5283]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 144/292 [00:23<00:23,  6.25batch/s, auc=0.8920, loss=0.9095]\u001b[A\n",
      "Training Epoch 4/25:  50%|████▉     | 145/292 [00:23<00:23,  6.25batch/s, auc=0.8920, loss=0.9095]\u001b[A\n",
      "Training Epoch 4/25:  50%|████▉     | 145/292 [00:24<00:23,  6.25batch/s, auc=0.8922, loss=0.7730]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 146/292 [00:24<00:23,  6.25batch/s, auc=0.8922, loss=0.7730]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 146/292 [00:24<00:23,  6.25batch/s, auc=0.8916, loss=0.9417]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 147/292 [00:24<00:23,  6.25batch/s, auc=0.8916, loss=0.9417]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 147/292 [00:24<00:23,  6.25batch/s, auc=0.8915, loss=0.6914]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 148/292 [00:24<00:23,  6.25batch/s, auc=0.8915, loss=0.6914]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 148/292 [00:24<00:23,  6.25batch/s, auc=0.8910, loss=1.0701]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 149/292 [00:24<00:22,  6.25batch/s, auc=0.8910, loss=1.0701]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 149/292 [00:24<00:22,  6.25batch/s, auc=0.8908, loss=0.8033]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.25batch/s, auc=0.8908, loss=0.8033]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.25batch/s, auc=0.8911, loss=0.6622]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.24batch/s, auc=0.8911, loss=0.6622]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.24batch/s, auc=0.8914, loss=0.5091]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 152/292 [00:24<00:22,  6.22batch/s, auc=0.8914, loss=0.5091]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.22batch/s, auc=0.8914, loss=0.7805]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8914, loss=0.7805]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8912, loss=0.8209]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.23batch/s, auc=0.8912, loss=0.8209]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.23batch/s, auc=0.8914, loss=0.6920]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 155/292 [00:25<00:21,  6.23batch/s, auc=0.8914, loss=0.6920]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 155/292 [00:25<00:21,  6.23batch/s, auc=0.8914, loss=0.6750]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.23batch/s, auc=0.8914, loss=0.6750]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.23batch/s, auc=0.8908, loss=1.3228]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.23batch/s, auc=0.8908, loss=1.3228]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.23batch/s, auc=0.8908, loss=0.6982]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 158/292 [00:25<00:21,  6.23batch/s, auc=0.8908, loss=0.6982]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.23batch/s, auc=0.8909, loss=0.9616]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.23batch/s, auc=0.8909, loss=0.9616]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.23batch/s, auc=0.8909, loss=0.6453]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.23batch/s, auc=0.8909, loss=0.6453]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.23batch/s, auc=0.8911, loss=0.6377]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.23batch/s, auc=0.8911, loss=0.6377]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.23batch/s, auc=0.8910, loss=0.7858]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.23batch/s, auc=0.8910, loss=0.7858]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.23batch/s, auc=0.8910, loss=0.7673]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.23batch/s, auc=0.8910, loss=0.7673]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.23batch/s, auc=0.8906, loss=0.8664]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 164/292 [00:26<00:20,  6.23batch/s, auc=0.8906, loss=0.8664]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.23batch/s, auc=0.8909, loss=0.6361]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.22batch/s, auc=0.8909, loss=0.6361]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.22batch/s, auc=0.8908, loss=0.6212]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.21batch/s, auc=0.8908, loss=0.6212]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.21batch/s, auc=0.8907, loss=0.6732]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.22batch/s, auc=0.8907, loss=0.6732]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.22batch/s, auc=0.8904, loss=0.7862]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 168/292 [00:27<00:19,  6.22batch/s, auc=0.8904, loss=0.7862]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 168/292 [00:27<00:19,  6.22batch/s, auc=0.8906, loss=0.6457]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.22batch/s, auc=0.8906, loss=0.6457]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.22batch/s, auc=0.8905, loss=0.6609]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 170/292 [00:27<00:19,  6.22batch/s, auc=0.8905, loss=0.6609]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.22batch/s, auc=0.8902, loss=0.7852]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.22batch/s, auc=0.8902, loss=0.7852]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.22batch/s, auc=0.8902, loss=0.6520]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.22batch/s, auc=0.8902, loss=0.6520]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.22batch/s, auc=0.8900, loss=0.8679]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.22batch/s, auc=0.8900, loss=0.8679]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.22batch/s, auc=0.8899, loss=0.7243]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 174/292 [00:28<00:18,  6.22batch/s, auc=0.8899, loss=0.7243]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 174/292 [00:28<00:18,  6.22batch/s, auc=0.8904, loss=0.4541]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.22batch/s, auc=0.8904, loss=0.4541]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.22batch/s, auc=0.8905, loss=0.7284]\u001b[A\n",
      "Training Epoch 4/25:  60%|██████    | 176/292 [00:28<00:18,  6.21batch/s, auc=0.8905, loss=0.7284]\u001b[A\n",
      "Training Epoch 4/25:  60%|██████    | 176/292 [00:28<00:18,  6.21batch/s, auc=0.8903, loss=0.9086]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 177/292 [00:28<00:18,  6.21batch/s, auc=0.8903, loss=0.9086]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 177/292 [00:29<00:18,  6.21batch/s, auc=0.8905, loss=0.5842]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 178/292 [00:29<00:18,  6.21batch/s, auc=0.8905, loss=0.5842]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 178/292 [00:29<00:18,  6.21batch/s, auc=0.8902, loss=0.7845]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.21batch/s, auc=0.8902, loss=0.7845]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.21batch/s, auc=0.8906, loss=0.5845]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.21batch/s, auc=0.8906, loss=0.5845]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.21batch/s, auc=0.8902, loss=0.8584]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.19batch/s, auc=0.8902, loss=0.8584]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.19batch/s, auc=0.8904, loss=0.7175]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 182/292 [00:29<00:17,  6.19batch/s, auc=0.8904, loss=0.7175]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 182/292 [00:29<00:17,  6.19batch/s, auc=0.8904, loss=0.7144]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 183/292 [00:29<00:17,  6.19batch/s, auc=0.8904, loss=0.7144]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.19batch/s, auc=0.8903, loss=0.7743]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.19batch/s, auc=0.8903, loss=0.7743]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.19batch/s, auc=0.8902, loss=0.6435]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.18batch/s, auc=0.8902, loss=0.6435]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.18batch/s, auc=0.8894, loss=1.2606]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.17batch/s, auc=0.8894, loss=1.2606]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.17batch/s, auc=0.8897, loss=0.5056]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 187/292 [00:30<00:16,  6.18batch/s, auc=0.8897, loss=0.5056]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 187/292 [00:30<00:16,  6.18batch/s, auc=0.8897, loss=0.6809]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 188/292 [00:30<00:16,  6.18batch/s, auc=0.8897, loss=0.6809]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 188/292 [00:30<00:16,  6.18batch/s, auc=0.8897, loss=0.7027]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▍   | 189/292 [00:30<00:16,  6.15batch/s, auc=0.8897, loss=0.7027]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.15batch/s, auc=0.8897, loss=0.6575]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.16batch/s, auc=0.8897, loss=0.6575]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.16batch/s, auc=0.8897, loss=0.6544]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.17batch/s, auc=0.8897, loss=0.6544]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.17batch/s, auc=0.8897, loss=0.7665]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.17batch/s, auc=0.8897, loss=0.7665]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.17batch/s, auc=0.8900, loss=0.4592]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.17batch/s, auc=0.8900, loss=0.4592]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.17batch/s, auc=0.8902, loss=0.6094]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▋   | 194/292 [00:31<00:15,  6.17batch/s, auc=0.8902, loss=0.6094]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▋   | 194/292 [00:31<00:15,  6.17batch/s, auc=0.8900, loss=0.7080]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 195/292 [00:31<00:15,  6.18batch/s, auc=0.8900, loss=0.7080]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.18batch/s, auc=0.8902, loss=0.5401]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.18batch/s, auc=0.8902, loss=0.5401]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.18batch/s, auc=0.8900, loss=0.7787]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.18batch/s, auc=0.8900, loss=0.7787]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.18batch/s, auc=0.8903, loss=0.4597]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.18batch/s, auc=0.8903, loss=0.4597]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.18batch/s, auc=0.8901, loss=0.9354]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.18batch/s, auc=0.8901, loss=0.9354]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.18batch/s, auc=0.8904, loss=0.4946]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 200/292 [00:32<00:14,  6.17batch/s, auc=0.8904, loss=0.4946]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 200/292 [00:32<00:14,  6.17batch/s, auc=0.8907, loss=0.5872]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 201/292 [00:32<00:14,  6.17batch/s, auc=0.8907, loss=0.5872]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.17batch/s, auc=0.8902, loss=0.9193]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.17batch/s, auc=0.8902, loss=0.9193]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.17batch/s, auc=0.8902, loss=0.6092]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.17batch/s, auc=0.8902, loss=0.6092]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.17batch/s, auc=0.8899, loss=0.9941]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.17batch/s, auc=0.8899, loss=0.9941]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.17batch/s, auc=0.8898, loss=0.6844]\u001b[A\n",
      "Training Epoch 4/25:  70%|███████   | 205/292 [00:33<00:14,  6.16batch/s, auc=0.8898, loss=0.6844]\u001b[A\n",
      "Training Epoch 4/25:  70%|███████   | 205/292 [00:33<00:14,  6.16batch/s, auc=0.8897, loss=0.8322]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 206/292 [00:33<00:13,  6.15batch/s, auc=0.8897, loss=0.8322]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 206/292 [00:33<00:13,  6.15batch/s, auc=0.8894, loss=0.8665]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 207/292 [00:33<00:13,  6.14batch/s, auc=0.8894, loss=0.8665]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 207/292 [00:34<00:13,  6.14batch/s, auc=0.8892, loss=0.7947]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 208/292 [00:34<00:13,  6.14batch/s, auc=0.8892, loss=0.7947]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 208/292 [00:34<00:13,  6.14batch/s, auc=0.8893, loss=0.7224]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.8893, loss=0.7224]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.15batch/s, auc=0.8886, loss=1.2771]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.15batch/s, auc=0.8886, loss=1.2771]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.15batch/s, auc=0.8884, loss=0.8000]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.16batch/s, auc=0.8884, loss=0.8000]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.16batch/s, auc=0.8883, loss=0.5875]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 212/292 [00:34<00:13,  6.15batch/s, auc=0.8883, loss=0.5875]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 212/292 [00:34<00:13,  6.15batch/s, auc=0.8882, loss=0.8166]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 213/292 [00:34<00:12,  6.15batch/s, auc=0.8882, loss=0.8166]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 213/292 [00:34<00:12,  6.15batch/s, auc=0.8881, loss=0.7018]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 214/292 [00:34<00:12,  6.16batch/s, auc=0.8881, loss=0.7018]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.16batch/s, auc=0.8881, loss=0.7611]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.16batch/s, auc=0.8881, loss=0.7611]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.16batch/s, auc=0.8881, loss=0.7527]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.14batch/s, auc=0.8881, loss=0.7527]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.14batch/s, auc=0.8883, loss=0.6047]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 217/292 [00:35<00:12,  6.14batch/s, auc=0.8883, loss=0.6047]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 217/292 [00:35<00:12,  6.14batch/s, auc=0.8883, loss=0.6814]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▍  | 218/292 [00:35<00:12,  6.14batch/s, auc=0.8883, loss=0.6814]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▍  | 218/292 [00:35<00:12,  6.14batch/s, auc=0.8878, loss=1.0733]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 219/292 [00:35<00:11,  6.14batch/s, auc=0.8878, loss=1.0733]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 219/292 [00:35<00:11,  6.14batch/s, auc=0.8881, loss=0.5712]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 220/292 [00:35<00:11,  6.14batch/s, auc=0.8881, loss=0.5712]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 220/292 [00:36<00:11,  6.14batch/s, auc=0.8883, loss=0.5932]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.14batch/s, auc=0.8883, loss=0.5932]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.14batch/s, auc=0.8881, loss=0.9123]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.14batch/s, auc=0.8881, loss=0.9123]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.14batch/s, auc=0.8884, loss=0.6175]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▋  | 223/292 [00:36<00:11,  6.14batch/s, auc=0.8884, loss=0.6175]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▋  | 223/292 [00:36<00:11,  6.14batch/s, auc=0.8882, loss=0.8318]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 224/292 [00:36<00:11,  6.14batch/s, auc=0.8882, loss=0.8318]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 224/292 [00:36<00:11,  6.14batch/s, auc=0.8884, loss=0.6018]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 225/292 [00:36<00:10,  6.13batch/s, auc=0.8884, loss=0.6018]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 225/292 [00:36<00:10,  6.13batch/s, auc=0.8885, loss=0.6571]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 226/292 [00:36<00:10,  6.14batch/s, auc=0.8885, loss=0.6571]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 226/292 [00:37<00:10,  6.14batch/s, auc=0.8882, loss=0.7418]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 227/292 [00:37<00:10,  6.14batch/s, auc=0.8882, loss=0.7418]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 227/292 [00:37<00:10,  6.14batch/s, auc=0.8883, loss=0.7390]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 228/292 [00:37<00:10,  6.01batch/s, auc=0.8883, loss=0.7390]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 228/292 [00:37<00:10,  6.01batch/s, auc=0.8881, loss=0.6465]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 229/292 [00:37<00:10,  6.05batch/s, auc=0.8881, loss=0.6465]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 229/292 [00:37<00:10,  6.05batch/s, auc=0.8877, loss=1.1250]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 230/292 [00:37<00:10,  5.94batch/s, auc=0.8877, loss=1.1250]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 230/292 [00:37<00:10,  5.94batch/s, auc=0.8878, loss=0.5950]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 231/292 [00:37<00:10,  6.00batch/s, auc=0.8878, loss=0.5950]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 231/292 [00:37<00:10,  6.00batch/s, auc=0.8877, loss=0.8834]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 232/292 [00:37<00:10,  5.91batch/s, auc=0.8877, loss=0.8834]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.91batch/s, auc=0.8879, loss=0.5215]\u001b[A\n",
      "Training Epoch 4/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.85batch/s, auc=0.8879, loss=0.5215]\u001b[A\n",
      "Training Epoch 4/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.85batch/s, auc=0.8880, loss=0.6767]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 234/292 [00:38<00:09,  5.80batch/s, auc=0.8880, loss=0.6767]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 234/292 [00:38<00:09,  5.80batch/s, auc=0.8881, loss=0.5137]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 235/292 [00:38<00:09,  5.77batch/s, auc=0.8881, loss=0.5137]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 235/292 [00:38<00:09,  5.77batch/s, auc=0.8880, loss=0.7970]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 236/292 [00:38<00:09,  5.83batch/s, auc=0.8880, loss=0.7970]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 236/292 [00:38<00:09,  5.83batch/s, auc=0.8878, loss=0.9675]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 237/292 [00:38<00:09,  5.91batch/s, auc=0.8878, loss=0.9675]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 237/292 [00:38<00:09,  5.91batch/s, auc=0.8881, loss=0.7060]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 238/292 [00:38<00:09,  5.97batch/s, auc=0.8881, loss=0.7060]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.97batch/s, auc=0.8885, loss=0.4748]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 239/292 [00:39<00:08,  6.02batch/s, auc=0.8885, loss=0.4748]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 239/292 [00:39<00:08,  6.02batch/s, auc=0.8883, loss=0.7461]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 240/292 [00:39<00:08,  6.02batch/s, auc=0.8883, loss=0.7461]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 240/292 [00:39<00:08,  6.02batch/s, auc=0.8886, loss=0.5079]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 241/292 [00:39<00:08,  5.94batch/s, auc=0.8886, loss=0.5079]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 241/292 [00:39<00:08,  5.94batch/s, auc=0.8890, loss=0.3959]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 242/292 [00:39<00:08,  5.78batch/s, auc=0.8890, loss=0.3959]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 242/292 [00:39<00:08,  5.78batch/s, auc=0.8890, loss=0.7362]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 243/292 [00:39<00:08,  5.76batch/s, auc=0.8890, loss=0.7362]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.76batch/s, auc=0.8889, loss=0.8063]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.86batch/s, auc=0.8889, loss=0.8063]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.86batch/s, auc=0.8891, loss=0.5510]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 245/292 [00:40<00:07,  5.93batch/s, auc=0.8891, loss=0.5510]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 245/292 [00:40<00:07,  5.93batch/s, auc=0.8890, loss=0.5986]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 246/292 [00:40<00:07,  5.87batch/s, auc=0.8890, loss=0.5986]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 246/292 [00:40<00:07,  5.87batch/s, auc=0.8890, loss=0.6198]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 247/292 [00:40<00:07,  5.70batch/s, auc=0.8890, loss=0.6198]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 247/292 [00:40<00:07,  5.70batch/s, auc=0.8891, loss=0.5556]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 248/292 [00:40<00:07,  5.81batch/s, auc=0.8891, loss=0.5556]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 248/292 [00:40<00:07,  5.81batch/s, auc=0.8894, loss=0.4158]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▌ | 249/292 [00:40<00:07,  5.78batch/s, auc=0.8894, loss=0.4158]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.78batch/s, auc=0.8893, loss=0.7525]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.86batch/s, auc=0.8893, loss=0.7525]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.86batch/s, auc=0.8893, loss=0.5674]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.72batch/s, auc=0.8893, loss=0.5674]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.72batch/s, auc=0.8890, loss=0.8660]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.72batch/s, auc=0.8890, loss=0.8660]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.72batch/s, auc=0.8891, loss=0.6755]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 253/292 [00:41<00:06,  5.70batch/s, auc=0.8891, loss=0.6755]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 253/292 [00:41<00:06,  5.70batch/s, auc=0.8890, loss=0.6516]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 254/292 [00:41<00:06,  5.81batch/s, auc=0.8890, loss=0.6516]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 254/292 [00:41<00:06,  5.81batch/s, auc=0.8888, loss=0.8121]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 255/292 [00:41<00:06,  5.66batch/s, auc=0.8888, loss=0.8121]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.66batch/s, auc=0.8888, loss=0.6266]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.71batch/s, auc=0.8888, loss=0.6266]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.71batch/s, auc=0.8888, loss=0.7215]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.71batch/s, auc=0.8888, loss=0.7215]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.71batch/s, auc=0.8888, loss=0.6713]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 258/292 [00:42<00:05,  5.71batch/s, auc=0.8888, loss=0.6713]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 258/292 [00:42<00:05,  5.71batch/s, auc=0.8884, loss=1.2852]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▊ | 259/292 [00:42<00:05,  5.70batch/s, auc=0.8884, loss=1.2852]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▊ | 259/292 [00:42<00:05,  5.70batch/s, auc=0.8883, loss=0.6942]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 260/292 [00:42<00:05,  5.74batch/s, auc=0.8883, loss=0.6942]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 260/292 [00:42<00:05,  5.74batch/s, auc=0.8885, loss=0.6219]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 261/292 [00:42<00:05,  5.73batch/s, auc=0.8885, loss=0.6219]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.73batch/s, auc=0.8887, loss=0.6004]\u001b[A\n",
      "Training Epoch 4/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.69batch/s, auc=0.8887, loss=0.6004]\u001b[A\n",
      "Training Epoch 4/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.69batch/s, auc=0.8888, loss=0.6570]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 263/292 [00:43<00:05,  5.70batch/s, auc=0.8888, loss=0.6570]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 263/292 [00:43<00:05,  5.70batch/s, auc=0.8889, loss=0.6032]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 264/292 [00:43<00:04,  5.77batch/s, auc=0.8889, loss=0.6032]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 264/292 [00:43<00:04,  5.77batch/s, auc=0.8891, loss=0.4039]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 265/292 [00:43<00:04,  5.74batch/s, auc=0.8891, loss=0.4039]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 265/292 [00:43<00:04,  5.74batch/s, auc=0.8891, loss=0.7505]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 266/292 [00:43<00:04,  5.83batch/s, auc=0.8891, loss=0.7505]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 266/292 [00:44<00:04,  5.83batch/s, auc=0.8889, loss=0.9855]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.68batch/s, auc=0.8889, loss=0.9855]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.68batch/s, auc=0.8889, loss=0.8276]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.70batch/s, auc=0.8889, loss=0.8276]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.70batch/s, auc=0.8887, loss=0.9004]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 269/292 [00:44<00:03,  5.79batch/s, auc=0.8887, loss=0.9004]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 269/292 [00:44<00:03,  5.79batch/s, auc=0.8888, loss=0.6779]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 270/292 [00:44<00:03,  5.86batch/s, auc=0.8888, loss=0.6779]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 270/292 [00:44<00:03,  5.86batch/s, auc=0.8890, loss=0.5455]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 271/292 [00:44<00:03,  5.70batch/s, auc=0.8890, loss=0.5455]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 271/292 [00:44<00:03,  5.70batch/s, auc=0.8891, loss=0.7347]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 272/292 [00:44<00:03,  5.75batch/s, auc=0.8891, loss=0.7347]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.75batch/s, auc=0.8892, loss=0.7048]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8892, loss=0.7048]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.73batch/s, auc=0.8890, loss=0.9022]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.71batch/s, auc=0.8890, loss=0.9022]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.71batch/s, auc=0.8893, loss=0.5037]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 275/292 [00:45<00:02,  5.69batch/s, auc=0.8893, loss=0.5037]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 275/292 [00:45<00:02,  5.69batch/s, auc=0.8895, loss=0.6688]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 276/292 [00:45<00:02,  5.76batch/s, auc=0.8895, loss=0.6688]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 276/292 [00:45<00:02,  5.76batch/s, auc=0.8895, loss=0.8072]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 277/292 [00:45<00:02,  5.72batch/s, auc=0.8895, loss=0.8072]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 277/292 [00:45<00:02,  5.72batch/s, auc=0.8895, loss=0.8563]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▌| 278/292 [00:45<00:02,  5.70batch/s, auc=0.8895, loss=0.8563]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.70batch/s, auc=0.8897, loss=0.6586]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.69batch/s, auc=0.8897, loss=0.6586]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.69batch/s, auc=0.8897, loss=0.6626]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.68batch/s, auc=0.8897, loss=0.6626]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.68batch/s, auc=0.8899, loss=0.5296]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 281/292 [00:46<00:01,  5.68batch/s, auc=0.8899, loss=0.5296]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 281/292 [00:46<00:01,  5.68batch/s, auc=0.8897, loss=0.9737]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 282/292 [00:46<00:01,  5.66batch/s, auc=0.8897, loss=0.9737]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 282/292 [00:46<00:01,  5.66batch/s, auc=0.8897, loss=0.6538]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 283/292 [00:46<00:01,  5.66batch/s, auc=0.8897, loss=0.6538]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 283/292 [00:46<00:01,  5.66batch/s, auc=0.8898, loss=0.6557]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 284/292 [00:46<00:01,  5.73batch/s, auc=0.8898, loss=0.6557]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.73batch/s, auc=0.8896, loss=0.8527]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.68batch/s, auc=0.8896, loss=0.8527]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.68batch/s, auc=0.8896, loss=0.6125]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.78batch/s, auc=0.8896, loss=0.6125]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.78batch/s, auc=0.8897, loss=0.6270]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 287/292 [00:47<00:00,  5.87batch/s, auc=0.8897, loss=0.6270]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 287/292 [00:47<00:00,  5.87batch/s, auc=0.8899, loss=0.6910]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▊| 288/292 [00:47<00:00,  5.92batch/s, auc=0.8899, loss=0.6910]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▊| 288/292 [00:47<00:00,  5.92batch/s, auc=0.8901, loss=0.6226]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 289/292 [00:47<00:00,  5.95batch/s, auc=0.8901, loss=0.6226]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 289/292 [00:47<00:00,  5.95batch/s, auc=0.8901, loss=0.7475]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 290/292 [00:47<00:00,  5.99batch/s, auc=0.8901, loss=0.7475]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.99batch/s, auc=0.8902, loss=0.5553]\u001b[A\n",
      "Training Epoch 4/25: 100%|█████████▉| 291/292 [00:48<00:00,  6.01batch/s, auc=0.8902, loss=0.5553]\u001b[A\n",
      "Training Epoch 4/25: 100%|█████████▉| 291/292 [00:48<00:00,  6.01batch/s, auc=0.8900, loss=1.1671]\u001b[A\n",
      "Training Epoch 4/25: 100%|██████████| 292/292 [00:48<00:00,  6.04batch/s, auc=0.8900, loss=1.1671]\u001b[A\n",
      "Epochs:  16%|█▌        | 4/25 [03:38<19:02, 54.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25] Train Loss: 0.7161 | Train AUROC: 0.8900 Val Loss: 0.7520 | Val AUROC: 0.8793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 5/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9095, loss=0.8069]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 1/292 [00:01<04:52,  1.01s/batch, auc=0.9095, loss=0.8069]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 1/292 [00:01<04:52,  1.01s/batch, auc=0.8602, loss=0.9223]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 2/292 [00:01<02:27,  1.97batch/s, auc=0.8602, loss=0.9223]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 2/292 [00:01<02:27,  1.97batch/s, auc=0.8746, loss=0.6916]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 3/292 [00:01<01:43,  2.81batch/s, auc=0.8746, loss=0.6916]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 3/292 [00:01<01:43,  2.81batch/s, auc=0.8809, loss=0.7665]\u001b[A\n",
      "Training Epoch 5/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.8809, loss=0.7665]\u001b[A\n",
      "Training Epoch 5/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.8823, loss=0.5887]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.8823, loss=0.5887]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.8801, loss=0.7055]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 6/292 [00:01<00:59,  4.83batch/s, auc=0.8801, loss=0.7055]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 6/292 [00:01<00:59,  4.83batch/s, auc=0.8836, loss=0.5645]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 7/292 [00:01<00:54,  5.23batch/s, auc=0.8836, loss=0.5645]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 7/292 [00:02<00:54,  5.23batch/s, auc=0.8792, loss=0.8451]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 8/292 [00:02<00:51,  5.56batch/s, auc=0.8792, loss=0.8451]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 8/292 [00:02<00:51,  5.56batch/s, auc=0.8842, loss=0.7171]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 9/292 [00:02<00:49,  5.72batch/s, auc=0.8842, loss=0.7171]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 9/292 [00:02<00:49,  5.72batch/s, auc=0.8800, loss=0.8092]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 10/292 [00:02<00:48,  5.87batch/s, auc=0.8800, loss=0.8092]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 10/292 [00:02<00:48,  5.87batch/s, auc=0.8821, loss=0.6630]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 11/292 [00:02<00:46,  6.03batch/s, auc=0.8821, loss=0.6630]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 11/292 [00:02<00:46,  6.03batch/s, auc=0.8861, loss=0.7398]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 12/292 [00:02<00:45,  6.15batch/s, auc=0.8861, loss=0.7398]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 12/292 [00:02<00:45,  6.15batch/s, auc=0.8853, loss=0.7682]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 13/292 [00:02<00:44,  6.23batch/s, auc=0.8853, loss=0.7682]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 13/292 [00:03<00:44,  6.23batch/s, auc=0.8859, loss=0.6818]\u001b[A\n",
      "Training Epoch 5/25:   5%|▍         | 14/292 [00:03<00:44,  6.28batch/s, auc=0.8859, loss=0.6818]\u001b[A\n",
      "Training Epoch 5/25:   5%|▍         | 14/292 [00:03<00:44,  6.28batch/s, auc=0.8881, loss=0.6234]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 15/292 [00:03<00:43,  6.31batch/s, auc=0.8881, loss=0.6234]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 15/292 [00:03<00:43,  6.31batch/s, auc=0.8915, loss=0.5411]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 16/292 [00:03<00:43,  6.33batch/s, auc=0.8915, loss=0.5411]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 16/292 [00:03<00:43,  6.33batch/s, auc=0.8889, loss=0.7571]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 17/292 [00:03<00:43,  6.34batch/s, auc=0.8889, loss=0.7571]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 17/292 [00:03<00:43,  6.34batch/s, auc=0.8886, loss=0.7321]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 18/292 [00:03<00:43,  6.34batch/s, auc=0.8886, loss=0.7321]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 18/292 [00:03<00:43,  6.34batch/s, auc=0.8880, loss=0.7070]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 19/292 [00:03<00:43,  6.33batch/s, auc=0.8880, loss=0.7070]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 19/292 [00:04<00:43,  6.33batch/s, auc=0.8864, loss=0.7045]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.8864, loss=0.7045]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.8892, loss=0.5347]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.8892, loss=0.5347]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.8880, loss=0.7378]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.8880, loss=0.7378]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.8915, loss=0.5087]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.8915, loss=0.5087]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.8916, loss=0.5615]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 24/292 [00:04<00:42,  6.28batch/s, auc=0.8916, loss=0.5615]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 24/292 [00:04<00:42,  6.28batch/s, auc=0.8922, loss=0.5929]\u001b[A\n",
      "Training Epoch 5/25:   9%|▊         | 25/292 [00:04<00:42,  6.26batch/s, auc=0.8922, loss=0.5929]\u001b[A\n",
      "Training Epoch 5/25:   9%|▊         | 25/292 [00:04<00:42,  6.26batch/s, auc=0.8933, loss=0.6054]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 26/292 [00:04<00:42,  6.30batch/s, auc=0.8933, loss=0.6054]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 26/292 [00:05<00:42,  6.30batch/s, auc=0.8923, loss=0.6420]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.8923, loss=0.6420]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.8935, loss=0.5675]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8935, loss=0.5675]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.8924, loss=0.6685]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8924, loss=0.6685]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.8945, loss=0.5776]\u001b[A\n",
      "Training Epoch 5/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.8945, loss=0.5776]\u001b[A\n",
      "Training Epoch 5/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.8953, loss=0.7017]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 31/292 [00:05<00:41,  6.35batch/s, auc=0.8953, loss=0.7017]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 31/292 [00:05<00:41,  6.35batch/s, auc=0.8953, loss=0.6215]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 32/292 [00:05<00:40,  6.36batch/s, auc=0.8953, loss=0.6215]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 32/292 [00:06<00:40,  6.36batch/s, auc=0.8964, loss=0.4752]\u001b[A\n",
      "Training Epoch 5/25:  11%|█▏        | 33/292 [00:06<00:40,  6.36batch/s, auc=0.8964, loss=0.4752]\u001b[A\n",
      "Training Epoch 5/25:  11%|█▏        | 33/292 [00:06<00:40,  6.36batch/s, auc=0.8964, loss=0.7510]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 34/292 [00:06<00:40,  6.37batch/s, auc=0.8964, loss=0.7510]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 34/292 [00:06<00:40,  6.37batch/s, auc=0.8962, loss=0.7322]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 35/292 [00:06<00:40,  6.35batch/s, auc=0.8962, loss=0.7322]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 35/292 [00:06<00:40,  6.35batch/s, auc=0.8967, loss=0.7811]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 36/292 [00:06<00:40,  6.35batch/s, auc=0.8967, loss=0.7811]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 36/292 [00:06<00:40,  6.35batch/s, auc=0.8969, loss=0.6908]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8969, loss=0.6908]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.8989, loss=0.4810]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 38/292 [00:06<00:39,  6.35batch/s, auc=0.8989, loss=0.4810]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 38/292 [00:07<00:39,  6.35batch/s, auc=0.9004, loss=0.5153]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.9004, loss=0.5153]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.9004, loss=0.7766]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▎        | 40/292 [00:07<00:39,  6.35batch/s, auc=0.9004, loss=0.7766]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▎        | 40/292 [00:07<00:39,  6.35batch/s, auc=0.9005, loss=0.6217]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 41/292 [00:07<00:39,  6.36batch/s, auc=0.9005, loss=0.6217]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 41/292 [00:07<00:39,  6.36batch/s, auc=0.9000, loss=0.7446]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 42/292 [00:07<00:39,  6.36batch/s, auc=0.9000, loss=0.7446]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 42/292 [00:07<00:39,  6.36batch/s, auc=0.9001, loss=0.6827]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9001, loss=0.6827]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9002, loss=0.6674]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9002, loss=0.6674]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9011, loss=0.4965]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 45/292 [00:07<00:39,  6.30batch/s, auc=0.9011, loss=0.4965]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9022, loss=0.4662]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 46/292 [00:08<00:39,  6.31batch/s, auc=0.9022, loss=0.4662]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 46/292 [00:08<00:39,  6.31batch/s, auc=0.9027, loss=0.5585]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 47/292 [00:08<00:38,  6.30batch/s, auc=0.9027, loss=0.5585]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 47/292 [00:08<00:38,  6.30batch/s, auc=0.9002, loss=1.0171]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9002, loss=1.0171]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9006, loss=0.5675]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.9006, loss=0.5675]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.8991, loss=0.8787]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.8991, loss=0.8787]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.9001, loss=0.4099]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 51/292 [00:08<00:38,  6.28batch/s, auc=0.9001, loss=0.4099]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 51/292 [00:09<00:38,  6.28batch/s, auc=0.9008, loss=0.6324]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.9008, loss=0.6324]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.9017, loss=0.5537]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9017, loss=0.5537]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9037, loss=0.3812]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 54/292 [00:09<00:37,  6.30batch/s, auc=0.9037, loss=0.3812]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 54/292 [00:09<00:37,  6.30batch/s, auc=0.9044, loss=0.5020]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9044, loss=0.5020]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9026, loss=1.0463]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9026, loss=1.0463]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9019, loss=0.8517]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 57/292 [00:09<00:37,  6.30batch/s, auc=0.9019, loss=0.8517]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 57/292 [00:10<00:37,  6.30batch/s, auc=0.9029, loss=0.4965]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 58/292 [00:10<00:37,  6.32batch/s, auc=0.9029, loss=0.4965]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 58/292 [00:10<00:37,  6.32batch/s, auc=0.9030, loss=0.5835]\u001b[A\n",
      "Training Epoch 5/25:  20%|██        | 59/292 [00:10<00:36,  6.32batch/s, auc=0.9030, loss=0.5835]\u001b[A\n",
      "Training Epoch 5/25:  20%|██        | 59/292 [00:10<00:36,  6.32batch/s, auc=0.9030, loss=0.6515]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.9030, loss=0.6515]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.9025, loss=0.6660]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 61/292 [00:10<00:36,  6.33batch/s, auc=0.9025, loss=0.6660]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 61/292 [00:10<00:36,  6.33batch/s, auc=0.9028, loss=0.7365]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.9028, loss=0.7365]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 62/292 [00:10<00:36,  6.33batch/s, auc=0.9022, loss=0.9075]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 63/292 [00:10<00:36,  6.33batch/s, auc=0.9022, loss=0.9075]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 63/292 [00:10<00:36,  6.33batch/s, auc=0.9024, loss=0.5109]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 64/292 [00:10<00:35,  6.33batch/s, auc=0.9024, loss=0.5109]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 64/292 [00:11<00:35,  6.33batch/s, auc=0.9029, loss=0.4946]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 65/292 [00:11<00:35,  6.33batch/s, auc=0.9029, loss=0.4946]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 65/292 [00:11<00:35,  6.33batch/s, auc=0.9027, loss=0.6095]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 66/292 [00:11<00:35,  6.34batch/s, auc=0.9027, loss=0.6095]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 66/292 [00:11<00:35,  6.34batch/s, auc=0.9028, loss=0.5889]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 67/292 [00:11<00:35,  6.32batch/s, auc=0.9028, loss=0.5889]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 67/292 [00:11<00:35,  6.32batch/s, auc=0.9028, loss=0.6572]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 68/292 [00:11<00:35,  6.33batch/s, auc=0.9028, loss=0.6572]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 68/292 [00:11<00:35,  6.33batch/s, auc=0.9021, loss=0.7835]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▎       | 69/292 [00:11<00:35,  6.33batch/s, auc=0.9021, loss=0.7835]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▎       | 69/292 [00:11<00:35,  6.33batch/s, auc=0.9027, loss=0.5284]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 70/292 [00:11<00:35,  6.33batch/s, auc=0.9027, loss=0.5284]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 70/292 [00:12<00:35,  6.33batch/s, auc=0.9031, loss=0.5017]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.9031, loss=0.5017]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 71/292 [00:12<00:34,  6.32batch/s, auc=0.9019, loss=1.1870]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▍       | 72/292 [00:12<00:34,  6.33batch/s, auc=0.9019, loss=1.1870]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▍       | 72/292 [00:12<00:34,  6.33batch/s, auc=0.9021, loss=0.5554]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 73/292 [00:12<00:34,  6.32batch/s, auc=0.9021, loss=0.5554]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 73/292 [00:12<00:34,  6.32batch/s, auc=0.9004, loss=1.2699]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 74/292 [00:12<00:34,  6.32batch/s, auc=0.9004, loss=1.2699]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 74/292 [00:12<00:34,  6.32batch/s, auc=0.8997, loss=0.8478]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 75/292 [00:12<00:34,  6.33batch/s, auc=0.8997, loss=0.8478]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 75/292 [00:12<00:34,  6.33batch/s, auc=0.8991, loss=0.7513]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 76/292 [00:12<00:34,  6.26batch/s, auc=0.8991, loss=0.7513]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 76/292 [00:13<00:34,  6.26batch/s, auc=0.8990, loss=0.7061]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.8990, loss=0.7061]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.8990, loss=0.6115]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 78/292 [00:13<00:33,  6.30batch/s, auc=0.8990, loss=0.6115]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 78/292 [00:13<00:33,  6.30batch/s, auc=0.8981, loss=0.9876]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 79/292 [00:13<00:33,  6.29batch/s, auc=0.8981, loss=0.9876]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 79/292 [00:13<00:33,  6.29batch/s, auc=0.8978, loss=0.6606]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 80/292 [00:13<00:33,  6.30batch/s, auc=0.8978, loss=0.6606]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 80/292 [00:13<00:33,  6.30batch/s, auc=0.8974, loss=0.9452]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 81/292 [00:13<00:33,  6.31batch/s, auc=0.8974, loss=0.9452]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 81/292 [00:13<00:33,  6.31batch/s, auc=0.8963, loss=0.8285]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 82/292 [00:13<00:33,  6.30batch/s, auc=0.8963, loss=0.8285]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 82/292 [00:13<00:33,  6.30batch/s, auc=0.8952, loss=1.0021]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 83/292 [00:13<00:33,  6.31batch/s, auc=0.8952, loss=1.0021]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 83/292 [00:14<00:33,  6.31batch/s, auc=0.8959, loss=0.4828]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 84/292 [00:14<00:33,  6.30batch/s, auc=0.8959, loss=0.4828]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 84/292 [00:14<00:33,  6.30batch/s, auc=0.8962, loss=0.6314]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 85/292 [00:14<00:32,  6.30batch/s, auc=0.8962, loss=0.6314]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 85/292 [00:14<00:32,  6.30batch/s, auc=0.8966, loss=0.5057]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 86/292 [00:14<00:32,  6.31batch/s, auc=0.8966, loss=0.5057]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 86/292 [00:14<00:32,  6.31batch/s, auc=0.8969, loss=0.5502]\u001b[A\n",
      "Training Epoch 5/25:  30%|██▉       | 87/292 [00:14<00:32,  6.31batch/s, auc=0.8969, loss=0.5502]\u001b[A\n",
      "Training Epoch 5/25:  30%|██▉       | 87/292 [00:14<00:32,  6.31batch/s, auc=0.8963, loss=0.7942]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 88/292 [00:14<00:32,  6.31batch/s, auc=0.8963, loss=0.7942]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 88/292 [00:14<00:32,  6.31batch/s, auc=0.8962, loss=0.6942]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 89/292 [00:14<00:32,  6.30batch/s, auc=0.8962, loss=0.6942]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 89/292 [00:15<00:32,  6.30batch/s, auc=0.8960, loss=0.7105]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 90/292 [00:15<00:32,  6.30batch/s, auc=0.8960, loss=0.7105]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 90/292 [00:15<00:32,  6.30batch/s, auc=0.8966, loss=0.5485]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 91/292 [00:15<00:31,  6.30batch/s, auc=0.8966, loss=0.5485]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 91/292 [00:15<00:31,  6.30batch/s, auc=0.8967, loss=0.6403]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 92/292 [00:15<00:31,  6.30batch/s, auc=0.8967, loss=0.6403]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 92/292 [00:15<00:31,  6.30batch/s, auc=0.8972, loss=0.5364]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 93/292 [00:15<00:31,  6.31batch/s, auc=0.8972, loss=0.5364]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 93/292 [00:15<00:31,  6.31batch/s, auc=0.8969, loss=0.7988]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 94/292 [00:15<00:31,  6.31batch/s, auc=0.8969, loss=0.7988]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 94/292 [00:15<00:31,  6.31batch/s, auc=0.8979, loss=0.4900]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 95/292 [00:15<00:31,  6.31batch/s, auc=0.8979, loss=0.4900]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 95/292 [00:16<00:31,  6.31batch/s, auc=0.8967, loss=0.9591]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 96/292 [00:16<00:31,  6.31batch/s, auc=0.8967, loss=0.9591]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 96/292 [00:16<00:31,  6.31batch/s, auc=0.8972, loss=0.5486]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8972, loss=0.5486]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 97/292 [00:16<00:30,  6.31batch/s, auc=0.8975, loss=0.6219]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8975, loss=0.6219]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▎      | 98/292 [00:16<00:30,  6.31batch/s, auc=0.8973, loss=0.5903]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.8973, loss=0.5903]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 99/292 [00:16<00:30,  6.31batch/s, auc=0.8973, loss=0.6357]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 100/292 [00:16<00:30,  6.30batch/s, auc=0.8973, loss=0.6357]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 100/292 [00:16<00:30,  6.30batch/s, auc=0.8966, loss=0.8907]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 101/292 [00:16<00:30,  6.30batch/s, auc=0.8966, loss=0.8907]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 101/292 [00:16<00:30,  6.30batch/s, auc=0.8971, loss=0.4812]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 102/292 [00:17<00:30,  6.30batch/s, auc=0.8971, loss=0.4812]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 102/292 [00:17<00:30,  6.30batch/s, auc=0.8968, loss=0.6503]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.8968, loss=0.6503]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▌      | 103/292 [00:17<00:29,  6.31batch/s, auc=0.8964, loss=1.0089]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 104/292 [00:17<00:29,  6.31batch/s, auc=0.8964, loss=1.0089]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 104/292 [00:17<00:29,  6.31batch/s, auc=0.8962, loss=0.7584]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8962, loss=0.7584]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 105/292 [00:17<00:29,  6.29batch/s, auc=0.8964, loss=0.5959]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▋      | 106/292 [00:17<00:29,  6.28batch/s, auc=0.8964, loss=0.5959]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▋      | 106/292 [00:17<00:29,  6.28batch/s, auc=0.8955, loss=1.0293]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 107/292 [00:17<00:29,  6.28batch/s, auc=0.8955, loss=1.0293]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 107/292 [00:17<00:29,  6.28batch/s, auc=0.8953, loss=0.9409]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 108/292 [00:17<00:29,  6.28batch/s, auc=0.8953, loss=0.9409]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 108/292 [00:18<00:29,  6.28batch/s, auc=0.8955, loss=0.5580]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 109/292 [00:18<00:29,  6.28batch/s, auc=0.8955, loss=0.5580]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 109/292 [00:18<00:29,  6.28batch/s, auc=0.8957, loss=0.6270]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 110/292 [00:18<00:28,  6.28batch/s, auc=0.8957, loss=0.6270]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 110/292 [00:18<00:28,  6.28batch/s, auc=0.8951, loss=0.9649]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 111/292 [00:18<00:28,  6.26batch/s, auc=0.8951, loss=0.9649]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 111/292 [00:18<00:28,  6.26batch/s, auc=0.8947, loss=0.9309]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 112/292 [00:18<00:28,  6.25batch/s, auc=0.8947, loss=0.9309]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 112/292 [00:18<00:28,  6.25batch/s, auc=0.8949, loss=0.6124]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▊      | 113/292 [00:18<00:28,  6.24batch/s, auc=0.8949, loss=0.6124]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▊      | 113/292 [00:18<00:28,  6.24batch/s, auc=0.8950, loss=0.6662]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 114/292 [00:18<00:28,  6.25batch/s, auc=0.8950, loss=0.6662]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 114/292 [00:19<00:28,  6.25batch/s, auc=0.8948, loss=0.6030]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 115/292 [00:19<00:28,  6.26batch/s, auc=0.8948, loss=0.6030]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 115/292 [00:19<00:28,  6.26batch/s, auc=0.8951, loss=0.7095]\u001b[A\n",
      "Training Epoch 5/25:  40%|███▉      | 116/292 [00:19<00:28,  6.27batch/s, auc=0.8951, loss=0.7095]\u001b[A\n",
      "Training Epoch 5/25:  40%|███▉      | 116/292 [00:19<00:28,  6.27batch/s, auc=0.8956, loss=0.5645]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 117/292 [00:19<00:27,  6.27batch/s, auc=0.8956, loss=0.5645]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 117/292 [00:19<00:27,  6.27batch/s, auc=0.8954, loss=0.7792]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 118/292 [00:19<00:27,  6.25batch/s, auc=0.8954, loss=0.7792]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 118/292 [00:19<00:27,  6.25batch/s, auc=0.8958, loss=0.5168]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 119/292 [00:19<00:27,  6.26batch/s, auc=0.8958, loss=0.5168]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 119/292 [00:19<00:27,  6.26batch/s, auc=0.8959, loss=0.7486]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 120/292 [00:19<00:27,  6.26batch/s, auc=0.8959, loss=0.7486]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 120/292 [00:20<00:27,  6.26batch/s, auc=0.8959, loss=0.6723]\u001b[A\n",
      "Training Epoch 5/25:  41%|████▏     | 121/292 [00:20<00:27,  6.26batch/s, auc=0.8959, loss=0.6723]\u001b[A\n",
      "Training Epoch 5/25:  41%|████▏     | 121/292 [00:20<00:27,  6.26batch/s, auc=0.8957, loss=0.8481]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 122/292 [00:20<00:27,  6.27batch/s, auc=0.8957, loss=0.8481]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 122/292 [00:20<00:27,  6.27batch/s, auc=0.8957, loss=0.6786]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 123/292 [00:20<00:26,  6.27batch/s, auc=0.8957, loss=0.6786]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 123/292 [00:20<00:26,  6.27batch/s, auc=0.8962, loss=0.6723]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8962, loss=0.6723]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 124/292 [00:20<00:26,  6.27batch/s, auc=0.8960, loss=0.5868]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.8960, loss=0.5868]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 125/292 [00:20<00:26,  6.27batch/s, auc=0.8955, loss=0.8449]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 126/292 [00:20<00:26,  6.27batch/s, auc=0.8955, loss=0.8449]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 126/292 [00:20<00:26,  6.27batch/s, auc=0.8952, loss=0.8445]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 127/292 [00:20<00:26,  6.27batch/s, auc=0.8952, loss=0.8445]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 127/292 [00:21<00:26,  6.27batch/s, auc=0.8946, loss=0.7249]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8946, loss=0.7249]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 128/292 [00:21<00:26,  6.27batch/s, auc=0.8943, loss=0.7782]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 129/292 [00:21<00:26,  6.26batch/s, auc=0.8943, loss=0.7782]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 129/292 [00:21<00:26,  6.26batch/s, auc=0.8942, loss=0.7201]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 130/292 [00:21<00:25,  6.26batch/s, auc=0.8942, loss=0.7201]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 130/292 [00:21<00:25,  6.26batch/s, auc=0.8944, loss=0.5270]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 131/292 [00:21<00:25,  6.26batch/s, auc=0.8944, loss=0.5270]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 131/292 [00:21<00:25,  6.26batch/s, auc=0.8946, loss=0.6209]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▌     | 132/292 [00:21<00:25,  6.26batch/s, auc=0.8946, loss=0.6209]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▌     | 132/292 [00:21<00:25,  6.26batch/s, auc=0.8943, loss=0.7475]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 133/292 [00:21<00:25,  6.26batch/s, auc=0.8943, loss=0.7475]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 133/292 [00:22<00:25,  6.26batch/s, auc=0.8947, loss=0.4802]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 134/292 [00:22<00:25,  6.25batch/s, auc=0.8947, loss=0.4802]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 134/292 [00:22<00:25,  6.25batch/s, auc=0.8951, loss=0.4513]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.8951, loss=0.4513]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 135/292 [00:22<00:25,  6.26batch/s, auc=0.8951, loss=0.6974]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8951, loss=0.6974]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 136/292 [00:22<00:24,  6.26batch/s, auc=0.8954, loss=0.5742]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 137/292 [00:22<00:24,  6.25batch/s, auc=0.8954, loss=0.5742]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 137/292 [00:22<00:24,  6.25batch/s, auc=0.8955, loss=0.6978]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 138/292 [00:22<00:24,  6.25batch/s, auc=0.8955, loss=0.6978]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 138/292 [00:22<00:24,  6.25batch/s, auc=0.8955, loss=0.7075]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 139/292 [00:22<00:24,  6.25batch/s, auc=0.8955, loss=0.7075]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 139/292 [00:23<00:24,  6.25batch/s, auc=0.8951, loss=0.9254]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 140/292 [00:23<00:24,  6.25batch/s, auc=0.8951, loss=0.9254]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 140/292 [00:23<00:24,  6.25batch/s, auc=0.8955, loss=0.4920]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 141/292 [00:23<00:24,  6.25batch/s, auc=0.8955, loss=0.4920]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 141/292 [00:23<00:24,  6.25batch/s, auc=0.8955, loss=0.7258]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▊     | 142/292 [00:23<00:24,  6.25batch/s, auc=0.8955, loss=0.7258]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▊     | 142/292 [00:23<00:24,  6.25batch/s, auc=0.8956, loss=0.7643]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.8956, loss=0.7643]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 143/292 [00:23<00:23,  6.24batch/s, auc=0.8958, loss=0.5663]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 144/292 [00:23<00:23,  6.24batch/s, auc=0.8958, loss=0.5663]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 144/292 [00:23<00:23,  6.24batch/s, auc=0.8960, loss=0.6948]\u001b[A\n",
      "Training Epoch 5/25:  50%|████▉     | 145/292 [00:23<00:23,  6.23batch/s, auc=0.8960, loss=0.6948]\u001b[A\n",
      "Training Epoch 5/25:  50%|████▉     | 145/292 [00:24<00:23,  6.23batch/s, auc=0.8959, loss=0.7430]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 146/292 [00:24<00:23,  6.23batch/s, auc=0.8959, loss=0.7430]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 146/292 [00:24<00:23,  6.23batch/s, auc=0.8959, loss=0.6976]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 147/292 [00:24<00:23,  6.24batch/s, auc=0.8959, loss=0.6976]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 147/292 [00:24<00:23,  6.24batch/s, auc=0.8962, loss=0.6456]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 148/292 [00:24<00:23,  6.24batch/s, auc=0.8962, loss=0.6456]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 148/292 [00:24<00:23,  6.24batch/s, auc=0.8961, loss=1.0264]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 149/292 [00:24<00:22,  6.24batch/s, auc=0.8961, loss=1.0264]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 149/292 [00:24<00:22,  6.24batch/s, auc=0.8964, loss=0.4648]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.23batch/s, auc=0.8964, loss=0.4648]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.23batch/s, auc=0.8972, loss=0.4007]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.23batch/s, auc=0.8972, loss=0.4007]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.23batch/s, auc=0.8977, loss=0.5060]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 152/292 [00:24<00:22,  6.23batch/s, auc=0.8977, loss=0.5060]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.23batch/s, auc=0.8978, loss=0.4574]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8978, loss=0.4574]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.23batch/s, auc=0.8982, loss=0.5890]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.23batch/s, auc=0.8982, loss=0.5890]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.23batch/s, auc=0.8981, loss=0.6728]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.23batch/s, auc=0.8981, loss=0.6728]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.23batch/s, auc=0.8980, loss=0.6125]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.21batch/s, auc=0.8980, loss=0.6125]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 156/292 [00:25<00:21,  6.21batch/s, auc=0.8983, loss=0.5331]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.21batch/s, auc=0.8983, loss=0.5331]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.21batch/s, auc=0.8985, loss=0.4839]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 158/292 [00:25<00:21,  6.21batch/s, auc=0.8985, loss=0.4839]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.21batch/s, auc=0.8988, loss=0.5370]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.21batch/s, auc=0.8988, loss=0.5370]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.21batch/s, auc=0.8987, loss=0.7767]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8987, loss=0.7767]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.21batch/s, auc=0.8987, loss=0.8097]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.21batch/s, auc=0.8987, loss=0.8097]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.21batch/s, auc=0.8988, loss=0.6322]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.21batch/s, auc=0.8988, loss=0.6322]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 162/292 [00:26<00:20,  6.21batch/s, auc=0.8991, loss=0.5308]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.20batch/s, auc=0.8991, loss=0.5308]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.20batch/s, auc=0.8993, loss=0.6185]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 164/292 [00:26<00:20,  6.20batch/s, auc=0.8993, loss=0.6185]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.20batch/s, auc=0.8996, loss=0.4799]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.21batch/s, auc=0.8996, loss=0.4799]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.21batch/s, auc=0.8995, loss=0.8098]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.18batch/s, auc=0.8995, loss=0.8098]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.18batch/s, auc=0.8988, loss=1.2101]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.17batch/s, auc=0.8988, loss=1.2101]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.17batch/s, auc=0.8988, loss=0.6264]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.17batch/s, auc=0.8988, loss=0.6264]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.17batch/s, auc=0.8978, loss=1.4804]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.17batch/s, auc=0.8978, loss=1.4804]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 169/292 [00:27<00:19,  6.17batch/s, auc=0.8978, loss=0.7575]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 170/292 [00:27<00:19,  6.18batch/s, auc=0.8978, loss=0.7575]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.18batch/s, auc=0.8979, loss=0.6172]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.18batch/s, auc=0.8979, loss=0.6172]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.18batch/s, auc=0.8977, loss=1.1135]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.18batch/s, auc=0.8977, loss=1.1135]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.18batch/s, auc=0.8982, loss=0.5429]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.17batch/s, auc=0.8982, loss=0.5429]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.17batch/s, auc=0.8981, loss=0.7209]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.18batch/s, auc=0.8981, loss=0.7209]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.18batch/s, auc=0.8983, loss=0.6120]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.18batch/s, auc=0.8983, loss=0.6120]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 175/292 [00:28<00:18,  6.18batch/s, auc=0.8983, loss=0.5744]\u001b[A\n",
      "Training Epoch 5/25:  60%|██████    | 176/292 [00:28<00:18,  6.18batch/s, auc=0.8983, loss=0.5744]\u001b[A\n",
      "Training Epoch 5/25:  60%|██████    | 176/292 [00:29<00:18,  6.18batch/s, auc=0.8986, loss=0.5236]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 177/292 [00:29<00:18,  6.19batch/s, auc=0.8986, loss=0.5236]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 177/292 [00:29<00:18,  6.19batch/s, auc=0.8980, loss=1.1010]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 178/292 [00:29<00:18,  6.17batch/s, auc=0.8980, loss=1.1010]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 178/292 [00:29<00:18,  6.17batch/s, auc=0.8982, loss=0.6082]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.15batch/s, auc=0.8982, loss=0.6082]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.15batch/s, auc=0.8983, loss=0.6614]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.16batch/s, auc=0.8983, loss=0.6614]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.16batch/s, auc=0.8981, loss=0.6612]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.17batch/s, auc=0.8981, loss=0.6612]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 181/292 [00:29<00:17,  6.17batch/s, auc=0.8985, loss=0.4982]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 182/292 [00:29<00:17,  6.16batch/s, auc=0.8985, loss=0.4982]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.16batch/s, auc=0.8987, loss=0.6238]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.16batch/s, auc=0.8987, loss=0.6238]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.16batch/s, auc=0.8986, loss=0.7412]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.16batch/s, auc=0.8986, loss=0.7412]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.16batch/s, auc=0.8987, loss=0.5130]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.16batch/s, auc=0.8987, loss=0.5130]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.16batch/s, auc=0.8989, loss=0.4861]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.15batch/s, auc=0.8989, loss=0.4861]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.15batch/s, auc=0.8986, loss=0.7361]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.15batch/s, auc=0.8986, loss=0.7361]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.15batch/s, auc=0.8984, loss=0.8600]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 188/292 [00:30<00:16,  6.16batch/s, auc=0.8984, loss=0.8600]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 188/292 [00:30<00:16,  6.16batch/s, auc=0.8981, loss=0.8660]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▍   | 189/292 [00:30<00:16,  6.15batch/s, auc=0.8981, loss=0.8660]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.15batch/s, auc=0.8980, loss=0.7305]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.95batch/s, auc=0.8980, loss=0.7305]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.95batch/s, auc=0.8982, loss=0.5976]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.89batch/s, auc=0.8982, loss=0.5976]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.89batch/s, auc=0.8982, loss=0.5465]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.84batch/s, auc=0.8982, loss=0.5465]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.84batch/s, auc=0.8984, loss=0.7602]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 193/292 [00:31<00:17,  5.82batch/s, auc=0.8984, loss=0.7602]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 193/292 [00:31<00:17,  5.82batch/s, auc=0.8987, loss=0.4713]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▋   | 194/292 [00:31<00:16,  5.82batch/s, auc=0.8987, loss=0.4713]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.82batch/s, auc=0.8985, loss=0.9167]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.76batch/s, auc=0.8985, loss=0.9167]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.76batch/s, auc=0.8988, loss=0.3989]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.73batch/s, auc=0.8988, loss=0.3989]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.73batch/s, auc=0.8991, loss=0.4970]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.84batch/s, auc=0.8991, loss=0.4970]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.84batch/s, auc=0.8994, loss=0.4579]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.71batch/s, auc=0.8994, loss=0.4579]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.71batch/s, auc=0.8994, loss=0.6015]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 199/292 [00:32<00:16,  5.72batch/s, auc=0.8994, loss=0.6015]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 199/292 [00:32<00:16,  5.72batch/s, auc=0.8993, loss=0.7867]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 200/292 [00:32<00:16,  5.74batch/s, auc=0.8993, loss=0.7867]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.74batch/s, auc=0.8996, loss=0.5975]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.76batch/s, auc=0.8996, loss=0.5975]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.76batch/s, auc=0.8994, loss=0.8884]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.75batch/s, auc=0.8994, loss=0.8884]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.75batch/s, auc=0.8996, loss=0.6342]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.74batch/s, auc=0.8996, loss=0.6342]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.74batch/s, auc=0.8997, loss=0.6535]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.73batch/s, auc=0.8997, loss=0.6535]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.73batch/s, auc=0.8994, loss=0.9791]\u001b[A\n",
      "Training Epoch 5/25:  70%|███████   | 205/292 [00:33<00:15,  5.73batch/s, auc=0.8994, loss=0.9791]\u001b[A\n",
      "Training Epoch 5/25:  70%|███████   | 205/292 [00:33<00:15,  5.73batch/s, auc=0.8996, loss=0.4830]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 206/292 [00:33<00:14,  5.78batch/s, auc=0.8996, loss=0.4830]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 206/292 [00:34<00:14,  5.78batch/s, auc=0.8999, loss=0.5267]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 207/292 [00:34<00:14,  5.76batch/s, auc=0.8999, loss=0.5267]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 207/292 [00:34<00:14,  5.76batch/s, auc=0.8996, loss=0.8301]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 208/292 [00:34<00:14,  5.74batch/s, auc=0.8996, loss=0.8301]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 208/292 [00:34<00:14,  5.74batch/s, auc=0.8997, loss=0.7066]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.72batch/s, auc=0.8997, loss=0.7066]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.72batch/s, auc=0.8995, loss=0.8024]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.77batch/s, auc=0.8995, loss=0.8024]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.77batch/s, auc=0.8997, loss=0.4465]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 211/292 [00:34<00:13,  5.87batch/s, auc=0.8997, loss=0.4465]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 211/292 [00:34<00:13,  5.87batch/s, auc=0.8992, loss=0.9104]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 212/292 [00:34<00:13,  5.94batch/s, auc=0.8992, loss=0.9104]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.94batch/s, auc=0.8992, loss=0.6073]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.88batch/s, auc=0.8992, loss=0.6073]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.88batch/s, auc=0.8991, loss=0.6284]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.83batch/s, auc=0.8991, loss=0.6284]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.83batch/s, auc=0.8995, loss=0.5238]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.8995, loss=0.5238]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.8995, loss=0.5352]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.77batch/s, auc=0.8995, loss=0.5352]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.77batch/s, auc=0.8996, loss=0.5639]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.77batch/s, auc=0.8996, loss=0.5639]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.77batch/s, auc=0.8992, loss=0.8218]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.77batch/s, auc=0.8992, loss=0.8218]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.77batch/s, auc=0.8992, loss=0.6169]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.75batch/s, auc=0.8992, loss=0.6169]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.75batch/s, auc=0.8994, loss=0.5331]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.8994, loss=0.5331]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.8991, loss=0.8654]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.78batch/s, auc=0.8991, loss=0.8654]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.78batch/s, auc=0.8993, loss=0.5998]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.76batch/s, auc=0.8993, loss=0.5998]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.76batch/s, auc=0.8990, loss=0.8760]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▋  | 223/292 [00:36<00:12,  5.75batch/s, auc=0.8990, loss=0.8760]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.75batch/s, auc=0.8988, loss=1.0063]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.74batch/s, auc=0.8988, loss=1.0063]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.74batch/s, auc=0.8990, loss=0.7612]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.73batch/s, auc=0.8990, loss=0.7612]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.73batch/s, auc=0.8994, loss=0.4753]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.73batch/s, auc=0.8994, loss=0.4753]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.73batch/s, auc=0.8996, loss=0.6443]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.73batch/s, auc=0.8996, loss=0.6443]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.73batch/s, auc=0.8997, loss=0.5914]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 228/292 [00:37<00:10,  5.85batch/s, auc=0.8997, loss=0.5914]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 228/292 [00:37<00:10,  5.85batch/s, auc=0.8996, loss=0.8093]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 229/292 [00:37<00:10,  5.79batch/s, auc=0.8996, loss=0.8093]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.79batch/s, auc=0.8999, loss=0.4646]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.76batch/s, auc=0.8999, loss=0.4646]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.76batch/s, auc=0.9001, loss=0.5016]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.73batch/s, auc=0.9001, loss=0.5016]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.73batch/s, auc=0.8997, loss=1.0084]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.77batch/s, auc=0.8997, loss=1.0084]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.77batch/s, auc=0.9000, loss=0.4562]\u001b[A\n",
      "Training Epoch 5/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.75batch/s, auc=0.9000, loss=0.4562]\u001b[A\n",
      "Training Epoch 5/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.75batch/s, auc=0.8996, loss=1.0167]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 234/292 [00:38<00:10,  5.74batch/s, auc=0.8996, loss=1.0167]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 234/292 [00:38<00:10,  5.74batch/s, auc=0.8998, loss=0.6061]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 235/292 [00:38<00:09,  5.74batch/s, auc=0.8998, loss=0.6061]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 235/292 [00:39<00:09,  5.74batch/s, auc=0.8999, loss=0.5477]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 236/292 [00:39<00:09,  5.82batch/s, auc=0.8999, loss=0.5477]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 236/292 [00:39<00:09,  5.82batch/s, auc=0.8997, loss=0.8624]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 237/292 [00:39<00:09,  5.73batch/s, auc=0.8997, loss=0.8624]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 237/292 [00:39<00:09,  5.73batch/s, auc=0.8997, loss=0.6356]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.73batch/s, auc=0.8997, loss=0.6356]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.73batch/s, auc=0.8998, loss=0.6010]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.71batch/s, auc=0.8998, loss=0.6010]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.71batch/s, auc=0.8998, loss=0.7487]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.70batch/s, auc=0.8998, loss=0.7487]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.70batch/s, auc=0.8999, loss=0.5695]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.76batch/s, auc=0.8999, loss=0.5695]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.76batch/s, auc=0.9002, loss=0.5023]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9002, loss=0.5023]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9001, loss=0.8113]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.73batch/s, auc=0.9001, loss=0.8113]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.73batch/s, auc=0.8997, loss=0.9188]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.72batch/s, auc=0.8997, loss=0.9188]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.72batch/s, auc=0.9000, loss=0.4772]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.75batch/s, auc=0.9000, loss=0.4772]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.75batch/s, auc=0.8998, loss=0.7026]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.73batch/s, auc=0.8998, loss=0.7026]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.73batch/s, auc=0.8996, loss=0.7883]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.72batch/s, auc=0.8996, loss=0.7883]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.72batch/s, auc=0.8999, loss=0.5027]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.69batch/s, auc=0.8999, loss=0.5027]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.69batch/s, auc=0.8998, loss=0.6149]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.79batch/s, auc=0.8998, loss=0.6149]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.79batch/s, auc=0.8996, loss=1.1080]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.76batch/s, auc=0.8996, loss=1.1080]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.76batch/s, auc=0.8994, loss=0.7203]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.85batch/s, auc=0.8994, loss=0.7203]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.85batch/s, auc=0.8993, loss=0.7523]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▋ | 252/292 [00:41<00:06,  5.75batch/s, auc=0.8993, loss=0.7523]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.75batch/s, auc=0.8992, loss=0.6966]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.72batch/s, auc=0.8992, loss=0.6966]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.72batch/s, auc=0.8995, loss=0.7033]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.71batch/s, auc=0.8995, loss=0.7033]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.71batch/s, auc=0.8997, loss=0.4506]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.71batch/s, auc=0.8997, loss=0.4506]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.71batch/s, auc=0.8992, loss=1.0634]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.82batch/s, auc=0.8992, loss=1.0634]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.82batch/s, auc=0.8994, loss=0.5013]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.72batch/s, auc=0.8994, loss=0.5013]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.72batch/s, auc=0.8995, loss=0.7602]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 258/292 [00:42<00:05,  5.71batch/s, auc=0.8995, loss=0.7602]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.71batch/s, auc=0.8995, loss=0.5774]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.70batch/s, auc=0.8995, loss=0.5774]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.70batch/s, auc=0.8993, loss=1.0879]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.70batch/s, auc=0.8993, loss=1.0879]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.70batch/s, auc=0.8995, loss=0.5206]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.71batch/s, auc=0.8995, loss=0.5206]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.71batch/s, auc=0.8994, loss=0.7397]\u001b[A\n",
      "Training Epoch 5/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.71batch/s, auc=0.8994, loss=0.7397]\u001b[A\n",
      "Training Epoch 5/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.71batch/s, auc=0.8996, loss=0.6039]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 263/292 [00:43<00:05,  5.71batch/s, auc=0.8996, loss=0.6039]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 263/292 [00:44<00:05,  5.71batch/s, auc=0.8993, loss=1.0469]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.8993, loss=1.0469]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.8992, loss=0.7623]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 265/292 [00:44<00:04,  5.69batch/s, auc=0.8992, loss=0.7623]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 265/292 [00:44<00:04,  5.69batch/s, auc=0.8990, loss=0.7136]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 266/292 [00:44<00:04,  5.68batch/s, auc=0.8990, loss=0.7136]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 266/292 [00:44<00:04,  5.68batch/s, auc=0.8993, loss=0.4546]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.67batch/s, auc=0.8993, loss=0.4546]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.67batch/s, auc=0.8991, loss=0.7925]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8991, loss=0.7925]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.67batch/s, auc=0.8994, loss=0.5628]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.68batch/s, auc=0.8994, loss=0.5628]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.68batch/s, auc=0.8992, loss=0.7356]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.67batch/s, auc=0.8992, loss=0.7356]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.67batch/s, auc=0.8991, loss=0.7202]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.55batch/s, auc=0.8991, loss=0.7202]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.55batch/s, auc=0.8992, loss=0.7716]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.48batch/s, auc=0.8992, loss=0.7716]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.48batch/s, auc=0.8990, loss=1.1237]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.42batch/s, auc=0.8990, loss=1.1237]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.42batch/s, auc=0.8991, loss=0.4421]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.47batch/s, auc=0.8991, loss=0.4421]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.47batch/s, auc=0.8992, loss=0.6413]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.48batch/s, auc=0.8992, loss=0.6413]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.48batch/s, auc=0.8993, loss=0.5888]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.50batch/s, auc=0.8993, loss=0.5888]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.50batch/s, auc=0.8993, loss=0.6721]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.53batch/s, auc=0.8993, loss=0.6721]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.53batch/s, auc=0.8993, loss=0.6623]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.56batch/s, auc=0.8993, loss=0.6623]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.56batch/s, auc=0.8993, loss=0.7241]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.57batch/s, auc=0.8993, loss=0.7241]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.57batch/s, auc=0.8995, loss=0.5114]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.59batch/s, auc=0.8995, loss=0.5114]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.59batch/s, auc=0.8994, loss=0.8152]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.60batch/s, auc=0.8994, loss=0.8152]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.60batch/s, auc=0.8996, loss=0.5013]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.65batch/s, auc=0.8996, loss=0.5013]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.65batch/s, auc=0.8995, loss=0.6411]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.8995, loss=0.6411]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.8994, loss=0.9113]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.73batch/s, auc=0.8994, loss=0.9113]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.73batch/s, auc=0.8995, loss=0.5241]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.69batch/s, auc=0.8995, loss=0.5241]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.69batch/s, auc=0.8994, loss=0.8665]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.77batch/s, auc=0.8994, loss=0.8665]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.77batch/s, auc=0.8996, loss=0.4656]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.8996, loss=0.4656]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.8997, loss=0.6508]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.87batch/s, auc=0.8997, loss=0.6508]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.87batch/s, auc=0.8997, loss=0.5443]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.90batch/s, auc=0.8997, loss=0.5443]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.90batch/s, auc=0.8999, loss=0.5311]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.92batch/s, auc=0.8999, loss=0.5311]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.92batch/s, auc=0.9001, loss=0.4961]\u001b[A\n",
      "Training Epoch 5/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.93batch/s, auc=0.9001, loss=0.4961]\u001b[A\n",
      "Training Epoch 5/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.93batch/s, auc=0.9002, loss=0.5765]\u001b[A\n",
      "Training Epoch 5/25: 100%|██████████| 292/292 [00:49<00:00,  5.96batch/s, auc=0.9002, loss=0.5765]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25] Train Loss: 0.6827 | Train AUROC: 0.9002 Val Loss: 0.6997 | Val AUROC: 0.8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 5/25 [04:33<18:12, 54.60s/it]\n",
      "Training Epoch 6/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.7989, loss=1.0317]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 1/292 [00:00<04:38,  1.04batch/s, auc=0.7989, loss=1.0317]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 1/292 [00:01<04:38,  1.04batch/s, auc=0.8702, loss=0.5063]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 2/292 [00:01<02:21,  2.05batch/s, auc=0.8702, loss=0.5063]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 2/292 [00:01<02:21,  2.05batch/s, auc=0.8777, loss=0.7324]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 3/292 [00:01<01:40,  2.87batch/s, auc=0.8777, loss=0.7324]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 3/292 [00:01<01:40,  2.87batch/s, auc=0.8774, loss=0.6850]\u001b[A\n",
      "Training Epoch 6/25:   1%|▏         | 4/292 [00:01<01:19,  3.65batch/s, auc=0.8774, loss=0.6850]\u001b[A\n",
      "Training Epoch 6/25:   1%|▏         | 4/292 [00:01<01:19,  3.65batch/s, auc=0.8924, loss=0.4907]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 5/292 [00:01<01:06,  4.31batch/s, auc=0.8924, loss=0.4907]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 5/292 [00:01<01:06,  4.31batch/s, auc=0.8650, loss=1.2770]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 6/292 [00:01<00:58,  4.85batch/s, auc=0.8650, loss=1.2770]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 6/292 [00:01<00:58,  4.85batch/s, auc=0.8665, loss=0.8259]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 7/292 [00:01<00:55,  5.16batch/s, auc=0.8665, loss=0.8259]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 7/292 [00:02<00:55,  5.16batch/s, auc=0.8699, loss=0.7336]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 8/292 [00:02<00:51,  5.50batch/s, auc=0.8699, loss=0.7336]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 8/292 [00:02<00:51,  5.50batch/s, auc=0.8797, loss=0.5142]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 9/292 [00:02<00:50,  5.61batch/s, auc=0.8797, loss=0.5142]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 9/292 [00:02<00:50,  5.61batch/s, auc=0.8850, loss=0.6038]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 10/292 [00:02<00:48,  5.83batch/s, auc=0.8850, loss=0.6038]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 10/292 [00:02<00:48,  5.83batch/s, auc=0.8883, loss=0.5860]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 11/292 [00:02<00:46,  5.99batch/s, auc=0.8883, loss=0.5860]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 11/292 [00:02<00:46,  5.99batch/s, auc=0.8891, loss=0.7082]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 12/292 [00:02<00:45,  6.11batch/s, auc=0.8891, loss=0.7082]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 12/292 [00:02<00:45,  6.11batch/s, auc=0.8881, loss=0.6179]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 13/292 [00:02<00:45,  6.19batch/s, auc=0.8881, loss=0.6179]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 13/292 [00:03<00:45,  6.19batch/s, auc=0.8894, loss=0.9573]\u001b[A\n",
      "Training Epoch 6/25:   5%|▍         | 14/292 [00:03<00:44,  6.25batch/s, auc=0.8894, loss=0.9573]\u001b[A\n",
      "Training Epoch 6/25:   5%|▍         | 14/292 [00:03<00:44,  6.25batch/s, auc=0.8900, loss=0.5820]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 15/292 [00:03<00:44,  6.28batch/s, auc=0.8900, loss=0.5820]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 15/292 [00:03<00:44,  6.28batch/s, auc=0.8928, loss=0.8513]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 16/292 [00:03<00:43,  6.28batch/s, auc=0.8928, loss=0.8513]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 16/292 [00:03<00:43,  6.28batch/s, auc=0.8914, loss=0.7535]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.8914, loss=0.7535]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.8940, loss=0.5244]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.8940, loss=0.5244]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.8985, loss=0.4170]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 19/292 [00:03<00:43,  6.32batch/s, auc=0.8985, loss=0.4170]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 19/292 [00:03<00:43,  6.32batch/s, auc=0.9005, loss=0.4952]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 20/292 [00:03<00:42,  6.33batch/s, auc=0.9005, loss=0.4952]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 20/292 [00:04<00:42,  6.33batch/s, auc=0.9023, loss=0.4989]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 21/292 [00:04<00:42,  6.34batch/s, auc=0.9023, loss=0.4989]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 21/292 [00:04<00:42,  6.34batch/s, auc=0.9025, loss=0.5152]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.9025, loss=0.5152]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 22/292 [00:04<00:42,  6.35batch/s, auc=0.9018, loss=0.8152]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.9018, loss=0.8152]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.9036, loss=0.4718]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 24/292 [00:04<00:42,  6.36batch/s, auc=0.9036, loss=0.4718]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 24/292 [00:04<00:42,  6.36batch/s, auc=0.9059, loss=0.5076]\u001b[A\n",
      "Training Epoch 6/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9059, loss=0.5076]\u001b[A\n",
      "Training Epoch 6/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9085, loss=0.3908]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 26/292 [00:04<00:42,  6.32batch/s, auc=0.9085, loss=0.3908]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 26/292 [00:05<00:42,  6.32batch/s, auc=0.9099, loss=0.5764]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.9099, loss=0.5764]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.9062, loss=1.0651]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9062, loss=1.0651]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9073, loss=0.4917]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 29/292 [00:05<00:41,  6.35batch/s, auc=0.9073, loss=0.4917]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 29/292 [00:05<00:41,  6.35batch/s, auc=0.9085, loss=0.4778]\u001b[A\n",
      "Training Epoch 6/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.9085, loss=0.4778]\u001b[A\n",
      "Training Epoch 6/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.9096, loss=0.4930]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.9096, loss=0.4930]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 31/292 [00:05<00:41,  6.32batch/s, auc=0.9103, loss=0.5101]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 32/292 [00:05<00:41,  6.28batch/s, auc=0.9103, loss=0.5101]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 32/292 [00:06<00:41,  6.28batch/s, auc=0.9101, loss=0.6455]\u001b[A\n",
      "Training Epoch 6/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9101, loss=0.6455]\u001b[A\n",
      "Training Epoch 6/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9067, loss=1.1980]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9067, loss=1.1980]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9072, loss=0.6946]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 35/292 [00:06<00:40,  6.29batch/s, auc=0.9072, loss=0.6946]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 35/292 [00:06<00:40,  6.29batch/s, auc=0.9080, loss=0.5510]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9080, loss=0.5510]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9073, loss=0.7189]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 37/292 [00:06<00:40,  6.27batch/s, auc=0.9073, loss=0.7189]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 37/292 [00:06<00:40,  6.27batch/s, auc=0.9071, loss=0.5772]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 38/292 [00:06<00:40,  6.24batch/s, auc=0.9071, loss=0.5772]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 38/292 [00:07<00:40,  6.24batch/s, auc=0.9063, loss=0.7797]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 39/292 [00:07<00:40,  6.26batch/s, auc=0.9063, loss=0.7797]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 39/292 [00:07<00:40,  6.26batch/s, auc=0.9076, loss=0.5481]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9076, loss=0.5481]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9090, loss=0.5905]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9090, loss=0.5905]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9103, loss=0.3969]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9103, loss=0.3969]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9097, loss=0.5448]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▍        | 43/292 [00:07<00:39,  6.27batch/s, auc=0.9097, loss=0.5448]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▍        | 43/292 [00:07<00:39,  6.27batch/s, auc=0.9094, loss=0.5322]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9094, loss=0.5322]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9088, loss=0.8851]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 45/292 [00:07<00:39,  6.30batch/s, auc=0.9088, loss=0.8851]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9080, loss=0.6672]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9080, loss=0.6672]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9080, loss=0.6029]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9080, loss=0.6029]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9082, loss=0.4973]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▋        | 48/292 [00:08<00:38,  6.33batch/s, auc=0.9082, loss=0.4973]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▋        | 48/292 [00:08<00:38,  6.33batch/s, auc=0.9078, loss=0.7853]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 49/292 [00:08<00:38,  6.27batch/s, auc=0.9078, loss=0.7853]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 49/292 [00:08<00:38,  6.27batch/s, auc=0.9051, loss=1.3127]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9051, loss=1.3127]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9045, loss=0.6812]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 51/292 [00:08<00:38,  6.31batch/s, auc=0.9045, loss=0.6812]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 51/292 [00:09<00:38,  6.31batch/s, auc=0.9065, loss=0.4194]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9065, loss=0.4194]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9078, loss=0.4710]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.9078, loss=0.4710]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.9062, loss=1.2106]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9062, loss=1.2106]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9059, loss=0.7657]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9059, loss=0.7657]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9066, loss=0.4399]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9066, loss=0.4399]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9066, loss=0.7210]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 57/292 [00:09<00:37,  6.29batch/s, auc=0.9066, loss=0.7210]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 57/292 [00:10<00:37,  6.29batch/s, auc=0.9054, loss=1.1848]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9054, loss=1.1848]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9052, loss=0.5845]\u001b[A\n",
      "Training Epoch 6/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9052, loss=0.5845]\u001b[A\n",
      "Training Epoch 6/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9063, loss=0.4653]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 60/292 [00:10<00:36,  6.31batch/s, auc=0.9063, loss=0.4653]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 60/292 [00:10<00:36,  6.31batch/s, auc=0.9080, loss=0.3341]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9080, loss=0.3341]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9078, loss=0.7613]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 62/292 [00:10<00:36,  6.32batch/s, auc=0.9078, loss=0.7613]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 62/292 [00:10<00:36,  6.32batch/s, auc=0.9083, loss=0.4551]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 63/292 [00:10<00:36,  6.31batch/s, auc=0.9083, loss=0.4551]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 63/292 [00:10<00:36,  6.31batch/s, auc=0.9089, loss=0.4610]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 64/292 [00:10<00:36,  6.28batch/s, auc=0.9089, loss=0.4610]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9071, loss=1.2723]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9071, loss=1.2723]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9076, loss=0.5037]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 66/292 [00:11<00:35,  6.29batch/s, auc=0.9076, loss=0.5037]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 66/292 [00:11<00:35,  6.29batch/s, auc=0.9079, loss=0.5022]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 67/292 [00:11<00:35,  6.30batch/s, auc=0.9079, loss=0.5022]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 67/292 [00:11<00:35,  6.30batch/s, auc=0.9073, loss=0.6607]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 68/292 [00:11<00:35,  6.30batch/s, auc=0.9073, loss=0.6607]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 68/292 [00:11<00:35,  6.30batch/s, auc=0.9061, loss=0.9475]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▎       | 69/292 [00:11<00:35,  6.23batch/s, auc=0.9061, loss=0.9475]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▎       | 69/292 [00:11<00:35,  6.23batch/s, auc=0.9055, loss=0.9875]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 70/292 [00:11<00:35,  6.26batch/s, auc=0.9055, loss=0.9875]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9054, loss=0.5113]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9054, loss=0.5113]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9061, loss=0.5651]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9061, loss=0.5651]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9062, loss=0.5473]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9062, loss=0.5473]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9065, loss=0.5129]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9065, loss=0.5129]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9065, loss=0.5527]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 75/292 [00:12<00:34,  6.29batch/s, auc=0.9065, loss=0.5527]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 75/292 [00:12<00:34,  6.29batch/s, auc=0.9061, loss=0.6679]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 76/292 [00:12<00:34,  6.26batch/s, auc=0.9061, loss=0.6679]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 76/292 [00:13<00:34,  6.26batch/s, auc=0.9055, loss=0.9018]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▋       | 77/292 [00:13<00:34,  6.26batch/s, auc=0.9055, loss=0.9018]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▋       | 77/292 [00:13<00:34,  6.26batch/s, auc=0.9061, loss=0.6344]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9061, loss=0.6344]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9063, loss=0.6303]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 79/292 [00:13<00:33,  6.28batch/s, auc=0.9063, loss=0.6303]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 79/292 [00:13<00:33,  6.28batch/s, auc=0.9062, loss=0.6571]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 80/292 [00:13<00:33,  6.29batch/s, auc=0.9062, loss=0.6571]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 80/292 [00:13<00:33,  6.29batch/s, auc=0.9058, loss=0.7266]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 81/292 [00:13<00:33,  6.28batch/s, auc=0.9058, loss=0.7266]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 81/292 [00:13<00:33,  6.28batch/s, auc=0.9059, loss=0.5717]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 82/292 [00:13<00:33,  6.28batch/s, auc=0.9059, loss=0.5717]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 82/292 [00:14<00:33,  6.28batch/s, auc=0.9060, loss=0.7168]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 83/292 [00:14<00:33,  6.29batch/s, auc=0.9060, loss=0.7168]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 83/292 [00:14<00:33,  6.29batch/s, auc=0.9059, loss=0.6148]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9059, loss=0.6148]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9062, loss=0.5243]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 85/292 [00:14<00:33,  6.27batch/s, auc=0.9062, loss=0.5243]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 85/292 [00:14<00:33,  6.27batch/s, auc=0.9061, loss=0.6603]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 86/292 [00:14<00:32,  6.28batch/s, auc=0.9061, loss=0.6603]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 86/292 [00:14<00:32,  6.28batch/s, auc=0.9061, loss=0.7078]\u001b[A\n",
      "Training Epoch 6/25:  30%|██▉       | 87/292 [00:14<00:32,  6.28batch/s, auc=0.9061, loss=0.7078]\u001b[A\n",
      "Training Epoch 6/25:  30%|██▉       | 87/292 [00:14<00:32,  6.28batch/s, auc=0.9057, loss=0.6443]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 88/292 [00:14<00:32,  6.26batch/s, auc=0.9057, loss=0.6443]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 88/292 [00:14<00:32,  6.26batch/s, auc=0.9058, loss=0.6007]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 89/292 [00:14<00:32,  6.27batch/s, auc=0.9058, loss=0.6007]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 89/292 [00:15<00:32,  6.27batch/s, auc=0.9063, loss=0.5394]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 90/292 [00:15<00:32,  6.27batch/s, auc=0.9063, loss=0.5394]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 90/292 [00:15<00:32,  6.27batch/s, auc=0.9072, loss=0.4699]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 91/292 [00:15<00:32,  6.27batch/s, auc=0.9072, loss=0.4699]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 91/292 [00:15<00:32,  6.27batch/s, auc=0.9075, loss=0.5183]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9075, loss=0.5183]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9079, loss=0.5743]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9079, loss=0.5743]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9081, loss=0.5756]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 94/292 [00:15<00:31,  6.26batch/s, auc=0.9081, loss=0.5756]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 94/292 [00:15<00:31,  6.26batch/s, auc=0.9081, loss=0.6833]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 95/292 [00:15<00:31,  6.26batch/s, auc=0.9081, loss=0.6833]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 95/292 [00:16<00:31,  6.26batch/s, auc=0.9079, loss=0.6185]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 96/292 [00:16<00:31,  6.26batch/s, auc=0.9079, loss=0.6185]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 96/292 [00:16<00:31,  6.26batch/s, auc=0.9080, loss=0.6251]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 97/292 [00:16<00:31,  6.25batch/s, auc=0.9080, loss=0.6251]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 97/292 [00:16<00:31,  6.25batch/s, auc=0.9078, loss=0.6880]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▎      | 98/292 [00:16<00:31,  6.26batch/s, auc=0.9078, loss=0.6880]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▎      | 98/292 [00:16<00:31,  6.26batch/s, auc=0.9076, loss=0.6281]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 99/292 [00:16<00:30,  6.26batch/s, auc=0.9076, loss=0.6281]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 99/292 [00:16<00:30,  6.26batch/s, auc=0.9074, loss=0.7220]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 100/292 [00:16<00:30,  6.23batch/s, auc=0.9074, loss=0.7220]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 100/292 [00:16<00:30,  6.23batch/s, auc=0.9074, loss=0.5532]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 101/292 [00:16<00:30,  6.23batch/s, auc=0.9074, loss=0.5532]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 101/292 [00:17<00:30,  6.23batch/s, auc=0.9078, loss=0.4285]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 102/292 [00:17<00:30,  6.24batch/s, auc=0.9078, loss=0.4285]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 102/292 [00:17<00:30,  6.24batch/s, auc=0.9070, loss=1.0099]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▌      | 103/292 [00:17<00:30,  6.24batch/s, auc=0.9070, loss=1.0099]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▌      | 103/292 [00:17<00:30,  6.24batch/s, auc=0.9070, loss=0.5642]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9070, loss=0.5642]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9066, loss=0.7922]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9066, loss=0.7922]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9052, loss=1.1609]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▋      | 106/292 [00:17<00:29,  6.24batch/s, auc=0.9052, loss=1.1609]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▋      | 106/292 [00:17<00:29,  6.24batch/s, auc=0.9055, loss=0.4866]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 107/292 [00:17<00:29,  6.25batch/s, auc=0.9055, loss=0.4866]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 107/292 [00:18<00:29,  6.25batch/s, auc=0.9059, loss=0.5036]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 108/292 [00:18<00:29,  6.24batch/s, auc=0.9059, loss=0.5036]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 108/292 [00:18<00:29,  6.24batch/s, auc=0.9064, loss=0.3869]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9064, loss=0.3869]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9067, loss=0.5917]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 110/292 [00:18<00:29,  6.24batch/s, auc=0.9067, loss=0.5917]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 110/292 [00:18<00:29,  6.24batch/s, auc=0.9073, loss=0.4265]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 111/292 [00:18<00:28,  6.24batch/s, auc=0.9073, loss=0.4265]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 111/292 [00:18<00:28,  6.24batch/s, auc=0.9071, loss=0.7787]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 112/292 [00:18<00:28,  6.24batch/s, auc=0.9071, loss=0.7787]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 112/292 [00:18<00:28,  6.24batch/s, auc=0.9067, loss=0.8303]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▊      | 113/292 [00:18<00:28,  6.23batch/s, auc=0.9067, loss=0.8303]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▊      | 113/292 [00:18<00:28,  6.23batch/s, auc=0.9071, loss=0.4147]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 114/292 [00:18<00:28,  6.24batch/s, auc=0.9071, loss=0.4147]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 114/292 [00:19<00:28,  6.24batch/s, auc=0.9072, loss=0.5363]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 115/292 [00:19<00:28,  6.24batch/s, auc=0.9072, loss=0.5363]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 115/292 [00:19<00:28,  6.24batch/s, auc=0.9069, loss=0.7737]\u001b[A\n",
      "Training Epoch 6/25:  40%|███▉      | 116/292 [00:19<00:28,  6.24batch/s, auc=0.9069, loss=0.7737]\u001b[A\n",
      "Training Epoch 6/25:  40%|███▉      | 116/292 [00:19<00:28,  6.24batch/s, auc=0.9075, loss=0.4210]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9075, loss=0.4210]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9079, loss=0.4666]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 118/292 [00:19<00:27,  6.23batch/s, auc=0.9079, loss=0.4666]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 118/292 [00:19<00:27,  6.23batch/s, auc=0.9078, loss=0.6200]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 119/292 [00:19<00:27,  6.23batch/s, auc=0.9078, loss=0.6200]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 119/292 [00:19<00:27,  6.23batch/s, auc=0.9082, loss=0.5425]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 120/292 [00:19<00:27,  6.22batch/s, auc=0.9082, loss=0.5425]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 120/292 [00:20<00:27,  6.22batch/s, auc=0.9081, loss=0.5795]\u001b[A\n",
      "Training Epoch 6/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9081, loss=0.5795]\u001b[A\n",
      "Training Epoch 6/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9081, loss=0.5458]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9081, loss=0.5458]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9079, loss=0.7438]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 123/292 [00:20<00:27,  6.23batch/s, auc=0.9079, loss=0.7438]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 123/292 [00:20<00:27,  6.23batch/s, auc=0.9082, loss=0.6194]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 124/292 [00:20<00:26,  6.23batch/s, auc=0.9082, loss=0.6194]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 124/292 [00:20<00:26,  6.23batch/s, auc=0.9088, loss=0.4453]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 125/292 [00:20<00:26,  6.22batch/s, auc=0.9088, loss=0.4453]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 125/292 [00:20<00:26,  6.22batch/s, auc=0.9084, loss=0.8455]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 126/292 [00:20<00:26,  6.22batch/s, auc=0.9084, loss=0.8455]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 126/292 [00:21<00:26,  6.22batch/s, auc=0.9082, loss=0.6528]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 127/292 [00:21<00:26,  6.22batch/s, auc=0.9082, loss=0.6528]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 127/292 [00:21<00:26,  6.22batch/s, auc=0.9075, loss=0.9619]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9075, loss=0.9619]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9069, loss=0.9770]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9069, loss=0.9770]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9068, loss=0.7269]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9068, loss=0.7269]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9067, loss=0.6482]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9067, loss=0.6482]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9070, loss=0.5615]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▌     | 132/292 [00:21<00:25,  6.20batch/s, auc=0.9070, loss=0.5615]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9071, loss=0.6193]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 133/292 [00:22<00:25,  6.22batch/s, auc=0.9071, loss=0.6193]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 133/292 [00:22<00:25,  6.22batch/s, auc=0.9072, loss=0.7490]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 134/292 [00:22<00:25,  6.22batch/s, auc=0.9072, loss=0.7490]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 134/292 [00:22<00:25,  6.22batch/s, auc=0.9075, loss=0.5550]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 135/292 [00:22<00:25,  6.22batch/s, auc=0.9075, loss=0.5550]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 135/292 [00:22<00:25,  6.22batch/s, auc=0.9076, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 136/292 [00:22<00:25,  6.21batch/s, auc=0.9076, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 136/292 [00:22<00:25,  6.21batch/s, auc=0.9074, loss=0.7869]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 137/292 [00:22<00:24,  6.20batch/s, auc=0.9074, loss=0.7869]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 137/292 [00:22<00:24,  6.20batch/s, auc=0.9075, loss=0.7305]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 138/292 [00:22<00:24,  6.20batch/s, auc=0.9075, loss=0.7305]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 138/292 [00:22<00:24,  6.20batch/s, auc=0.9078, loss=0.5159]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 139/292 [00:22<00:24,  6.20batch/s, auc=0.9078, loss=0.5159]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 139/292 [00:23<00:24,  6.20batch/s, auc=0.9077, loss=0.6357]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 140/292 [00:23<00:24,  6.20batch/s, auc=0.9077, loss=0.6357]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 140/292 [00:23<00:24,  6.20batch/s, auc=0.9081, loss=0.4344]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9081, loss=0.4344]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9077, loss=0.9503]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▊     | 142/292 [00:23<00:24,  6.20batch/s, auc=0.9077, loss=0.9503]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▊     | 142/292 [00:23<00:24,  6.20batch/s, auc=0.9076, loss=0.8351]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 143/292 [00:23<00:24,  6.20batch/s, auc=0.9076, loss=0.8351]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 143/292 [00:23<00:24,  6.20batch/s, auc=0.9063, loss=1.5860]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 144/292 [00:23<00:23,  6.20batch/s, auc=0.9063, loss=1.5860]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 144/292 [00:23<00:23,  6.20batch/s, auc=0.9067, loss=0.4544]\u001b[A\n",
      "Training Epoch 6/25:  50%|████▉     | 145/292 [00:23<00:23,  6.19batch/s, auc=0.9067, loss=0.4544]\u001b[A\n",
      "Training Epoch 6/25:  50%|████▉     | 145/292 [00:24<00:23,  6.19batch/s, auc=0.9068, loss=0.5449]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 146/292 [00:24<00:23,  6.19batch/s, auc=0.9068, loss=0.5449]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 146/292 [00:24<00:23,  6.19batch/s, auc=0.9070, loss=0.6217]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9070, loss=0.6217]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9073, loss=0.5274]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 148/292 [00:24<00:23,  6.19batch/s, auc=0.9073, loss=0.5274]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 148/292 [00:24<00:23,  6.19batch/s, auc=0.9072, loss=0.7728]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 149/292 [00:24<00:23,  6.18batch/s, auc=0.9072, loss=0.7728]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 149/292 [00:24<00:23,  6.18batch/s, auc=0.9071, loss=0.7717]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.17batch/s, auc=0.9071, loss=0.7717]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████▏    | 150/292 [00:24<00:22,  6.17batch/s, auc=0.9073, loss=0.6575]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.17batch/s, auc=0.9073, loss=0.6575]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9074, loss=0.5254]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.18batch/s, auc=0.9074, loss=0.5254]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.18batch/s, auc=0.9072, loss=0.8107]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.15batch/s, auc=0.9072, loss=0.8107]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.15batch/s, auc=0.9071, loss=0.7391]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9071, loss=0.7391]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9066, loss=0.9494]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9066, loss=0.9494]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9067, loss=0.5875]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9067, loss=0.5875]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9067, loss=0.6855]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.15batch/s, auc=0.9067, loss=0.6855]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9067, loss=0.6790]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.17batch/s, auc=0.9067, loss=0.6790]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.17batch/s, auc=0.9070, loss=0.6471]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9070, loss=0.6471]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9074, loss=0.4448]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.16batch/s, auc=0.9074, loss=0.4448]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.16batch/s, auc=0.9068, loss=0.9994]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.15batch/s, auc=0.9068, loss=0.9994]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.15batch/s, auc=0.9065, loss=0.7681]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.16batch/s, auc=0.9065, loss=0.7681]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.16batch/s, auc=0.9068, loss=0.5433]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.16batch/s, auc=0.9068, loss=0.5433]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 163/292 [00:27<00:20,  6.16batch/s, auc=0.9069, loss=0.6718]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.16batch/s, auc=0.9069, loss=0.6718]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.16batch/s, auc=0.9070, loss=0.5668]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9070, loss=0.5668]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9070, loss=0.6789]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.86batch/s, auc=0.9070, loss=0.6789]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.86batch/s, auc=0.9072, loss=0.6585]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.83batch/s, auc=0.9072, loss=0.6585]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.83batch/s, auc=0.9073, loss=0.6054]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 168/292 [00:27<00:21,  5.81batch/s, auc=0.9073, loss=0.6054]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 168/292 [00:27<00:21,  5.81batch/s, auc=0.9072, loss=0.6624]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 169/292 [00:27<00:20,  5.90batch/s, auc=0.9072, loss=0.6624]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.90batch/s, auc=0.9066, loss=1.2020]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 170/292 [00:28<00:20,  5.98batch/s, auc=0.9066, loss=1.2020]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 170/292 [00:28<00:20,  5.98batch/s, auc=0.9065, loss=0.8289]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▊    | 171/292 [00:28<00:20,  6.03batch/s, auc=0.9065, loss=0.8289]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▊    | 171/292 [00:28<00:20,  6.03batch/s, auc=0.9065, loss=0.7521]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9065, loss=0.7521]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9058, loss=1.1831]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.07batch/s, auc=0.9058, loss=1.1831]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.07batch/s, auc=0.9057, loss=0.7652]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.09batch/s, auc=0.9057, loss=0.7652]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.09batch/s, auc=0.9055, loss=0.9107]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.11batch/s, auc=0.9055, loss=0.9107]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.11batch/s, auc=0.9057, loss=0.5640]\u001b[A\n",
      "Training Epoch 6/25:  60%|██████    | 176/292 [00:29<00:18,  6.11batch/s, auc=0.9057, loss=0.5640]\u001b[A\n",
      "Training Epoch 6/25:  60%|██████    | 176/292 [00:29<00:18,  6.11batch/s, auc=0.9058, loss=0.7057]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9058, loss=0.7057]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9059, loss=0.7006]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 178/292 [00:29<00:18,  6.10batch/s, auc=0.9059, loss=0.7006]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 178/292 [00:29<00:18,  6.10batch/s, auc=0.9059, loss=0.6312]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.9059, loss=0.6312]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.9059, loss=0.6932]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9059, loss=0.6932]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9058, loss=0.7157]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.09batch/s, auc=0.9058, loss=0.7157]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.09batch/s, auc=0.9059, loss=0.7110]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.01batch/s, auc=0.9059, loss=0.7110]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.01batch/s, auc=0.9057, loss=0.6528]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.04batch/s, auc=0.9057, loss=0.6528]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.04batch/s, auc=0.9052, loss=0.9763]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.07batch/s, auc=0.9052, loss=0.9763]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.07batch/s, auc=0.9051, loss=0.5963]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9051, loss=0.5963]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9052, loss=0.5600]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.07batch/s, auc=0.9052, loss=0.5600]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.07batch/s, auc=0.9052, loss=0.5485]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.09batch/s, auc=0.9052, loss=0.5485]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.09batch/s, auc=0.9048, loss=0.8860]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9048, loss=0.8860]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9049, loss=0.6717]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.12batch/s, auc=0.9049, loss=0.6717]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.12batch/s, auc=0.9044, loss=1.0013]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.14batch/s, auc=0.9044, loss=1.0013]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.14batch/s, auc=0.9046, loss=0.5066]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.13batch/s, auc=0.9046, loss=0.5066]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.13batch/s, auc=0.9047, loss=0.5445]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.12batch/s, auc=0.9047, loss=0.5445]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.12batch/s, auc=0.9047, loss=0.4941]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.09batch/s, auc=0.9047, loss=0.4941]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.09batch/s, auc=0.9049, loss=0.5267]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.12batch/s, auc=0.9049, loss=0.5267]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.12batch/s, auc=0.9049, loss=0.6940]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.12batch/s, auc=0.9049, loss=0.6940]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.12batch/s, auc=0.9045, loss=0.8491]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.12batch/s, auc=0.9045, loss=0.8491]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.12batch/s, auc=0.9043, loss=0.8010]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.11batch/s, auc=0.9043, loss=0.8010]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.11batch/s, auc=0.9042, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.12batch/s, auc=0.9042, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.12batch/s, auc=0.9045, loss=0.4565]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.11batch/s, auc=0.9045, loss=0.4565]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.11batch/s, auc=0.9046, loss=0.5898]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 200/292 [00:32<00:15,  6.11batch/s, auc=0.9046, loss=0.5898]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.11batch/s, auc=0.9047, loss=0.5333]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.10batch/s, auc=0.9047, loss=0.5333]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.10batch/s, auc=0.9048, loss=0.6045]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.10batch/s, auc=0.9048, loss=0.6045]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.10batch/s, auc=0.9049, loss=0.5919]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.09batch/s, auc=0.9049, loss=0.5919]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.09batch/s, auc=0.9048, loss=0.6871]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.10batch/s, auc=0.9048, loss=0.6871]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.10batch/s, auc=0.9048, loss=0.5180]\u001b[A\n",
      "Training Epoch 6/25:  70%|███████   | 205/292 [00:33<00:14,  6.11batch/s, auc=0.9048, loss=0.5180]\u001b[A\n",
      "Training Epoch 6/25:  70%|███████   | 205/292 [00:33<00:14,  6.11batch/s, auc=0.9049, loss=0.6592]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 206/292 [00:33<00:14,  6.12batch/s, auc=0.9049, loss=0.6592]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 206/292 [00:34<00:14,  6.12batch/s, auc=0.9049, loss=0.6274]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 207/292 [00:34<00:13,  6.11batch/s, auc=0.9049, loss=0.6274]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 207/292 [00:34<00:13,  6.11batch/s, auc=0.9048, loss=0.6629]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 208/292 [00:34<00:13,  6.10batch/s, auc=0.9048, loss=0.6629]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 208/292 [00:34<00:13,  6.10batch/s, auc=0.9046, loss=0.7992]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.09batch/s, auc=0.9046, loss=0.7992]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.09batch/s, auc=0.9048, loss=0.5124]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.09batch/s, auc=0.9048, loss=0.5124]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.09batch/s, auc=0.9050, loss=0.5673]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.08batch/s, auc=0.9050, loss=0.5673]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.08batch/s, auc=0.9054, loss=0.3716]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 212/292 [00:34<00:13,  6.08batch/s, auc=0.9054, loss=0.3716]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.08batch/s, auc=0.9055, loss=0.7275]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.06batch/s, auc=0.9055, loss=0.7275]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.06batch/s, auc=0.9056, loss=0.5174]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.08batch/s, auc=0.9056, loss=0.5174]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.08batch/s, auc=0.9054, loss=0.8742]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.08batch/s, auc=0.9054, loss=0.8742]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.08batch/s, auc=0.9056, loss=0.5952]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.08batch/s, auc=0.9056, loss=0.5952]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.08batch/s, auc=0.9056, loss=0.8756]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 217/292 [00:35<00:12,  6.06batch/s, auc=0.9056, loss=0.8756]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 217/292 [00:35<00:12,  6.06batch/s, auc=0.9055, loss=0.7432]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▍  | 218/292 [00:35<00:12,  5.94batch/s, auc=0.9055, loss=0.7432]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.94batch/s, auc=0.9054, loss=0.6269]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.77batch/s, auc=0.9054, loss=0.6269]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.77batch/s, auc=0.9052, loss=0.9220]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.9052, loss=0.9220]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.9052, loss=0.4787]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.74batch/s, auc=0.9052, loss=0.4787]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.74batch/s, auc=0.9051, loss=0.6023]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.72batch/s, auc=0.9051, loss=0.6023]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.72batch/s, auc=0.9051, loss=0.5684]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▋  | 223/292 [00:36<00:12,  5.71batch/s, auc=0.9051, loss=0.5684]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.71batch/s, auc=0.9051, loss=0.5809]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.70batch/s, auc=0.9051, loss=0.5809]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.70batch/s, auc=0.9050, loss=0.7442]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.70batch/s, auc=0.9050, loss=0.7442]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.70batch/s, auc=0.9053, loss=0.4607]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.68batch/s, auc=0.9053, loss=0.4607]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.68batch/s, auc=0.9055, loss=0.4334]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.78batch/s, auc=0.9055, loss=0.4334]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.78batch/s, auc=0.9054, loss=0.6372]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.65batch/s, auc=0.9054, loss=0.6372]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.65batch/s, auc=0.9057, loss=0.4208]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 229/292 [00:37<00:11,  5.68batch/s, auc=0.9057, loss=0.4208]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.68batch/s, auc=0.9057, loss=0.8414]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.65batch/s, auc=0.9057, loss=0.8414]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.65batch/s, auc=0.9056, loss=0.8031]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.65batch/s, auc=0.9056, loss=0.8031]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.65batch/s, auc=0.9056, loss=0.6589]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.66batch/s, auc=0.9056, loss=0.6589]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.66batch/s, auc=0.9054, loss=0.9182]\u001b[A\n",
      "Training Epoch 6/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.66batch/s, auc=0.9054, loss=0.9182]\u001b[A\n",
      "Training Epoch 6/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.66batch/s, auc=0.9057, loss=0.4909]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 234/292 [00:38<00:10,  5.66batch/s, auc=0.9057, loss=0.4909]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 234/292 [00:38<00:10,  5.66batch/s, auc=0.9056, loss=0.6694]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 235/292 [00:38<00:10,  5.65batch/s, auc=0.9056, loss=0.6694]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 235/292 [00:39<00:10,  5.65batch/s, auc=0.9055, loss=0.8491]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 236/292 [00:39<00:09,  5.65batch/s, auc=0.9055, loss=0.8491]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 236/292 [00:39<00:09,  5.65batch/s, auc=0.9056, loss=0.5610]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 237/292 [00:39<00:09,  5.66batch/s, auc=0.9056, loss=0.5610]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 237/292 [00:39<00:09,  5.66batch/s, auc=0.9054, loss=0.9031]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.68batch/s, auc=0.9054, loss=0.9031]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.68batch/s, auc=0.9053, loss=0.6112]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.68batch/s, auc=0.9053, loss=0.6112]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.68batch/s, auc=0.9056, loss=0.4090]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.68batch/s, auc=0.9056, loss=0.4090]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.68batch/s, auc=0.9053, loss=0.8767]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.68batch/s, auc=0.9053, loss=0.8767]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.68batch/s, auc=0.9052, loss=0.6815]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.68batch/s, auc=0.9052, loss=0.6815]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.68batch/s, auc=0.9053, loss=0.5375]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.68batch/s, auc=0.9053, loss=0.5375]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.68batch/s, auc=0.9056, loss=0.5357]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.66batch/s, auc=0.9056, loss=0.5357]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.66batch/s, auc=0.9055, loss=0.8426]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.72batch/s, auc=0.9055, loss=0.8426]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.72batch/s, auc=0.9052, loss=1.1300]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 246/292 [00:40<00:07,  5.82batch/s, auc=0.9052, loss=1.1300]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.82batch/s, auc=0.9053, loss=0.5140]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9053, loss=0.5140]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9054, loss=0.5778]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9054, loss=0.5778]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9051, loss=0.8352]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.62batch/s, auc=0.9051, loss=0.8352]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.62batch/s, auc=0.9050, loss=0.6380]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.64batch/s, auc=0.9050, loss=0.6380]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.64batch/s, auc=0.9049, loss=0.9828]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.65batch/s, auc=0.9049, loss=0.9828]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.65batch/s, auc=0.9049, loss=0.4814]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▋ | 252/292 [00:41<00:07,  5.64batch/s, auc=0.9049, loss=0.4814]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.64batch/s, auc=0.9049, loss=0.8571]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9049, loss=0.8571]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9050, loss=0.5054]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9050, loss=0.5054]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9052, loss=0.4509]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.63batch/s, auc=0.9052, loss=0.4509]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.63batch/s, auc=0.9051, loss=0.6080]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.61batch/s, auc=0.9051, loss=0.6080]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.61batch/s, auc=0.9052, loss=0.5371]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.60batch/s, auc=0.9052, loss=0.5371]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.60batch/s, auc=0.9052, loss=0.8612]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.61batch/s, auc=0.9052, loss=0.8612]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.61batch/s, auc=0.9052, loss=0.5849]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.61batch/s, auc=0.9052, loss=0.5849]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.61batch/s, auc=0.9048, loss=1.0508]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.60batch/s, auc=0.9048, loss=1.0508]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.60batch/s, auc=0.9048, loss=0.6244]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.60batch/s, auc=0.9048, loss=0.6244]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.60batch/s, auc=0.9049, loss=0.5709]\u001b[A\n",
      "Training Epoch 6/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9049, loss=0.5709]\u001b[A\n",
      "Training Epoch 6/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9047, loss=0.7825]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 263/292 [00:43<00:05,  5.61batch/s, auc=0.9047, loss=0.7825]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 263/292 [00:44<00:05,  5.61batch/s, auc=0.9050, loss=0.4094]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 264/292 [00:44<00:04,  5.60batch/s, auc=0.9050, loss=0.4094]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 264/292 [00:44<00:04,  5.60batch/s, auc=0.9050, loss=0.5964]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 265/292 [00:44<00:04,  5.61batch/s, auc=0.9050, loss=0.5964]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 265/292 [00:44<00:04,  5.61batch/s, auc=0.9052, loss=0.5588]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 266/292 [00:44<00:04,  5.63batch/s, auc=0.9052, loss=0.5588]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 266/292 [00:44<00:04,  5.63batch/s, auc=0.9053, loss=0.5184]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.62batch/s, auc=0.9053, loss=0.5184]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.62batch/s, auc=0.9055, loss=0.4688]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.63batch/s, auc=0.9055, loss=0.4688]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.63batch/s, auc=0.9056, loss=0.5495]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.73batch/s, auc=0.9056, loss=0.5495]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.73batch/s, auc=0.9056, loss=0.5586]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.80batch/s, auc=0.9056, loss=0.5586]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.80batch/s, auc=0.9055, loss=0.7317]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.62batch/s, auc=0.9055, loss=0.7317]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.62batch/s, auc=0.9051, loss=1.1951]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.35batch/s, auc=0.9051, loss=1.1951]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.35batch/s, auc=0.9048, loss=0.9949]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.53batch/s, auc=0.9048, loss=0.9949]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.53batch/s, auc=0.9046, loss=0.9508]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.56batch/s, auc=0.9046, loss=0.9508]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.56batch/s, auc=0.9044, loss=1.0223]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.70batch/s, auc=0.9044, loss=1.0223]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.70batch/s, auc=0.9044, loss=0.5336]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.78batch/s, auc=0.9044, loss=0.5336]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.78batch/s, auc=0.9044, loss=0.7458]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.83batch/s, auc=0.9044, loss=0.7458]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.83batch/s, auc=0.9042, loss=1.0335]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.85batch/s, auc=0.9042, loss=1.0335]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.85batch/s, auc=0.9045, loss=0.5871]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.89batch/s, auc=0.9045, loss=0.5871]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.89batch/s, auc=0.9045, loss=0.6777]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.90batch/s, auc=0.9045, loss=0.6777]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.90batch/s, auc=0.9046, loss=0.5990]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.69batch/s, auc=0.9046, loss=0.5990]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.69batch/s, auc=0.9044, loss=1.0090]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.9044, loss=1.0090]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.66batch/s, auc=0.9043, loss=0.6049]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.9043, loss=0.6049]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.9044, loss=0.6632]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.61batch/s, auc=0.9044, loss=0.6632]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.61batch/s, auc=0.9042, loss=0.7395]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.61batch/s, auc=0.9042, loss=0.7395]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.61batch/s, auc=0.9041, loss=0.6613]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.72batch/s, auc=0.9041, loss=0.6613]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.72batch/s, auc=0.9042, loss=0.6320]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9042, loss=0.6320]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9042, loss=0.6081]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9042, loss=0.6081]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9040, loss=0.8410]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9040, loss=0.8410]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9037, loss=1.0626]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9037, loss=1.0626]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9038, loss=0.5939]\u001b[A\n",
      "Training Epoch 6/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.94batch/s, auc=0.9038, loss=0.5939]\u001b[A\n",
      "Training Epoch 6/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.94batch/s, auc=0.9038, loss=0.5302]\u001b[A\n",
      "Training Epoch 6/25: 100%|██████████| 292/292 [00:49<00:00,  5.95batch/s, auc=0.9038, loss=0.5302]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25] Train Loss: 0.6729 | Train AUROC: 0.9038 Val Loss: 0.6992 | Val AUROC: 0.8867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|██▍       | 6/25 [05:28<17:19, 54.73s/it]\n",
      "Training Epoch 7/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9741, loss=0.4373]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 1/292 [00:00<04:43,  1.03batch/s, auc=0.9741, loss=0.4373]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 1/292 [00:01<04:43,  1.03batch/s, auc=0.9141, loss=0.8315]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.9141, loss=0.8315]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.9236, loss=0.5179]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9236, loss=0.5179]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9386, loss=0.4803]\u001b[A\n",
      "Training Epoch 7/25:   1%|▏         | 4/292 [00:01<01:19,  3.64batch/s, auc=0.9386, loss=0.4803]\u001b[A\n",
      "Training Epoch 7/25:   1%|▏         | 4/292 [00:01<01:19,  3.64batch/s, auc=0.9418, loss=0.5538]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 5/292 [00:01<01:06,  4.30batch/s, auc=0.9418, loss=0.5538]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 5/292 [00:01<01:06,  4.30batch/s, auc=0.9303, loss=0.7884]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9303, loss=0.7884]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9094, loss=1.0960]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 7/292 [00:01<00:56,  5.06batch/s, auc=0.9094, loss=1.0960]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 7/292 [00:02<00:56,  5.06batch/s, auc=0.9060, loss=0.7526]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.9060, loss=0.7526]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.9034, loss=0.7478]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.9034, loss=0.7478]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.8974, loss=0.7862]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 10/292 [00:02<00:49,  5.67batch/s, auc=0.8974, loss=0.7862]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 10/292 [00:02<00:49,  5.67batch/s, auc=0.9039, loss=0.3790]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 11/292 [00:02<00:48,  5.85batch/s, auc=0.9039, loss=0.3790]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 11/292 [00:02<00:48,  5.85batch/s, auc=0.8939, loss=1.0648]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 12/292 [00:02<00:46,  5.99batch/s, auc=0.8939, loss=1.0648]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 12/292 [00:02<00:46,  5.99batch/s, auc=0.8969, loss=0.5590]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 13/292 [00:02<00:46,  6.06batch/s, auc=0.8969, loss=0.5590]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 13/292 [00:03<00:46,  6.06batch/s, auc=0.9005, loss=0.5439]\u001b[A\n",
      "Training Epoch 7/25:   5%|▍         | 14/292 [00:03<00:45,  6.15batch/s, auc=0.9005, loss=0.5439]\u001b[A\n",
      "Training Epoch 7/25:   5%|▍         | 14/292 [00:03<00:45,  6.15batch/s, auc=0.9033, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 15/292 [00:03<00:44,  6.16batch/s, auc=0.9033, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 15/292 [00:03<00:44,  6.16batch/s, auc=0.9031, loss=0.5703]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 16/292 [00:03<00:44,  6.22batch/s, auc=0.9031, loss=0.5703]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 16/292 [00:03<00:44,  6.22batch/s, auc=0.9029, loss=0.6220]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9029, loss=0.6220]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9067, loss=0.5012]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9067, loss=0.5012]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9047, loss=0.8846]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 19/292 [00:03<00:43,  6.27batch/s, auc=0.9047, loss=0.8846]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 19/292 [00:04<00:43,  6.27batch/s, auc=0.9042, loss=0.6781]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9042, loss=0.6781]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9040, loss=0.5383]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.9040, loss=0.5383]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.8978, loss=1.1511]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.8978, loss=1.1511]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9016, loss=0.5443]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9016, loss=0.5443]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9040, loss=0.5930]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.9040, loss=0.5930]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.9075, loss=0.5423]\u001b[A\n",
      "Training Epoch 7/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9075, loss=0.5423]\u001b[A\n",
      "Training Epoch 7/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9085, loss=0.5425]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 26/292 [00:04<00:41,  6.34batch/s, auc=0.9085, loss=0.5425]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.9046, loss=0.9918]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9046, loss=0.9918]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9050, loss=0.6269]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.9050, loss=0.6269]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.9047, loss=0.6376]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.9047, loss=0.6376]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 29/292 [00:05<00:41,  6.36batch/s, auc=0.9064, loss=0.5205]\u001b[A\n",
      "Training Epoch 7/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.9064, loss=0.5205]\u001b[A\n",
      "Training Epoch 7/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.9062, loss=0.8719]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 31/292 [00:05<00:41,  6.36batch/s, auc=0.9062, loss=0.8719]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 31/292 [00:05<00:41,  6.36batch/s, auc=0.9048, loss=0.7480]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 32/292 [00:05<00:41,  6.28batch/s, auc=0.9048, loss=0.7480]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 32/292 [00:06<00:41,  6.28batch/s, auc=0.9060, loss=0.4609]\u001b[A\n",
      "Training Epoch 7/25:  11%|█▏        | 33/292 [00:06<00:41,  6.29batch/s, auc=0.9060, loss=0.4609]\u001b[A\n",
      "Training Epoch 7/25:  11%|█▏        | 33/292 [00:06<00:41,  6.29batch/s, auc=0.9038, loss=1.0191]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9038, loss=1.0191]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9066, loss=0.4136]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 35/292 [00:06<00:40,  6.31batch/s, auc=0.9066, loss=0.4136]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 35/292 [00:06<00:40,  6.31batch/s, auc=0.9063, loss=0.6760]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9063, loss=0.6760]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9071, loss=0.6408]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9071, loss=0.6408]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9080, loss=0.5241]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 38/292 [00:06<00:40,  6.26batch/s, auc=0.9080, loss=0.5241]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 38/292 [00:07<00:40,  6.26batch/s, auc=0.9087, loss=0.5552]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9087, loss=0.5552]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9091, loss=0.6424]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▎        | 40/292 [00:07<00:40,  6.30batch/s, auc=0.9091, loss=0.6424]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▎        | 40/292 [00:07<00:40,  6.30batch/s, auc=0.9098, loss=0.5187]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 41/292 [00:07<00:39,  6.30batch/s, auc=0.9098, loss=0.5187]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 41/292 [00:07<00:39,  6.30batch/s, auc=0.9105, loss=0.5048]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9105, loss=0.5048]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9109, loss=0.6636]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9109, loss=0.6636]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9083, loss=1.0045]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 44/292 [00:07<00:39,  6.25batch/s, auc=0.9083, loss=1.0045]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 44/292 [00:08<00:39,  6.25batch/s, auc=0.9085, loss=0.6916]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 45/292 [00:08<00:39,  6.24batch/s, auc=0.9085, loss=0.6916]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 45/292 [00:08<00:39,  6.24batch/s, auc=0.9097, loss=0.4705]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 46/292 [00:08<00:39,  6.28batch/s, auc=0.9097, loss=0.4705]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 46/292 [00:08<00:39,  6.28batch/s, auc=0.9093, loss=0.7970]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9093, loss=0.7970]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9101, loss=0.4975]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9101, loss=0.4975]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9104, loss=0.5762]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9104, loss=0.5762]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9113, loss=0.4612]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9113, loss=0.4612]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9123, loss=0.5585]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 51/292 [00:08<00:38,  6.22batch/s, auc=0.9123, loss=0.5585]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 51/292 [00:09<00:38,  6.22batch/s, auc=0.9108, loss=0.8777]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 52/292 [00:09<00:38,  6.23batch/s, auc=0.9108, loss=0.8777]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 52/292 [00:09<00:38,  6.23batch/s, auc=0.9093, loss=1.0546]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 53/292 [00:09<00:38,  6.20batch/s, auc=0.9093, loss=1.0546]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 53/292 [00:09<00:38,  6.20batch/s, auc=0.9095, loss=0.6416]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 54/292 [00:09<00:38,  6.24batch/s, auc=0.9095, loss=0.6416]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 54/292 [00:09<00:38,  6.24batch/s, auc=0.9102, loss=0.4131]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9102, loss=0.4131]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9077, loss=1.1372]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9077, loss=1.1372]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9076, loss=0.7042]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 57/292 [00:09<00:37,  6.33batch/s, auc=0.9076, loss=0.7042]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 57/292 [00:10<00:37,  6.33batch/s, auc=0.9065, loss=0.8912]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9065, loss=0.8912]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9058, loss=0.6821]\u001b[A\n",
      "Training Epoch 7/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9058, loss=0.6821]\u001b[A\n",
      "Training Epoch 7/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9049, loss=0.8898]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9049, loss=0.8898]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9053, loss=0.5998]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9053, loss=0.5998]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9055, loss=0.7227]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 62/292 [00:10<00:36,  6.31batch/s, auc=0.9055, loss=0.7227]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 62/292 [00:10<00:36,  6.31batch/s, auc=0.9050, loss=0.8966]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 63/292 [00:10<00:36,  6.31batch/s, auc=0.9050, loss=0.8966]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 63/292 [00:11<00:36,  6.31batch/s, auc=0.9051, loss=0.5797]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 64/292 [00:11<00:36,  6.31batch/s, auc=0.9051, loss=0.5797]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 64/292 [00:11<00:36,  6.31batch/s, auc=0.9051, loss=0.5857]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 65/292 [00:11<00:35,  6.31batch/s, auc=0.9051, loss=0.5857]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 65/292 [00:11<00:35,  6.31batch/s, auc=0.9046, loss=0.6561]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 66/292 [00:11<00:35,  6.32batch/s, auc=0.9046, loss=0.6561]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 66/292 [00:11<00:35,  6.32batch/s, auc=0.9028, loss=1.1900]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 67/292 [00:11<00:35,  6.30batch/s, auc=0.9028, loss=1.1900]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 67/292 [00:11<00:35,  6.30batch/s, auc=0.9036, loss=0.5577]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 68/292 [00:11<00:35,  6.25batch/s, auc=0.9036, loss=0.5577]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 68/292 [00:11<00:35,  6.25batch/s, auc=0.9040, loss=0.5268]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▎       | 69/292 [00:11<00:35,  6.21batch/s, auc=0.9040, loss=0.5268]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▎       | 69/292 [00:11<00:35,  6.21batch/s, auc=0.9043, loss=0.5629]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 70/292 [00:11<00:35,  6.24batch/s, auc=0.9043, loss=0.5629]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 70/292 [00:12<00:35,  6.24batch/s, auc=0.9039, loss=0.5865]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 71/292 [00:12<00:35,  6.26batch/s, auc=0.9039, loss=0.5865]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 71/292 [00:12<00:35,  6.26batch/s, auc=0.9040, loss=0.7342]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9040, loss=0.7342]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9047, loss=0.5214]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9047, loss=0.5214]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9047, loss=0.7132]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9047, loss=0.7132]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9055, loss=0.6329]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9055, loss=0.6329]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9043, loss=1.1062]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 76/292 [00:12<00:34,  6.26batch/s, auc=0.9043, loss=1.1062]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 76/292 [00:13<00:34,  6.26batch/s, auc=0.9052, loss=0.4485]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9052, loss=0.4485]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9055, loss=0.6348]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 78/292 [00:13<00:34,  6.17batch/s, auc=0.9055, loss=0.6348]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 78/292 [00:13<00:34,  6.17batch/s, auc=0.9051, loss=0.7572]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 79/292 [00:13<00:34,  6.21batch/s, auc=0.9051, loss=0.7572]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 79/292 [00:13<00:34,  6.21batch/s, auc=0.9054, loss=0.5598]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9054, loss=0.5598]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9052, loss=0.5645]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9052, loss=0.5645]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9051, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 82/292 [00:13<00:33,  6.28batch/s, auc=0.9051, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 82/292 [00:14<00:33,  6.28batch/s, auc=0.9058, loss=0.4380]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 83/292 [00:14<00:33,  6.27batch/s, auc=0.9058, loss=0.4380]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 83/292 [00:14<00:33,  6.27batch/s, auc=0.9057, loss=0.6195]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9057, loss=0.6195]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9060, loss=0.4759]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 85/292 [00:14<00:32,  6.27batch/s, auc=0.9060, loss=0.4759]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 85/292 [00:14<00:32,  6.27batch/s, auc=0.9047, loss=1.2084]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 86/292 [00:14<00:32,  6.28batch/s, auc=0.9047, loss=1.2084]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 86/292 [00:14<00:32,  6.28batch/s, auc=0.9050, loss=0.5100]\u001b[A\n",
      "Training Epoch 7/25:  30%|██▉       | 87/292 [00:14<00:32,  6.29batch/s, auc=0.9050, loss=0.5100]\u001b[A\n",
      "Training Epoch 7/25:  30%|██▉       | 87/292 [00:14<00:32,  6.29batch/s, auc=0.9050, loss=0.6218]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 88/292 [00:14<00:32,  6.29batch/s, auc=0.9050, loss=0.6218]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 88/292 [00:15<00:32,  6.29batch/s, auc=0.9047, loss=0.7070]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 89/292 [00:15<00:32,  6.28batch/s, auc=0.9047, loss=0.7070]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 89/292 [00:15<00:32,  6.28batch/s, auc=0.9055, loss=0.3738]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9055, loss=0.3738]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9058, loss=0.5913]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9058, loss=0.5913]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9061, loss=0.5472]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9061, loss=0.5472]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9060, loss=0.6504]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 93/292 [00:15<00:32,  6.20batch/s, auc=0.9060, loss=0.6504]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 93/292 [00:15<00:32,  6.20batch/s, auc=0.9048, loss=1.0743]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 94/292 [00:15<00:31,  6.21batch/s, auc=0.9048, loss=1.0743]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 94/292 [00:15<00:31,  6.21batch/s, auc=0.9043, loss=1.0727]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 95/292 [00:15<00:31,  6.20batch/s, auc=0.9043, loss=1.0727]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 95/292 [00:16<00:31,  6.20batch/s, auc=0.9041, loss=0.6726]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 96/292 [00:16<00:31,  6.21batch/s, auc=0.9041, loss=0.6726]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 96/292 [00:16<00:31,  6.21batch/s, auc=0.9050, loss=0.4100]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9050, loss=0.4100]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9045, loss=0.7353]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9045, loss=0.7353]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9049, loss=0.6259]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9049, loss=0.6259]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9056, loss=0.3797]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9056, loss=0.3797]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9048, loss=1.0309]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 101/292 [00:16<00:31,  6.15batch/s, auc=0.9048, loss=1.0309]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 101/292 [00:17<00:31,  6.15batch/s, auc=0.9049, loss=0.5984]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 102/292 [00:17<00:30,  6.18batch/s, auc=0.9049, loss=0.5984]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 102/292 [00:17<00:30,  6.18batch/s, auc=0.9048, loss=0.7129]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9048, loss=0.7129]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9045, loss=0.7469]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9045, loss=0.7469]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9047, loss=0.4548]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9047, loss=0.4548]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9039, loss=0.8031]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9039, loss=0.8031]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9034, loss=0.7586]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9034, loss=0.7586]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9038, loss=0.4538]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9038, loss=0.4538]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9040, loss=0.6448]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9040, loss=0.6448]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9039, loss=0.7352]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9039, loss=0.7352]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9040, loss=0.5591]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 111/292 [00:18<00:29,  6.24batch/s, auc=0.9040, loss=0.5591]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 111/292 [00:18<00:29,  6.24batch/s, auc=0.9046, loss=0.5000]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9046, loss=0.5000]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9055, loss=0.4289]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▊      | 113/292 [00:18<00:28,  6.23batch/s, auc=0.9055, loss=0.4289]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▊      | 113/292 [00:19<00:28,  6.23batch/s, auc=0.9056, loss=0.6453]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9056, loss=0.6453]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9055, loss=0.6098]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 115/292 [00:19<00:28,  6.23batch/s, auc=0.9055, loss=0.6098]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 115/292 [00:19<00:28,  6.23batch/s, auc=0.9051, loss=0.9896]\u001b[A\n",
      "Training Epoch 7/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9051, loss=0.9896]\u001b[A\n",
      "Training Epoch 7/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9052, loss=0.5353]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 117/292 [00:19<00:28,  6.20batch/s, auc=0.9052, loss=0.5353]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 117/292 [00:19<00:28,  6.20batch/s, auc=0.9053, loss=0.5323]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9053, loss=0.5323]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9046, loss=1.1332]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9046, loss=1.1332]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 119/292 [00:20<00:27,  6.22batch/s, auc=0.9046, loss=0.4503]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 120/292 [00:20<00:27,  6.22batch/s, auc=0.9046, loss=0.4503]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 120/292 [00:20<00:27,  6.22batch/s, auc=0.9049, loss=0.4691]\u001b[A\n",
      "Training Epoch 7/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9049, loss=0.4691]\u001b[A\n",
      "Training Epoch 7/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9051, loss=0.5236]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9051, loss=0.5236]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9056, loss=0.4910]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9056, loss=0.4910]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9058, loss=0.4522]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 124/292 [00:20<00:27,  6.22batch/s, auc=0.9058, loss=0.4522]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 124/292 [00:20<00:27,  6.22batch/s, auc=0.9062, loss=0.4589]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 125/292 [00:20<00:26,  6.22batch/s, auc=0.9062, loss=0.4589]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 125/292 [00:20<00:26,  6.22batch/s, auc=0.9059, loss=0.7964]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 126/292 [00:20<00:26,  6.21batch/s, auc=0.9059, loss=0.7964]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 126/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.5601]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.5601]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.6186]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.6186]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.5579]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 129/292 [00:21<00:26,  6.21batch/s, auc=0.9059, loss=0.5579]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 129/292 [00:21<00:26,  6.21batch/s, auc=0.9065, loss=0.4603]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 130/292 [00:21<00:26,  6.22batch/s, auc=0.9065, loss=0.4603]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 130/292 [00:21<00:26,  6.22batch/s, auc=0.9066, loss=0.6885]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 131/292 [00:21<00:25,  6.22batch/s, auc=0.9066, loss=0.6885]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 131/292 [00:21<00:25,  6.22batch/s, auc=0.9071, loss=0.4049]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▌     | 132/292 [00:21<00:26,  6.15batch/s, auc=0.9071, loss=0.4049]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▌     | 132/292 [00:22<00:26,  6.15batch/s, auc=0.9065, loss=1.0557]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9065, loss=1.0557]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9068, loss=0.4547]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9068, loss=0.4547]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9067, loss=0.4692]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 135/292 [00:22<00:25,  6.20batch/s, auc=0.9067, loss=0.4692]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 135/292 [00:22<00:25,  6.20batch/s, auc=0.9069, loss=0.4134]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 136/292 [00:22<00:25,  6.20batch/s, auc=0.9069, loss=0.4134]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 136/292 [00:22<00:25,  6.20batch/s, auc=0.9067, loss=0.7630]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9067, loss=0.7630]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9061, loss=1.1957]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 138/292 [00:22<00:24,  6.19batch/s, auc=0.9061, loss=1.1957]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 138/292 [00:23<00:24,  6.19batch/s, auc=0.9063, loss=0.7282]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 139/292 [00:23<00:24,  6.20batch/s, auc=0.9063, loss=0.7282]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 139/292 [00:23<00:24,  6.20batch/s, auc=0.9067, loss=0.3718]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9067, loss=0.3718]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9067, loss=0.8059]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9067, loss=0.8059]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9065, loss=0.6788]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▊     | 142/292 [00:23<00:24,  6.20batch/s, auc=0.9065, loss=0.6788]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▊     | 142/292 [00:23<00:24,  6.20batch/s, auc=0.9065, loss=0.5337]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 143/292 [00:23<00:24,  6.18batch/s, auc=0.9065, loss=0.5337]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 143/292 [00:23<00:24,  6.18batch/s, auc=0.9066, loss=0.5237]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 144/292 [00:23<00:23,  6.18batch/s, auc=0.9066, loss=0.5237]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 144/292 [00:24<00:23,  6.18batch/s, auc=0.9066, loss=0.5381]\u001b[A\n",
      "Training Epoch 7/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9066, loss=0.5381]\u001b[A\n",
      "Training Epoch 7/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9066, loss=0.5909]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9066, loss=0.5909]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9062, loss=0.8279]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 147/292 [00:24<00:23,  6.17batch/s, auc=0.9062, loss=0.8279]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 147/292 [00:24<00:23,  6.17batch/s, auc=0.9068, loss=0.3551]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9068, loss=0.3551]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9066, loss=0.7584]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9066, loss=0.7584]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9069, loss=0.4192]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.14batch/s, auc=0.9069, loss=0.4192]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.14batch/s, auc=0.9072, loss=0.4588]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.16batch/s, auc=0.9072, loss=0.4588]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.16batch/s, auc=0.9072, loss=0.5580]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9072, loss=0.5580]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9078, loss=0.3937]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9078, loss=0.3937]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9079, loss=0.6051]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9079, loss=0.6051]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9080, loss=0.7275]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9080, loss=0.7275]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9077, loss=0.9472]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9077, loss=0.9472]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9081, loss=0.4965]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.16batch/s, auc=0.9081, loss=0.4965]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.16batch/s, auc=0.9077, loss=1.0688]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9077, loss=1.0688]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9076, loss=0.7408]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9076, loss=0.7408]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9074, loss=0.7830]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.17batch/s, auc=0.9074, loss=0.7830]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.17batch/s, auc=0.9073, loss=0.5242]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9073, loss=0.5242]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9074, loss=0.5460]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9074, loss=0.5460]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9076, loss=0.6006]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.14batch/s, auc=0.9076, loss=0.6006]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 163/292 [00:27<00:20,  6.14batch/s, auc=0.9077, loss=0.6642]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9077, loss=0.6642]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9078, loss=0.4922]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9078, loss=0.4922]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9077, loss=0.7634]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9077, loss=0.7634]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9082, loss=0.4452]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.12batch/s, auc=0.9082, loss=0.4452]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.12batch/s, auc=0.9082, loss=0.7846]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9082, loss=0.7846]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9082, loss=0.4939]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.13batch/s, auc=0.9082, loss=0.4939]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9082, loss=0.6768]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.13batch/s, auc=0.9082, loss=0.6768]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.13batch/s, auc=0.9081, loss=0.8172]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.14batch/s, auc=0.9081, loss=0.8172]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.14batch/s, auc=0.9078, loss=0.7705]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9078, loss=0.7705]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9079, loss=0.6522]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.15batch/s, auc=0.9079, loss=0.6522]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.15batch/s, auc=0.9080, loss=0.7887]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.14batch/s, auc=0.9080, loss=0.7887]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.14batch/s, auc=0.9080, loss=0.5715]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.13batch/s, auc=0.9080, loss=0.5715]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.13batch/s, auc=0.9082, loss=0.6034]\u001b[A\n",
      "Training Epoch 7/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9082, loss=0.6034]\u001b[A\n",
      "Training Epoch 7/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9086, loss=0.5008]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 177/292 [00:29<00:18,  6.10batch/s, auc=0.9086, loss=0.5008]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 177/292 [00:29<00:18,  6.10batch/s, auc=0.9084, loss=0.6626]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 178/292 [00:29<00:18,  6.10batch/s, auc=0.9084, loss=0.6626]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 178/292 [00:29<00:18,  6.10batch/s, auc=0.9089, loss=0.3730]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.9089, loss=0.3730]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.10batch/s, auc=0.9087, loss=0.6042]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.10batch/s, auc=0.9087, loss=0.6042]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.10batch/s, auc=0.9091, loss=0.4609]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.10batch/s, auc=0.9091, loss=0.4609]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.10batch/s, auc=0.9093, loss=0.6156]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.09batch/s, auc=0.9093, loss=0.6156]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.09batch/s, auc=0.9094, loss=0.6433]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9094, loss=0.6433]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9097, loss=0.4816]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.10batch/s, auc=0.9097, loss=0.4816]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.10batch/s, auc=0.9101, loss=0.4555]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.05batch/s, auc=0.9101, loss=0.4555]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.05batch/s, auc=0.9101, loss=0.6568]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▎   | 186/292 [00:30<00:17,  5.93batch/s, auc=0.9101, loss=0.6568]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▎   | 186/292 [00:30<00:17,  5.93batch/s, auc=0.9099, loss=0.6742]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 187/292 [00:30<00:17,  5.98batch/s, auc=0.9099, loss=0.6742]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 187/292 [00:31<00:17,  5.98batch/s, auc=0.9099, loss=0.6619]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.88batch/s, auc=0.9099, loss=0.6619]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.88batch/s, auc=0.9099, loss=0.5872]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.93batch/s, auc=0.9099, loss=0.5872]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.93batch/s, auc=0.9098, loss=0.7580]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.78batch/s, auc=0.9098, loss=0.7580]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.78batch/s, auc=0.9099, loss=0.4861]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.75batch/s, auc=0.9099, loss=0.4861]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.75batch/s, auc=0.9098, loss=0.5677]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.72batch/s, auc=0.9098, loss=0.5677]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.72batch/s, auc=0.9092, loss=1.2050]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 193/292 [00:31<00:17,  5.70batch/s, auc=0.9092, loss=1.2050]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.70batch/s, auc=0.9091, loss=0.7697]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.70batch/s, auc=0.9091, loss=0.7697]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.70batch/s, auc=0.9091, loss=0.5892]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 195/292 [00:32<00:17,  5.69batch/s, auc=0.9091, loss=0.5892]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 195/292 [00:32<00:17,  5.69batch/s, auc=0.9091, loss=0.8788]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.70batch/s, auc=0.9091, loss=0.8788]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.70batch/s, auc=0.9091, loss=0.6752]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.69batch/s, auc=0.9091, loss=0.6752]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.69batch/s, auc=0.9091, loss=0.7040]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.69batch/s, auc=0.9091, loss=0.7040]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.69batch/s, auc=0.9090, loss=0.6957]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.69batch/s, auc=0.9090, loss=0.6957]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.69batch/s, auc=0.9091, loss=0.6554]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.69batch/s, auc=0.9091, loss=0.6554]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.69batch/s, auc=0.9086, loss=0.9585]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.80batch/s, auc=0.9086, loss=0.9585]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.80batch/s, auc=0.9087, loss=0.5290]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.71batch/s, auc=0.9087, loss=0.5290]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.71batch/s, auc=0.9090, loss=0.6415]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.70batch/s, auc=0.9090, loss=0.6415]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.70batch/s, auc=0.9091, loss=0.5329]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.69batch/s, auc=0.9091, loss=0.5329]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.69batch/s, auc=0.9089, loss=0.8217]\u001b[A\n",
      "Training Epoch 7/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9089, loss=0.8217]\u001b[A\n",
      "Training Epoch 7/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9090, loss=0.5062]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 206/292 [00:34<00:15,  5.69batch/s, auc=0.9090, loss=0.5062]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 206/292 [00:34<00:15,  5.69batch/s, auc=0.9085, loss=1.1651]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 207/292 [00:34<00:14,  5.69batch/s, auc=0.9085, loss=1.1651]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 207/292 [00:34<00:14,  5.69batch/s, auc=0.9087, loss=0.4377]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 208/292 [00:34<00:14,  5.69batch/s, auc=0.9087, loss=0.4377]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 208/292 [00:34<00:14,  5.69batch/s, auc=0.9081, loss=1.1412]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.72batch/s, auc=0.9081, loss=1.1412]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.72batch/s, auc=0.9078, loss=0.9338]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.70batch/s, auc=0.9078, loss=0.9338]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.70batch/s, auc=0.9075, loss=0.8856]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.70batch/s, auc=0.9075, loss=0.8856]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.70batch/s, auc=0.9075, loss=0.6343]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.70batch/s, auc=0.9075, loss=0.6343]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.70batch/s, auc=0.9077, loss=0.5178]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.71batch/s, auc=0.9077, loss=0.5178]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.71batch/s, auc=0.9079, loss=0.5970]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.70batch/s, auc=0.9079, loss=0.5970]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.70batch/s, auc=0.9079, loss=0.7121]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.80batch/s, auc=0.9079, loss=0.7121]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.80batch/s, auc=0.9079, loss=0.6914]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 216/292 [00:35<00:12,  5.87batch/s, auc=0.9079, loss=0.6914]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 216/292 [00:36<00:12,  5.87batch/s, auc=0.9078, loss=0.7175]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.92batch/s, auc=0.9078, loss=0.7175]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.92batch/s, auc=0.9076, loss=0.7448]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.96batch/s, auc=0.9076, loss=0.7448]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.96batch/s, auc=0.9075, loss=0.6875]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.99batch/s, auc=0.9075, loss=0.6875]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.99batch/s, auc=0.9071, loss=1.0876]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.98batch/s, auc=0.9071, loss=1.0876]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.98batch/s, auc=0.9070, loss=0.5893]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.00batch/s, auc=0.9070, loss=0.5893]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.00batch/s, auc=0.9072, loss=0.5459]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.94batch/s, auc=0.9072, loss=0.5459]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.94batch/s, auc=0.9075, loss=0.4923]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.98batch/s, auc=0.9075, loss=0.4923]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.98batch/s, auc=0.9075, loss=0.6235]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.88batch/s, auc=0.9075, loss=0.6235]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.88batch/s, auc=0.9075, loss=0.5814]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.82batch/s, auc=0.9075, loss=0.5814]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.82batch/s, auc=0.9077, loss=0.5181]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.82batch/s, auc=0.9077, loss=0.5181]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.82batch/s, auc=0.9076, loss=0.6087]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.77batch/s, auc=0.9076, loss=0.6087]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.77batch/s, auc=0.9080, loss=0.5636]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.75batch/s, auc=0.9080, loss=0.5636]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.75batch/s, auc=0.9082, loss=0.5819]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.72batch/s, auc=0.9082, loss=0.5819]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.72batch/s, auc=0.9080, loss=0.5931]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.70batch/s, auc=0.9080, loss=0.5931]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.70batch/s, auc=0.9079, loss=0.7192]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.66batch/s, auc=0.9079, loss=0.7192]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.66batch/s, auc=0.9082, loss=0.5376]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.69batch/s, auc=0.9082, loss=0.5376]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.69batch/s, auc=0.9085, loss=0.4173]\u001b[A\n",
      "Training Epoch 7/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.79batch/s, auc=0.9085, loss=0.4173]\u001b[A\n",
      "Training Epoch 7/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.79batch/s, auc=0.9086, loss=0.4604]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 234/292 [00:39<00:10,  5.65batch/s, auc=0.9086, loss=0.4604]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 234/292 [00:39<00:10,  5.65batch/s, auc=0.9086, loss=0.6386]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 235/292 [00:39<00:10,  5.64batch/s, auc=0.9086, loss=0.6386]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 235/292 [00:39<00:10,  5.64batch/s, auc=0.9088, loss=0.4605]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 236/292 [00:39<00:09,  5.64batch/s, auc=0.9088, loss=0.4605]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 236/292 [00:39<00:09,  5.64batch/s, auc=0.9090, loss=0.5469]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 237/292 [00:39<00:09,  5.67batch/s, auc=0.9090, loss=0.5469]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 237/292 [00:39<00:09,  5.67batch/s, auc=0.9091, loss=0.5506]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.67batch/s, auc=0.9091, loss=0.5506]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.67batch/s, auc=0.9092, loss=0.7264]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.78batch/s, auc=0.9092, loss=0.7264]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.78batch/s, auc=0.9090, loss=0.7954]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.65batch/s, auc=0.9090, loss=0.7954]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.65batch/s, auc=0.9087, loss=0.8107]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.66batch/s, auc=0.9087, loss=0.8107]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.66batch/s, auc=0.9084, loss=0.8596]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9084, loss=0.8596]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9086, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.82batch/s, auc=0.9086, loss=0.5985]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.82batch/s, auc=0.9087, loss=0.6184]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.86batch/s, auc=0.9087, loss=0.6184]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.86batch/s, auc=0.9084, loss=1.0156]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 245/292 [00:40<00:07,  5.92batch/s, auc=0.9084, loss=1.0156]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 245/292 [00:41<00:07,  5.92batch/s, auc=0.9084, loss=0.5552]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.82batch/s, auc=0.9084, loss=0.5552]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.82batch/s, auc=0.9085, loss=0.5816]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.86batch/s, auc=0.9085, loss=0.5816]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.86batch/s, auc=0.9080, loss=1.2149]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.63batch/s, auc=0.9080, loss=1.2149]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.63batch/s, auc=0.9079, loss=0.7129]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.67batch/s, auc=0.9079, loss=0.7129]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.67batch/s, auc=0.9081, loss=0.5293]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.75batch/s, auc=0.9081, loss=0.5293]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.75batch/s, auc=0.9081, loss=0.6537]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.84batch/s, auc=0.9081, loss=0.6537]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.84batch/s, auc=0.9075, loss=1.2434]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.89batch/s, auc=0.9075, loss=1.2434]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.89batch/s, auc=0.9076, loss=0.6820]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.93batch/s, auc=0.9076, loss=0.6820]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.93batch/s, auc=0.9075, loss=0.7009]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.83batch/s, auc=0.9075, loss=0.7009]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.83batch/s, auc=0.9078, loss=0.4548]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.76batch/s, auc=0.9078, loss=0.4548]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.76batch/s, auc=0.9079, loss=0.5586]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.83batch/s, auc=0.9079, loss=0.5586]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.83batch/s, auc=0.9076, loss=0.8892]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.67batch/s, auc=0.9076, loss=0.8892]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.67batch/s, auc=0.9076, loss=0.8408]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.65batch/s, auc=0.9076, loss=0.8408]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.65batch/s, auc=0.9073, loss=0.6896]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.65batch/s, auc=0.9073, loss=0.6896]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.65batch/s, auc=0.9074, loss=0.4760]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.63batch/s, auc=0.9074, loss=0.4760]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.63batch/s, auc=0.9075, loss=0.5625]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.62batch/s, auc=0.9075, loss=0.5625]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.62batch/s, auc=0.9076, loss=0.6560]\u001b[A\n",
      "Training Epoch 7/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.68batch/s, auc=0.9076, loss=0.6560]\u001b[A\n",
      "Training Epoch 7/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.68batch/s, auc=0.9075, loss=0.5580]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 263/292 [00:44<00:05,  5.66batch/s, auc=0.9075, loss=0.5580]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 263/292 [00:44<00:05,  5.66batch/s, auc=0.9072, loss=0.9219]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9072, loss=0.9219]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9072, loss=0.5563]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 265/292 [00:44<00:04,  5.62batch/s, auc=0.9072, loss=0.5563]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 265/292 [00:44<00:04,  5.62batch/s, auc=0.9070, loss=0.6665]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 266/292 [00:44<00:04,  5.62batch/s, auc=0.9070, loss=0.6665]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 266/292 [00:44<00:04,  5.62batch/s, auc=0.9072, loss=0.5106]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.59batch/s, auc=0.9072, loss=0.5106]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.59batch/s, auc=0.9074, loss=0.4308]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9074, loss=0.4308]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9073, loss=0.6133]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.63batch/s, auc=0.9073, loss=0.6133]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.63batch/s, auc=0.9074, loss=0.6078]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9074, loss=0.6078]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9075, loss=0.5076]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9075, loss=0.5076]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9074, loss=0.8015]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.59batch/s, auc=0.9074, loss=0.8015]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.59batch/s, auc=0.9068, loss=1.1766]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.61batch/s, auc=0.9068, loss=1.1766]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.61batch/s, auc=0.9069, loss=0.5392]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.60batch/s, auc=0.9069, loss=0.5392]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.60batch/s, auc=0.9069, loss=0.7156]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9069, loss=0.7156]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9069, loss=0.6985]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.67batch/s, auc=0.9069, loss=0.6985]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.67batch/s, auc=0.9069, loss=0.6103]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.65batch/s, auc=0.9069, loss=0.6103]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.65batch/s, auc=0.9070, loss=0.4711]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.63batch/s, auc=0.9070, loss=0.4711]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.63batch/s, auc=0.9071, loss=0.5044]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.61batch/s, auc=0.9071, loss=0.5044]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.61batch/s, auc=0.9073, loss=0.5667]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.59batch/s, auc=0.9073, loss=0.5667]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.59batch/s, auc=0.9073, loss=0.5748]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.59batch/s, auc=0.9073, loss=0.5748]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.59batch/s, auc=0.9071, loss=0.7644]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9071, loss=0.7644]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9071, loss=0.6427]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.57batch/s, auc=0.9071, loss=0.6427]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.57batch/s, auc=0.9072, loss=0.4516]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9072, loss=0.4516]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.55batch/s, auc=0.9073, loss=0.5972]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.56batch/s, auc=0.9073, loss=0.5972]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.56batch/s, auc=0.9074, loss=0.5687]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.67batch/s, auc=0.9074, loss=0.5687]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.67batch/s, auc=0.9076, loss=0.4740]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.76batch/s, auc=0.9076, loss=0.4740]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.76batch/s, auc=0.9076, loss=0.6107]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9076, loss=0.6107]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9074, loss=0.7910]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9074, loss=0.7910]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9077, loss=0.4096]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.89batch/s, auc=0.9077, loss=0.4096]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.89batch/s, auc=0.9079, loss=0.4517]\u001b[A\n",
      "Training Epoch 7/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9079, loss=0.4517]\u001b[A\n",
      "Training Epoch 7/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9078, loss=0.9595]\u001b[A\n",
      "Training Epoch 7/25: 100%|██████████| 292/292 [00:49<00:00,  5.92batch/s, auc=0.9078, loss=0.9595]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25] Train Loss: 0.6572 | Train AUROC: 0.9078 Val Loss: 0.6790 | Val AUROC: 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|██▊       | 7/25 [06:23<16:28, 54.93s/it]\n",
      "Training Epoch 8/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9219, loss=0.5721]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 1/292 [00:00<04:38,  1.05batch/s, auc=0.9219, loss=0.5721]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 1/292 [00:01<04:38,  1.05batch/s, auc=0.9074, loss=0.6897]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 2/292 [00:01<02:20,  2.06batch/s, auc=0.9074, loss=0.6897]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 2/292 [00:01<02:20,  2.06batch/s, auc=0.9136, loss=0.5946]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 3/292 [00:01<01:37,  2.98batch/s, auc=0.9136, loss=0.5946]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 3/292 [00:01<01:37,  2.98batch/s, auc=0.9100, loss=0.6660]\u001b[A\n",
      "Training Epoch 8/25:   1%|▏         | 4/292 [00:01<01:17,  3.71batch/s, auc=0.9100, loss=0.6660]\u001b[A\n",
      "Training Epoch 8/25:   1%|▏         | 4/292 [00:01<01:17,  3.71batch/s, auc=0.9011, loss=0.8056]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 5/292 [00:01<01:05,  4.37batch/s, auc=0.9011, loss=0.8056]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 5/292 [00:01<01:05,  4.37batch/s, auc=0.8877, loss=1.0226]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 6/292 [00:01<00:58,  4.89batch/s, auc=0.8877, loss=1.0226]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 6/292 [00:01<00:58,  4.89batch/s, auc=0.8831, loss=0.7771]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 7/292 [00:01<00:54,  5.26batch/s, auc=0.8831, loss=0.7771]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 7/292 [00:02<00:54,  5.26batch/s, auc=0.8937, loss=0.6709]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 8/292 [00:02<00:50,  5.57batch/s, auc=0.8937, loss=0.6709]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 8/292 [00:02<00:50,  5.57batch/s, auc=0.8903, loss=0.8543]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 9/292 [00:02<00:48,  5.81batch/s, auc=0.8903, loss=0.8543]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 9/292 [00:02<00:48,  5.81batch/s, auc=0.8986, loss=0.5843]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 10/292 [00:02<00:47,  5.97batch/s, auc=0.8986, loss=0.5843]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 10/292 [00:02<00:47,  5.97batch/s, auc=0.9030, loss=0.4688]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 11/292 [00:02<00:46,  5.99batch/s, auc=0.9030, loss=0.4688]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 11/292 [00:02<00:46,  5.99batch/s, auc=0.9019, loss=0.6362]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 12/292 [00:02<00:45,  6.10batch/s, auc=0.9019, loss=0.6362]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 12/292 [00:02<00:45,  6.10batch/s, auc=0.9025, loss=0.6072]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 13/292 [00:02<00:45,  6.18batch/s, auc=0.9025, loss=0.6072]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 13/292 [00:03<00:45,  6.18batch/s, auc=0.8975, loss=0.9935]\u001b[A\n",
      "Training Epoch 8/25:   5%|▍         | 14/292 [00:03<00:44,  6.24batch/s, auc=0.8975, loss=0.9935]\u001b[A\n",
      "Training Epoch 8/25:   5%|▍         | 14/292 [00:03<00:44,  6.24batch/s, auc=0.8996, loss=0.7035]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 15/292 [00:03<00:44,  6.28batch/s, auc=0.8996, loss=0.7035]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 15/292 [00:03<00:44,  6.28batch/s, auc=0.9024, loss=0.5829]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 16/292 [00:03<00:44,  6.18batch/s, auc=0.9024, loss=0.5829]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 16/292 [00:03<00:44,  6.18batch/s, auc=0.9050, loss=0.4852]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 17/292 [00:03<00:44,  6.18batch/s, auc=0.9050, loss=0.4852]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 17/292 [00:03<00:44,  6.18batch/s, auc=0.9047, loss=0.6815]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 18/292 [00:03<00:44,  6.20batch/s, auc=0.9047, loss=0.6815]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 18/292 [00:03<00:44,  6.20batch/s, auc=0.9048, loss=0.5138]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 19/292 [00:03<00:43,  6.25batch/s, auc=0.9048, loss=0.5138]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 19/292 [00:03<00:43,  6.25batch/s, auc=0.9065, loss=0.4913]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 20/292 [00:03<00:43,  6.29batch/s, auc=0.9065, loss=0.4913]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 20/292 [00:04<00:43,  6.29batch/s, auc=0.9099, loss=0.5175]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9099, loss=0.5175]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9102, loss=0.7612]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 22/292 [00:04<00:42,  6.33batch/s, auc=0.9102, loss=0.7612]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 22/292 [00:04<00:42,  6.33batch/s, auc=0.9124, loss=0.6178]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9124, loss=0.6178]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9121, loss=0.6435]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9121, loss=0.6435]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9131, loss=0.5532]\u001b[A\n",
      "Training Epoch 8/25:   9%|▊         | 25/292 [00:04<00:42,  6.31batch/s, auc=0.9131, loss=0.5532]\u001b[A\n",
      "Training Epoch 8/25:   9%|▊         | 25/292 [00:04<00:42,  6.31batch/s, auc=0.9137, loss=0.6076]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 26/292 [00:04<00:42,  6.32batch/s, auc=0.9137, loss=0.6076]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 26/292 [00:05<00:42,  6.32batch/s, auc=0.9132, loss=0.7447]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9132, loss=0.7447]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9129, loss=0.6242]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9129, loss=0.6242]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9134, loss=0.5285]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 29/292 [00:05<00:41,  6.28batch/s, auc=0.9134, loss=0.5285]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 29/292 [00:05<00:41,  6.28batch/s, auc=0.9141, loss=0.6163]\u001b[A\n",
      "Training Epoch 8/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9141, loss=0.6163]\u001b[A\n",
      "Training Epoch 8/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9110, loss=0.8909]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9110, loss=0.8909]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9113, loss=0.5643]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 32/292 [00:05<00:41,  6.30batch/s, auc=0.9113, loss=0.5643]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 32/292 [00:06<00:41,  6.30batch/s, auc=0.9111, loss=0.6645]\u001b[A\n",
      "Training Epoch 8/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9111, loss=0.6645]\u001b[A\n",
      "Training Epoch 8/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9094, loss=0.7586]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9094, loss=0.7586]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9118, loss=0.5169]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9118, loss=0.5169]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9122, loss=0.6073]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9122, loss=0.6073]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9075, loss=1.4075]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9075, loss=1.4075]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9063, loss=1.0028]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9063, loss=1.0028]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9078, loss=0.5564]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 39/292 [00:06<00:40,  6.30batch/s, auc=0.9078, loss=0.5564]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 39/292 [00:07<00:40,  6.30batch/s, auc=0.9092, loss=0.4729]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▎        | 40/292 [00:07<00:40,  6.29batch/s, auc=0.9092, loss=0.4729]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▎        | 40/292 [00:07<00:40,  6.29batch/s, auc=0.9088, loss=0.5673]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9088, loss=0.5673]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9085, loss=0.6937]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9085, loss=0.6937]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9094, loss=0.4989]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▍        | 43/292 [00:07<00:39,  6.27batch/s, auc=0.9094, loss=0.4989]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▍        | 43/292 [00:07<00:39,  6.27batch/s, auc=0.9090, loss=0.6729]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9090, loss=0.6729]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9077, loss=0.7419]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 45/292 [00:07<00:39,  6.30batch/s, auc=0.9077, loss=0.7419]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9057, loss=0.9632]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9057, loss=0.9632]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9049, loss=0.7874]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 47/292 [00:08<00:39,  6.24batch/s, auc=0.9049, loss=0.7874]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 47/292 [00:08<00:39,  6.24batch/s, auc=0.9059, loss=0.5505]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9059, loss=0.5505]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9067, loss=0.5153]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.9067, loss=0.5153]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.9079, loss=0.4386]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 50/292 [00:08<00:38,  6.21batch/s, auc=0.9079, loss=0.4386]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 50/292 [00:08<00:38,  6.21batch/s, auc=0.9074, loss=0.6455]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 51/292 [00:08<00:38,  6.24batch/s, auc=0.9074, loss=0.6455]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 51/292 [00:09<00:38,  6.24batch/s, auc=0.9083, loss=0.4655]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9083, loss=0.4655]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9074, loss=0.9428]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9074, loss=0.9428]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9071, loss=0.6621]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9071, loss=0.6621]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9072, loss=0.6187]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9072, loss=0.6187]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9081, loss=0.5255]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9081, loss=0.5255]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9084, loss=0.4947]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 57/292 [00:09<00:37,  6.31batch/s, auc=0.9084, loss=0.4947]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 57/292 [00:10<00:37,  6.31batch/s, auc=0.9071, loss=1.0683]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.9071, loss=1.0683]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.9072, loss=0.5772]\u001b[A\n",
      "Training Epoch 8/25:  20%|██        | 59/292 [00:10<00:36,  6.32batch/s, auc=0.9072, loss=0.5772]\u001b[A\n",
      "Training Epoch 8/25:  20%|██        | 59/292 [00:10<00:36,  6.32batch/s, auc=0.9088, loss=0.3362]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.9088, loss=0.3362]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 60/292 [00:10<00:36,  6.32batch/s, auc=0.9093, loss=0.4969]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9093, loss=0.4969]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 61/292 [00:10<00:36,  6.31batch/s, auc=0.9102, loss=0.4828]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 62/292 [00:10<00:36,  6.31batch/s, auc=0.9102, loss=0.4828]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 62/292 [00:10<00:36,  6.31batch/s, auc=0.9091, loss=0.8845]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 63/292 [00:10<00:36,  6.25batch/s, auc=0.9091, loss=0.8845]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 63/292 [00:10<00:36,  6.25batch/s, auc=0.9099, loss=0.5794]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 64/292 [00:10<00:36,  6.26batch/s, auc=0.9099, loss=0.5794]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 64/292 [00:11<00:36,  6.26batch/s, auc=0.9085, loss=1.1403]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 65/292 [00:11<00:36,  6.14batch/s, auc=0.9085, loss=1.1403]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 65/292 [00:11<00:36,  6.14batch/s, auc=0.9080, loss=0.7233]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 66/292 [00:11<00:36,  6.18batch/s, auc=0.9080, loss=0.7233]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 66/292 [00:11<00:36,  6.18batch/s, auc=0.9081, loss=0.5889]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 67/292 [00:11<00:36,  6.22batch/s, auc=0.9081, loss=0.5889]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 67/292 [00:11<00:36,  6.22batch/s, auc=0.9076, loss=0.6410]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 68/292 [00:11<00:35,  6.24batch/s, auc=0.9076, loss=0.6410]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 68/292 [00:11<00:35,  6.24batch/s, auc=0.9064, loss=0.9596]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▎       | 69/292 [00:11<00:35,  6.25batch/s, auc=0.9064, loss=0.9596]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▎       | 69/292 [00:11<00:35,  6.25batch/s, auc=0.9064, loss=0.6871]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 70/292 [00:11<00:35,  6.25batch/s, auc=0.9064, loss=0.6871]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 70/292 [00:12<00:35,  6.25batch/s, auc=0.9063, loss=0.6086]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 71/292 [00:12<00:35,  6.24batch/s, auc=0.9063, loss=0.6086]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 71/292 [00:12<00:35,  6.24batch/s, auc=0.9068, loss=0.4956]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▍       | 72/292 [00:12<00:35,  6.26batch/s, auc=0.9068, loss=0.4956]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▍       | 72/292 [00:12<00:35,  6.26batch/s, auc=0.9062, loss=0.6871]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9062, loss=0.6871]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9066, loss=0.5677]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9066, loss=0.5677]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9055, loss=0.9412]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9055, loss=0.9412]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9064, loss=0.4757]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 76/292 [00:12<00:34,  6.28batch/s, auc=0.9064, loss=0.4757]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 76/292 [00:13<00:34,  6.28batch/s, auc=0.9066, loss=0.5874]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▋       | 77/292 [00:13<00:34,  6.28batch/s, auc=0.9066, loss=0.5874]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▋       | 77/292 [00:13<00:34,  6.28batch/s, auc=0.9065, loss=0.5634]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9065, loss=0.5634]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9066, loss=0.5612]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9066, loss=0.5612]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9073, loss=0.4851]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9073, loss=0.4851]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9072, loss=0.5647]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 81/292 [00:13<00:33,  6.24batch/s, auc=0.9072, loss=0.5647]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 81/292 [00:13<00:33,  6.24batch/s, auc=0.9077, loss=0.5504]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 82/292 [00:13<00:33,  6.24batch/s, auc=0.9077, loss=0.5504]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 82/292 [00:14<00:33,  6.24batch/s, auc=0.9072, loss=0.7485]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9072, loss=0.7485]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9073, loss=0.5169]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9073, loss=0.5169]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9075, loss=0.6102]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9075, loss=0.6102]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9077, loss=0.5947]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 86/292 [00:14<00:32,  6.24batch/s, auc=0.9077, loss=0.5947]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 86/292 [00:14<00:32,  6.24batch/s, auc=0.9075, loss=0.6980]\u001b[A\n",
      "Training Epoch 8/25:  30%|██▉       | 87/292 [00:14<00:33,  6.19batch/s, auc=0.9075, loss=0.6980]\u001b[A\n",
      "Training Epoch 8/25:  30%|██▉       | 87/292 [00:14<00:33,  6.19batch/s, auc=0.9074, loss=0.7913]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 88/292 [00:14<00:32,  6.21batch/s, auc=0.9074, loss=0.7913]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 88/292 [00:14<00:32,  6.21batch/s, auc=0.9069, loss=0.7687]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 89/292 [00:14<00:32,  6.21batch/s, auc=0.9069, loss=0.7687]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 89/292 [00:15<00:32,  6.21batch/s, auc=0.9075, loss=0.3822]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 90/292 [00:15<00:32,  6.21batch/s, auc=0.9075, loss=0.3822]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 90/292 [00:15<00:32,  6.21batch/s, auc=0.9077, loss=0.7034]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9077, loss=0.7034]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9069, loss=0.9164]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9069, loss=0.9164]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9072, loss=0.4424]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9072, loss=0.4424]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9073, loss=0.5005]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 94/292 [00:15<00:31,  6.22batch/s, auc=0.9073, loss=0.5005]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 94/292 [00:15<00:31,  6.22batch/s, auc=0.9071, loss=0.7837]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 95/292 [00:15<00:31,  6.16batch/s, auc=0.9071, loss=0.7837]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 95/292 [00:16<00:31,  6.16batch/s, auc=0.9064, loss=0.7642]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 96/292 [00:16<00:31,  6.17batch/s, auc=0.9064, loss=0.7642]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 96/292 [00:16<00:31,  6.17batch/s, auc=0.9064, loss=0.9388]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 97/292 [00:16<00:31,  6.13batch/s, auc=0.9064, loss=0.9388]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 97/292 [00:16<00:31,  6.13batch/s, auc=0.9060, loss=0.7176]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▎      | 98/292 [00:16<00:31,  6.15batch/s, auc=0.9060, loss=0.7176]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▎      | 98/292 [00:16<00:31,  6.15batch/s, auc=0.9060, loss=0.7786]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 99/292 [00:16<00:31,  6.17batch/s, auc=0.9060, loss=0.7786]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 99/292 [00:16<00:31,  6.17batch/s, auc=0.9063, loss=0.4579]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 100/292 [00:16<00:30,  6.19batch/s, auc=0.9063, loss=0.4579]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 100/292 [00:16<00:30,  6.19batch/s, auc=0.9062, loss=0.8069]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 101/292 [00:16<00:30,  6.21batch/s, auc=0.9062, loss=0.8069]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 101/292 [00:17<00:30,  6.21batch/s, auc=0.9062, loss=0.7979]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9062, loss=0.7979]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9065, loss=0.4850]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▌      | 103/292 [00:17<00:30,  6.17batch/s, auc=0.9065, loss=0.4850]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▌      | 103/292 [00:17<00:30,  6.17batch/s, auc=0.9068, loss=0.6424]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9068, loss=0.6424]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9061, loss=0.9881]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9061, loss=0.9881]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9052, loss=1.1231]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9052, loss=1.1231]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9057, loss=0.4744]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 107/292 [00:17<00:29,  6.23batch/s, auc=0.9057, loss=0.4744]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 107/292 [00:18<00:29,  6.23batch/s, auc=0.9065, loss=0.4096]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9065, loss=0.4096]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9071, loss=0.5803]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9071, loss=0.5803]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9074, loss=0.5629]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 110/292 [00:18<00:29,  6.24batch/s, auc=0.9074, loss=0.5629]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 110/292 [00:18<00:29,  6.24batch/s, auc=0.9081, loss=0.4472]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 111/292 [00:18<00:29,  6.15batch/s, auc=0.9081, loss=0.4472]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 111/292 [00:18<00:29,  6.15batch/s, auc=0.9085, loss=0.6971]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9085, loss=0.6971]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9077, loss=0.7214]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▊      | 113/292 [00:18<00:28,  6.17batch/s, auc=0.9077, loss=0.7214]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▊      | 113/292 [00:19<00:28,  6.17batch/s, auc=0.9078, loss=0.5303]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 114/292 [00:19<00:28,  6.18batch/s, auc=0.9078, loss=0.5303]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 114/292 [00:19<00:28,  6.18batch/s, auc=0.9080, loss=0.6125]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 115/292 [00:19<00:28,  6.16batch/s, auc=0.9080, loss=0.6125]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 115/292 [00:19<00:28,  6.16batch/s, auc=0.9068, loss=0.9935]\u001b[A\n",
      "Training Epoch 8/25:  40%|███▉      | 116/292 [00:19<00:28,  6.17batch/s, auc=0.9068, loss=0.9935]\u001b[A\n",
      "Training Epoch 8/25:  40%|███▉      | 116/292 [00:19<00:28,  6.17batch/s, auc=0.9075, loss=0.4744]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 117/292 [00:19<00:28,  6.18batch/s, auc=0.9075, loss=0.4744]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 117/292 [00:19<00:28,  6.18batch/s, auc=0.9074, loss=0.6680]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 118/292 [00:19<00:28,  6.17batch/s, auc=0.9074, loss=0.6680]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 118/292 [00:19<00:28,  6.17batch/s, auc=0.9073, loss=0.6090]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 119/292 [00:19<00:28,  6.18batch/s, auc=0.9073, loss=0.6090]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 119/292 [00:19<00:28,  6.18batch/s, auc=0.9077, loss=0.4777]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 120/292 [00:19<00:28,  6.12batch/s, auc=0.9077, loss=0.4777]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 120/292 [00:20<00:28,  6.12batch/s, auc=0.9072, loss=0.9274]\u001b[A\n",
      "Training Epoch 8/25:  41%|████▏     | 121/292 [00:20<00:27,  6.14batch/s, auc=0.9072, loss=0.9274]\u001b[A\n",
      "Training Epoch 8/25:  41%|████▏     | 121/292 [00:20<00:27,  6.14batch/s, auc=0.9076, loss=0.4971]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 122/292 [00:20<00:27,  6.16batch/s, auc=0.9076, loss=0.4971]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 122/292 [00:20<00:27,  6.16batch/s, auc=0.9079, loss=0.5692]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 123/292 [00:20<00:27,  6.18batch/s, auc=0.9079, loss=0.5692]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 123/292 [00:20<00:27,  6.18batch/s, auc=0.9077, loss=0.6144]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9077, loss=0.6144]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9084, loss=0.3821]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 125/292 [00:20<00:27,  6.18batch/s, auc=0.9084, loss=0.3821]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 125/292 [00:20<00:27,  6.18batch/s, auc=0.9089, loss=0.4313]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 126/292 [00:20<00:26,  6.18batch/s, auc=0.9089, loss=0.4313]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 126/292 [00:21<00:26,  6.18batch/s, auc=0.9088, loss=0.7491]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 127/292 [00:21<00:27,  6.11batch/s, auc=0.9088, loss=0.7491]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 127/292 [00:21<00:27,  6.11batch/s, auc=0.9091, loss=0.5058]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 128/292 [00:21<00:26,  6.12batch/s, auc=0.9091, loss=0.5058]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 128/292 [00:21<00:26,  6.12batch/s, auc=0.9095, loss=0.5324]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 129/292 [00:21<00:26,  6.13batch/s, auc=0.9095, loss=0.5324]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 129/292 [00:21<00:26,  6.13batch/s, auc=0.9093, loss=0.6666]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 130/292 [00:21<00:26,  6.13batch/s, auc=0.9093, loss=0.6666]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 130/292 [00:21<00:26,  6.13batch/s, auc=0.9090, loss=0.8054]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 131/292 [00:21<00:26,  6.15batch/s, auc=0.9090, loss=0.8054]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 131/292 [00:21<00:26,  6.15batch/s, auc=0.9093, loss=0.4392]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▌     | 132/292 [00:21<00:25,  6.16batch/s, auc=0.9093, loss=0.4392]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9093, loss=0.6528]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9093, loss=0.6528]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9094, loss=0.5818]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 134/292 [00:22<00:25,  6.14batch/s, auc=0.9094, loss=0.5818]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 134/292 [00:22<00:25,  6.14batch/s, auc=0.9093, loss=0.8603]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 135/292 [00:22<00:25,  6.15batch/s, auc=0.9093, loss=0.8603]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 135/292 [00:22<00:25,  6.15batch/s, auc=0.9099, loss=0.3941]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 136/292 [00:22<00:25,  6.14batch/s, auc=0.9099, loss=0.3941]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 136/292 [00:22<00:25,  6.14batch/s, auc=0.9086, loss=1.3714]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9086, loss=1.3714]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9087, loss=0.6226]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 138/292 [00:22<00:25,  6.08batch/s, auc=0.9087, loss=0.6226]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 138/292 [00:23<00:25,  6.08batch/s, auc=0.9083, loss=0.7951]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9083, loss=0.7951]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9081, loss=0.6664]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 140/292 [00:23<00:24,  6.12batch/s, auc=0.9081, loss=0.6664]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 140/292 [00:23<00:24,  6.12batch/s, auc=0.9081, loss=0.5013]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9081, loss=0.5013]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9075, loss=0.8636]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▊     | 142/292 [00:23<00:24,  6.12batch/s, auc=0.9075, loss=0.8636]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▊     | 142/292 [00:23<00:24,  6.12batch/s, auc=0.9079, loss=0.3802]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 143/292 [00:23<00:24,  6.14batch/s, auc=0.9079, loss=0.3802]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 143/292 [00:23<00:24,  6.14batch/s, auc=0.9082, loss=0.5232]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 144/292 [00:23<00:24,  6.14batch/s, auc=0.9082, loss=0.5232]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 144/292 [00:24<00:24,  6.14batch/s, auc=0.9084, loss=0.5092]\u001b[A\n",
      "Training Epoch 8/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9084, loss=0.5092]\u001b[A\n",
      "Training Epoch 8/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9086, loss=0.4983]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 146/292 [00:24<00:23,  6.14batch/s, auc=0.9086, loss=0.4983]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 146/292 [00:24<00:23,  6.14batch/s, auc=0.9088, loss=0.5628]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 147/292 [00:24<00:23,  6.10batch/s, auc=0.9088, loss=0.5628]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 147/292 [00:24<00:23,  6.10batch/s, auc=0.9085, loss=0.8849]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 148/292 [00:24<00:23,  6.12batch/s, auc=0.9085, loss=0.8849]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 148/292 [00:24<00:23,  6.12batch/s, auc=0.9089, loss=0.5379]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 149/292 [00:24<00:23,  6.13batch/s, auc=0.9089, loss=0.5379]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 149/292 [00:24<00:23,  6.13batch/s, auc=0.9088, loss=0.6339]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.11batch/s, auc=0.9088, loss=0.6339]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.11batch/s, auc=0.9090, loss=0.4318]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.13batch/s, auc=0.9090, loss=0.4318]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.13batch/s, auc=0.9094, loss=0.4275]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9094, loss=0.4275]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9095, loss=0.5523]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9095, loss=0.5523]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9093, loss=0.9143]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.09batch/s, auc=0.9093, loss=0.9143]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.09batch/s, auc=0.9092, loss=0.6202]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.10batch/s, auc=0.9092, loss=0.6202]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.10batch/s, auc=0.9092, loss=0.6167]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.10batch/s, auc=0.9092, loss=0.6167]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.10batch/s, auc=0.9095, loss=0.4260]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.09batch/s, auc=0.9095, loss=0.4260]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.09batch/s, auc=0.9099, loss=0.4227]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 158/292 [00:26<00:22,  6.09batch/s, auc=0.9099, loss=0.4227]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 158/292 [00:26<00:22,  6.09batch/s, auc=0.9100, loss=0.5618]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.06batch/s, auc=0.9100, loss=0.5618]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.06batch/s, auc=0.9100, loss=0.6460]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.07batch/s, auc=0.9100, loss=0.6460]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.07batch/s, auc=0.9099, loss=0.7825]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.09batch/s, auc=0.9099, loss=0.7825]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.09batch/s, auc=0.9101, loss=0.6955]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.08batch/s, auc=0.9101, loss=0.6955]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.08batch/s, auc=0.9103, loss=0.4366]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9103, loss=0.4366]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9103, loss=0.5528]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9103, loss=0.5528]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9104, loss=0.5292]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9104, loss=0.5292]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9102, loss=0.8889]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.10batch/s, auc=0.9102, loss=0.8889]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.10batch/s, auc=0.9101, loss=0.6358]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9101, loss=0.6358]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9101, loss=0.4696]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.11batch/s, auc=0.9101, loss=0.4696]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.11batch/s, auc=0.9104, loss=0.4037]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.11batch/s, auc=0.9104, loss=0.4037]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.11batch/s, auc=0.9106, loss=0.5691]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9106, loss=0.5691]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9108, loss=0.4842]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9108, loss=0.4842]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9108, loss=0.5209]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.13batch/s, auc=0.9108, loss=0.5209]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.13batch/s, auc=0.9107, loss=0.6713]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.13batch/s, auc=0.9107, loss=0.6713]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.13batch/s, auc=0.9111, loss=0.4000]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.13batch/s, auc=0.9111, loss=0.4000]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.13batch/s, auc=0.9115, loss=0.4465]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.13batch/s, auc=0.9115, loss=0.4465]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.13batch/s, auc=0.9107, loss=1.2539]\u001b[A\n",
      "Training Epoch 8/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9107, loss=1.2539]\u001b[A\n",
      "Training Epoch 8/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9112, loss=0.3164]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 177/292 [00:29<00:18,  6.11batch/s, auc=0.9112, loss=0.3164]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 177/292 [00:29<00:18,  6.11batch/s, auc=0.9114, loss=0.3566]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9114, loss=0.3566]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9115, loss=0.6763]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9115, loss=0.6763]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9112, loss=0.8312]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.08batch/s, auc=0.9112, loss=0.8312]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.08batch/s, auc=0.9112, loss=0.5809]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.10batch/s, auc=0.9112, loss=0.5809]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.10batch/s, auc=0.9111, loss=0.5893]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9111, loss=0.5893]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9110, loss=0.5902]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9110, loss=0.5902]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9107, loss=0.8534]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9107, loss=0.8534]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9109, loss=0.4543]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.09batch/s, auc=0.9109, loss=0.4543]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.09batch/s, auc=0.9110, loss=0.5231]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.08batch/s, auc=0.9110, loss=0.5231]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.08batch/s, auc=0.9107, loss=0.6404]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.09batch/s, auc=0.9107, loss=0.6404]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.09batch/s, auc=0.9111, loss=0.3821]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.08batch/s, auc=0.9111, loss=0.3821]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.08batch/s, auc=0.9111, loss=0.5518]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.08batch/s, auc=0.9111, loss=0.5518]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.08batch/s, auc=0.9111, loss=0.6074]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.08batch/s, auc=0.9111, loss=0.6074]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.08batch/s, auc=0.9113, loss=0.4884]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9113, loss=0.4884]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9114, loss=0.6966]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.08batch/s, auc=0.9114, loss=0.6966]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.08batch/s, auc=0.9114, loss=0.5880]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.08batch/s, auc=0.9114, loss=0.5880]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.9110, loss=0.7720]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9110, loss=0.7720]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9108, loss=0.8858]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9108, loss=0.8858]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9106, loss=0.8786]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.08batch/s, auc=0.9106, loss=0.8786]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.08batch/s, auc=0.9106, loss=0.6015]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9106, loss=0.6015]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9110, loss=0.4047]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.82batch/s, auc=0.9110, loss=0.4047]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.82batch/s, auc=0.9111, loss=0.7085]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 199/292 [00:32<00:16,  5.80batch/s, auc=0.9111, loss=0.7085]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.80batch/s, auc=0.9115, loss=0.3921]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.76batch/s, auc=0.9115, loss=0.3921]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.76batch/s, auc=0.9113, loss=0.7485]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.74batch/s, auc=0.9113, loss=0.7485]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.74batch/s, auc=0.9112, loss=0.6226]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.72batch/s, auc=0.9112, loss=0.6226]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.72batch/s, auc=0.9109, loss=1.0770]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.71batch/s, auc=0.9109, loss=1.0770]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.71batch/s, auc=0.9109, loss=0.6954]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.68batch/s, auc=0.9109, loss=0.6954]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9108, loss=0.6651]\u001b[A\n",
      "Training Epoch 8/25:  70%|███████   | 205/292 [00:34<00:15,  5.67batch/s, auc=0.9108, loss=0.6651]\u001b[A\n",
      "Training Epoch 8/25:  70%|███████   | 205/292 [00:34<00:15,  5.67batch/s, auc=0.9110, loss=0.5118]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 206/292 [00:34<00:15,  5.67batch/s, auc=0.9110, loss=0.5118]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 206/292 [00:34<00:15,  5.67batch/s, auc=0.9109, loss=0.6870]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 207/292 [00:34<00:14,  5.68batch/s, auc=0.9109, loss=0.6870]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 207/292 [00:34<00:14,  5.68batch/s, auc=0.9111, loss=0.4132]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 208/292 [00:34<00:14,  5.68batch/s, auc=0.9111, loss=0.4132]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 208/292 [00:34<00:14,  5.68batch/s, auc=0.9111, loss=0.5823]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.67batch/s, auc=0.9111, loss=0.5823]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.67batch/s, auc=0.9109, loss=0.4777]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.67batch/s, auc=0.9109, loss=0.4777]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.67batch/s, auc=0.9112, loss=0.4501]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.67batch/s, auc=0.9112, loss=0.4501]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.67batch/s, auc=0.9116, loss=0.3438]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.69batch/s, auc=0.9116, loss=0.3438]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.69batch/s, auc=0.9121, loss=0.3836]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.67batch/s, auc=0.9121, loss=0.3836]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.67batch/s, auc=0.9119, loss=0.7740]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.67batch/s, auc=0.9119, loss=0.7740]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.67batch/s, auc=0.9122, loss=0.5259]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.65batch/s, auc=0.9122, loss=0.5259]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.65batch/s, auc=0.9121, loss=0.6003]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.64batch/s, auc=0.9121, loss=0.6003]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.64batch/s, auc=0.9121, loss=0.7504]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.64batch/s, auc=0.9121, loss=0.7504]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.64batch/s, auc=0.9121, loss=0.6733]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.66batch/s, auc=0.9121, loss=0.6733]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.66batch/s, auc=0.9121, loss=0.6036]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.65batch/s, auc=0.9121, loss=0.6036]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.65batch/s, auc=0.9123, loss=0.4984]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.63batch/s, auc=0.9123, loss=0.4984]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.63batch/s, auc=0.9125, loss=0.4745]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.57batch/s, auc=0.9125, loss=0.4745]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.57batch/s, auc=0.9126, loss=0.5558]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.59batch/s, auc=0.9126, loss=0.5558]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.59batch/s, auc=0.9128, loss=0.4651]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.61batch/s, auc=0.9128, loss=0.4651]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.61batch/s, auc=0.9128, loss=0.7683]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.72batch/s, auc=0.9128, loss=0.7683]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.72batch/s, auc=0.9130, loss=0.4410]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.80batch/s, auc=0.9130, loss=0.4410]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.80batch/s, auc=0.9128, loss=0.9631]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.89batch/s, auc=0.9128, loss=0.9631]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.89batch/s, auc=0.9128, loss=0.6421]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.94batch/s, auc=0.9128, loss=0.6421]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 227/292 [00:38<00:10,  5.94batch/s, auc=0.9129, loss=0.5728]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.96batch/s, auc=0.9129, loss=0.5728]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.96batch/s, auc=0.9131, loss=0.4150]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.98batch/s, auc=0.9131, loss=0.4150]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.98batch/s, auc=0.9123, loss=1.6226]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.90batch/s, auc=0.9123, loss=1.6226]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.90batch/s, auc=0.9124, loss=0.6265]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.70batch/s, auc=0.9124, loss=0.6265]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.70batch/s, auc=0.9127, loss=0.4948]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.69batch/s, auc=0.9127, loss=0.4948]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.69batch/s, auc=0.9121, loss=1.2211]\u001b[A\n",
      "Training Epoch 8/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.75batch/s, auc=0.9121, loss=1.2211]\u001b[A\n",
      "Training Epoch 8/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.75batch/s, auc=0.9122, loss=0.5246]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 234/292 [00:39<00:10,  5.73batch/s, auc=0.9122, loss=0.5246]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 234/292 [00:39<00:10,  5.73batch/s, auc=0.9120, loss=0.6499]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 235/292 [00:39<00:09,  5.82batch/s, auc=0.9120, loss=0.6499]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 235/292 [00:39<00:09,  5.82batch/s, auc=0.9120, loss=0.6540]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 236/292 [00:39<00:09,  5.74batch/s, auc=0.9120, loss=0.6540]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 236/292 [00:39<00:09,  5.74batch/s, auc=0.9118, loss=0.7257]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 237/292 [00:39<00:09,  5.71batch/s, auc=0.9118, loss=0.7257]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 237/292 [00:39<00:09,  5.71batch/s, auc=0.9120, loss=0.4674]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.68batch/s, auc=0.9120, loss=0.4674]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.68batch/s, auc=0.9121, loss=0.4829]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.65batch/s, auc=0.9121, loss=0.4829]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.65batch/s, auc=0.9119, loss=0.9655]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.74batch/s, auc=0.9119, loss=0.9655]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.74batch/s, auc=0.9116, loss=0.8121]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.60batch/s, auc=0.9116, loss=0.8121]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.60batch/s, auc=0.9114, loss=0.9228]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.56batch/s, auc=0.9114, loss=0.9228]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.56batch/s, auc=0.9115, loss=0.4913]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.68batch/s, auc=0.9115, loss=0.4913]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.68batch/s, auc=0.9119, loss=0.5067]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.64batch/s, auc=0.9119, loss=0.5067]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.64batch/s, auc=0.9120, loss=0.5088]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.58batch/s, auc=0.9120, loss=0.5088]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.58batch/s, auc=0.9123, loss=0.5051]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.58batch/s, auc=0.9123, loss=0.5051]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.58batch/s, auc=0.9123, loss=0.5996]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.59batch/s, auc=0.9123, loss=0.5996]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.59batch/s, auc=0.9125, loss=0.4974]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.60batch/s, auc=0.9125, loss=0.4974]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.60batch/s, auc=0.9124, loss=0.6929]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.61batch/s, auc=0.9124, loss=0.6929]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.61batch/s, auc=0.9123, loss=0.6622]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.60batch/s, auc=0.9123, loss=0.6622]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.60batch/s, auc=0.9122, loss=0.7317]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9122, loss=0.7317]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9119, loss=0.9068]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9119, loss=0.9068]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9120, loss=0.5831]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.59batch/s, auc=0.9120, loss=0.5831]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.59batch/s, auc=0.9120, loss=0.5328]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.59batch/s, auc=0.9120, loss=0.5328]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.59batch/s, auc=0.9119, loss=0.6851]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.72batch/s, auc=0.9119, loss=0.6851]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.72batch/s, auc=0.9118, loss=0.5384]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.74batch/s, auc=0.9118, loss=0.5384]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.74batch/s, auc=0.9117, loss=0.7959]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.82batch/s, auc=0.9117, loss=0.7959]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.82batch/s, auc=0.9118, loss=0.4415]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.87batch/s, auc=0.9118, loss=0.4415]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.87batch/s, auc=0.9116, loss=0.8398]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.92batch/s, auc=0.9116, loss=0.8398]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.92batch/s, auc=0.9114, loss=0.8245]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.94batch/s, auc=0.9114, loss=0.8245]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.94batch/s, auc=0.9115, loss=0.6287]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.95batch/s, auc=0.9115, loss=0.6287]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.95batch/s, auc=0.9113, loss=0.7493]\u001b[A\n",
      "Training Epoch 8/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.81batch/s, auc=0.9113, loss=0.7493]\u001b[A\n",
      "Training Epoch 8/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.81batch/s, auc=0.9116, loss=0.5186]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 263/292 [00:44<00:05,  5.75batch/s, auc=0.9116, loss=0.5186]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 263/292 [00:44<00:05,  5.75batch/s, auc=0.9116, loss=0.5834]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.9116, loss=0.5834]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.9118, loss=0.4516]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 265/292 [00:44<00:04,  5.67batch/s, auc=0.9118, loss=0.4516]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 265/292 [00:44<00:04,  5.67batch/s, auc=0.9118, loss=0.5874]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 266/292 [00:44<00:04,  5.66batch/s, auc=0.9118, loss=0.5874]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 266/292 [00:44<00:04,  5.66batch/s, auc=0.9117, loss=0.7632]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.66batch/s, auc=0.9117, loss=0.7632]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.66batch/s, auc=0.9116, loss=0.5772]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.76batch/s, auc=0.9116, loss=0.5772]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.76batch/s, auc=0.9116, loss=0.5120]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.82batch/s, auc=0.9116, loss=0.5120]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.82batch/s, auc=0.9116, loss=0.5541]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9116, loss=0.5541]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9117, loss=0.6003]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9117, loss=0.6003]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9119, loss=0.4155]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.57batch/s, auc=0.9119, loss=0.4155]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.57batch/s, auc=0.9118, loss=0.7411]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.68batch/s, auc=0.9118, loss=0.7411]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.68batch/s, auc=0.9119, loss=0.5789]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.55batch/s, auc=0.9119, loss=0.5789]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.55batch/s, auc=0.9118, loss=0.6859]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9118, loss=0.6859]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9119, loss=0.6243]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.55batch/s, auc=0.9119, loss=0.6243]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.55batch/s, auc=0.9118, loss=0.5535]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.57batch/s, auc=0.9118, loss=0.5535]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.57batch/s, auc=0.9119, loss=0.5387]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.59batch/s, auc=0.9119, loss=0.5387]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.59batch/s, auc=0.9118, loss=0.8187]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.58batch/s, auc=0.9118, loss=0.8187]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.58batch/s, auc=0.9117, loss=0.7065]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9117, loss=0.7065]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9114, loss=0.9058]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.56batch/s, auc=0.9114, loss=0.9058]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.56batch/s, auc=0.9115, loss=0.5075]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9115, loss=0.5075]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9111, loss=1.1847]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.59batch/s, auc=0.9111, loss=1.1847]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.59batch/s, auc=0.9110, loss=0.6448]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.57batch/s, auc=0.9110, loss=0.6448]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.57batch/s, auc=0.9112, loss=0.4423]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.58batch/s, auc=0.9112, loss=0.4423]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.58batch/s, auc=0.9112, loss=0.6867]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9112, loss=0.6867]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9112, loss=0.5458]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.79batch/s, auc=0.9112, loss=0.5458]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.79batch/s, auc=0.9109, loss=1.2887]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9109, loss=1.2887]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9109, loss=0.6988]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.91batch/s, auc=0.9109, loss=0.6988]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.91batch/s, auc=0.9109, loss=0.6664]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.94batch/s, auc=0.9109, loss=0.6664]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.94batch/s, auc=0.9111, loss=0.4686]\u001b[A\n",
      "Training Epoch 8/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.97batch/s, auc=0.9111, loss=0.4686]\u001b[A\n",
      "Training Epoch 8/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.97batch/s, auc=0.9112, loss=0.4050]\u001b[A\n",
      "Training Epoch 8/25: 100%|██████████| 292/292 [00:49<00:00,  5.92batch/s, auc=0.9112, loss=0.4050]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25] Train Loss: 0.6422 | Train AUROC: 0.9112 Val Loss: 0.6703 | Val AUROC: 0.8954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|███▏      | 8/25 [07:18<15:35, 55.02s/it]\n",
      "Training Epoch 9/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9578, loss=0.6014]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 1/292 [00:00<04:44,  1.02batch/s, auc=0.9578, loss=0.6014]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 1/292 [00:01<04:44,  1.02batch/s, auc=0.8818, loss=0.8593]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.8818, loss=0.8593]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.9061, loss=0.3425]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 3/292 [00:01<01:41,  2.86batch/s, auc=0.9061, loss=0.3425]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 3/292 [00:01<01:41,  2.86batch/s, auc=0.9085, loss=0.6353]\u001b[A\n",
      "Training Epoch 9/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9085, loss=0.6353]\u001b[A\n",
      "Training Epoch 9/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9097, loss=0.5574]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9097, loss=0.5574]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9149, loss=0.5468]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9149, loss=0.5468]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9054, loss=0.7804]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 7/292 [00:01<00:55,  5.14batch/s, auc=0.9054, loss=0.7804]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 7/292 [00:02<00:55,  5.14batch/s, auc=0.9019, loss=0.7551]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 8/292 [00:02<00:51,  5.48batch/s, auc=0.9019, loss=0.7551]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 8/292 [00:02<00:51,  5.48batch/s, auc=0.9026, loss=0.5774]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 9/292 [00:02<00:49,  5.70batch/s, auc=0.9026, loss=0.5774]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 9/292 [00:02<00:49,  5.70batch/s, auc=0.9088, loss=0.4872]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 10/292 [00:02<00:48,  5.84batch/s, auc=0.9088, loss=0.4872]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 10/292 [00:02<00:48,  5.84batch/s, auc=0.9125, loss=0.5515]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9125, loss=0.5515]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9147, loss=0.6382]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9147, loss=0.6382]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9169, loss=0.5042]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 13/292 [00:02<00:45,  6.11batch/s, auc=0.9169, loss=0.5042]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 13/292 [00:03<00:45,  6.11batch/s, auc=0.9206, loss=0.4470]\u001b[A\n",
      "Training Epoch 9/25:   5%|▍         | 14/292 [00:03<00:44,  6.19batch/s, auc=0.9206, loss=0.4470]\u001b[A\n",
      "Training Epoch 9/25:   5%|▍         | 14/292 [00:03<00:44,  6.19batch/s, auc=0.9157, loss=0.8405]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9157, loss=0.8405]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9151, loss=0.7113]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 16/292 [00:03<00:44,  6.26batch/s, auc=0.9151, loss=0.7113]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 16/292 [00:03<00:44,  6.26batch/s, auc=0.9057, loss=1.1458]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 17/292 [00:03<00:43,  6.29batch/s, auc=0.9057, loss=1.1458]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 17/292 [00:03<00:43,  6.29batch/s, auc=0.9084, loss=0.5335]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.9084, loss=0.5335]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.9048, loss=1.0034]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 19/292 [00:03<00:43,  6.32batch/s, auc=0.9048, loss=1.0034]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 19/292 [00:04<00:43,  6.32batch/s, auc=0.9042, loss=0.6205]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 20/292 [00:04<00:43,  6.25batch/s, auc=0.9042, loss=0.6205]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 20/292 [00:04<00:43,  6.25batch/s, auc=0.9035, loss=0.5890]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9035, loss=0.5890]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9067, loss=0.4763]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 22/292 [00:04<00:42,  6.29batch/s, auc=0.9067, loss=0.4763]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 22/292 [00:04<00:42,  6.29batch/s, auc=0.9073, loss=0.5723]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.9073, loss=0.5723]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.9072, loss=0.5732]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 24/292 [00:04<00:42,  6.30batch/s, auc=0.9072, loss=0.5732]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 24/292 [00:04<00:42,  6.30batch/s, auc=0.9085, loss=0.5486]\u001b[A\n",
      "Training Epoch 9/25:   9%|▊         | 25/292 [00:04<00:42,  6.30batch/s, auc=0.9085, loss=0.5486]\u001b[A\n",
      "Training Epoch 9/25:   9%|▊         | 25/292 [00:04<00:42,  6.30batch/s, auc=0.9072, loss=0.8301]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 26/292 [00:04<00:42,  6.31batch/s, auc=0.9072, loss=0.8301]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 26/292 [00:05<00:42,  6.31batch/s, auc=0.9064, loss=0.6757]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 27/292 [00:05<00:42,  6.31batch/s, auc=0.9064, loss=0.6757]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 27/292 [00:05<00:42,  6.31batch/s, auc=0.9078, loss=0.5421]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9078, loss=0.5421]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9090, loss=0.5550]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 29/292 [00:05<00:41,  6.29batch/s, auc=0.9090, loss=0.5550]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 29/292 [00:05<00:41,  6.29batch/s, auc=0.9104, loss=0.4555]\u001b[A\n",
      "Training Epoch 9/25:  10%|█         | 30/292 [00:05<00:41,  6.28batch/s, auc=0.9104, loss=0.4555]\u001b[A\n",
      "Training Epoch 9/25:  10%|█         | 30/292 [00:05<00:41,  6.28batch/s, auc=0.9120, loss=0.4532]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9120, loss=0.4532]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9118, loss=0.5896]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 32/292 [00:05<00:41,  6.29batch/s, auc=0.9118, loss=0.5896]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 32/292 [00:06<00:41,  6.29batch/s, auc=0.9122, loss=0.5674]\u001b[A\n",
      "Training Epoch 9/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9122, loss=0.5674]\u001b[A\n",
      "Training Epoch 9/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9115, loss=0.8206]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9115, loss=0.8206]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9127, loss=0.5110]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9127, loss=0.5110]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9113, loss=0.7357]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9113, loss=0.7357]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9126, loss=0.5445]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9126, loss=0.5445]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9134, loss=0.4527]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 38/292 [00:06<00:40,  6.27batch/s, auc=0.9134, loss=0.4527]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 38/292 [00:07<00:40,  6.27batch/s, auc=0.9137, loss=0.6284]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9137, loss=0.6284]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9106, loss=1.2368]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▎        | 40/292 [00:07<00:40,  6.27batch/s, auc=0.9106, loss=1.2368]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▎        | 40/292 [00:07<00:40,  6.27batch/s, auc=0.9113, loss=0.5686]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9113, loss=0.5686]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9130, loss=0.4991]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 42/292 [00:07<00:39,  6.29batch/s, auc=0.9130, loss=0.4991]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 42/292 [00:07<00:39,  6.29batch/s, auc=0.9130, loss=0.5109]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9130, loss=0.5109]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9116, loss=1.0555]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.9116, loss=1.0555]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.9122, loss=0.6472]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9122, loss=0.6472]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9131, loss=0.5458]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9131, loss=0.5458]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9129, loss=0.6035]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9129, loss=0.6035]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9140, loss=0.4417]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9140, loss=0.4417]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9146, loss=0.5893]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9146, loss=0.5893]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9139, loss=0.6871]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9139, loss=0.6871]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9137, loss=0.5628]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 51/292 [00:08<00:38,  6.30batch/s, auc=0.9137, loss=0.5628]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 51/292 [00:09<00:38,  6.30batch/s, auc=0.9130, loss=0.7322]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9130, loss=0.7322]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9129, loss=0.5154]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9129, loss=0.5154]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9126, loss=0.6551]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 54/292 [00:09<00:37,  6.28batch/s, auc=0.9126, loss=0.6551]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 54/292 [00:09<00:37,  6.28batch/s, auc=0.9127, loss=0.5280]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9127, loss=0.5280]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9128, loss=0.5599]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9128, loss=0.5599]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9123, loss=0.7250]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 57/292 [00:09<00:37,  6.29batch/s, auc=0.9123, loss=0.7250]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 57/292 [00:10<00:37,  6.29batch/s, auc=0.9139, loss=0.4341]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9139, loss=0.4341]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9134, loss=0.7119]\u001b[A\n",
      "Training Epoch 9/25:  20%|██        | 59/292 [00:10<00:37,  6.22batch/s, auc=0.9134, loss=0.7119]\u001b[A\n",
      "Training Epoch 9/25:  20%|██        | 59/292 [00:10<00:37,  6.22batch/s, auc=0.9132, loss=0.5876]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 60/292 [00:10<00:37,  6.25batch/s, auc=0.9132, loss=0.5876]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 60/292 [00:10<00:37,  6.25batch/s, auc=0.9135, loss=0.5655]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9135, loss=0.5655]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9140, loss=0.5169]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9140, loss=0.5169]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9142, loss=0.6137]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 63/292 [00:10<00:36,  6.26batch/s, auc=0.9142, loss=0.6137]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 63/292 [00:11<00:36,  6.26batch/s, auc=0.9139, loss=0.7214]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 64/292 [00:11<00:36,  6.22batch/s, auc=0.9139, loss=0.7214]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 64/292 [00:11<00:36,  6.22batch/s, auc=0.9144, loss=0.5455]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 65/292 [00:11<00:36,  6.24batch/s, auc=0.9144, loss=0.5455]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 65/292 [00:11<00:36,  6.24batch/s, auc=0.9147, loss=0.5590]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9147, loss=0.5590]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9148, loss=0.5422]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9148, loss=0.5422]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9150, loss=0.4739]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 68/292 [00:11<00:35,  6.29batch/s, auc=0.9150, loss=0.4739]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 68/292 [00:11<00:35,  6.29batch/s, auc=0.9147, loss=0.7924]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9147, loss=0.7924]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9143, loss=0.8382]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 70/292 [00:11<00:35,  6.27batch/s, auc=0.9143, loss=0.8382]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9137, loss=0.5882]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9137, loss=0.5882]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9137, loss=0.6494]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9137, loss=0.6494]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9147, loss=0.3694]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 73/292 [00:12<00:34,  6.30batch/s, auc=0.9147, loss=0.3694]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 73/292 [00:12<00:34,  6.30batch/s, auc=0.9135, loss=0.8452]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9135, loss=0.8452]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9131, loss=0.8448]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9131, loss=0.8448]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9132, loss=0.6738]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 76/292 [00:12<00:34,  6.23batch/s, auc=0.9132, loss=0.6738]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 76/292 [00:13<00:34,  6.23batch/s, auc=0.9131, loss=0.6245]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9131, loss=0.6245]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9130, loss=0.5662]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9130, loss=0.5662]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9128, loss=0.6982]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 79/292 [00:13<00:34,  6.26batch/s, auc=0.9128, loss=0.6982]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 79/292 [00:13<00:34,  6.26batch/s, auc=0.9124, loss=0.8269]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 80/292 [00:13<00:33,  6.27batch/s, auc=0.9124, loss=0.8269]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 80/292 [00:13<00:33,  6.27batch/s, auc=0.9128, loss=0.4461]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 81/292 [00:13<00:33,  6.27batch/s, auc=0.9128, loss=0.4461]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 81/292 [00:13<00:33,  6.27batch/s, auc=0.9125, loss=0.8810]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 82/292 [00:13<00:33,  6.28batch/s, auc=0.9125, loss=0.8810]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 82/292 [00:14<00:33,  6.28batch/s, auc=0.9125, loss=0.5564]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 83/292 [00:14<00:33,  6.26batch/s, auc=0.9125, loss=0.5564]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 83/292 [00:14<00:33,  6.26batch/s, auc=0.9126, loss=0.5939]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 84/292 [00:14<00:33,  6.27batch/s, auc=0.9126, loss=0.5939]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 84/292 [00:14<00:33,  6.27batch/s, auc=0.9125, loss=0.6638]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 85/292 [00:14<00:33,  6.27batch/s, auc=0.9125, loss=0.6638]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 85/292 [00:14<00:33,  6.27batch/s, auc=0.9126, loss=0.6424]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 86/292 [00:14<00:32,  6.27batch/s, auc=0.9126, loss=0.6424]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 86/292 [00:14<00:32,  6.27batch/s, auc=0.9125, loss=0.6396]\u001b[A\n",
      "Training Epoch 9/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.9125, loss=0.6396]\u001b[A\n",
      "Training Epoch 9/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.9128, loss=0.5943]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 88/292 [00:14<00:32,  6.28batch/s, auc=0.9128, loss=0.5943]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 88/292 [00:15<00:32,  6.28batch/s, auc=0.9126, loss=0.9546]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 89/292 [00:15<00:32,  6.28batch/s, auc=0.9126, loss=0.9546]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 89/292 [00:15<00:32,  6.28batch/s, auc=0.9127, loss=0.6728]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 90/292 [00:15<00:32,  6.26batch/s, auc=0.9127, loss=0.6728]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 90/292 [00:15<00:32,  6.26batch/s, auc=0.9127, loss=0.5113]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9127, loss=0.5113]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9126, loss=0.6019]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9126, loss=0.6019]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9123, loss=0.8132]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9123, loss=0.8132]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9126, loss=0.5441]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9126, loss=0.5441]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9126, loss=0.5510]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 95/292 [00:15<00:31,  6.23batch/s, auc=0.9126, loss=0.5510]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 95/292 [00:16<00:31,  6.23batch/s, auc=0.9123, loss=0.6818]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9123, loss=0.6818]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9123, loss=0.7046]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9123, loss=0.7046]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9121, loss=0.6196]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▎      | 98/292 [00:16<00:31,  6.21batch/s, auc=0.9121, loss=0.6196]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▎      | 98/292 [00:16<00:31,  6.21batch/s, auc=0.9121, loss=0.5230]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 99/292 [00:16<00:31,  6.22batch/s, auc=0.9121, loss=0.5230]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 99/292 [00:16<00:31,  6.22batch/s, auc=0.9121, loss=0.6425]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 100/292 [00:16<00:30,  6.22batch/s, auc=0.9121, loss=0.6425]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 100/292 [00:16<00:30,  6.22batch/s, auc=0.9124, loss=0.4726]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 101/292 [00:16<00:30,  6.22batch/s, auc=0.9124, loss=0.4726]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 101/292 [00:17<00:30,  6.22batch/s, auc=0.9121, loss=0.7852]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9121, loss=0.7852]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9119, loss=0.7756]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▌      | 103/292 [00:17<00:30,  6.21batch/s, auc=0.9119, loss=0.7756]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▌      | 103/292 [00:17<00:30,  6.21batch/s, auc=0.9115, loss=0.8314]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 104/292 [00:17<00:30,  6.13batch/s, auc=0.9115, loss=0.8314]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 104/292 [00:17<00:30,  6.13batch/s, auc=0.9112, loss=0.7218]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 105/292 [00:17<00:30,  6.16batch/s, auc=0.9112, loss=0.7218]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 105/292 [00:17<00:30,  6.16batch/s, auc=0.9105, loss=0.8803]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▋      | 106/292 [00:17<00:30,  6.17batch/s, auc=0.9105, loss=0.8803]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▋      | 106/292 [00:17<00:30,  6.17batch/s, auc=0.9106, loss=0.5653]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 107/292 [00:17<00:29,  6.19batch/s, auc=0.9106, loss=0.5653]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 107/292 [00:18<00:29,  6.19batch/s, auc=0.9103, loss=0.6377]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9103, loss=0.6377]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9107, loss=0.5413]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9107, loss=0.5413]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9108, loss=0.5349]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9108, loss=0.5349]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9104, loss=0.7280]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 111/292 [00:18<00:29,  6.22batch/s, auc=0.9104, loss=0.7280]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 111/292 [00:18<00:29,  6.22batch/s, auc=0.9107, loss=0.5828]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 112/292 [00:18<00:29,  6.12batch/s, auc=0.9107, loss=0.5828]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 112/292 [00:18<00:29,  6.12batch/s, auc=0.9106, loss=0.5739]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▊      | 113/292 [00:18<00:29,  6.14batch/s, auc=0.9106, loss=0.5739]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▊      | 113/292 [00:19<00:29,  6.14batch/s, auc=0.9111, loss=0.3856]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9111, loss=0.3856]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9107, loss=0.7689]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 115/292 [00:19<00:28,  6.17batch/s, auc=0.9107, loss=0.7689]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 115/292 [00:19<00:28,  6.17batch/s, auc=0.9106, loss=0.7557]\u001b[A\n",
      "Training Epoch 9/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9106, loss=0.7557]\u001b[A\n",
      "Training Epoch 9/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9107, loss=0.5935]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9107, loss=0.5935]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9110, loss=0.5784]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9110, loss=0.5784]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9112, loss=0.5867]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 119/292 [00:19<00:27,  6.20batch/s, auc=0.9112, loss=0.5867]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 119/292 [00:20<00:27,  6.20batch/s, auc=0.9117, loss=0.5199]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9117, loss=0.5199]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9119, loss=0.5555]\u001b[A\n",
      "Training Epoch 9/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9119, loss=0.5555]\u001b[A\n",
      "Training Epoch 9/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9122, loss=0.5081]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 122/292 [00:20<00:27,  6.20batch/s, auc=0.9122, loss=0.5081]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 122/292 [00:20<00:27,  6.20batch/s, auc=0.9128, loss=0.4704]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 123/292 [00:20<00:27,  6.20batch/s, auc=0.9128, loss=0.4704]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 123/292 [00:20<00:27,  6.20batch/s, auc=0.9128, loss=0.4993]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9128, loss=0.4993]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9126, loss=0.7875]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9126, loss=0.7875]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9127, loss=0.5694]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 126/292 [00:20<00:26,  6.18batch/s, auc=0.9127, loss=0.5694]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 126/292 [00:21<00:26,  6.18batch/s, auc=0.9122, loss=0.9580]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 127/292 [00:21<00:26,  6.18batch/s, auc=0.9122, loss=0.9580]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 127/292 [00:21<00:26,  6.18batch/s, auc=0.9118, loss=0.7932]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 128/292 [00:21<00:26,  6.15batch/s, auc=0.9118, loss=0.7932]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 128/292 [00:21<00:26,  6.15batch/s, auc=0.9122, loss=0.3961]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 129/292 [00:21<00:26,  6.15batch/s, auc=0.9122, loss=0.3961]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 129/292 [00:21<00:26,  6.15batch/s, auc=0.9124, loss=0.4628]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 130/292 [00:21<00:26,  6.17batch/s, auc=0.9124, loss=0.4628]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 130/292 [00:21<00:26,  6.17batch/s, auc=0.9124, loss=0.5583]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9124, loss=0.5583]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9130, loss=0.4454]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▌     | 132/292 [00:21<00:25,  6.18batch/s, auc=0.9130, loss=0.4454]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▌     | 132/292 [00:22<00:25,  6.18batch/s, auc=0.9128, loss=0.6655]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9128, loss=0.6655]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9130, loss=0.4846]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9130, loss=0.4846]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9122, loss=1.1225]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9122, loss=1.1225]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9126, loss=0.4644]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9126, loss=0.4644]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9127, loss=0.5390]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 137/292 [00:22<00:25,  6.16batch/s, auc=0.9127, loss=0.5390]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 137/292 [00:22<00:25,  6.16batch/s, auc=0.9126, loss=0.7270]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 138/292 [00:22<00:25,  6.16batch/s, auc=0.9126, loss=0.7270]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 138/292 [00:23<00:25,  6.16batch/s, auc=0.9121, loss=0.6087]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 139/292 [00:23<00:24,  6.15batch/s, auc=0.9121, loss=0.6087]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 139/292 [00:23<00:24,  6.15batch/s, auc=0.9122, loss=0.4937]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9122, loss=0.4937]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9124, loss=0.6076]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9124, loss=0.6076]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9124, loss=0.5214]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9124, loss=0.5214]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9124, loss=0.6936]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9124, loss=0.6936]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9128, loss=0.5581]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 144/292 [00:23<00:24,  6.17batch/s, auc=0.9128, loss=0.5581]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 144/292 [00:24<00:24,  6.17batch/s, auc=0.9130, loss=0.4726]\u001b[A\n",
      "Training Epoch 9/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9130, loss=0.4726]\u001b[A\n",
      "Training Epoch 9/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9132, loss=0.6140]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9132, loss=0.6140]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9128, loss=0.7632]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 147/292 [00:24<00:23,  6.17batch/s, auc=0.9128, loss=0.7632]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 147/292 [00:24<00:23,  6.17batch/s, auc=0.9123, loss=1.0174]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 148/292 [00:24<00:23,  6.17batch/s, auc=0.9123, loss=1.0174]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 148/292 [00:24<00:23,  6.17batch/s, auc=0.9121, loss=0.7309]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9121, loss=0.7309]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9122, loss=0.5950]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.16batch/s, auc=0.9122, loss=0.5950]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.16batch/s, auc=0.9123, loss=0.5621]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.11batch/s, auc=0.9123, loss=0.5621]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.11batch/s, auc=0.9120, loss=0.8201]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9120, loss=0.8201]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9124, loss=0.4246]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9124, loss=0.4246]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9127, loss=0.4463]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9127, loss=0.4463]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9121, loss=0.8827]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9121, loss=0.8827]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9115, loss=0.8977]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9115, loss=0.8977]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.14batch/s, auc=0.9114, loss=0.7705]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.14batch/s, auc=0.9114, loss=0.7705]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.14batch/s, auc=0.9114, loss=0.5079]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9114, loss=0.5079]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9116, loss=0.6943]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9116, loss=0.6943]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9115, loss=0.6171]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9115, loss=0.6171]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9116, loss=0.5705]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9116, loss=0.5705]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9117, loss=0.5294]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9117, loss=0.5294]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.13batch/s, auc=0.9115, loss=0.9985]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9115, loss=0.9985]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9116, loss=0.5219]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.11batch/s, auc=0.9116, loss=0.5219]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.11batch/s, auc=0.9118, loss=0.5028]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.08batch/s, auc=0.9118, loss=0.5028]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.08batch/s, auc=0.9116, loss=0.7387]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.02batch/s, auc=0.9116, loss=0.7387]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.02batch/s, auc=0.9116, loss=0.5128]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.04batch/s, auc=0.9116, loss=0.5128]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.04batch/s, auc=0.9116, loss=0.5612]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.05batch/s, auc=0.9116, loss=0.5612]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.05batch/s, auc=0.9118, loss=0.6047]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.05batch/s, auc=0.9118, loss=0.6047]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.05batch/s, auc=0.9119, loss=0.4750]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.08batch/s, auc=0.9119, loss=0.4750]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.08batch/s, auc=0.9119, loss=0.6855]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.10batch/s, auc=0.9119, loss=0.6855]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.10batch/s, auc=0.9121, loss=0.5508]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9121, loss=0.5508]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9118, loss=0.7940]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.09batch/s, auc=0.9118, loss=0.7940]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.09batch/s, auc=0.9118, loss=0.6267]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.10batch/s, auc=0.9118, loss=0.6267]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.10batch/s, auc=0.9117, loss=0.5894]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.08batch/s, auc=0.9117, loss=0.5894]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.08batch/s, auc=0.9117, loss=0.5490]\u001b[A\n",
      "Training Epoch 9/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9117, loss=0.5490]\u001b[A\n",
      "Training Epoch 9/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9113, loss=0.8767]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9113, loss=0.8767]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9115, loss=0.5562]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9115, loss=0.5562]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9112, loss=1.0692]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9112, loss=1.0692]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9107, loss=1.0603]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9107, loss=1.0603]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9104, loss=0.7499]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.11batch/s, auc=0.9104, loss=0.7499]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9101, loss=0.6235]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.11batch/s, auc=0.9101, loss=0.6235]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.11batch/s, auc=0.9104, loss=0.4466]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.10batch/s, auc=0.9104, loss=0.4466]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.10batch/s, auc=0.9107, loss=0.4985]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9107, loss=0.4985]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9106, loss=0.7127]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.08batch/s, auc=0.9106, loss=0.7127]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.08batch/s, auc=0.9106, loss=0.6761]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.02batch/s, auc=0.9106, loss=0.6761]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.02batch/s, auc=0.9103, loss=0.9781]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.04batch/s, auc=0.9103, loss=0.9781]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.04batch/s, auc=0.9107, loss=0.4024]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.05batch/s, auc=0.9107, loss=0.4024]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.05batch/s, auc=0.9106, loss=0.6980]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.06batch/s, auc=0.9106, loss=0.6980]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.06batch/s, auc=0.9110, loss=0.4867]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.02batch/s, auc=0.9110, loss=0.4867]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.02batch/s, auc=0.9114, loss=0.3670]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.03batch/s, auc=0.9114, loss=0.3670]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.03batch/s, auc=0.9113, loss=0.5911]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.05batch/s, auc=0.9113, loss=0.5911]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.05batch/s, auc=0.9107, loss=1.4788]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.04batch/s, auc=0.9107, loss=1.4788]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.04batch/s, auc=0.9109, loss=0.6323]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.06batch/s, auc=0.9109, loss=0.6323]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.06batch/s, auc=0.9111, loss=0.5187]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.06batch/s, auc=0.9111, loss=0.5187]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.06batch/s, auc=0.9115, loss=0.4579]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.06batch/s, auc=0.9115, loss=0.4579]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.06batch/s, auc=0.9118, loss=0.4586]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.06batch/s, auc=0.9118, loss=0.4586]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.06batch/s, auc=0.9117, loss=0.6647]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.05batch/s, auc=0.9117, loss=0.6647]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.05batch/s, auc=0.9119, loss=0.6548]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.06batch/s, auc=0.9119, loss=0.6548]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.06batch/s, auc=0.9118, loss=0.5481]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9118, loss=0.5481]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9116, loss=0.8163]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.04batch/s, auc=0.9116, loss=0.8163]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.04batch/s, auc=0.9117, loss=0.4821]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.06batch/s, auc=0.9117, loss=0.4821]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.06batch/s, auc=0.9120, loss=0.5541]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.06batch/s, auc=0.9120, loss=0.5541]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.06batch/s, auc=0.9122, loss=0.6512]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.03batch/s, auc=0.9122, loss=0.6512]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.03batch/s, auc=0.9121, loss=0.6061]\u001b[A\n",
      "Training Epoch 9/25:  70%|███████   | 205/292 [00:33<00:14,  5.92batch/s, auc=0.9121, loss=0.6061]\u001b[A\n",
      "Training Epoch 9/25:  70%|███████   | 205/292 [00:34<00:14,  5.92batch/s, auc=0.9120, loss=0.7669]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 206/292 [00:34<00:14,  5.75batch/s, auc=0.9120, loss=0.7669]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 206/292 [00:34<00:14,  5.75batch/s, auc=0.9121, loss=0.5996]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 207/292 [00:34<00:14,  5.71batch/s, auc=0.9121, loss=0.5996]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 207/292 [00:34<00:14,  5.71batch/s, auc=0.9121, loss=0.5444]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9121, loss=0.5444]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9124, loss=0.4899]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.66batch/s, auc=0.9124, loss=0.4899]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.66batch/s, auc=0.9125, loss=0.5790]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.65batch/s, auc=0.9125, loss=0.5790]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.65batch/s, auc=0.9128, loss=0.4044]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.66batch/s, auc=0.9128, loss=0.4044]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.66batch/s, auc=0.9126, loss=0.8115]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.64batch/s, auc=0.9126, loss=0.8115]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.64batch/s, auc=0.9127, loss=0.5500]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.74batch/s, auc=0.9127, loss=0.5500]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.74batch/s, auc=0.9122, loss=1.1055]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.62batch/s, auc=0.9122, loss=1.1055]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.62batch/s, auc=0.9122, loss=0.6589]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.74batch/s, auc=0.9122, loss=0.6589]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.74batch/s, auc=0.9120, loss=0.6948]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.84batch/s, auc=0.9120, loss=0.6948]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.84batch/s, auc=0.9122, loss=0.4959]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.89batch/s, auc=0.9122, loss=0.4959]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.89batch/s, auc=0.9123, loss=0.5765]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.93batch/s, auc=0.9123, loss=0.5765]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.93batch/s, auc=0.9122, loss=0.6099]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.96batch/s, auc=0.9122, loss=0.6099]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.96batch/s, auc=0.9122, loss=0.6034]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.98batch/s, auc=0.9122, loss=0.6034]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.98batch/s, auc=0.9121, loss=0.8370]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.99batch/s, auc=0.9121, loss=0.8370]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.99batch/s, auc=0.9120, loss=0.6220]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.99batch/s, auc=0.9120, loss=0.6220]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.99batch/s, auc=0.9121, loss=0.4543]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.97batch/s, auc=0.9121, loss=0.4543]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.97batch/s, auc=0.9120, loss=0.6063]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.00batch/s, auc=0.9120, loss=0.6063]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.00batch/s, auc=0.9119, loss=0.8060]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.01batch/s, auc=0.9119, loss=0.8060]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.01batch/s, auc=0.9121, loss=0.4778]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 226/292 [00:37<00:11,  6.00batch/s, auc=0.9121, loss=0.4778]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 226/292 [00:37<00:11,  6.00batch/s, auc=0.9121, loss=0.7857]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.99batch/s, auc=0.9121, loss=0.7857]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.99batch/s, auc=0.9121, loss=0.6102]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.79batch/s, auc=0.9121, loss=0.6102]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.79batch/s, auc=0.9120, loss=0.5839]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.85batch/s, auc=0.9120, loss=0.5839]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.85batch/s, auc=0.9120, loss=0.5553]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.79batch/s, auc=0.9120, loss=0.5553]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.79batch/s, auc=0.9121, loss=0.5345]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.85batch/s, auc=0.9121, loss=0.5345]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.85batch/s, auc=0.9120, loss=0.6665]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.91batch/s, auc=0.9120, loss=0.6665]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.91batch/s, auc=0.9119, loss=0.6671]\u001b[A\n",
      "Training Epoch 9/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.70batch/s, auc=0.9119, loss=0.6671]\u001b[A\n",
      "Training Epoch 9/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.70batch/s, auc=0.9120, loss=0.5107]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 234/292 [00:38<00:10,  5.68batch/s, auc=0.9120, loss=0.5107]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 234/292 [00:39<00:10,  5.68batch/s, auc=0.9122, loss=0.4561]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 235/292 [00:39<00:09,  5.71batch/s, auc=0.9122, loss=0.4561]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 235/292 [00:39<00:09,  5.71batch/s, auc=0.9121, loss=0.6477]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 236/292 [00:39<00:09,  5.69batch/s, auc=0.9121, loss=0.6477]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 236/292 [00:39<00:09,  5.69batch/s, auc=0.9120, loss=0.7496]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9120, loss=0.7496]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9121, loss=0.6105]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.64batch/s, auc=0.9121, loss=0.6105]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.64batch/s, auc=0.9120, loss=0.8025]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.63batch/s, auc=0.9120, loss=0.8025]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.63batch/s, auc=0.9121, loss=0.5453]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.63batch/s, auc=0.9121, loss=0.5453]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.63batch/s, auc=0.9123, loss=0.4382]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.61batch/s, auc=0.9123, loss=0.4382]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.61batch/s, auc=0.9124, loss=0.5561]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.60batch/s, auc=0.9124, loss=0.5561]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.60batch/s, auc=0.9127, loss=0.4103]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.60batch/s, auc=0.9127, loss=0.4103]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.60batch/s, auc=0.9128, loss=0.5416]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9128, loss=0.5416]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9130, loss=0.5512]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.72batch/s, auc=0.9130, loss=0.5512]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.72batch/s, auc=0.9130, loss=0.4928]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.65batch/s, auc=0.9130, loss=0.4928]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.65batch/s, auc=0.9130, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.64batch/s, auc=0.9130, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.64batch/s, auc=0.9129, loss=0.7465]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.65batch/s, auc=0.9129, loss=0.7465]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.65batch/s, auc=0.9130, loss=0.7154]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.75batch/s, auc=0.9130, loss=0.7154]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.75batch/s, auc=0.9131, loss=0.4938]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.70batch/s, auc=0.9131, loss=0.4938]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.70batch/s, auc=0.9133, loss=0.4045]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.67batch/s, auc=0.9133, loss=0.4045]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.67batch/s, auc=0.9130, loss=0.9082]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.66batch/s, auc=0.9130, loss=0.9082]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.66batch/s, auc=0.9132, loss=0.5960]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.63batch/s, auc=0.9132, loss=0.5960]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.63batch/s, auc=0.9132, loss=0.5126]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.74batch/s, auc=0.9132, loss=0.5126]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.74batch/s, auc=0.9130, loss=0.6623]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.80batch/s, auc=0.9130, loss=0.6623]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.80batch/s, auc=0.9130, loss=0.6593]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.86batch/s, auc=0.9130, loss=0.6593]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.86batch/s, auc=0.9131, loss=0.6067]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 257/292 [00:42<00:05,  5.89batch/s, auc=0.9131, loss=0.6067]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 257/292 [00:43<00:05,  5.89batch/s, auc=0.9130, loss=0.7683]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.91batch/s, auc=0.9130, loss=0.7683]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.91batch/s, auc=0.9131, loss=0.6389]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.94batch/s, auc=0.9131, loss=0.6389]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.94batch/s, auc=0.9133, loss=0.4659]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.90batch/s, auc=0.9133, loss=0.4659]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.90batch/s, auc=0.9134, loss=0.6326]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.87batch/s, auc=0.9134, loss=0.6326]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.87batch/s, auc=0.9134, loss=0.8009]\u001b[A\n",
      "Training Epoch 9/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.90batch/s, auc=0.9134, loss=0.8009]\u001b[A\n",
      "Training Epoch 9/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.90batch/s, auc=0.9135, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 263/292 [00:43<00:04,  5.84batch/s, auc=0.9135, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 263/292 [00:44<00:04,  5.84batch/s, auc=0.9134, loss=0.5506]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9134, loss=0.5506]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9136, loss=0.5186]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 265/292 [00:44<00:04,  5.61batch/s, auc=0.9136, loss=0.5186]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 265/292 [00:44<00:04,  5.61batch/s, auc=0.9136, loss=0.5661]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 266/292 [00:44<00:04,  5.61batch/s, auc=0.9136, loss=0.5661]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 266/292 [00:44<00:04,  5.61batch/s, auc=0.9136, loss=0.5199]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.58batch/s, auc=0.9136, loss=0.5199]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.58batch/s, auc=0.9136, loss=0.8851]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.59batch/s, auc=0.9136, loss=0.8851]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.59batch/s, auc=0.9136, loss=0.5064]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.60batch/s, auc=0.9136, loss=0.5064]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.60batch/s, auc=0.9139, loss=0.4986]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.59batch/s, auc=0.9139, loss=0.4986]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.59batch/s, auc=0.9138, loss=0.6284]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9138, loss=0.6284]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9139, loss=0.5547]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.60batch/s, auc=0.9139, loss=0.5547]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.60batch/s, auc=0.9139, loss=0.5174]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.59batch/s, auc=0.9139, loss=0.5174]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.59batch/s, auc=0.9139, loss=0.5348]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.57batch/s, auc=0.9139, loss=0.5348]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.57batch/s, auc=0.9140, loss=0.4995]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.61batch/s, auc=0.9140, loss=0.4995]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.61batch/s, auc=0.9138, loss=0.8098]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.60batch/s, auc=0.9138, loss=0.8098]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.60batch/s, auc=0.9139, loss=0.7420]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.61batch/s, auc=0.9139, loss=0.7420]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.61batch/s, auc=0.9139, loss=0.6546]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.59batch/s, auc=0.9139, loss=0.6546]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.59batch/s, auc=0.9138, loss=0.7174]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.56batch/s, auc=0.9138, loss=0.7174]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.56batch/s, auc=0.9137, loss=0.8885]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9137, loss=0.8885]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9137, loss=0.6001]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.54batch/s, auc=0.9137, loss=0.6001]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.54batch/s, auc=0.9136, loss=0.6902]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.53batch/s, auc=0.9136, loss=0.6902]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.53batch/s, auc=0.9137, loss=0.6048]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.54batch/s, auc=0.9137, loss=0.6048]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.54batch/s, auc=0.9134, loss=0.7350]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.52batch/s, auc=0.9134, loss=0.7350]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.52batch/s, auc=0.9136, loss=0.3828]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.51batch/s, auc=0.9136, loss=0.3828]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.51batch/s, auc=0.9138, loss=0.5559]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.63batch/s, auc=0.9138, loss=0.5559]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.63batch/s, auc=0.9141, loss=0.3483]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.70batch/s, auc=0.9141, loss=0.3483]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.70batch/s, auc=0.9140, loss=0.6878]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.75batch/s, auc=0.9140, loss=0.6878]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.75batch/s, auc=0.9139, loss=0.7929]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.80batch/s, auc=0.9139, loss=0.7929]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.80batch/s, auc=0.9140, loss=0.3954]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.84batch/s, auc=0.9140, loss=0.3954]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.84batch/s, auc=0.9142, loss=0.6308]\u001b[A\n",
      "Training Epoch 9/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.87batch/s, auc=0.9142, loss=0.6308]\u001b[A\n",
      "Training Epoch 9/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.87batch/s, auc=0.9143, loss=0.6097]\u001b[A\n",
      "Training Epoch 9/25: 100%|██████████| 292/292 [00:49<00:00,  5.94batch/s, auc=0.9143, loss=0.6097]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25] Train Loss: 0.6300 | Train AUROC: 0.9143 Val Loss: 0.6548 | Val AUROC: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|███▌      | 9/25 [08:13<14:40, 55.06s/it]\n",
      "Training Epoch 10/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9627, loss=0.4103]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 1/292 [00:00<04:47,  1.01batch/s, auc=0.9627, loss=0.4103]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 1/292 [00:01<04:47,  1.01batch/s, auc=0.9221, loss=0.7916]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9221, loss=0.7916]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9195, loss=0.5443]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 3/292 [00:01<01:39,  2.91batch/s, auc=0.9195, loss=0.5443]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 3/292 [00:01<01:39,  2.91batch/s, auc=0.9237, loss=0.5058]\u001b[A\n",
      "Training Epoch 10/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9237, loss=0.5058]\u001b[A\n",
      "Training Epoch 10/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9293, loss=0.4185]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9293, loss=0.4185]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9327, loss=0.4880]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 6/292 [00:01<00:59,  4.80batch/s, auc=0.9327, loss=0.4880]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 6/292 [00:01<00:59,  4.80batch/s, auc=0.9273, loss=0.6001]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 7/292 [00:01<00:55,  5.18batch/s, auc=0.9273, loss=0.6001]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 7/292 [00:02<00:55,  5.18batch/s, auc=0.9086, loss=1.0014]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 8/292 [00:02<00:51,  5.51batch/s, auc=0.9086, loss=1.0014]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 8/292 [00:02<00:51,  5.51batch/s, auc=0.9123, loss=0.4601]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 9/292 [00:02<00:49,  5.66batch/s, auc=0.9123, loss=0.4601]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 9/292 [00:02<00:49,  5.66batch/s, auc=0.9173, loss=0.5291]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 10/292 [00:02<00:49,  5.74batch/s, auc=0.9173, loss=0.5291]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 10/292 [00:02<00:49,  5.74batch/s, auc=0.9116, loss=0.8517]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9116, loss=0.8517]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9083, loss=0.8232]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 12/292 [00:02<00:46,  6.05batch/s, auc=0.9083, loss=0.8232]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 12/292 [00:02<00:46,  6.05batch/s, auc=0.9064, loss=0.8950]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 13/292 [00:02<00:45,  6.15batch/s, auc=0.9064, loss=0.8950]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 13/292 [00:03<00:45,  6.15batch/s, auc=0.9066, loss=0.4966]\u001b[A\n",
      "Training Epoch 10/25:   5%|▍         | 14/292 [00:03<00:45,  6.17batch/s, auc=0.9066, loss=0.4966]\u001b[A\n",
      "Training Epoch 10/25:   5%|▍         | 14/292 [00:03<00:45,  6.17batch/s, auc=0.9115, loss=0.3992]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9115, loss=0.3992]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9130, loss=0.5068]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9130, loss=0.5068]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9153, loss=0.5391]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9153, loss=0.5391]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9161, loss=0.4746]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 18/292 [00:03<00:43,  6.27batch/s, auc=0.9161, loss=0.4746]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 18/292 [00:03<00:43,  6.27batch/s, auc=0.9113, loss=1.1284]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 19/292 [00:03<00:43,  6.29batch/s, auc=0.9113, loss=1.1284]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 19/292 [00:04<00:43,  6.29batch/s, auc=0.9114, loss=0.4765]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 20/292 [00:04<00:43,  6.25batch/s, auc=0.9114, loss=0.4765]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 20/292 [00:04<00:43,  6.25batch/s, auc=0.9128, loss=0.6586]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 21/292 [00:04<00:43,  6.29batch/s, auc=0.9128, loss=0.6586]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 21/292 [00:04<00:43,  6.29batch/s, auc=0.9096, loss=0.8206]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 22/292 [00:04<00:43,  6.26batch/s, auc=0.9096, loss=0.8206]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 22/292 [00:04<00:43,  6.26batch/s, auc=0.9098, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 23/292 [00:04<00:42,  6.29batch/s, auc=0.9098, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 23/292 [00:04<00:42,  6.29batch/s, auc=0.9116, loss=0.5165]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9116, loss=0.5165]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9112, loss=0.6353]\u001b[A\n",
      "Training Epoch 10/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9112, loss=0.6353]\u001b[A\n",
      "Training Epoch 10/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9108, loss=0.7608]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 26/292 [00:04<00:41,  6.34batch/s, auc=0.9108, loss=0.7608]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.9094, loss=0.8973]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.9094, loss=0.8973]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.9085, loss=0.6423]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9085, loss=0.6423]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9099, loss=0.5053]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9099, loss=0.5053]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9098, loss=0.5548]\u001b[A\n",
      "Training Epoch 10/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.9098, loss=0.5548]\u001b[A\n",
      "Training Epoch 10/25:  10%|█         | 30/292 [00:05<00:41,  6.35batch/s, auc=0.9097, loss=0.5690]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 31/292 [00:05<00:41,  6.29batch/s, auc=0.9097, loss=0.5690]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 31/292 [00:05<00:41,  6.29batch/s, auc=0.9117, loss=0.3978]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 32/292 [00:05<00:41,  6.31batch/s, auc=0.9117, loss=0.3978]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 32/292 [00:06<00:41,  6.31batch/s, auc=0.9072, loss=1.3038]\u001b[A\n",
      "Training Epoch 10/25:  11%|█▏        | 33/292 [00:06<00:40,  6.32batch/s, auc=0.9072, loss=1.3038]\u001b[A\n",
      "Training Epoch 10/25:  11%|█▏        | 33/292 [00:06<00:40,  6.32batch/s, auc=0.9068, loss=0.8326]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9068, loss=0.8326]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9076, loss=0.4664]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.9076, loss=0.4664]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.9085, loss=0.4847]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9085, loss=0.4847]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9085, loss=0.5361]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 37/292 [00:06<00:40,  6.32batch/s, auc=0.9085, loss=0.5361]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 37/292 [00:06<00:40,  6.32batch/s, auc=0.9073, loss=0.6680]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 38/292 [00:06<00:40,  6.33batch/s, auc=0.9073, loss=0.6680]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 38/292 [00:07<00:40,  6.33batch/s, auc=0.9081, loss=0.5406]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.9081, loss=0.5406]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 39/292 [00:07<00:39,  6.34batch/s, auc=0.9091, loss=0.5799]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▎        | 40/292 [00:07<00:39,  6.34batch/s, auc=0.9091, loss=0.5799]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▎        | 40/292 [00:07<00:39,  6.34batch/s, auc=0.9107, loss=0.4666]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.9107, loss=0.4666]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.9113, loss=0.4641]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 42/292 [00:07<00:39,  6.33batch/s, auc=0.9113, loss=0.4641]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 42/292 [00:07<00:39,  6.33batch/s, auc=0.9113, loss=0.6051]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.9113, loss=0.6051]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▍        | 43/292 [00:07<00:39,  6.32batch/s, auc=0.9130, loss=0.4815]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 44/292 [00:07<00:39,  6.33batch/s, auc=0.9130, loss=0.4815]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 44/292 [00:07<00:39,  6.33batch/s, auc=0.9119, loss=0.6472]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 45/292 [00:07<00:39,  6.31batch/s, auc=0.9119, loss=0.6472]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9123, loss=0.5775]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9123, loss=0.5775]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9125, loss=0.5192]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9125, loss=0.5192]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9136, loss=0.4164]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.9136, loss=0.4164]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.9145, loss=0.6006]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 49/292 [00:08<00:38,  6.33batch/s, auc=0.9145, loss=0.6006]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 49/292 [00:08<00:38,  6.33batch/s, auc=0.9151, loss=0.4973]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 50/292 [00:08<00:38,  6.32batch/s, auc=0.9151, loss=0.4973]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 50/292 [00:08<00:38,  6.32batch/s, auc=0.9137, loss=1.0123]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 51/292 [00:08<00:38,  6.24batch/s, auc=0.9137, loss=1.0123]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 51/292 [00:09<00:38,  6.24batch/s, auc=0.9153, loss=0.3725]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 52/292 [00:09<00:38,  6.26batch/s, auc=0.9153, loss=0.3725]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 52/292 [00:09<00:38,  6.26batch/s, auc=0.9151, loss=0.5952]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9151, loss=0.5952]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9148, loss=0.5737]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9148, loss=0.5737]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9126, loss=1.1431]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9126, loss=1.1431]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9124, loss=0.7049]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 56/292 [00:09<00:37,  6.26batch/s, auc=0.9124, loss=0.7049]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 56/292 [00:09<00:37,  6.26batch/s, auc=0.9118, loss=0.9302]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 57/292 [00:09<00:37,  6.28batch/s, auc=0.9118, loss=0.9302]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 57/292 [00:10<00:37,  6.28batch/s, auc=0.9127, loss=0.4231]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9127, loss=0.4231]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9123, loss=0.7685]\u001b[A\n",
      "Training Epoch 10/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9123, loss=0.7685]\u001b[A\n",
      "Training Epoch 10/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9124, loss=0.4916]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9124, loss=0.4916]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9129, loss=0.5311]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 61/292 [00:10<00:37,  6.19batch/s, auc=0.9129, loss=0.5311]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 61/292 [00:10<00:37,  6.19batch/s, auc=0.9126, loss=0.6085]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 62/292 [00:10<00:36,  6.22batch/s, auc=0.9126, loss=0.6085]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 62/292 [00:10<00:36,  6.22batch/s, auc=0.9119, loss=0.8335]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 63/292 [00:10<00:36,  6.25batch/s, auc=0.9119, loss=0.8335]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 63/292 [00:11<00:36,  6.25batch/s, auc=0.9113, loss=0.6086]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 64/292 [00:11<00:36,  6.26batch/s, auc=0.9113, loss=0.6086]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 64/292 [00:11<00:36,  6.26batch/s, auc=0.9116, loss=0.4818]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9116, loss=0.4818]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9124, loss=0.4659]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9124, loss=0.4659]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9127, loss=0.4651]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9127, loss=0.4651]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9136, loss=0.3344]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9136, loss=0.3344]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9129, loss=0.7772]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▎       | 69/292 [00:11<00:35,  6.29batch/s, auc=0.9129, loss=0.7772]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▎       | 69/292 [00:11<00:35,  6.29batch/s, auc=0.9133, loss=0.6882]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 70/292 [00:11<00:35,  6.27batch/s, auc=0.9133, loss=0.6882]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9129, loss=0.7439]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9129, loss=0.7439]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9124, loss=0.6696]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9124, loss=0.6696]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9118, loss=0.8105]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9118, loss=0.8105]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9113, loss=0.6914]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9113, loss=0.6914]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9123, loss=0.3934]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9123, loss=0.3934]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9130, loss=0.4029]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 76/292 [00:12<00:34,  6.29batch/s, auc=0.9130, loss=0.4029]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 76/292 [00:13<00:34,  6.29batch/s, auc=0.9138, loss=0.4154]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9138, loss=0.4154]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9134, loss=0.7032]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 78/292 [00:13<00:34,  6.22batch/s, auc=0.9134, loss=0.7032]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 78/292 [00:13<00:34,  6.22batch/s, auc=0.9145, loss=0.5401]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 79/292 [00:13<00:34,  6.24batch/s, auc=0.9145, loss=0.5401]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 79/292 [00:13<00:34,  6.24batch/s, auc=0.9127, loss=1.2460]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9127, loss=1.2460]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9135, loss=0.3836]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9135, loss=0.3836]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9133, loss=0.5424]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 82/292 [00:13<00:33,  6.25batch/s, auc=0.9133, loss=0.5424]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 82/292 [00:14<00:33,  6.25batch/s, auc=0.9133, loss=0.5683]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9133, loss=0.5683]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9135, loss=0.4901]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.9135, loss=0.4901]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.9126, loss=0.8596]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9126, loss=0.8596]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9131, loss=0.5078]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 86/292 [00:14<00:32,  6.27batch/s, auc=0.9131, loss=0.5078]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 86/292 [00:14<00:32,  6.27batch/s, auc=0.9131, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.9131, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:  30%|██▉       | 87/292 [00:14<00:32,  6.27batch/s, auc=0.9131, loss=0.6256]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 88/292 [00:14<00:32,  6.27batch/s, auc=0.9131, loss=0.6256]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 88/292 [00:15<00:32,  6.27batch/s, auc=0.9127, loss=0.7103]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9127, loss=0.7103]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9126, loss=0.6478]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9126, loss=0.6478]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9126, loss=0.5803]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9126, loss=0.5803]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9124, loss=0.6856]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9124, loss=0.6856]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9122, loss=0.7746]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 93/292 [00:15<00:31,  6.24batch/s, auc=0.9122, loss=0.7746]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 93/292 [00:15<00:31,  6.24batch/s, auc=0.9125, loss=0.6666]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9125, loss=0.6666]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9130, loss=0.4283]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 95/292 [00:15<00:31,  6.25batch/s, auc=0.9130, loss=0.4283]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 95/292 [00:16<00:31,  6.25batch/s, auc=0.9135, loss=0.5164]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9135, loss=0.5164]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9137, loss=0.5383]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 97/292 [00:16<00:31,  6.25batch/s, auc=0.9137, loss=0.5383]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 97/292 [00:16<00:31,  6.25batch/s, auc=0.9138, loss=0.5099]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▎      | 98/292 [00:16<00:30,  6.26batch/s, auc=0.9138, loss=0.5099]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▎      | 98/292 [00:16<00:30,  6.26batch/s, auc=0.9136, loss=0.8051]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 99/292 [00:16<00:30,  6.26batch/s, auc=0.9136, loss=0.8051]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 99/292 [00:16<00:30,  6.26batch/s, auc=0.9135, loss=0.5842]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 100/292 [00:16<00:30,  6.26batch/s, auc=0.9135, loss=0.5842]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 100/292 [00:16<00:30,  6.26batch/s, auc=0.9131, loss=0.7173]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 101/292 [00:16<00:30,  6.25batch/s, auc=0.9131, loss=0.7173]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 101/292 [00:17<00:30,  6.25batch/s, auc=0.9131, loss=0.6181]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 102/292 [00:17<00:30,  6.25batch/s, auc=0.9131, loss=0.6181]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 102/292 [00:17<00:30,  6.25batch/s, auc=0.9130, loss=0.5607]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▌      | 103/292 [00:17<00:30,  6.25batch/s, auc=0.9130, loss=0.5607]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▌      | 103/292 [00:17<00:30,  6.25batch/s, auc=0.9134, loss=0.5715]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9134, loss=0.5715]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9141, loss=0.4339]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9141, loss=0.4339]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9132, loss=1.0888]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▋      | 106/292 [00:17<00:30,  6.18batch/s, auc=0.9132, loss=1.0888]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▋      | 106/292 [00:17<00:30,  6.18batch/s, auc=0.9131, loss=0.6507]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 107/292 [00:17<00:29,  6.20batch/s, auc=0.9131, loss=0.6507]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 107/292 [00:18<00:29,  6.20batch/s, auc=0.9133, loss=0.5036]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9133, loss=0.5036]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9135, loss=0.5102]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 109/292 [00:18<00:29,  6.25batch/s, auc=0.9135, loss=0.5102]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 109/292 [00:18<00:29,  6.25batch/s, auc=0.9130, loss=0.9270]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 110/292 [00:18<00:29,  6.25batch/s, auc=0.9130, loss=0.9270]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 110/292 [00:18<00:29,  6.25batch/s, auc=0.9129, loss=0.7603]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 111/292 [00:18<00:29,  6.24batch/s, auc=0.9129, loss=0.7603]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 111/292 [00:18<00:29,  6.24batch/s, auc=0.9132, loss=0.4687]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9132, loss=0.4687]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9134, loss=0.6703]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▊      | 113/292 [00:18<00:28,  6.22batch/s, auc=0.9134, loss=0.6703]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9136, loss=0.5080]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9136, loss=0.5080]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9140, loss=0.4440]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9140, loss=0.4440]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9139, loss=0.7898]\u001b[A\n",
      "Training Epoch 10/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9139, loss=0.7898]\u001b[A\n",
      "Training Epoch 10/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9138, loss=0.5389]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9138, loss=0.5389]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9141, loss=0.5495]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9141, loss=0.5495]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9146, loss=0.4478]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9146, loss=0.4478]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9148, loss=0.6344]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 120/292 [00:19<00:27,  6.21batch/s, auc=0.9148, loss=0.6344]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9153, loss=0.3779]\u001b[A\n",
      "Training Epoch 10/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9153, loss=0.3779]\u001b[A\n",
      "Training Epoch 10/25:  41%|████▏     | 121/292 [00:20<00:27,  6.22batch/s, auc=0.9154, loss=0.5696]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9154, loss=0.5696]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 122/292 [00:20<00:27,  6.22batch/s, auc=0.9154, loss=0.6200]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9154, loss=0.6200]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9156, loss=0.4743]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9156, loss=0.4743]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9159, loss=0.4060]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9159, loss=0.4060]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9163, loss=0.5826]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 126/292 [00:20<00:26,  6.20batch/s, auc=0.9163, loss=0.5826]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9162, loss=0.4237]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 127/292 [00:21<00:26,  6.20batch/s, auc=0.9162, loss=0.4237]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 127/292 [00:21<00:26,  6.20batch/s, auc=0.9153, loss=1.0705]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9153, loss=1.0705]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9159, loss=0.3241]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9159, loss=0.3241]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9155, loss=0.8729]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9155, loss=0.8729]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9148, loss=0.9991]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 131/292 [00:21<00:26,  6.19batch/s, auc=0.9148, loss=0.9991]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 131/292 [00:21<00:26,  6.19batch/s, auc=0.9151, loss=0.5266]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▌     | 132/292 [00:21<00:25,  6.19batch/s, auc=0.9151, loss=0.5266]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▌     | 132/292 [00:22<00:25,  6.19batch/s, auc=0.9153, loss=0.5002]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9153, loss=0.5002]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9159, loss=0.3588]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9159, loss=0.3588]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9159, loss=0.6498]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9159, loss=0.6498]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9156, loss=0.6867]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9156, loss=0.6867]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9161, loss=0.3745]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9161, loss=0.3745]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9162, loss=0.5512]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 138/292 [00:22<00:24,  6.18batch/s, auc=0.9162, loss=0.5512]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 138/292 [00:23<00:24,  6.18batch/s, auc=0.9164, loss=0.5461]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 139/292 [00:23<00:24,  6.15batch/s, auc=0.9164, loss=0.5461]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 139/292 [00:23<00:24,  6.15batch/s, auc=0.9165, loss=0.7458]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9165, loss=0.7458]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9168, loss=0.4268]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 141/292 [00:23<00:24,  6.14batch/s, auc=0.9168, loss=0.4268]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 141/292 [00:23<00:24,  6.14batch/s, auc=0.9165, loss=0.7563]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▊     | 142/292 [00:23<00:24,  6.16batch/s, auc=0.9165, loss=0.7563]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▊     | 142/292 [00:23<00:24,  6.16batch/s, auc=0.9165, loss=0.7149]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9165, loss=0.7149]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9163, loss=0.6707]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 144/292 [00:23<00:24,  6.15batch/s, auc=0.9163, loss=0.6707]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 144/292 [00:24<00:24,  6.15batch/s, auc=0.9162, loss=0.8027]\u001b[A\n",
      "Training Epoch 10/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9162, loss=0.8027]\u001b[A\n",
      "Training Epoch 10/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9159, loss=0.7845]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9159, loss=0.7845]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9157, loss=0.6955]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 147/292 [00:24<00:23,  6.13batch/s, auc=0.9157, loss=0.6955]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 147/292 [00:24<00:23,  6.13batch/s, auc=0.9153, loss=0.7754]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 148/292 [00:24<00:23,  6.12batch/s, auc=0.9153, loss=0.7754]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 148/292 [00:24<00:23,  6.12batch/s, auc=0.9155, loss=0.5818]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9155, loss=0.5818]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9155, loss=0.6509]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.15batch/s, auc=0.9155, loss=0.6509]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.15batch/s, auc=0.9160, loss=0.4242]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9160, loss=0.4242]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9161, loss=0.5224]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9161, loss=0.5224]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9159, loss=0.9097]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9159, loss=0.9097]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9159, loss=0.6578]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9159, loss=0.6578]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9158, loss=0.7700]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9158, loss=0.7700]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9160, loss=0.4397]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9160, loss=0.4397]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9157, loss=0.6670]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.15batch/s, auc=0.9157, loss=0.6670]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9160, loss=0.4944]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9160, loss=0.4944]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9160, loss=0.6772]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.15batch/s, auc=0.9160, loss=0.6772]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.15batch/s, auc=0.9162, loss=0.4671]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9162, loss=0.4671]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9165, loss=0.5694]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.16batch/s, auc=0.9165, loss=0.5694]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.16batch/s, auc=0.9168, loss=0.4511]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9168, loss=0.4511]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9167, loss=0.8347]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 163/292 [00:26<00:21,  6.14batch/s, auc=0.9167, loss=0.8347]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9169, loss=0.5076]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9169, loss=0.5076]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9167, loss=0.8303]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9167, loss=0.8303]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9168, loss=0.5088]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9168, loss=0.5088]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9160, loss=1.1171]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9160, loss=1.1171]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9161, loss=0.6290]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.13batch/s, auc=0.9161, loss=0.6290]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.13batch/s, auc=0.9164, loss=0.4418]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.13batch/s, auc=0.9164, loss=0.4418]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9163, loss=0.6245]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9163, loss=0.6245]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9159, loss=0.7500]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9159, loss=0.7500]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9159, loss=0.6350]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.05batch/s, auc=0.9159, loss=0.6350]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.05batch/s, auc=0.9155, loss=0.8783]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.91batch/s, auc=0.9155, loss=0.8783]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.91batch/s, auc=0.9153, loss=0.6238]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 174/292 [00:28<00:19,  5.98batch/s, auc=0.9153, loss=0.6238]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 174/292 [00:28<00:19,  5.98batch/s, auc=0.9154, loss=0.5510]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.01batch/s, auc=0.9154, loss=0.5510]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.01batch/s, auc=0.9157, loss=0.4351]\u001b[A\n",
      "Training Epoch 10/25:  60%|██████    | 176/292 [00:29<00:19,  6.04batch/s, auc=0.9157, loss=0.4351]\u001b[A\n",
      "Training Epoch 10/25:  60%|██████    | 176/292 [00:29<00:19,  6.04batch/s, auc=0.9156, loss=0.8399]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 177/292 [00:29<00:19,  6.05batch/s, auc=0.9156, loss=0.8399]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 177/292 [00:29<00:19,  6.05batch/s, auc=0.9159, loss=0.4456]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 178/292 [00:29<00:18,  6.07batch/s, auc=0.9159, loss=0.4456]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 178/292 [00:29<00:18,  6.07batch/s, auc=0.9159, loss=0.6197]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.08batch/s, auc=0.9159, loss=0.6197]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.08batch/s, auc=0.9155, loss=0.8783]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.08batch/s, auc=0.9155, loss=0.8783]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.08batch/s, auc=0.9153, loss=0.7867]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.09batch/s, auc=0.9153, loss=0.7867]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.09batch/s, auc=0.9150, loss=0.9936]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9150, loss=0.9936]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9148, loss=0.9131]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9148, loss=0.9131]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9146, loss=0.6963]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.10batch/s, auc=0.9146, loss=0.6963]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.10batch/s, auc=0.9144, loss=0.8110]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.11batch/s, auc=0.9144, loss=0.8110]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.11batch/s, auc=0.9145, loss=0.4944]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.12batch/s, auc=0.9145, loss=0.4944]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.12batch/s, auc=0.9143, loss=0.7639]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.12batch/s, auc=0.9143, loss=0.7639]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.12batch/s, auc=0.9137, loss=1.0378]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9137, loss=1.0378]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9139, loss=0.6023]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.11batch/s, auc=0.9139, loss=0.6023]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.11batch/s, auc=0.9140, loss=0.5314]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9140, loss=0.5314]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9143, loss=0.5936]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.10batch/s, auc=0.9143, loss=0.5936]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.10batch/s, auc=0.9146, loss=0.6334]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9146, loss=0.6334]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9144, loss=0.6787]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.09batch/s, auc=0.9144, loss=0.6787]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.09batch/s, auc=0.9147, loss=0.5402]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9147, loss=0.5402]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9146, loss=0.6423]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.09batch/s, auc=0.9146, loss=0.6423]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.09batch/s, auc=0.9149, loss=0.4853]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.08batch/s, auc=0.9149, loss=0.4853]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.08batch/s, auc=0.9150, loss=0.5984]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.08batch/s, auc=0.9150, loss=0.5984]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.08batch/s, auc=0.9152, loss=0.5087]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.08batch/s, auc=0.9152, loss=0.5087]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.08batch/s, auc=0.9146, loss=1.0550]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.08batch/s, auc=0.9146, loss=1.0550]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.08batch/s, auc=0.9146, loss=0.6167]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9146, loss=0.6167]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9148, loss=0.5341]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.02batch/s, auc=0.9148, loss=0.5341]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.02batch/s, auc=0.9147, loss=0.6020]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.03batch/s, auc=0.9147, loss=0.6020]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.03batch/s, auc=0.9147, loss=0.6094]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.03batch/s, auc=0.9147, loss=0.6094]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.03batch/s, auc=0.9151, loss=0.3775]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.04batch/s, auc=0.9151, loss=0.3775]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.04batch/s, auc=0.9152, loss=0.7276]\u001b[A\n",
      "Training Epoch 10/25:  70%|███████   | 205/292 [00:33<00:14,  6.05batch/s, auc=0.9152, loss=0.7276]\u001b[A\n",
      "Training Epoch 10/25:  70%|███████   | 205/292 [00:34<00:14,  6.05batch/s, auc=0.9151, loss=0.5453]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 206/292 [00:34<00:14,  6.05batch/s, auc=0.9151, loss=0.5453]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 206/292 [00:34<00:14,  6.05batch/s, auc=0.9152, loss=0.5017]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 207/292 [00:34<00:14,  6.05batch/s, auc=0.9152, loss=0.5017]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 207/292 [00:34<00:14,  6.05batch/s, auc=0.9153, loss=0.5905]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 208/292 [00:34<00:13,  6.04batch/s, auc=0.9153, loss=0.5905]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 208/292 [00:34<00:13,  6.04batch/s, auc=0.9156, loss=0.4561]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.04batch/s, auc=0.9156, loss=0.4561]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.04batch/s, auc=0.9158, loss=0.5073]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.04batch/s, auc=0.9158, loss=0.5073]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.04batch/s, auc=0.9157, loss=0.6283]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.05batch/s, auc=0.9157, loss=0.6283]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.05batch/s, auc=0.9158, loss=0.6074]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.06batch/s, auc=0.9158, loss=0.6074]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.06batch/s, auc=0.9158, loss=0.5749]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.04batch/s, auc=0.9158, loss=0.5749]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.04batch/s, auc=0.9155, loss=0.8585]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.04batch/s, auc=0.9155, loss=0.8585]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.04batch/s, auc=0.9156, loss=0.5941]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.05batch/s, auc=0.9156, loss=0.5941]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.05batch/s, auc=0.9156, loss=0.6725]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.05batch/s, auc=0.9156, loss=0.6725]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.05batch/s, auc=0.9158, loss=0.4616]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.99batch/s, auc=0.9158, loss=0.4616]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.99batch/s, auc=0.9155, loss=0.8509]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.89batch/s, auc=0.9155, loss=0.8509]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.89batch/s, auc=0.9152, loss=0.9490]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.76batch/s, auc=0.9152, loss=0.9490]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.76batch/s, auc=0.9149, loss=0.6703]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.9149, loss=0.6703]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.74batch/s, auc=0.9148, loss=0.5766]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.75batch/s, auc=0.9148, loss=0.5766]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.75batch/s, auc=0.9147, loss=0.6707]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.72batch/s, auc=0.9147, loss=0.6707]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.72batch/s, auc=0.9147, loss=0.6778]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▋  | 223/292 [00:36<00:12,  5.71batch/s, auc=0.9147, loss=0.6778]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.71batch/s, auc=0.9148, loss=0.4520]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.71batch/s, auc=0.9148, loss=0.4520]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.71batch/s, auc=0.9148, loss=0.5926]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.63batch/s, auc=0.9148, loss=0.5926]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.63batch/s, auc=0.9148, loss=0.5594]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.64batch/s, auc=0.9148, loss=0.5594]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.64batch/s, auc=0.9150, loss=0.4801]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.64batch/s, auc=0.9150, loss=0.4801]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.64batch/s, auc=0.9146, loss=0.9646]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.65batch/s, auc=0.9146, loss=0.9646]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.65batch/s, auc=0.9149, loss=0.4146]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 229/292 [00:37<00:10,  5.73batch/s, auc=0.9149, loss=0.4146]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.73batch/s, auc=0.9151, loss=0.3853]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.57batch/s, auc=0.9151, loss=0.3853]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.57batch/s, auc=0.9151, loss=0.6821]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.60batch/s, auc=0.9151, loss=0.6821]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.60batch/s, auc=0.9151, loss=0.7843]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.64batch/s, auc=0.9151, loss=0.7843]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.64batch/s, auc=0.9152, loss=0.4238]\u001b[A\n",
      "Training Epoch 10/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.75batch/s, auc=0.9152, loss=0.4238]\u001b[A\n",
      "Training Epoch 10/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.75batch/s, auc=0.9154, loss=0.3652]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 234/292 [00:38<00:09,  5.84batch/s, auc=0.9154, loss=0.3652]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 234/292 [00:39<00:09,  5.84batch/s, auc=0.9153, loss=0.7036]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9153, loss=0.7036]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9153, loss=0.7111]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 236/292 [00:39<00:09,  5.76batch/s, auc=0.9153, loss=0.7111]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 236/292 [00:39<00:09,  5.76batch/s, auc=0.9154, loss=0.4859]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 237/292 [00:39<00:09,  5.83batch/s, auc=0.9154, loss=0.4859]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 237/292 [00:39<00:09,  5.83batch/s, auc=0.9157, loss=0.3634]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.76batch/s, auc=0.9157, loss=0.3634]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.76batch/s, auc=0.9155, loss=0.7805]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.82batch/s, auc=0.9155, loss=0.7805]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.82batch/s, auc=0.9157, loss=0.5563]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.76batch/s, auc=0.9157, loss=0.5563]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.76batch/s, auc=0.9158, loss=0.4390]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.74batch/s, auc=0.9158, loss=0.4390]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.74batch/s, auc=0.9159, loss=0.6873]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.71batch/s, auc=0.9159, loss=0.6873]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.71batch/s, auc=0.9159, loss=0.7122]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.80batch/s, auc=0.9159, loss=0.7122]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.80batch/s, auc=0.9159, loss=0.4605]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.9159, loss=0.4605]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.9160, loss=0.4291]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.69batch/s, auc=0.9160, loss=0.4291]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.69batch/s, auc=0.9159, loss=0.7191]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.67batch/s, auc=0.9159, loss=0.7191]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.67batch/s, auc=0.9158, loss=0.7642]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.74batch/s, auc=0.9158, loss=0.7642]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.74batch/s, auc=0.9159, loss=0.5633]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.70batch/s, auc=0.9159, loss=0.5633]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.70batch/s, auc=0.9156, loss=1.1195]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.67batch/s, auc=0.9156, loss=1.1195]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.67batch/s, auc=0.9156, loss=0.6117]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.65batch/s, auc=0.9156, loss=0.6117]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.65batch/s, auc=0.9155, loss=0.6858]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.64batch/s, auc=0.9155, loss=0.6858]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.64batch/s, auc=0.9155, loss=0.5176]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9155, loss=0.5176]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9154, loss=0.6838]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9154, loss=0.6838]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9155, loss=0.4026]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.64batch/s, auc=0.9155, loss=0.4026]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.64batch/s, auc=0.9156, loss=0.4535]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.64batch/s, auc=0.9156, loss=0.4535]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.64batch/s, auc=0.9156, loss=0.6245]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.62batch/s, auc=0.9156, loss=0.6245]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.62batch/s, auc=0.9156, loss=0.6997]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.60batch/s, auc=0.9156, loss=0.6997]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.60batch/s, auc=0.9156, loss=0.7113]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9156, loss=0.7113]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9158, loss=0.4276]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9158, loss=0.4276]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9157, loss=0.7408]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.57batch/s, auc=0.9157, loss=0.7408]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.57batch/s, auc=0.9157, loss=0.5568]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.59batch/s, auc=0.9157, loss=0.5568]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.59batch/s, auc=0.9156, loss=0.7184]\u001b[A\n",
      "Training Epoch 10/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9156, loss=0.7184]\u001b[A\n",
      "Training Epoch 10/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9158, loss=0.4924]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 263/292 [00:43<00:05,  5.50batch/s, auc=0.9158, loss=0.4924]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 263/292 [00:44<00:05,  5.50batch/s, auc=0.9158, loss=0.6383]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 264/292 [00:44<00:04,  5.61batch/s, auc=0.9158, loss=0.6383]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 264/292 [00:44<00:04,  5.61batch/s, auc=0.9157, loss=0.7267]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 265/292 [00:44<00:04,  5.60batch/s, auc=0.9157, loss=0.7267]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 265/292 [00:44<00:04,  5.60batch/s, auc=0.9158, loss=0.5771]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.9158, loss=0.5771]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.9159, loss=0.4890]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.53batch/s, auc=0.9159, loss=0.4890]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.53batch/s, auc=0.9158, loss=0.6100]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.54batch/s, auc=0.9158, loss=0.6100]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.54batch/s, auc=0.9161, loss=0.4207]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.55batch/s, auc=0.9161, loss=0.4207]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.55batch/s, auc=0.9162, loss=0.6063]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9162, loss=0.6063]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9164, loss=0.5265]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.57batch/s, auc=0.9164, loss=0.5265]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.57batch/s, auc=0.9163, loss=0.6838]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.55batch/s, auc=0.9163, loss=0.6838]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.55batch/s, auc=0.9163, loss=0.7403]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.57batch/s, auc=0.9163, loss=0.7403]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.57batch/s, auc=0.9163, loss=0.6326]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.59batch/s, auc=0.9163, loss=0.6326]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.59batch/s, auc=0.9165, loss=0.5439]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.69batch/s, auc=0.9165, loss=0.5439]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.69batch/s, auc=0.9167, loss=0.4325]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.62batch/s, auc=0.9167, loss=0.4325]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.62batch/s, auc=0.9170, loss=0.5269]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.59batch/s, auc=0.9170, loss=0.5269]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.59batch/s, auc=0.9169, loss=0.6341]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.59batch/s, auc=0.9169, loss=0.6341]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.59batch/s, auc=0.9170, loss=0.5976]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.58batch/s, auc=0.9170, loss=0.5976]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.58batch/s, auc=0.9171, loss=0.6087]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9171, loss=0.6087]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9170, loss=0.6711]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.58batch/s, auc=0.9170, loss=0.6711]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.58batch/s, auc=0.9166, loss=1.1571]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.56batch/s, auc=0.9166, loss=1.1571]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.56batch/s, auc=0.9166, loss=0.6940]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.55batch/s, auc=0.9166, loss=0.6940]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.55batch/s, auc=0.9166, loss=0.7065]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9166, loss=0.7065]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9165, loss=0.5168]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.55batch/s, auc=0.9165, loss=0.5168]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.55batch/s, auc=0.9166, loss=0.4773]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9166, loss=0.4773]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9163, loss=0.8493]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.76batch/s, auc=0.9163, loss=0.8493]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.76batch/s, auc=0.9163, loss=0.6107]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9163, loss=0.6107]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9163, loss=0.4673]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.85batch/s, auc=0.9163, loss=0.4673]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.85batch/s, auc=0.9165, loss=0.4812]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.88batch/s, auc=0.9165, loss=0.4812]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.88batch/s, auc=0.9164, loss=0.6162]\u001b[A\n",
      "Training Epoch 10/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.90batch/s, auc=0.9164, loss=0.6162]\u001b[A\n",
      "Training Epoch 10/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.90batch/s, auc=0.9165, loss=0.6138]\u001b[A\n",
      "Training Epoch 10/25: 100%|██████████| 292/292 [00:49<00:00,  5.94batch/s, auc=0.9165, loss=0.6138]\u001b[A\n",
      "Epochs:  40%|████      | 10/25 [09:08<13:44, 54.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25] Train Loss: 0.6237 | Train AUROC: 0.9165 Val Loss: 0.6886 | Val AUROC: 0.8920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 11/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.8937, loss=0.5886]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 1/292 [00:00<04:47,  1.01batch/s, auc=0.8937, loss=0.5886]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 1/292 [00:01<04:47,  1.01batch/s, auc=0.9054, loss=0.6173]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 2/292 [00:01<02:25,  2.00batch/s, auc=0.9054, loss=0.6173]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 2/292 [00:01<02:25,  2.00batch/s, auc=0.8986, loss=0.7077]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 3/292 [00:01<01:41,  2.85batch/s, auc=0.8986, loss=0.7077]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 3/292 [00:01<01:41,  2.85batch/s, auc=0.9127, loss=0.5142]\u001b[A\n",
      "Training Epoch 11/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9127, loss=0.5142]\u001b[A\n",
      "Training Epoch 11/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9184, loss=0.4990]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9184, loss=0.4990]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9201, loss=0.7017]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9201, loss=0.7017]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9167, loss=0.6483]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 7/292 [00:01<00:55,  5.16batch/s, auc=0.9167, loss=0.6483]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 7/292 [00:02<00:55,  5.16batch/s, auc=0.9089, loss=0.7916]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 8/292 [00:02<00:51,  5.50batch/s, auc=0.9089, loss=0.7916]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 8/292 [00:02<00:51,  5.50batch/s, auc=0.9170, loss=0.5089]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 9/292 [00:02<00:50,  5.58batch/s, auc=0.9170, loss=0.5089]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 9/292 [00:02<00:50,  5.58batch/s, auc=0.9262, loss=0.3538]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9262, loss=0.3538]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9272, loss=0.5059]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9272, loss=0.5059]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9287, loss=0.4468]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 12/292 [00:02<00:47,  5.93batch/s, auc=0.9287, loss=0.4468]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 12/292 [00:02<00:47,  5.93batch/s, auc=0.9280, loss=0.5430]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 13/292 [00:02<00:46,  6.06batch/s, auc=0.9280, loss=0.5430]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 13/292 [00:03<00:46,  6.06batch/s, auc=0.9235, loss=0.8675]\u001b[A\n",
      "Training Epoch 11/25:   5%|▍         | 14/292 [00:03<00:45,  6.14batch/s, auc=0.9235, loss=0.8675]\u001b[A\n",
      "Training Epoch 11/25:   5%|▍         | 14/292 [00:03<00:45,  6.14batch/s, auc=0.9241, loss=0.6020]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 15/292 [00:03<00:45,  6.13batch/s, auc=0.9241, loss=0.6020]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 15/292 [00:03<00:45,  6.13batch/s, auc=0.9246, loss=0.6057]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 16/292 [00:03<00:44,  6.19batch/s, auc=0.9246, loss=0.6057]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 16/292 [00:03<00:44,  6.19batch/s, auc=0.9257, loss=0.4775]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9257, loss=0.4775]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9268, loss=0.4828]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 18/292 [00:03<00:44,  6.21batch/s, auc=0.9268, loss=0.4828]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 18/292 [00:03<00:44,  6.21batch/s, auc=0.9252, loss=0.7078]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 19/292 [00:03<00:43,  6.26batch/s, auc=0.9252, loss=0.7078]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 19/292 [00:04<00:43,  6.26batch/s, auc=0.9239, loss=0.7453]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 20/292 [00:04<00:44,  6.17batch/s, auc=0.9239, loss=0.7453]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 20/292 [00:04<00:44,  6.17batch/s, auc=0.9235, loss=0.6544]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 21/292 [00:04<00:44,  6.15batch/s, auc=0.9235, loss=0.6544]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 21/292 [00:04<00:44,  6.15batch/s, auc=0.9257, loss=0.4318]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 22/292 [00:04<00:43,  6.21batch/s, auc=0.9257, loss=0.4318]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 22/292 [00:04<00:43,  6.21batch/s, auc=0.9254, loss=0.6363]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 23/292 [00:04<00:43,  6.21batch/s, auc=0.9254, loss=0.6363]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 23/292 [00:04<00:43,  6.21batch/s, auc=0.9239, loss=0.6612]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9239, loss=0.6612]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9215, loss=1.0435]\u001b[A\n",
      "Training Epoch 11/25:   9%|▊         | 25/292 [00:04<00:42,  6.28batch/s, auc=0.9215, loss=1.0435]\u001b[A\n",
      "Training Epoch 11/25:   9%|▊         | 25/292 [00:05<00:42,  6.28batch/s, auc=0.9189, loss=1.0772]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 26/292 [00:05<00:42,  6.25batch/s, auc=0.9189, loss=1.0772]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 26/292 [00:05<00:42,  6.25batch/s, auc=0.9177, loss=0.6708]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 27/292 [00:05<00:42,  6.28batch/s, auc=0.9177, loss=0.6708]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 27/292 [00:05<00:42,  6.28batch/s, auc=0.9172, loss=0.7671]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 28/292 [00:05<00:41,  6.29batch/s, auc=0.9172, loss=0.7671]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 28/292 [00:05<00:41,  6.29batch/s, auc=0.9188, loss=0.4286]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 29/292 [00:05<00:42,  6.19batch/s, auc=0.9188, loss=0.4286]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 29/292 [00:05<00:42,  6.19batch/s, auc=0.9173, loss=0.8726]\u001b[A\n",
      "Training Epoch 11/25:  10%|█         | 30/292 [00:05<00:42,  6.23batch/s, auc=0.9173, loss=0.8726]\u001b[A\n",
      "Training Epoch 11/25:  10%|█         | 30/292 [00:05<00:42,  6.23batch/s, auc=0.9194, loss=0.3967]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 31/292 [00:05<00:41,  6.26batch/s, auc=0.9194, loss=0.3967]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 31/292 [00:05<00:41,  6.26batch/s, auc=0.9198, loss=0.5335]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 32/292 [00:05<00:41,  6.28batch/s, auc=0.9198, loss=0.5335]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 32/292 [00:06<00:41,  6.28batch/s, auc=0.9202, loss=0.5494]\u001b[A\n",
      "Training Epoch 11/25:  11%|█▏        | 33/292 [00:06<00:41,  6.26batch/s, auc=0.9202, loss=0.5494]\u001b[A\n",
      "Training Epoch 11/25:  11%|█▏        | 33/292 [00:06<00:41,  6.26batch/s, auc=0.9200, loss=0.5448]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 34/292 [00:06<00:41,  6.28batch/s, auc=0.9200, loss=0.5448]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 34/292 [00:06<00:41,  6.28batch/s, auc=0.9190, loss=0.6524]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9190, loss=0.6524]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9189, loss=0.5623]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9189, loss=0.5623]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9188, loss=0.8237]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9188, loss=0.8237]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9182, loss=0.6528]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9182, loss=0.6528]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9187, loss=0.5423]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9187, loss=0.5423]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9191, loss=0.4266]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.9191, loss=0.4266]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.9176, loss=0.7884]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.9176, loss=0.7884]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 41/292 [00:07<00:39,  6.32batch/s, auc=0.9174, loss=0.7193]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 42/292 [00:07<00:39,  6.33batch/s, auc=0.9174, loss=0.7193]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 42/292 [00:07<00:39,  6.33batch/s, auc=0.9184, loss=0.4514]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▍        | 43/292 [00:07<00:39,  6.33batch/s, auc=0.9184, loss=0.4514]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▍        | 43/292 [00:07<00:39,  6.33batch/s, auc=0.9184, loss=0.5941]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 44/292 [00:07<00:39,  6.33batch/s, auc=0.9184, loss=0.5941]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 44/292 [00:08<00:39,  6.33batch/s, auc=0.9180, loss=0.6723]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 45/292 [00:08<00:39,  6.33batch/s, auc=0.9180, loss=0.6723]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 45/292 [00:08<00:39,  6.33batch/s, auc=0.9184, loss=0.5835]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.9184, loss=0.5835]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.9184, loss=0.8464]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9184, loss=0.8464]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9193, loss=0.5493]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9193, loss=0.5493]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▋        | 48/292 [00:08<00:38,  6.26batch/s, auc=0.9194, loss=0.5450]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 49/292 [00:08<00:38,  6.27batch/s, auc=0.9194, loss=0.5450]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 49/292 [00:08<00:38,  6.27batch/s, auc=0.9189, loss=0.5438]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9189, loss=0.5438]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9191, loss=0.4952]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 51/292 [00:08<00:38,  6.26batch/s, auc=0.9191, loss=0.4952]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 51/292 [00:09<00:38,  6.26batch/s, auc=0.9166, loss=1.1335]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 52/292 [00:09<00:38,  6.22batch/s, auc=0.9166, loss=1.1335]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 52/292 [00:09<00:38,  6.22batch/s, auc=0.9167, loss=0.5697]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 53/292 [00:09<00:38,  6.25batch/s, auc=0.9167, loss=0.5697]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 53/292 [00:09<00:38,  6.25batch/s, auc=0.9152, loss=0.8809]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9152, loss=0.8809]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9140, loss=0.8257]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9140, loss=0.8257]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9149, loss=0.4955]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 56/292 [00:09<00:37,  6.28batch/s, auc=0.9149, loss=0.4955]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 56/292 [00:09<00:37,  6.28batch/s, auc=0.9143, loss=0.9575]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 57/292 [00:09<00:37,  6.29batch/s, auc=0.9143, loss=0.9575]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 57/292 [00:10<00:37,  6.29batch/s, auc=0.9139, loss=0.5509]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 58/292 [00:10<00:37,  6.29batch/s, auc=0.9139, loss=0.5509]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 58/292 [00:10<00:37,  6.29batch/s, auc=0.9143, loss=0.4774]\u001b[A\n",
      "Training Epoch 11/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9143, loss=0.4774]\u001b[A\n",
      "Training Epoch 11/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9144, loss=0.5758]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9144, loss=0.5758]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9141, loss=0.6494]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 61/292 [00:10<00:36,  6.25batch/s, auc=0.9141, loss=0.6494]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 61/292 [00:10<00:36,  6.25batch/s, auc=0.9146, loss=0.4611]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9146, loss=0.4611]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9152, loss=0.5729]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9152, loss=0.5729]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9156, loss=0.6041]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9156, loss=0.6041]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9163, loss=0.4184]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9163, loss=0.4184]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9162, loss=0.6581]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9162, loss=0.6581]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9170, loss=0.4272]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9170, loss=0.4272]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9173, loss=0.5265]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 68/292 [00:11<00:35,  6.26batch/s, auc=0.9173, loss=0.5265]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 68/292 [00:11<00:35,  6.26batch/s, auc=0.9175, loss=0.6586]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9175, loss=0.6586]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▎       | 69/292 [00:12<00:35,  6.28batch/s, auc=0.9171, loss=0.6148]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9171, loss=0.6148]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9169, loss=0.6690]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9169, loss=0.6690]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9172, loss=0.5332]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9172, loss=0.5332]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9181, loss=0.3922]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 73/292 [00:12<00:34,  6.29batch/s, auc=0.9181, loss=0.3922]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 73/292 [00:12<00:34,  6.29batch/s, auc=0.9182, loss=0.6863]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9182, loss=0.6863]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9189, loss=0.4286]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 75/292 [00:12<00:34,  6.21batch/s, auc=0.9189, loss=0.4286]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 75/292 [00:12<00:34,  6.21batch/s, auc=0.9194, loss=0.4722]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 76/292 [00:12<00:34,  6.23batch/s, auc=0.9194, loss=0.4722]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 76/292 [00:13<00:34,  6.23batch/s, auc=0.9196, loss=0.4587]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9196, loss=0.4587]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9191, loss=0.9636]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 78/292 [00:13<00:34,  6.19batch/s, auc=0.9191, loss=0.9636]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 78/292 [00:13<00:34,  6.19batch/s, auc=0.9190, loss=0.7004]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 79/292 [00:13<00:34,  6.21batch/s, auc=0.9190, loss=0.7004]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 79/292 [00:13<00:34,  6.21batch/s, auc=0.9196, loss=0.4253]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 80/292 [00:13<00:34,  6.18batch/s, auc=0.9196, loss=0.4253]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 80/292 [00:13<00:34,  6.18batch/s, auc=0.9195, loss=0.6686]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 81/292 [00:13<00:34,  6.20batch/s, auc=0.9195, loss=0.6686]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 81/292 [00:13<00:34,  6.20batch/s, auc=0.9184, loss=1.1021]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 82/292 [00:13<00:33,  6.21batch/s, auc=0.9184, loss=1.1021]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 82/292 [00:14<00:33,  6.21batch/s, auc=0.9186, loss=0.6889]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 83/292 [00:14<00:33,  6.23batch/s, auc=0.9186, loss=0.6889]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 83/292 [00:14<00:33,  6.23batch/s, auc=0.9190, loss=0.5703]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9190, loss=0.5703]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9196, loss=0.4231]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 85/292 [00:14<00:33,  6.25batch/s, auc=0.9196, loss=0.4231]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 85/292 [00:14<00:33,  6.25batch/s, auc=0.9200, loss=0.4888]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 86/292 [00:14<00:32,  6.25batch/s, auc=0.9200, loss=0.4888]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 86/292 [00:14<00:32,  6.25batch/s, auc=0.9205, loss=0.5607]\u001b[A\n",
      "Training Epoch 11/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9205, loss=0.5607]\u001b[A\n",
      "Training Epoch 11/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9190, loss=0.9987]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 88/292 [00:14<00:32,  6.25batch/s, auc=0.9190, loss=0.9987]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 88/292 [00:15<00:32,  6.25batch/s, auc=0.9192, loss=0.4692]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9192, loss=0.4692]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9189, loss=0.5701]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9189, loss=0.5701]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9184, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 91/292 [00:15<00:32,  6.25batch/s, auc=0.9184, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 91/292 [00:15<00:32,  6.25batch/s, auc=0.9174, loss=0.8510]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 92/292 [00:15<00:31,  6.25batch/s, auc=0.9174, loss=0.8510]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 92/292 [00:15<00:31,  6.25batch/s, auc=0.9179, loss=0.4896]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9179, loss=0.4896]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9186, loss=0.5332]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9186, loss=0.5332]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 94/292 [00:16<00:31,  6.25batch/s, auc=0.9191, loss=0.4844]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 95/292 [00:16<00:31,  6.16batch/s, auc=0.9191, loss=0.4844]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 95/292 [00:16<00:31,  6.16batch/s, auc=0.9195, loss=0.4212]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 96/292 [00:16<00:31,  6.14batch/s, auc=0.9195, loss=0.4212]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 96/292 [00:16<00:31,  6.14batch/s, auc=0.9198, loss=0.5510]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 97/292 [00:16<00:31,  6.17batch/s, auc=0.9198, loss=0.5510]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 97/292 [00:16<00:31,  6.17batch/s, auc=0.9209, loss=0.3143]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▎      | 98/292 [00:16<00:31,  6.16batch/s, auc=0.9209, loss=0.3143]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▎      | 98/292 [00:16<00:31,  6.16batch/s, auc=0.9210, loss=0.5650]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 99/292 [00:16<00:31,  6.19batch/s, auc=0.9210, loss=0.5650]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 99/292 [00:16<00:31,  6.19batch/s, auc=0.9214, loss=0.4647]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9214, loss=0.4647]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 100/292 [00:17<00:30,  6.21batch/s, auc=0.9210, loss=0.8330]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 101/292 [00:17<00:30,  6.22batch/s, auc=0.9210, loss=0.8330]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 101/292 [00:17<00:30,  6.22batch/s, auc=0.9214, loss=0.4802]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9214, loss=0.4802]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9212, loss=0.6746]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9212, loss=0.6746]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9213, loss=0.5911]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9213, loss=0.5911]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9216, loss=0.6026]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 105/292 [00:17<00:30,  6.19batch/s, auc=0.9216, loss=0.6026]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 105/292 [00:17<00:30,  6.19batch/s, auc=0.9217, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▋      | 106/292 [00:17<00:30,  6.19batch/s, auc=0.9217, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▋      | 106/292 [00:17<00:30,  6.19batch/s, auc=0.9206, loss=1.1424]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 107/292 [00:17<00:29,  6.19batch/s, auc=0.9206, loss=1.1424]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 107/292 [00:18<00:29,  6.19batch/s, auc=0.9208, loss=0.6870]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9208, loss=0.6870]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9206, loss=0.7114]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9206, loss=0.7114]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9210, loss=0.4888]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 110/292 [00:18<00:29,  6.19batch/s, auc=0.9210, loss=0.4888]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 110/292 [00:18<00:29,  6.19batch/s, auc=0.9214, loss=0.5587]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9214, loss=0.5587]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9212, loss=0.5563]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 112/292 [00:18<00:28,  6.21batch/s, auc=0.9212, loss=0.5563]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 112/292 [00:18<00:28,  6.21batch/s, auc=0.9214, loss=0.4540]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▊      | 113/292 [00:18<00:28,  6.21batch/s, auc=0.9214, loss=0.4540]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▊      | 113/292 [00:19<00:28,  6.21batch/s, auc=0.9215, loss=0.6251]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 114/292 [00:19<00:28,  6.20batch/s, auc=0.9215, loss=0.6251]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 114/292 [00:19<00:28,  6.20batch/s, auc=0.9215, loss=0.5837]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9215, loss=0.5837]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9218, loss=0.5832]\u001b[A\n",
      "Training Epoch 11/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9218, loss=0.5832]\u001b[A\n",
      "Training Epoch 11/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9222, loss=0.4090]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9222, loss=0.4090]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9218, loss=0.6942]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9218, loss=0.6942]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9209, loss=1.2424]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 119/292 [00:19<00:28,  6.17batch/s, auc=0.9209, loss=1.2424]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 119/292 [00:20<00:28,  6.17batch/s, auc=0.9208, loss=0.5103]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 120/292 [00:20<00:27,  6.17batch/s, auc=0.9208, loss=0.5103]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 120/292 [00:20<00:27,  6.17batch/s, auc=0.9209, loss=0.4703]\u001b[A\n",
      "Training Epoch 11/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9209, loss=0.4703]\u001b[A\n",
      "Training Epoch 11/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9211, loss=0.4741]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 122/292 [00:20<00:27,  6.17batch/s, auc=0.9211, loss=0.4741]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 122/292 [00:20<00:27,  6.17batch/s, auc=0.9213, loss=0.4484]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9213, loss=0.4484]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9217, loss=0.3610]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 124/292 [00:20<00:27,  6.14batch/s, auc=0.9217, loss=0.3610]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 124/292 [00:20<00:27,  6.14batch/s, auc=0.9219, loss=0.5392]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 125/292 [00:20<00:27,  6.15batch/s, auc=0.9219, loss=0.5392]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 125/292 [00:21<00:27,  6.15batch/s, auc=0.9220, loss=0.6076]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 126/292 [00:21<00:26,  6.16batch/s, auc=0.9220, loss=0.6076]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 126/292 [00:21<00:26,  6.16batch/s, auc=0.9222, loss=0.5103]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9222, loss=0.5103]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9223, loss=0.4060]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9223, loss=0.4060]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9220, loss=0.6191]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9220, loss=0.6191]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9221, loss=0.6220]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 130/292 [00:21<00:26,  6.14batch/s, auc=0.9221, loss=0.6220]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 130/292 [00:21<00:26,  6.14batch/s, auc=0.9218, loss=0.8331]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 131/292 [00:21<00:26,  6.14batch/s, auc=0.9218, loss=0.8331]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 131/292 [00:22<00:26,  6.14batch/s, auc=0.9222, loss=0.4244]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▌     | 132/292 [00:22<00:26,  6.15batch/s, auc=0.9222, loss=0.4244]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▌     | 132/292 [00:22<00:26,  6.15batch/s, auc=0.9217, loss=0.8086]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9217, loss=0.8086]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9216, loss=0.6594]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9216, loss=0.6594]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9216, loss=0.5138]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9216, loss=0.5138]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9218, loss=0.5168]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9218, loss=0.5168]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9216, loss=0.7400]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9216, loss=0.7400]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9214, loss=0.5455]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 138/292 [00:22<00:24,  6.19batch/s, auc=0.9214, loss=0.5455]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 138/292 [00:23<00:24,  6.19batch/s, auc=0.9218, loss=0.3694]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9218, loss=0.3694]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9221, loss=0.3814]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9221, loss=0.3814]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9224, loss=0.4284]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9224, loss=0.4284]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9224, loss=0.5180]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9224, loss=0.5180]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9223, loss=0.7175]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9223, loss=0.7175]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9220, loss=0.7134]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 144/292 [00:23<00:24,  6.15batch/s, auc=0.9220, loss=0.7134]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 144/292 [00:24<00:24,  6.15batch/s, auc=0.9222, loss=0.4870]\u001b[A\n",
      "Training Epoch 11/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9222, loss=0.4870]\u001b[A\n",
      "Training Epoch 11/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9222, loss=0.5222]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9222, loss=0.5222]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9227, loss=0.3044]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9227, loss=0.3044]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9226, loss=0.5522]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9226, loss=0.5522]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9229, loss=0.5482]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 149/292 [00:24<00:23,  6.10batch/s, auc=0.9229, loss=0.5482]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 149/292 [00:24<00:23,  6.10batch/s, auc=0.9231, loss=0.3856]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.11batch/s, auc=0.9231, loss=0.3856]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.11batch/s, auc=0.9230, loss=0.6815]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.13batch/s, auc=0.9230, loss=0.6815]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.13batch/s, auc=0.9224, loss=0.9168]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9224, loss=0.9168]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9227, loss=0.3254]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9227, loss=0.3254]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9227, loss=0.6992]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9227, loss=0.6992]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9229, loss=0.3951]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9229, loss=0.3951]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9226, loss=0.9732]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.15batch/s, auc=0.9226, loss=0.9732]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.15batch/s, auc=0.9229, loss=0.4283]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9229, loss=0.4283]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9221, loss=1.3350]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9221, loss=1.3350]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9223, loss=0.3808]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9223, loss=0.3808]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9221, loss=0.6104]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9221, loss=0.6104]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9220, loss=0.6907]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9220, loss=0.6907]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9215, loss=0.8687]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.09batch/s, auc=0.9215, loss=0.8687]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.09batch/s, auc=0.9213, loss=0.6917]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.12batch/s, auc=0.9213, loss=0.6917]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.12batch/s, auc=0.9210, loss=0.6028]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9210, loss=0.6028]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9214, loss=0.3639]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9214, loss=0.3639]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9216, loss=0.4974]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.12batch/s, auc=0.9216, loss=0.4974]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.12batch/s, auc=0.9217, loss=0.4904]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9217, loss=0.4904]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9215, loss=0.5778]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9215, loss=0.5778]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.12batch/s, auc=0.9216, loss=0.6111]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.08batch/s, auc=0.9216, loss=0.6111]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.08batch/s, auc=0.9216, loss=0.5012]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.10batch/s, auc=0.9216, loss=0.5012]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.10batch/s, auc=0.9217, loss=0.6088]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.11batch/s, auc=0.9217, loss=0.6088]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.11batch/s, auc=0.9218, loss=0.6654]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9218, loss=0.6654]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9217, loss=0.5931]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.08batch/s, auc=0.9217, loss=0.5931]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.08batch/s, auc=0.9216, loss=0.5264]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.01batch/s, auc=0.9216, loss=0.5264]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.01batch/s, auc=0.9215, loss=0.7514]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.82batch/s, auc=0.9215, loss=0.7514]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.82batch/s, auc=0.9213, loss=0.8041]\u001b[A\n",
      "Training Epoch 11/25:  60%|██████    | 176/292 [00:29<00:19,  5.80batch/s, auc=0.9213, loss=0.8041]\u001b[A\n",
      "Training Epoch 11/25:  60%|██████    | 176/292 [00:29<00:19,  5.80batch/s, auc=0.9217, loss=0.3517]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 177/292 [00:29<00:19,  5.78batch/s, auc=0.9217, loss=0.3517]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 177/292 [00:29<00:19,  5.78batch/s, auc=0.9220, loss=0.3508]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 178/292 [00:29<00:19,  5.76batch/s, auc=0.9220, loss=0.3508]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 178/292 [00:29<00:19,  5.76batch/s, auc=0.9219, loss=0.8470]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.76batch/s, auc=0.9219, loss=0.8470]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.76batch/s, auc=0.9220, loss=0.4942]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.74batch/s, auc=0.9220, loss=0.4942]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.74batch/s, auc=0.9222, loss=0.4430]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.68batch/s, auc=0.9222, loss=0.4430]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.68batch/s, auc=0.9220, loss=0.6863]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.78batch/s, auc=0.9220, loss=0.6863]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.78batch/s, auc=0.9220, loss=0.6255]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 183/292 [00:30<00:19,  5.66batch/s, auc=0.9220, loss=0.6255]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 183/292 [00:30<00:19,  5.66batch/s, auc=0.9219, loss=0.6774]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 184/292 [00:30<00:19,  5.68batch/s, auc=0.9219, loss=0.6774]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 184/292 [00:30<00:19,  5.68batch/s, auc=0.9217, loss=0.7319]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.70batch/s, auc=0.9217, loss=0.7319]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.70batch/s, auc=0.9220, loss=0.4256]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▎   | 186/292 [00:30<00:18,  5.68batch/s, auc=0.9220, loss=0.4256]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▎   | 186/292 [00:31<00:18,  5.68batch/s, auc=0.9222, loss=0.4656]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.68batch/s, auc=0.9222, loss=0.4656]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.68batch/s, auc=0.9221, loss=0.7129]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.67batch/s, auc=0.9221, loss=0.7129]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.67batch/s, auc=0.9220, loss=0.7827]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▍   | 189/292 [00:31<00:18,  5.58batch/s, auc=0.9220, loss=0.7827]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▍   | 189/292 [00:31<00:18,  5.58batch/s, auc=0.9221, loss=0.5011]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 190/292 [00:31<00:18,  5.60batch/s, auc=0.9221, loss=0.5011]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 190/292 [00:31<00:18,  5.60batch/s, auc=0.9219, loss=0.6054]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.64batch/s, auc=0.9219, loss=0.6054]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 191/292 [00:32<00:17,  5.64batch/s, auc=0.9219, loss=0.5128]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.66batch/s, auc=0.9219, loss=0.5128]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.66batch/s, auc=0.9221, loss=0.4616]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.68batch/s, auc=0.9221, loss=0.4616]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.68batch/s, auc=0.9223, loss=0.4253]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.81batch/s, auc=0.9223, loss=0.4253]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.81batch/s, auc=0.9225, loss=0.4772]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.91batch/s, auc=0.9225, loss=0.4772]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.91batch/s, auc=0.9225, loss=0.6339]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.97batch/s, auc=0.9225, loss=0.6339]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.97batch/s, auc=0.9228, loss=0.4227]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.01batch/s, auc=0.9228, loss=0.4227]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 197/292 [00:33<00:15,  6.01batch/s, auc=0.9228, loss=0.6310]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 198/292 [00:33<00:15,  6.04batch/s, auc=0.9228, loss=0.6310]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 198/292 [00:33<00:15,  6.04batch/s, auc=0.9229, loss=0.4436]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.07batch/s, auc=0.9229, loss=0.4436]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.07batch/s, auc=0.9228, loss=0.6449]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.07batch/s, auc=0.9228, loss=0.6449]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.07batch/s, auc=0.9228, loss=0.5140]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.06batch/s, auc=0.9228, loss=0.5140]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 201/292 [00:33<00:15,  6.06batch/s, auc=0.9227, loss=0.6330]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.97batch/s, auc=0.9227, loss=0.6330]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.97batch/s, auc=0.9227, loss=0.5956]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.89batch/s, auc=0.9227, loss=0.5956]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.89batch/s, auc=0.9227, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.82batch/s, auc=0.9227, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.82batch/s, auc=0.9229, loss=0.5410]\u001b[A\n",
      "Training Epoch 11/25:  70%|███████   | 205/292 [00:34<00:15,  5.77batch/s, auc=0.9229, loss=0.5410]\u001b[A\n",
      "Training Epoch 11/25:  70%|███████   | 205/292 [00:34<00:15,  5.77batch/s, auc=0.9232, loss=0.4563]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 206/292 [00:34<00:14,  5.85batch/s, auc=0.9232, loss=0.4563]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 206/292 [00:34<00:14,  5.85batch/s, auc=0.9229, loss=0.8738]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 207/292 [00:34<00:14,  5.91batch/s, auc=0.9229, loss=0.8738]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 207/292 [00:34<00:14,  5.91batch/s, auc=0.9233, loss=0.3933]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 208/292 [00:34<00:14,  5.94batch/s, auc=0.9233, loss=0.3933]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 208/292 [00:34<00:14,  5.94batch/s, auc=0.9231, loss=1.0559]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 209/292 [00:34<00:13,  5.98batch/s, auc=0.9231, loss=1.0559]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 209/292 [00:35<00:13,  5.98batch/s, auc=0.9226, loss=1.1944]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 210/292 [00:35<00:13,  6.00batch/s, auc=0.9226, loss=1.1944]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 210/292 [00:35<00:13,  6.00batch/s, auc=0.9226, loss=0.5823]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.02batch/s, auc=0.9226, loss=0.5823]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.02batch/s, auc=0.9228, loss=0.4167]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.03batch/s, auc=0.9228, loss=0.4167]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.03batch/s, auc=0.9226, loss=0.6924]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.03batch/s, auc=0.9226, loss=0.6924]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.03batch/s, auc=0.9226, loss=0.6080]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.05batch/s, auc=0.9226, loss=0.6080]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.05batch/s, auc=0.9226, loss=0.4874]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.06batch/s, auc=0.9226, loss=0.4874]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▎  | 215/292 [00:36<00:12,  6.06batch/s, auc=0.9226, loss=0.6931]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 216/292 [00:36<00:12,  6.02batch/s, auc=0.9226, loss=0.6931]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 216/292 [00:36<00:12,  6.02batch/s, auc=0.9223, loss=0.8265]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 217/292 [00:36<00:12,  6.02batch/s, auc=0.9223, loss=0.8265]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 217/292 [00:36<00:12,  6.02batch/s, auc=0.9224, loss=0.4729]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.03batch/s, auc=0.9224, loss=0.4729]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.03batch/s, auc=0.9226, loss=0.5941]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 219/292 [00:36<00:12,  6.04batch/s, auc=0.9226, loss=0.5941]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 219/292 [00:36<00:12,  6.04batch/s, auc=0.9226, loss=0.4843]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.90batch/s, auc=0.9226, loss=0.4843]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.90batch/s, auc=0.9228, loss=0.3987]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.81batch/s, auc=0.9228, loss=0.3987]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.81batch/s, auc=0.9226, loss=0.9356]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.87batch/s, auc=0.9226, loss=0.9356]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.87batch/s, auc=0.9230, loss=0.3384]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.93batch/s, auc=0.9230, loss=0.3384]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.93batch/s, auc=0.9227, loss=0.6691]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.95batch/s, auc=0.9227, loss=0.6691]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.95batch/s, auc=0.9227, loss=0.4702]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.98batch/s, auc=0.9227, loss=0.4702]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.98batch/s, auc=0.9226, loss=0.5785]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.99batch/s, auc=0.9226, loss=0.5785]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.99batch/s, auc=0.9227, loss=0.4226]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.99batch/s, auc=0.9227, loss=0.4226]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 227/292 [00:38<00:10,  5.99batch/s, auc=0.9228, loss=0.4539]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.99batch/s, auc=0.9228, loss=0.4539]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.99batch/s, auc=0.9229, loss=0.4338]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.96batch/s, auc=0.9229, loss=0.4338]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.96batch/s, auc=0.9226, loss=0.7439]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.75batch/s, auc=0.9226, loss=0.7439]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.75batch/s, auc=0.9226, loss=0.5702]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.69batch/s, auc=0.9226, loss=0.5702]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.69batch/s, auc=0.9225, loss=0.5790]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9225, loss=0.5790]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9227, loss=0.4552]\u001b[A\n",
      "Training Epoch 11/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.70batch/s, auc=0.9227, loss=0.4552]\u001b[A\n",
      "Training Epoch 11/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.70batch/s, auc=0.9228, loss=0.5112]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 234/292 [00:39<00:10,  5.79batch/s, auc=0.9228, loss=0.5112]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 234/292 [00:39<00:10,  5.79batch/s, auc=0.9227, loss=0.7687]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 235/292 [00:39<00:09,  5.88batch/s, auc=0.9227, loss=0.7687]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 235/292 [00:39<00:09,  5.88batch/s, auc=0.9224, loss=0.8517]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 236/292 [00:39<00:09,  5.70batch/s, auc=0.9224, loss=0.8517]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 236/292 [00:39<00:09,  5.70batch/s, auc=0.9226, loss=0.4892]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 237/292 [00:39<00:09,  5.68batch/s, auc=0.9226, loss=0.4892]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 237/292 [00:39<00:09,  5.68batch/s, auc=0.9228, loss=0.4413]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.67batch/s, auc=0.9228, loss=0.4413]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.67batch/s, auc=0.9227, loss=0.6824]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.68batch/s, auc=0.9227, loss=0.6824]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.68batch/s, auc=0.9228, loss=0.4578]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.68batch/s, auc=0.9228, loss=0.4578]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.68batch/s, auc=0.9227, loss=0.6574]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.66batch/s, auc=0.9227, loss=0.6574]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.66batch/s, auc=0.9228, loss=0.4254]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.64batch/s, auc=0.9228, loss=0.4254]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.64batch/s, auc=0.9230, loss=0.4488]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.65batch/s, auc=0.9230, loss=0.4488]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.65batch/s, auc=0.9231, loss=0.4921]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.67batch/s, auc=0.9231, loss=0.4921]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.67batch/s, auc=0.9230, loss=0.5345]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.68batch/s, auc=0.9230, loss=0.5345]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.68batch/s, auc=0.9230, loss=0.5923]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9230, loss=0.5923]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9231, loss=0.4873]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.64batch/s, auc=0.9231, loss=0.4873]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.64batch/s, auc=0.9231, loss=0.6971]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9231, loss=0.6971]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9224, loss=1.6010]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.68batch/s, auc=0.9224, loss=1.6010]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.68batch/s, auc=0.9225, loss=0.4450]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.78batch/s, auc=0.9225, loss=0.4450]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.78batch/s, auc=0.9226, loss=0.4048]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.84batch/s, auc=0.9226, loss=0.4048]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.84batch/s, auc=0.9226, loss=0.7430]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.90batch/s, auc=0.9226, loss=0.7430]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.90batch/s, auc=0.9224, loss=0.7673]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.93batch/s, auc=0.9224, loss=0.7673]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.93batch/s, auc=0.9223, loss=0.9188]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.95batch/s, auc=0.9223, loss=0.9188]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.95batch/s, auc=0.9223, loss=0.6648]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.96batch/s, auc=0.9223, loss=0.6648]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.96batch/s, auc=0.9218, loss=1.2633]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.87batch/s, auc=0.9218, loss=1.2633]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.87batch/s, auc=0.9219, loss=0.5073]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.78batch/s, auc=0.9219, loss=0.5073]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.78batch/s, auc=0.9218, loss=0.6587]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.73batch/s, auc=0.9218, loss=0.6587]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.73batch/s, auc=0.9218, loss=0.5614]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.69batch/s, auc=0.9218, loss=0.5614]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.69batch/s, auc=0.9214, loss=0.9891]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9214, loss=0.9891]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9216, loss=0.4301]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.63batch/s, auc=0.9216, loss=0.4301]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.63batch/s, auc=0.9215, loss=0.5836]\u001b[A\n",
      "Training Epoch 11/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.57batch/s, auc=0.9215, loss=0.5836]\u001b[A\n",
      "Training Epoch 11/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.57batch/s, auc=0.9216, loss=0.6424]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 263/292 [00:44<00:05,  5.56batch/s, auc=0.9216, loss=0.6424]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 263/292 [00:44<00:05,  5.56batch/s, auc=0.9215, loss=0.6329]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 264/292 [00:44<00:05,  5.57batch/s, auc=0.9215, loss=0.6329]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 264/292 [00:44<00:05,  5.57batch/s, auc=0.9213, loss=0.7143]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 265/292 [00:44<00:04,  5.56batch/s, auc=0.9213, loss=0.7143]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 265/292 [00:44<00:04,  5.56batch/s, auc=0.9214, loss=0.4816]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 266/292 [00:44<00:04,  5.59batch/s, auc=0.9214, loss=0.4816]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 266/292 [00:44<00:04,  5.59batch/s, auc=0.9211, loss=0.9681]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.60batch/s, auc=0.9211, loss=0.9681]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.60batch/s, auc=0.9213, loss=0.4203]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9213, loss=0.4203]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9213, loss=0.5964]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.65batch/s, auc=0.9213, loss=0.5964]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.65batch/s, auc=0.9211, loss=0.8046]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9211, loss=0.8046]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9210, loss=0.7441]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9210, loss=0.7441]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9209, loss=0.6356]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.61batch/s, auc=0.9209, loss=0.6356]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.61batch/s, auc=0.9207, loss=1.0008]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.60batch/s, auc=0.9207, loss=1.0008]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.60batch/s, auc=0.9206, loss=0.5705]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.58batch/s, auc=0.9206, loss=0.5705]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.58batch/s, auc=0.9206, loss=0.6252]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.57batch/s, auc=0.9206, loss=0.6252]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.57batch/s, auc=0.9207, loss=0.4520]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.69batch/s, auc=0.9207, loss=0.4520]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.69batch/s, auc=0.9208, loss=0.5020]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.66batch/s, auc=0.9208, loss=0.5020]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.66batch/s, auc=0.9207, loss=0.9412]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.75batch/s, auc=0.9207, loss=0.9412]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.75batch/s, auc=0.9207, loss=0.5419]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.69batch/s, auc=0.9207, loss=0.5419]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.69batch/s, auc=0.9208, loss=0.5663]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9208, loss=0.5663]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9209, loss=0.4927]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9209, loss=0.4927]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9208, loss=0.7626]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.60batch/s, auc=0.9208, loss=0.7626]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.60batch/s, auc=0.9207, loss=0.5794]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.60batch/s, auc=0.9207, loss=0.5794]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.60batch/s, auc=0.9208, loss=0.5138]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.56batch/s, auc=0.9208, loss=0.5138]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.56batch/s, auc=0.9206, loss=0.7092]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9206, loss=0.7092]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9207, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.72batch/s, auc=0.9207, loss=0.5351]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.72batch/s, auc=0.9206, loss=0.5593]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9206, loss=0.5593]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9207, loss=0.4275]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.88batch/s, auc=0.9207, loss=0.4275]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.88batch/s, auc=0.9207, loss=0.6031]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.93batch/s, auc=0.9207, loss=0.6031]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.93batch/s, auc=0.9207, loss=0.5284]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.94batch/s, auc=0.9207, loss=0.5284]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.94batch/s, auc=0.9208, loss=0.4228]\u001b[A\n",
      "Training Epoch 11/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.95batch/s, auc=0.9208, loss=0.4228]\u001b[A\n",
      "Training Epoch 11/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.95batch/s, auc=0.9205, loss=1.6987]\u001b[A\n",
      "Training Epoch 11/25: 100%|██████████| 292/292 [00:49<00:00,  5.92batch/s, auc=0.9205, loss=1.6987]\u001b[A\n",
      "Epochs:  44%|████▍     | 11/25 [10:03<12:50, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25] Train Loss: 0.6122 | Train AUROC: 0.9205 Val Loss: 0.6908 | Val AUROC: 0.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 12/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8396, loss=0.8210]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 1/292 [00:01<04:52,  1.01s/batch, auc=0.8396, loss=0.8210]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 1/292 [00:01<04:52,  1.01s/batch, auc=0.8815, loss=0.4846]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 2/292 [00:01<02:26,  1.97batch/s, auc=0.8815, loss=0.4846]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 2/292 [00:01<02:26,  1.97batch/s, auc=0.8825, loss=0.6313]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.8825, loss=0.6313]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.9021, loss=0.5473]\u001b[A\n",
      "Training Epoch 12/25:   1%|▏         | 4/292 [00:01<01:20,  3.57batch/s, auc=0.9021, loss=0.5473]\u001b[A\n",
      "Training Epoch 12/25:   1%|▏         | 4/292 [00:01<01:20,  3.57batch/s, auc=0.9030, loss=0.5252]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 5/292 [00:01<01:07,  4.24batch/s, auc=0.9030, loss=0.5252]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 5/292 [00:01<01:07,  4.24batch/s, auc=0.9124, loss=0.5172]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 6/292 [00:01<01:01,  4.67batch/s, auc=0.9124, loss=0.5172]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 6/292 [00:01<01:01,  4.67batch/s, auc=0.9067, loss=0.7053]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 7/292 [00:01<00:56,  5.03batch/s, auc=0.9067, loss=0.7053]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 7/292 [00:02<00:56,  5.03batch/s, auc=0.9041, loss=0.7548]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 8/292 [00:02<00:52,  5.39batch/s, auc=0.9041, loss=0.7548]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 8/292 [00:02<00:52,  5.39batch/s, auc=0.9096, loss=0.4827]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 9/292 [00:02<00:51,  5.53batch/s, auc=0.9096, loss=0.4827]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 9/292 [00:02<00:51,  5.53batch/s, auc=0.9076, loss=0.6941]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 10/292 [00:02<00:49,  5.71batch/s, auc=0.9076, loss=0.6941]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 10/292 [00:02<00:49,  5.71batch/s, auc=0.9118, loss=0.5551]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 11/292 [00:02<00:47,  5.90batch/s, auc=0.9118, loss=0.5551]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 11/292 [00:02<00:47,  5.90batch/s, auc=0.9140, loss=0.5275]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 12/292 [00:02<00:46,  6.04batch/s, auc=0.9140, loss=0.5275]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 12/292 [00:02<00:46,  6.04batch/s, auc=0.9200, loss=0.4886]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 13/292 [00:02<00:45,  6.10batch/s, auc=0.9200, loss=0.4886]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 13/292 [00:03<00:45,  6.10batch/s, auc=0.9178, loss=0.7623]\u001b[A\n",
      "Training Epoch 12/25:   5%|▍         | 14/292 [00:03<00:45,  6.13batch/s, auc=0.9178, loss=0.7623]\u001b[A\n",
      "Training Epoch 12/25:   5%|▍         | 14/292 [00:03<00:45,  6.13batch/s, auc=0.9179, loss=0.6810]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 15/292 [00:03<00:44,  6.20batch/s, auc=0.9179, loss=0.6810]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 15/292 [00:03<00:44,  6.20batch/s, auc=0.9204, loss=0.4978]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9204, loss=0.4978]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9236, loss=0.4218]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 17/292 [00:03<00:44,  6.25batch/s, auc=0.9236, loss=0.4218]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 17/292 [00:03<00:44,  6.25batch/s, auc=0.9221, loss=0.6281]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9221, loss=0.6281]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9199, loss=0.6296]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 19/292 [00:03<00:43,  6.28batch/s, auc=0.9199, loss=0.6296]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 19/292 [00:04<00:43,  6.28batch/s, auc=0.9215, loss=0.5271]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 20/292 [00:04<00:43,  6.31batch/s, auc=0.9215, loss=0.5271]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 20/292 [00:04<00:43,  6.31batch/s, auc=0.9218, loss=0.5416]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9218, loss=0.5416]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9228, loss=0.4434]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.9228, loss=0.4434]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.9235, loss=0.4711]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9235, loss=0.4711]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9240, loss=0.4994]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9240, loss=0.4994]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9251, loss=0.5640]\u001b[A\n",
      "Training Epoch 12/25:   9%|▊         | 25/292 [00:04<00:42,  6.35batch/s, auc=0.9251, loss=0.5640]\u001b[A\n",
      "Training Epoch 12/25:   9%|▊         | 25/292 [00:05<00:42,  6.35batch/s, auc=0.9277, loss=0.4279]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 26/292 [00:05<00:41,  6.35batch/s, auc=0.9277, loss=0.4279]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 26/292 [00:05<00:41,  6.35batch/s, auc=0.9286, loss=0.4147]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9286, loss=0.4147]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9288, loss=0.5426]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9288, loss=0.5426]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9297, loss=0.5517]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9297, loss=0.5517]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9301, loss=0.5141]\u001b[A\n",
      "Training Epoch 12/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9301, loss=0.5141]\u001b[A\n",
      "Training Epoch 12/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9308, loss=0.4458]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9308, loss=0.4458]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9301, loss=0.6833]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 32/292 [00:05<00:41,  6.34batch/s, auc=0.9301, loss=0.6833]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 32/292 [00:06<00:41,  6.34batch/s, auc=0.9302, loss=0.4898]\u001b[A\n",
      "Training Epoch 12/25:  11%|█▏        | 33/292 [00:06<00:40,  6.35batch/s, auc=0.9302, loss=0.4898]\u001b[A\n",
      "Training Epoch 12/25:  11%|█▏        | 33/292 [00:06<00:40,  6.35batch/s, auc=0.9305, loss=0.4885]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9305, loss=0.4885]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9318, loss=0.4397]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9318, loss=0.4397]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9321, loss=0.6734]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 36/292 [00:06<00:40,  6.28batch/s, auc=0.9321, loss=0.6734]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 36/292 [00:06<00:40,  6.28batch/s, auc=0.9312, loss=0.5846]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9312, loss=0.5846]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9321, loss=0.3665]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9321, loss=0.3665]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9319, loss=0.5777]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9319, loss=0.5777]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9329, loss=0.3852]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▎        | 40/292 [00:07<00:39,  6.31batch/s, auc=0.9329, loss=0.3852]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▎        | 40/292 [00:07<00:39,  6.31batch/s, auc=0.9320, loss=0.7046]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 41/292 [00:07<00:39,  6.31batch/s, auc=0.9320, loss=0.7046]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 41/292 [00:07<00:39,  6.31batch/s, auc=0.9327, loss=0.4086]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 42/292 [00:07<00:39,  6.31batch/s, auc=0.9327, loss=0.4086]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 42/292 [00:07<00:39,  6.31batch/s, auc=0.9312, loss=0.8937]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9312, loss=0.8937]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9303, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.9303, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.9302, loss=0.5815]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9302, loss=0.5815]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9304, loss=0.4501]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9304, loss=0.4501]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9290, loss=0.7569]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 47/292 [00:08<00:38,  6.31batch/s, auc=0.9290, loss=0.7569]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 47/292 [00:08<00:38,  6.31batch/s, auc=0.9300, loss=0.3860]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9300, loss=0.3860]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9294, loss=0.7598]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9294, loss=0.7598]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9289, loss=0.5613]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9289, loss=0.5613]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9288, loss=0.6401]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 51/292 [00:08<00:38,  6.29batch/s, auc=0.9288, loss=0.6401]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9275, loss=0.8087]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.9275, loss=0.8087]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 52/292 [00:09<00:38,  6.30batch/s, auc=0.9279, loss=0.4706]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9279, loss=0.4706]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9279, loss=0.6040]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9279, loss=0.6040]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9273, loss=0.6931]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9273, loss=0.6931]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9280, loss=0.4749]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 56/292 [00:09<00:37,  6.28batch/s, auc=0.9280, loss=0.4749]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 56/292 [00:09<00:37,  6.28batch/s, auc=0.9272, loss=0.7982]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 57/292 [00:09<00:37,  6.28batch/s, auc=0.9272, loss=0.7982]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 57/292 [00:10<00:37,  6.28batch/s, auc=0.9276, loss=0.4824]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 58/292 [00:10<00:37,  6.27batch/s, auc=0.9276, loss=0.4824]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 58/292 [00:10<00:37,  6.27batch/s, auc=0.9269, loss=0.5979]\u001b[A\n",
      "Training Epoch 12/25:  20%|██        | 59/292 [00:10<00:37,  6.28batch/s, auc=0.9269, loss=0.5979]\u001b[A\n",
      "Training Epoch 12/25:  20%|██        | 59/292 [00:10<00:37,  6.28batch/s, auc=0.9264, loss=0.6384]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 60/292 [00:10<00:36,  6.27batch/s, auc=0.9264, loss=0.6384]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 60/292 [00:10<00:36,  6.27batch/s, auc=0.9264, loss=0.6935]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9264, loss=0.6935]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9244, loss=1.0360]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 62/292 [00:10<00:36,  6.26batch/s, auc=0.9244, loss=1.0360]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 62/292 [00:10<00:36,  6.26batch/s, auc=0.9245, loss=0.6283]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 63/292 [00:10<00:36,  6.26batch/s, auc=0.9245, loss=0.6283]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 63/292 [00:11<00:36,  6.26batch/s, auc=0.9235, loss=1.0544]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 64/292 [00:11<00:36,  6.27batch/s, auc=0.9235, loss=1.0544]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 64/292 [00:11<00:36,  6.27batch/s, auc=0.9244, loss=0.4778]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9244, loss=0.4778]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 65/292 [00:11<00:36,  6.27batch/s, auc=0.9236, loss=0.9702]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9236, loss=0.9702]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9236, loss=0.5471]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 67/292 [00:11<00:35,  6.25batch/s, auc=0.9236, loss=0.5471]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 67/292 [00:11<00:35,  6.25batch/s, auc=0.9236, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9236, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9240, loss=0.4466]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9240, loss=0.4466]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▎       | 69/292 [00:12<00:35,  6.28batch/s, auc=0.9241, loss=0.6216]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9241, loss=0.6216]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9236, loss=0.6188]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9236, loss=0.6188]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9234, loss=0.5635]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9234, loss=0.5635]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9232, loss=0.5400]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9232, loss=0.5400]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9232, loss=0.5610]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9232, loss=0.5610]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9235, loss=0.4898]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9235, loss=0.4898]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9234, loss=0.5561]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 76/292 [00:12<00:34,  6.28batch/s, auc=0.9234, loss=0.5561]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 76/292 [00:13<00:34,  6.28batch/s, auc=0.9236, loss=0.4925]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.9236, loss=0.4925]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▋       | 77/292 [00:13<00:34,  6.29batch/s, auc=0.9230, loss=0.7350]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 78/292 [00:13<00:34,  6.29batch/s, auc=0.9230, loss=0.7350]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 78/292 [00:13<00:34,  6.29batch/s, auc=0.9226, loss=0.8351]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 79/292 [00:13<00:33,  6.29batch/s, auc=0.9226, loss=0.8351]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 79/292 [00:13<00:33,  6.29batch/s, auc=0.9223, loss=0.6046]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 80/292 [00:13<00:33,  6.29batch/s, auc=0.9223, loss=0.6046]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 80/292 [00:13<00:33,  6.29batch/s, auc=0.9220, loss=0.6293]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 81/292 [00:13<00:33,  6.29batch/s, auc=0.9220, loss=0.6293]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 81/292 [00:13<00:33,  6.29batch/s, auc=0.9212, loss=0.9858]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 82/292 [00:13<00:33,  6.29batch/s, auc=0.9212, loss=0.9858]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 82/292 [00:14<00:33,  6.29batch/s, auc=0.9208, loss=0.7168]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 83/292 [00:14<00:33,  6.29batch/s, auc=0.9208, loss=0.7168]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 83/292 [00:14<00:33,  6.29batch/s, auc=0.9209, loss=0.6211]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9209, loss=0.6211]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9212, loss=0.5203]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 85/292 [00:14<00:32,  6.28batch/s, auc=0.9212, loss=0.5203]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 85/292 [00:14<00:32,  6.28batch/s, auc=0.9214, loss=0.4617]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.9214, loss=0.4617]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.9222, loss=0.4107]\u001b[A\n",
      "Training Epoch 12/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9222, loss=0.4107]\u001b[A\n",
      "Training Epoch 12/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9227, loss=0.3733]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 88/292 [00:14<00:32,  6.24batch/s, auc=0.9227, loss=0.3733]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 88/292 [00:15<00:32,  6.24batch/s, auc=0.9230, loss=0.4574]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9230, loss=0.4574]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9235, loss=0.5495]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9235, loss=0.5495]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9240, loss=0.4246]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9240, loss=0.4246]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9241, loss=0.4925]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 92/292 [00:15<00:31,  6.25batch/s, auc=0.9241, loss=0.4925]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 92/292 [00:15<00:31,  6.25batch/s, auc=0.9242, loss=0.5272]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9242, loss=0.5272]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9240, loss=0.6699]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9240, loss=0.6699]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9244, loss=0.4655]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 95/292 [00:15<00:31,  6.26batch/s, auc=0.9244, loss=0.4655]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 95/292 [00:16<00:31,  6.26batch/s, auc=0.9244, loss=0.4770]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 96/292 [00:16<00:31,  6.24batch/s, auc=0.9244, loss=0.4770]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 96/292 [00:16<00:31,  6.24batch/s, auc=0.9242, loss=0.6808]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9242, loss=0.6808]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9244, loss=0.5776]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▎      | 98/292 [00:16<00:31,  6.22batch/s, auc=0.9244, loss=0.5776]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▎      | 98/292 [00:16<00:31,  6.22batch/s, auc=0.9246, loss=0.4565]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9246, loss=0.4565]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9252, loss=0.4038]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 100/292 [00:16<00:30,  6.23batch/s, auc=0.9252, loss=0.4038]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 100/292 [00:16<00:30,  6.23batch/s, auc=0.9258, loss=0.3011]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 101/292 [00:16<00:30,  6.24batch/s, auc=0.9258, loss=0.3011]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 101/292 [00:17<00:30,  6.24batch/s, auc=0.9260, loss=0.4696]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9260, loss=0.4696]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9264, loss=0.5139]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▌      | 103/292 [00:17<00:30,  6.23batch/s, auc=0.9264, loss=0.5139]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▌      | 103/292 [00:17<00:30,  6.23batch/s, auc=0.9269, loss=0.3889]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9269, loss=0.3889]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9275, loss=0.4438]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9275, loss=0.4438]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9277, loss=0.4300]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▋      | 106/292 [00:17<00:29,  6.20batch/s, auc=0.9277, loss=0.4300]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▋      | 106/292 [00:17<00:29,  6.20batch/s, auc=0.9276, loss=0.7374]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 107/292 [00:17<00:29,  6.20batch/s, auc=0.9276, loss=0.7374]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 107/292 [00:18<00:29,  6.20batch/s, auc=0.9271, loss=0.6797]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9271, loss=0.6797]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9269, loss=0.5747]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9269, loss=0.5747]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9268, loss=0.6782]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9268, loss=0.6782]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9272, loss=0.3825]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9272, loss=0.3825]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9271, loss=0.5257]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9271, loss=0.5257]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9269, loss=0.7602]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▊      | 113/292 [00:18<00:28,  6.19batch/s, auc=0.9269, loss=0.7602]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▊      | 113/292 [00:19<00:28,  6.19batch/s, auc=0.9269, loss=0.4934]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9269, loss=0.4934]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9270, loss=0.5245]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 115/292 [00:19<00:28,  6.18batch/s, auc=0.9270, loss=0.5245]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 115/292 [00:19<00:28,  6.18batch/s, auc=0.9273, loss=0.3326]\u001b[A\n",
      "Training Epoch 12/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9273, loss=0.3326]\u001b[A\n",
      "Training Epoch 12/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9273, loss=0.5141]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 117/292 [00:19<00:28,  6.20batch/s, auc=0.9273, loss=0.5141]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 117/292 [00:19<00:28,  6.20batch/s, auc=0.9273, loss=0.4563]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9273, loss=0.4563]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9273, loss=0.5838]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 119/292 [00:19<00:27,  6.21batch/s, auc=0.9273, loss=0.5838]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 119/292 [00:20<00:27,  6.21batch/s, auc=0.9273, loss=0.8963]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9273, loss=0.8963]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9272, loss=0.5242]\u001b[A\n",
      "Training Epoch 12/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9272, loss=0.5242]\u001b[A\n",
      "Training Epoch 12/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9275, loss=0.4126]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9275, loss=0.4126]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9270, loss=0.8357]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 123/292 [00:20<00:27,  6.20batch/s, auc=0.9270, loss=0.8357]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 123/292 [00:20<00:27,  6.20batch/s, auc=0.9264, loss=1.0344]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9264, loss=1.0344]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9265, loss=0.4553]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 125/292 [00:20<00:26,  6.20batch/s, auc=0.9265, loss=0.4553]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 125/292 [00:20<00:26,  6.20batch/s, auc=0.9265, loss=0.6723]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 126/292 [00:20<00:26,  6.20batch/s, auc=0.9265, loss=0.6723]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9267, loss=0.4786]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 127/292 [00:21<00:26,  6.20batch/s, auc=0.9267, loss=0.4786]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 127/292 [00:21<00:26,  6.20batch/s, auc=0.9269, loss=0.4178]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9269, loss=0.4178]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9264, loss=1.0345]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9264, loss=1.0345]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9265, loss=0.4823]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9265, loss=0.4823]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9265, loss=0.5395]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 131/292 [00:21<00:25,  6.19batch/s, auc=0.9265, loss=0.5395]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 131/292 [00:21<00:25,  6.19batch/s, auc=0.9265, loss=0.5341]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▌     | 132/292 [00:21<00:25,  6.16batch/s, auc=0.9265, loss=0.5341]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9266, loss=0.4335]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9266, loss=0.4335]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 133/292 [00:22<00:25,  6.17batch/s, auc=0.9265, loss=0.5443]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9265, loss=0.5443]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9267, loss=0.4784]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9267, loss=0.4784]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9267, loss=0.5471]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 136/292 [00:22<00:25,  6.07batch/s, auc=0.9267, loss=0.5471]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 136/292 [00:22<00:25,  6.07batch/s, auc=0.9267, loss=0.5715]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 137/292 [00:22<00:25,  6.11batch/s, auc=0.9267, loss=0.5715]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 137/292 [00:22<00:25,  6.11batch/s, auc=0.9264, loss=0.6694]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 138/292 [00:22<00:25,  6.11batch/s, auc=0.9264, loss=0.6694]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 138/292 [00:23<00:25,  6.11batch/s, auc=0.9264, loss=0.6266]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 139/292 [00:23<00:25,  6.12batch/s, auc=0.9264, loss=0.6266]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 139/292 [00:23<00:25,  6.12batch/s, auc=0.9264, loss=0.5718]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 140/292 [00:23<00:24,  6.13batch/s, auc=0.9264, loss=0.5718]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 140/292 [00:23<00:24,  6.13batch/s, auc=0.9266, loss=0.4244]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 141/292 [00:23<00:24,  6.13batch/s, auc=0.9266, loss=0.4244]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 141/292 [00:23<00:24,  6.13batch/s, auc=0.9264, loss=0.6488]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▊     | 142/292 [00:23<00:24,  6.13batch/s, auc=0.9264, loss=0.6488]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▊     | 142/292 [00:23<00:24,  6.13batch/s, auc=0.9266, loss=0.4404]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 143/292 [00:23<00:24,  6.14batch/s, auc=0.9266, loss=0.4404]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 143/292 [00:23<00:24,  6.14batch/s, auc=0.9263, loss=0.7477]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 144/292 [00:23<00:24,  6.15batch/s, auc=0.9263, loss=0.7477]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 144/292 [00:24<00:24,  6.15batch/s, auc=0.9263, loss=0.4418]\u001b[A\n",
      "Training Epoch 12/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9263, loss=0.4418]\u001b[A\n",
      "Training Epoch 12/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9263, loss=0.4821]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9263, loss=0.4821]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9265, loss=0.5461]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9265, loss=0.5461]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9265, loss=0.5406]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9265, loss=0.5406]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9266, loss=0.4422]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9266, loss=0.4422]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9263, loss=0.8882]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.17batch/s, auc=0.9263, loss=0.8882]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.17batch/s, auc=0.9263, loss=0.5753]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.18batch/s, auc=0.9263, loss=0.5753]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.18batch/s, auc=0.9263, loss=0.4362]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9263, loss=0.4362]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9266, loss=0.4087]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9266, loss=0.4087]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9260, loss=0.9403]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9260, loss=0.9403]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9258, loss=0.7173]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9258, loss=0.7173]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9258, loss=0.5109]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9258, loss=0.5109]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.14batch/s, auc=0.9255, loss=0.7484]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.14batch/s, auc=0.9255, loss=0.7484]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.14batch/s, auc=0.9250, loss=0.9107]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.16batch/s, auc=0.9250, loss=0.9107]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.16batch/s, auc=0.9251, loss=0.4816]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9251, loss=0.4816]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.17batch/s, auc=0.9252, loss=0.5505]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9252, loss=0.5505]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9254, loss=0.5284]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.16batch/s, auc=0.9254, loss=0.5284]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.16batch/s, auc=0.9252, loss=0.7363]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9252, loss=0.7363]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.14batch/s, auc=0.9251, loss=0.7935]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9251, loss=0.7935]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9255, loss=0.3305]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9255, loss=0.3305]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9257, loss=0.4579]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9257, loss=0.4579]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9255, loss=0.7176]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9255, loss=0.7176]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9257, loss=0.5008]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9257, loss=0.5008]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9260, loss=0.3409]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9260, loss=0.3409]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9262, loss=0.4568]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.10batch/s, auc=0.9262, loss=0.4568]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.10batch/s, auc=0.9260, loss=0.7628]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.10batch/s, auc=0.9260, loss=0.7628]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.10batch/s, auc=0.9262, loss=0.5278]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.09batch/s, auc=0.9262, loss=0.5278]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.09batch/s, auc=0.9263, loss=0.5634]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9263, loss=0.5634]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9264, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.09batch/s, auc=0.9264, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.09batch/s, auc=0.9268, loss=0.3034]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.10batch/s, auc=0.9268, loss=0.3034]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.10batch/s, auc=0.9266, loss=0.6303]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.11batch/s, auc=0.9266, loss=0.6303]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.11batch/s, auc=0.9265, loss=0.6690]\u001b[A\n",
      "Training Epoch 12/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9265, loss=0.6690]\u001b[A\n",
      "Training Epoch 12/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9267, loss=0.5818]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 177/292 [00:29<00:18,  6.11batch/s, auc=0.9267, loss=0.5818]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 177/292 [00:29<00:18,  6.11batch/s, auc=0.9266, loss=0.6025]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9266, loss=0.6025]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9266, loss=0.6379]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.12batch/s, auc=0.9266, loss=0.6379]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.12batch/s, auc=0.9262, loss=0.6389]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.13batch/s, auc=0.9262, loss=0.6389]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.13batch/s, auc=0.9260, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.14batch/s, auc=0.9260, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.14batch/s, auc=0.9262, loss=0.6060]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9262, loss=0.6060]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9263, loss=0.5566]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9263, loss=0.5566]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9265, loss=0.5105]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9265, loss=0.5105]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9264, loss=0.6103]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.11batch/s, auc=0.9264, loss=0.6103]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.11batch/s, auc=0.9265, loss=0.6101]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.11batch/s, auc=0.9265, loss=0.6101]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.11batch/s, auc=0.9267, loss=0.4168]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.11batch/s, auc=0.9267, loss=0.4168]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.11batch/s, auc=0.9267, loss=0.4838]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.06batch/s, auc=0.9267, loss=0.4838]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.06batch/s, auc=0.9264, loss=0.8296]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.07batch/s, auc=0.9264, loss=0.8296]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.07batch/s, auc=0.9265, loss=0.5195]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.07batch/s, auc=0.9265, loss=0.5195]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.07batch/s, auc=0.9267, loss=0.4931]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9267, loss=0.4931]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9268, loss=0.4686]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.08batch/s, auc=0.9268, loss=0.4686]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.08batch/s, auc=0.9264, loss=0.8688]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.08batch/s, auc=0.9264, loss=0.8688]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.9267, loss=0.3862]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9267, loss=0.3862]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9263, loss=0.9218]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9263, loss=0.9218]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9261, loss=0.9188]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9261, loss=0.9188]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9257, loss=1.0329]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.09batch/s, auc=0.9257, loss=1.0329]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.09batch/s, auc=0.9257, loss=0.6021]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.09batch/s, auc=0.9257, loss=0.6021]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.09batch/s, auc=0.9255, loss=0.7148]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.10batch/s, auc=0.9255, loss=0.7148]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.10batch/s, auc=0.9252, loss=0.7481]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.10batch/s, auc=0.9252, loss=0.7481]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.10batch/s, auc=0.9250, loss=0.6423]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.09batch/s, auc=0.9250, loss=0.6423]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.09batch/s, auc=0.9250, loss=0.5704]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.09batch/s, auc=0.9250, loss=0.5704]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.09batch/s, auc=0.9248, loss=0.7310]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.07batch/s, auc=0.9248, loss=0.7310]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.07batch/s, auc=0.9247, loss=0.6326]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.07batch/s, auc=0.9247, loss=0.6326]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.07batch/s, auc=0.9246, loss=0.7025]\u001b[A\n",
      "Training Epoch 12/25:  70%|███████   | 205/292 [00:33<00:14,  6.06batch/s, auc=0.9246, loss=0.7025]\u001b[A\n",
      "Training Epoch 12/25:  70%|███████   | 205/292 [00:34<00:14,  6.06batch/s, auc=0.9244, loss=0.6565]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 206/292 [00:34<00:14,  5.82batch/s, auc=0.9244, loss=0.6565]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 206/292 [00:34<00:14,  5.82batch/s, auc=0.9236, loss=1.4409]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 207/292 [00:34<00:14,  5.90batch/s, auc=0.9236, loss=1.4409]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 207/292 [00:34<00:14,  5.90batch/s, auc=0.9233, loss=0.8898]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 208/292 [00:34<00:14,  5.82batch/s, auc=0.9233, loss=0.8898]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 208/292 [00:34<00:14,  5.82batch/s, auc=0.9233, loss=0.6392]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.88batch/s, auc=0.9233, loss=0.6392]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.88batch/s, auc=0.9235, loss=0.4749]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.80batch/s, auc=0.9235, loss=0.4749]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.80batch/s, auc=0.9236, loss=0.5776]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 211/292 [00:34<00:14,  5.77batch/s, auc=0.9236, loss=0.5776]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.77batch/s, auc=0.9234, loss=0.7525]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.76batch/s, auc=0.9234, loss=0.7525]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.76batch/s, auc=0.9233, loss=0.5706]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.84batch/s, auc=0.9233, loss=0.5706]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.84batch/s, auc=0.9231, loss=0.7885]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.66batch/s, auc=0.9231, loss=0.7885]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.66batch/s, auc=0.9233, loss=0.4806]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.78batch/s, auc=0.9233, loss=0.4806]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.78batch/s, auc=0.9234, loss=0.4288]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.73batch/s, auc=0.9234, loss=0.4288]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.73batch/s, auc=0.9232, loss=0.6246]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.71batch/s, auc=0.9232, loss=0.6246]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.71batch/s, auc=0.9233, loss=0.5344]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.72batch/s, auc=0.9233, loss=0.5344]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.72batch/s, auc=0.9234, loss=0.5442]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9234, loss=0.5442]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9235, loss=0.5119]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.64batch/s, auc=0.9235, loss=0.5119]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.64batch/s, auc=0.9235, loss=0.7241]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.65batch/s, auc=0.9235, loss=0.7241]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.65batch/s, auc=0.9236, loss=0.4263]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.61batch/s, auc=0.9236, loss=0.4263]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.61batch/s, auc=0.9236, loss=0.5165]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.74batch/s, auc=0.9236, loss=0.5165]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.74batch/s, auc=0.9236, loss=0.6003]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.68batch/s, auc=0.9236, loss=0.6003]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.68batch/s, auc=0.9236, loss=0.4493]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.80batch/s, auc=0.9236, loss=0.4493]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.80batch/s, auc=0.9234, loss=0.7513]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.79batch/s, auc=0.9234, loss=0.7513]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.79batch/s, auc=0.9231, loss=0.8242]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.85batch/s, auc=0.9231, loss=0.8242]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.85batch/s, auc=0.9232, loss=0.6172]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.78batch/s, auc=0.9232, loss=0.6172]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.78batch/s, auc=0.9231, loss=0.5926]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.74batch/s, auc=0.9231, loss=0.5926]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.74batch/s, auc=0.9234, loss=0.3914]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.71batch/s, auc=0.9234, loss=0.3914]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.71batch/s, auc=0.9234, loss=0.6552]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.68batch/s, auc=0.9234, loss=0.6552]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.68batch/s, auc=0.9233, loss=0.7960]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.66batch/s, auc=0.9233, loss=0.7960]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.66batch/s, auc=0.9232, loss=0.6459]\u001b[A\n",
      "Training Epoch 12/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.66batch/s, auc=0.9232, loss=0.6459]\u001b[A\n",
      "Training Epoch 12/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.66batch/s, auc=0.9234, loss=0.4897]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 234/292 [00:38<00:10,  5.68batch/s, auc=0.9234, loss=0.4897]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 234/292 [00:39<00:10,  5.68batch/s, auc=0.9235, loss=0.5013]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 235/292 [00:39<00:10,  5.68batch/s, auc=0.9235, loss=0.5013]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 235/292 [00:39<00:10,  5.68batch/s, auc=0.9234, loss=0.7913]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 236/292 [00:39<00:09,  5.77batch/s, auc=0.9234, loss=0.7913]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 236/292 [00:39<00:09,  5.77batch/s, auc=0.9232, loss=0.7524]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 237/292 [00:39<00:09,  5.66batch/s, auc=0.9232, loss=0.7524]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 237/292 [00:39<00:09,  5.66batch/s, auc=0.9234, loss=0.3298]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.66batch/s, auc=0.9234, loss=0.3298]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.66batch/s, auc=0.9237, loss=0.4406]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.65batch/s, auc=0.9237, loss=0.4406]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.65batch/s, auc=0.9238, loss=0.4844]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.63batch/s, auc=0.9238, loss=0.4844]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.63batch/s, auc=0.9238, loss=0.6395]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.62batch/s, auc=0.9238, loss=0.6395]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.62batch/s, auc=0.9235, loss=0.9269]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.59batch/s, auc=0.9235, loss=0.9269]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.59batch/s, auc=0.9234, loss=0.7235]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.61batch/s, auc=0.9234, loss=0.7235]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.61batch/s, auc=0.9232, loss=0.7064]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9232, loss=0.7064]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9230, loss=0.8956]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.64batch/s, auc=0.9230, loss=0.8956]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.64batch/s, auc=0.9230, loss=0.5887]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9230, loss=0.5887]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9227, loss=0.8355]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9227, loss=0.8355]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9226, loss=0.7142]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.66batch/s, auc=0.9226, loss=0.7142]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.66batch/s, auc=0.9227, loss=0.4924]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.66batch/s, auc=0.9227, loss=0.4924]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.66batch/s, auc=0.9225, loss=0.6375]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.70batch/s, auc=0.9225, loss=0.6375]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.70batch/s, auc=0.9225, loss=0.5489]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.81batch/s, auc=0.9225, loss=0.5489]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.81batch/s, auc=0.9226, loss=0.5563]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.88batch/s, auc=0.9226, loss=0.5563]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.88batch/s, auc=0.9226, loss=0.5550]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.88batch/s, auc=0.9226, loss=0.5550]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.88batch/s, auc=0.9228, loss=0.5058]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.86batch/s, auc=0.9228, loss=0.5058]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.86batch/s, auc=0.9229, loss=0.5199]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.77batch/s, auc=0.9229, loss=0.5199]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.77batch/s, auc=0.9229, loss=0.6664]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.73batch/s, auc=0.9229, loss=0.6664]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.73batch/s, auc=0.9225, loss=1.0939]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.81batch/s, auc=0.9225, loss=1.0939]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.81batch/s, auc=0.9226, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.71batch/s, auc=0.9226, loss=0.5524]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.71batch/s, auc=0.9228, loss=0.4185]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.66batch/s, auc=0.9228, loss=0.4185]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.66batch/s, auc=0.9229, loss=0.5113]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.64batch/s, auc=0.9229, loss=0.5113]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.64batch/s, auc=0.9226, loss=0.9375]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.74batch/s, auc=0.9226, loss=0.9375]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.74batch/s, auc=0.9227, loss=0.4820]\u001b[A\n",
      "Training Epoch 12/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.72batch/s, auc=0.9227, loss=0.4820]\u001b[A\n",
      "Training Epoch 12/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.72batch/s, auc=0.9229, loss=0.4339]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 263/292 [00:44<00:04,  5.80batch/s, auc=0.9229, loss=0.4339]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 263/292 [00:44<00:04,  5.80batch/s, auc=0.9231, loss=0.3712]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 264/292 [00:44<00:04,  5.85batch/s, auc=0.9231, loss=0.3712]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 264/292 [00:44<00:04,  5.85batch/s, auc=0.9231, loss=0.5093]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 265/292 [00:44<00:04,  5.89batch/s, auc=0.9231, loss=0.5093]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 265/292 [00:44<00:04,  5.89batch/s, auc=0.9226, loss=1.0018]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 266/292 [00:44<00:04,  5.81batch/s, auc=0.9226, loss=1.0018]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 266/292 [00:44<00:04,  5.81batch/s, auc=0.9226, loss=0.5986]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.86batch/s, auc=0.9226, loss=0.5986]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.86batch/s, auc=0.9227, loss=0.6451]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.74batch/s, auc=0.9227, loss=0.6451]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.74batch/s, auc=0.9226, loss=0.5858]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9226, loss=0.5858]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9228, loss=0.4417]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.75batch/s, auc=0.9228, loss=0.4417]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.75batch/s, auc=0.9226, loss=0.7187]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.9226, loss=0.7187]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.69batch/s, auc=0.9229, loss=0.3818]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.77batch/s, auc=0.9229, loss=0.3818]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.77batch/s, auc=0.9229, loss=0.7318]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.83batch/s, auc=0.9229, loss=0.7318]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.83batch/s, auc=0.9229, loss=0.6610]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.76batch/s, auc=0.9229, loss=0.6610]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.76batch/s, auc=0.9231, loss=0.4686]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.71batch/s, auc=0.9231, loss=0.4686]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.71batch/s, auc=0.9232, loss=0.4108]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.66batch/s, auc=0.9232, loss=0.4108]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.66batch/s, auc=0.9232, loss=0.5895]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.64batch/s, auc=0.9232, loss=0.5895]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.64batch/s, auc=0.9231, loss=0.6786]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.72batch/s, auc=0.9231, loss=0.6786]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.72batch/s, auc=0.9232, loss=0.4886]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.68batch/s, auc=0.9232, loss=0.4886]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.68batch/s, auc=0.9231, loss=0.7908]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9231, loss=0.7908]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9230, loss=0.6772]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.74batch/s, auc=0.9230, loss=0.6772]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.74batch/s, auc=0.9229, loss=0.7082]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.70batch/s, auc=0.9229, loss=0.7082]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.70batch/s, auc=0.9229, loss=0.6016]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.77batch/s, auc=0.9229, loss=0.6016]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.77batch/s, auc=0.9230, loss=0.4089]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.65batch/s, auc=0.9230, loss=0.4089]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.65batch/s, auc=0.9230, loss=0.5670]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.63batch/s, auc=0.9230, loss=0.5670]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.63batch/s, auc=0.9229, loss=0.7560]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.75batch/s, auc=0.9229, loss=0.7560]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.75batch/s, auc=0.9230, loss=0.4644]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.82batch/s, auc=0.9230, loss=0.4644]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.82batch/s, auc=0.9230, loss=0.6038]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.89batch/s, auc=0.9230, loss=0.6038]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.89batch/s, auc=0.9227, loss=0.8483]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.91batch/s, auc=0.9227, loss=0.8483]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.91batch/s, auc=0.9228, loss=0.4140]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.95batch/s, auc=0.9228, loss=0.4140]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.95batch/s, auc=0.9228, loss=0.6206]\u001b[A\n",
      "Training Epoch 12/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.95batch/s, auc=0.9228, loss=0.6206]\u001b[A\n",
      "Training Epoch 12/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.95batch/s, auc=0.9228, loss=0.4837]\u001b[A\n",
      "Training Epoch 12/25: 100%|██████████| 292/292 [00:49<00:00,  5.95batch/s, auc=0.9228, loss=0.4837]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25] Train Loss: 0.5961 | Train AUROC: 0.9228 Val Loss: 0.6470 | Val AUROC: 0.9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|████▊     | 12/25 [10:58<11:54, 54.99s/it]\n",
      "Training Epoch 13/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.8868, loss=0.8259]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 1/292 [00:01<04:51,  1.00s/batch, auc=0.8868, loss=0.8259]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 1/292 [00:01<04:51,  1.00s/batch, auc=0.9018, loss=0.4882]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9018, loss=0.4882]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9185, loss=0.5919]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.9185, loss=0.5919]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.9068, loss=0.6168]\u001b[A\n",
      "Training Epoch 13/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9068, loss=0.6168]\u001b[A\n",
      "Training Epoch 13/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9124, loss=0.5282]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9124, loss=0.5282]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9124, loss=0.6186]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 6/292 [00:01<01:01,  4.68batch/s, auc=0.9124, loss=0.6186]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 6/292 [00:01<01:01,  4.68batch/s, auc=0.9179, loss=0.4312]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 7/292 [00:01<00:55,  5.10batch/s, auc=0.9179, loss=0.4312]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 7/292 [00:02<00:55,  5.10batch/s, auc=0.9209, loss=0.4761]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 8/292 [00:02<00:52,  5.45batch/s, auc=0.9209, loss=0.4761]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 8/292 [00:02<00:52,  5.45batch/s, auc=0.9181, loss=0.7383]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 9/292 [00:02<00:50,  5.59batch/s, auc=0.9181, loss=0.7383]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 9/292 [00:02<00:50,  5.59batch/s, auc=0.9191, loss=0.6419]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9191, loss=0.6419]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9165, loss=0.6271]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9165, loss=0.6271]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9154, loss=0.6980]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9154, loss=0.6980]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 12/292 [00:02<00:46,  6.00batch/s, auc=0.9144, loss=0.5844]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 13/292 [00:02<00:45,  6.11batch/s, auc=0.9144, loss=0.5844]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 13/292 [00:03<00:45,  6.11batch/s, auc=0.9165, loss=0.5695]\u001b[A\n",
      "Training Epoch 13/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9165, loss=0.5695]\u001b[A\n",
      "Training Epoch 13/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9202, loss=0.4202]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9202, loss=0.4202]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9180, loss=0.6749]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9180, loss=0.6749]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9185, loss=0.5414]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 17/292 [00:03<00:43,  6.29batch/s, auc=0.9185, loss=0.5414]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 17/292 [00:03<00:43,  6.29batch/s, auc=0.9186, loss=0.5625]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 18/292 [00:03<00:43,  6.32batch/s, auc=0.9186, loss=0.5625]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 18/292 [00:03<00:43,  6.32batch/s, auc=0.9160, loss=1.0157]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 19/292 [00:03<00:43,  6.27batch/s, auc=0.9160, loss=1.0157]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 19/292 [00:04<00:43,  6.27batch/s, auc=0.9159, loss=0.4827]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9159, loss=0.4827]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9175, loss=0.5025]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9175, loss=0.5025]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9196, loss=0.4728]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9196, loss=0.4728]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9199, loss=0.4907]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9199, loss=0.4907]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9183, loss=0.6872]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 24/292 [00:04<00:42,  6.29batch/s, auc=0.9183, loss=0.6872]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 24/292 [00:04<00:42,  6.29batch/s, auc=0.9186, loss=0.7033]\u001b[A\n",
      "Training Epoch 13/25:   9%|▊         | 25/292 [00:04<00:42,  6.31batch/s, auc=0.9186, loss=0.7033]\u001b[A\n",
      "Training Epoch 13/25:   9%|▊         | 25/292 [00:04<00:42,  6.31batch/s, auc=0.9202, loss=0.4328]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 26/292 [00:04<00:42,  6.31batch/s, auc=0.9202, loss=0.4328]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 26/292 [00:05<00:42,  6.31batch/s, auc=0.9179, loss=1.0338]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 27/292 [00:05<00:42,  6.30batch/s, auc=0.9179, loss=1.0338]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 27/292 [00:05<00:42,  6.30batch/s, auc=0.9186, loss=0.3869]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 28/292 [00:05<00:41,  6.30batch/s, auc=0.9186, loss=0.3869]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 28/292 [00:05<00:41,  6.30batch/s, auc=0.9188, loss=0.6563]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9188, loss=0.6563]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9155, loss=1.2385]\u001b[A\n",
      "Training Epoch 13/25:  10%|█         | 30/292 [00:05<00:41,  6.28batch/s, auc=0.9155, loss=1.2385]\u001b[A\n",
      "Training Epoch 13/25:  10%|█         | 30/292 [00:05<00:41,  6.28batch/s, auc=0.9162, loss=0.6290]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 31/292 [00:05<00:41,  6.25batch/s, auc=0.9162, loss=0.6290]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 31/292 [00:05<00:41,  6.25batch/s, auc=0.9174, loss=0.4812]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 32/292 [00:05<00:41,  6.29batch/s, auc=0.9174, loss=0.4812]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 32/292 [00:06<00:41,  6.29batch/s, auc=0.9183, loss=0.5395]\u001b[A\n",
      "Training Epoch 13/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9183, loss=0.5395]\u001b[A\n",
      "Training Epoch 13/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9174, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 34/292 [00:06<00:41,  6.27batch/s, auc=0.9174, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 34/292 [00:06<00:41,  6.27batch/s, auc=0.9172, loss=0.7292]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 35/292 [00:06<00:40,  6.29batch/s, auc=0.9172, loss=0.7292]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 35/292 [00:06<00:40,  6.29batch/s, auc=0.9161, loss=0.6495]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9161, loss=0.6495]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9145, loss=0.5878]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9145, loss=0.5878]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9165, loss=0.4901]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 38/292 [00:06<00:40,  6.32batch/s, auc=0.9165, loss=0.4901]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 38/292 [00:07<00:40,  6.32batch/s, auc=0.9163, loss=0.6808]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9163, loss=0.6808]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9153, loss=0.6373]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▎        | 40/292 [00:07<00:40,  6.29batch/s, auc=0.9153, loss=0.6373]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▎        | 40/292 [00:07<00:40,  6.29batch/s, auc=0.9153, loss=0.4991]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 41/292 [00:07<00:40,  6.26batch/s, auc=0.9153, loss=0.4991]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 41/292 [00:07<00:40,  6.26batch/s, auc=0.9171, loss=0.4510]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9171, loss=0.4510]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9179, loss=0.4282]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9179, loss=0.4282]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9184, loss=0.4424]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 44/292 [00:07<00:39,  6.30batch/s, auc=0.9184, loss=0.4424]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 44/292 [00:08<00:39,  6.30batch/s, auc=0.9196, loss=0.3999]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 45/292 [00:08<00:39,  6.27batch/s, auc=0.9196, loss=0.3999]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 45/292 [00:08<00:39,  6.27batch/s, auc=0.9176, loss=0.8441]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 46/292 [00:08<00:39,  6.23batch/s, auc=0.9176, loss=0.8441]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 46/292 [00:08<00:39,  6.23batch/s, auc=0.9172, loss=0.7133]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9172, loss=0.7133]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9167, loss=0.7773]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9167, loss=0.7773]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9172, loss=0.5137]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9172, loss=0.5137]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9179, loss=0.4665]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 50/292 [00:08<00:38,  6.25batch/s, auc=0.9179, loss=0.4665]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 50/292 [00:08<00:38,  6.25batch/s, auc=0.9170, loss=0.8899]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 51/292 [00:08<00:38,  6.26batch/s, auc=0.9170, loss=0.8899]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 51/292 [00:09<00:38,  6.26batch/s, auc=0.9171, loss=0.5888]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9171, loss=0.5888]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9177, loss=0.5332]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 53/292 [00:09<00:38,  6.29batch/s, auc=0.9177, loss=0.5332]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 53/292 [00:09<00:38,  6.29batch/s, auc=0.9192, loss=0.4062]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9192, loss=0.4062]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9193, loss=0.4837]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9193, loss=0.4837]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9177, loss=1.0624]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9177, loss=1.0624]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9179, loss=0.5528]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 57/292 [00:09<00:37,  6.30batch/s, auc=0.9179, loss=0.5528]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 57/292 [00:10<00:37,  6.30batch/s, auc=0.9183, loss=0.5169]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9183, loss=0.5169]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9181, loss=0.6823]\u001b[A\n",
      "Training Epoch 13/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9181, loss=0.6823]\u001b[A\n",
      "Training Epoch 13/25:  20%|██        | 59/292 [00:10<00:36,  6.30batch/s, auc=0.9184, loss=0.5619]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9184, loss=0.5619]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 60/292 [00:10<00:36,  6.30batch/s, auc=0.9192, loss=0.3207]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9192, loss=0.3207]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9188, loss=0.6381]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 62/292 [00:10<00:36,  6.29batch/s, auc=0.9188, loss=0.6381]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 62/292 [00:10<00:36,  6.29batch/s, auc=0.9195, loss=0.4187]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 63/292 [00:10<00:36,  6.31batch/s, auc=0.9195, loss=0.4187]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 63/292 [00:11<00:36,  6.31batch/s, auc=0.9196, loss=0.4617]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 64/292 [00:11<00:36,  6.31batch/s, auc=0.9196, loss=0.4617]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 64/292 [00:11<00:36,  6.31batch/s, auc=0.9197, loss=0.5250]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 65/292 [00:11<00:36,  6.30batch/s, auc=0.9197, loss=0.5250]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 65/292 [00:11<00:36,  6.30batch/s, auc=0.9198, loss=0.6566]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 66/292 [00:11<00:35,  6.29batch/s, auc=0.9198, loss=0.6566]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 66/292 [00:11<00:35,  6.29batch/s, auc=0.9195, loss=0.6843]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 67/292 [00:11<00:35,  6.29batch/s, auc=0.9195, loss=0.6843]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 67/292 [00:11<00:35,  6.29batch/s, auc=0.9199, loss=0.5678]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 68/292 [00:11<00:35,  6.29batch/s, auc=0.9199, loss=0.5678]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 68/292 [00:11<00:35,  6.29batch/s, auc=0.9196, loss=0.5381]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▎       | 69/292 [00:11<00:35,  6.29batch/s, auc=0.9196, loss=0.5381]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▎       | 69/292 [00:11<00:35,  6.29batch/s, auc=0.9193, loss=0.4819]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9193, loss=0.4819]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 70/292 [00:12<00:35,  6.28batch/s, auc=0.9197, loss=0.3899]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 71/292 [00:12<00:35,  6.29batch/s, auc=0.9197, loss=0.3899]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 71/292 [00:12<00:35,  6.29batch/s, auc=0.9201, loss=0.4833]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▍       | 72/292 [00:12<00:34,  6.29batch/s, auc=0.9201, loss=0.4833]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▍       | 72/292 [00:12<00:34,  6.29batch/s, auc=0.9200, loss=0.5288]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 73/292 [00:12<00:34,  6.30batch/s, auc=0.9200, loss=0.5288]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 73/292 [00:12<00:34,  6.30batch/s, auc=0.9206, loss=0.5957]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9206, loss=0.5957]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9211, loss=0.5773]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 75/292 [00:12<00:34,  6.29batch/s, auc=0.9211, loss=0.5773]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 75/292 [00:12<00:34,  6.29batch/s, auc=0.9213, loss=0.4526]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 76/292 [00:12<00:34,  6.27batch/s, auc=0.9213, loss=0.4526]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 76/292 [00:13<00:34,  6.27batch/s, auc=0.9217, loss=0.4078]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9217, loss=0.4078]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9213, loss=0.6295]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9213, loss=0.6295]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9209, loss=0.6976]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9209, loss=0.6976]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9215, loss=0.4493]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9215, loss=0.4493]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9217, loss=0.6677]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9217, loss=0.6677]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9217, loss=0.5306]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 82/292 [00:13<00:33,  6.27batch/s, auc=0.9217, loss=0.5306]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 82/292 [00:14<00:33,  6.27batch/s, auc=0.9220, loss=0.4461]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 83/292 [00:14<00:33,  6.27batch/s, auc=0.9220, loss=0.4461]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 83/292 [00:14<00:33,  6.27batch/s, auc=0.9230, loss=0.3697]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.9230, loss=0.3697]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 84/292 [00:14<00:33,  6.26batch/s, auc=0.9238, loss=0.4114]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9238, loss=0.4114]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9245, loss=0.3754]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 86/292 [00:14<00:32,  6.26batch/s, auc=0.9245, loss=0.3754]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 86/292 [00:14<00:32,  6.26batch/s, auc=0.9247, loss=0.5653]\u001b[A\n",
      "Training Epoch 13/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9247, loss=0.5653]\u001b[A\n",
      "Training Epoch 13/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9254, loss=0.3910]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 88/292 [00:14<00:32,  6.27batch/s, auc=0.9254, loss=0.3910]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 88/292 [00:15<00:32,  6.27batch/s, auc=0.9249, loss=0.7712]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 89/292 [00:15<00:32,  6.26batch/s, auc=0.9249, loss=0.7712]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 89/292 [00:15<00:32,  6.26batch/s, auc=0.9257, loss=0.3337]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 90/292 [00:15<00:32,  6.26batch/s, auc=0.9257, loss=0.3337]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 90/292 [00:15<00:32,  6.26batch/s, auc=0.9248, loss=0.9073]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9248, loss=0.9073]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 91/292 [00:15<00:32,  6.26batch/s, auc=0.9252, loss=0.3827]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9252, loss=0.3827]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9252, loss=0.5528]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9252, loss=0.5528]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 93/292 [00:15<00:31,  6.25batch/s, auc=0.9247, loss=0.7024]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9247, loss=0.7024]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9249, loss=0.5496]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 95/292 [00:15<00:31,  6.24batch/s, auc=0.9249, loss=0.5496]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 95/292 [00:16<00:31,  6.24batch/s, auc=0.9251, loss=0.5930]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9251, loss=0.5930]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9248, loss=0.6392]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 97/292 [00:16<00:31,  6.27batch/s, auc=0.9248, loss=0.6392]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 97/292 [00:16<00:31,  6.27batch/s, auc=0.9239, loss=1.2561]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▎      | 98/292 [00:16<00:30,  6.28batch/s, auc=0.9239, loss=1.2561]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▎      | 98/292 [00:16<00:30,  6.28batch/s, auc=0.9239, loss=0.5721]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 99/292 [00:16<00:30,  6.28batch/s, auc=0.9239, loss=0.5721]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 99/292 [00:16<00:30,  6.28batch/s, auc=0.9230, loss=0.9192]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 100/292 [00:16<00:30,  6.28batch/s, auc=0.9230, loss=0.9192]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 100/292 [00:16<00:30,  6.28batch/s, auc=0.9230, loss=0.5226]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 101/292 [00:16<00:30,  6.29batch/s, auc=0.9230, loss=0.5226]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 101/292 [00:17<00:30,  6.29batch/s, auc=0.9240, loss=0.3234]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 102/292 [00:17<00:30,  6.29batch/s, auc=0.9240, loss=0.3234]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 102/292 [00:17<00:30,  6.29batch/s, auc=0.9239, loss=0.6029]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▌      | 103/292 [00:17<00:30,  6.28batch/s, auc=0.9239, loss=0.6029]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▌      | 103/292 [00:17<00:30,  6.28batch/s, auc=0.9236, loss=0.6663]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 104/292 [00:17<00:30,  6.27batch/s, auc=0.9236, loss=0.6663]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 104/292 [00:17<00:30,  6.27batch/s, auc=0.9233, loss=0.5526]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 105/292 [00:17<00:29,  6.25batch/s, auc=0.9233, loss=0.5526]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 105/292 [00:17<00:29,  6.25batch/s, auc=0.9225, loss=0.9480]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▋      | 106/292 [00:17<00:29,  6.24batch/s, auc=0.9225, loss=0.9480]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▋      | 106/292 [00:17<00:29,  6.24batch/s, auc=0.9225, loss=0.5796]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 107/292 [00:17<00:29,  6.25batch/s, auc=0.9225, loss=0.5796]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 107/292 [00:18<00:29,  6.25batch/s, auc=0.9225, loss=0.6669]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 108/292 [00:18<00:29,  6.25batch/s, auc=0.9225, loss=0.6669]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 108/292 [00:18<00:29,  6.25batch/s, auc=0.9222, loss=0.6057]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9222, loss=0.6057]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9223, loss=0.4628]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9223, loss=0.4628]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 110/292 [00:18<00:29,  6.23batch/s, auc=0.9228, loss=0.4477]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 111/292 [00:18<00:28,  6.24batch/s, auc=0.9228, loss=0.4477]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 111/292 [00:18<00:28,  6.24batch/s, auc=0.9223, loss=0.7745]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 112/292 [00:18<00:28,  6.25batch/s, auc=0.9223, loss=0.7745]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 112/292 [00:18<00:28,  6.25batch/s, auc=0.9221, loss=0.8241]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▊      | 113/292 [00:18<00:28,  6.25batch/s, auc=0.9221, loss=0.8241]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▊      | 113/292 [00:19<00:28,  6.25batch/s, auc=0.9225, loss=0.4314]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 114/292 [00:19<00:28,  6.24batch/s, auc=0.9225, loss=0.4314]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 114/292 [00:19<00:28,  6.24batch/s, auc=0.9225, loss=0.5926]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 115/292 [00:19<00:28,  6.24batch/s, auc=0.9225, loss=0.5926]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 115/292 [00:19<00:28,  6.24batch/s, auc=0.9229, loss=0.4868]\u001b[A\n",
      "Training Epoch 13/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9229, loss=0.4868]\u001b[A\n",
      "Training Epoch 13/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9222, loss=0.8197]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9222, loss=0.8197]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9227, loss=0.3924]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9227, loss=0.3924]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9225, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9225, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9232, loss=0.4018]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 120/292 [00:19<00:27,  6.21batch/s, auc=0.9232, loss=0.4018]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9233, loss=0.6267]\u001b[A\n",
      "Training Epoch 13/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9233, loss=0.6267]\u001b[A\n",
      "Training Epoch 13/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9238, loss=0.4696]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 122/292 [00:20<00:27,  6.20batch/s, auc=0.9238, loss=0.4696]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 122/292 [00:20<00:27,  6.20batch/s, auc=0.9241, loss=0.4811]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 123/292 [00:20<00:27,  6.19batch/s, auc=0.9241, loss=0.4811]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 123/292 [00:20<00:27,  6.19batch/s, auc=0.9234, loss=0.9656]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9234, loss=0.9656]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9234, loss=0.5721]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9234, loss=0.5721]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9239, loss=0.4188]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 126/292 [00:20<00:26,  6.19batch/s, auc=0.9239, loss=0.4188]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9243, loss=0.3990]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9243, loss=0.3990]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9239, loss=0.9994]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.9994]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.6419]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.6419]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9240, loss=0.5717]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9240, loss=0.5717]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.5118]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 131/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.5118]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 131/292 [00:21<00:26,  6.19batch/s, auc=0.9242, loss=0.4771]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▌     | 132/292 [00:21<00:25,  6.19batch/s, auc=0.9242, loss=0.4771]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▌     | 132/292 [00:22<00:25,  6.19batch/s, auc=0.9241, loss=0.5699]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9241, loss=0.5699]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9242, loss=0.5672]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5672]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5051]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5051]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5610]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5610]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9239, loss=0.7317]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9239, loss=0.7317]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9240, loss=0.4821]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 138/292 [00:22<00:24,  6.18batch/s, auc=0.9240, loss=0.4821]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 138/292 [00:23<00:24,  6.18batch/s, auc=0.9239, loss=0.6428]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 139/292 [00:23<00:24,  6.17batch/s, auc=0.9239, loss=0.6428]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 139/292 [00:23<00:24,  6.17batch/s, auc=0.9242, loss=0.5806]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9242, loss=0.5806]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9248, loss=0.4110]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9248, loss=0.4110]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9251, loss=0.4375]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9251, loss=0.4375]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9251, loss=0.6186]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 143/292 [00:23<00:24,  6.16batch/s, auc=0.9251, loss=0.6186]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 143/292 [00:23<00:24,  6.16batch/s, auc=0.9254, loss=0.4313]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 144/292 [00:23<00:24,  6.16batch/s, auc=0.9254, loss=0.4313]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 144/292 [00:24<00:24,  6.16batch/s, auc=0.9251, loss=0.6522]\u001b[A\n",
      "Training Epoch 13/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9251, loss=0.6522]\u001b[A\n",
      "Training Epoch 13/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9249, loss=0.6637]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9249, loss=0.6637]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9249, loss=0.6748]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 147/292 [00:24<00:23,  6.18batch/s, auc=0.9249, loss=0.6748]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 147/292 [00:24<00:23,  6.18batch/s, auc=0.9250, loss=0.4877]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 148/292 [00:24<00:23,  6.18batch/s, auc=0.9250, loss=0.4877]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 148/292 [00:24<00:23,  6.18batch/s, auc=0.9249, loss=0.6200]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 149/292 [00:24<00:23,  6.18batch/s, auc=0.9249, loss=0.6200]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 149/292 [00:24<00:23,  6.18batch/s, auc=0.9250, loss=0.6074]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.17batch/s, auc=0.9250, loss=0.6074]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.17batch/s, auc=0.9250, loss=0.6198]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.18batch/s, auc=0.9250, loss=0.6198]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.18batch/s, auc=0.9245, loss=0.9269]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.18batch/s, auc=0.9245, loss=0.9269]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.18batch/s, auc=0.9244, loss=0.5905]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.18batch/s, auc=0.9244, loss=0.5905]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.18batch/s, auc=0.9249, loss=0.3449]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9249, loss=0.3449]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9249, loss=0.5038]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9249, loss=0.5038]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9249, loss=0.5548]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9249, loss=0.5548]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9249, loss=0.5818]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.17batch/s, auc=0.9249, loss=0.5818]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.17batch/s, auc=0.9252, loss=0.3532]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.18batch/s, auc=0.9252, loss=0.3532]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.18batch/s, auc=0.9255, loss=0.4213]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.18batch/s, auc=0.9255, loss=0.4213]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.18batch/s, auc=0.9259, loss=0.4452]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.18batch/s, auc=0.9259, loss=0.4452]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.18batch/s, auc=0.9254, loss=0.8803]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.18batch/s, auc=0.9254, loss=0.8803]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.18batch/s, auc=0.9255, loss=0.6252]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.16batch/s, auc=0.9255, loss=0.6252]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.16batch/s, auc=0.9255, loss=0.6694]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 163/292 [00:26<00:20,  6.15batch/s, auc=0.9255, loss=0.6694]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 163/292 [00:27<00:20,  6.15batch/s, auc=0.9257, loss=0.5737]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9257, loss=0.5737]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9252, loss=0.8637]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9252, loss=0.8637]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9249, loss=0.7704]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9249, loss=0.7704]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9252, loss=0.5002]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9252, loss=0.5002]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9254, loss=0.4001]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.10batch/s, auc=0.9254, loss=0.4001]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.10batch/s, auc=0.9253, loss=0.5867]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.11batch/s, auc=0.9253, loss=0.5867]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.11batch/s, auc=0.9255, loss=0.4255]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9255, loss=0.4255]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9254, loss=0.7112]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.11batch/s, auc=0.9254, loss=0.7112]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.11batch/s, auc=0.9255, loss=0.6502]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.12batch/s, auc=0.9255, loss=0.6502]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.12batch/s, auc=0.9256, loss=0.5225]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.12batch/s, auc=0.9256, loss=0.5225]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.12batch/s, auc=0.9256, loss=0.5292]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.11batch/s, auc=0.9256, loss=0.5292]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.11batch/s, auc=0.9259, loss=0.4555]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.11batch/s, auc=0.9259, loss=0.4555]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.11batch/s, auc=0.9260, loss=0.5403]\u001b[A\n",
      "Training Epoch 13/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9260, loss=0.5403]\u001b[A\n",
      "Training Epoch 13/25:  60%|██████    | 176/292 [00:29<00:18,  6.12batch/s, auc=0.9261, loss=0.4098]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 177/292 [00:29<00:18,  6.12batch/s, auc=0.9261, loss=0.4098]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 177/292 [00:29<00:18,  6.12batch/s, auc=0.9260, loss=0.6465]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 178/292 [00:29<00:18,  6.13batch/s, auc=0.9260, loss=0.6465]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 178/292 [00:29<00:18,  6.13batch/s, auc=0.9262, loss=0.6116]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.13batch/s, auc=0.9262, loss=0.6116]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.13batch/s, auc=0.9262, loss=0.4406]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9262, loss=0.4406]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.11batch/s, auc=0.9260, loss=0.8280]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.11batch/s, auc=0.9260, loss=0.8280]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9264, loss=0.3912]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.97batch/s, auc=0.9264, loss=0.3912]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.97batch/s, auc=0.9265, loss=0.5012]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.00batch/s, auc=0.9265, loss=0.5012]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.00batch/s, auc=0.9267, loss=0.5111]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.81batch/s, auc=0.9267, loss=0.5111]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.81batch/s, auc=0.9268, loss=0.4714]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.90batch/s, auc=0.9268, loss=0.4714]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.90batch/s, auc=0.9269, loss=0.5279]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▎   | 186/292 [00:30<00:18,  5.83batch/s, auc=0.9269, loss=0.5279]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▎   | 186/292 [00:30<00:18,  5.83batch/s, auc=0.9270, loss=0.4756]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 187/292 [00:30<00:18,  5.80batch/s, auc=0.9270, loss=0.4756]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.80batch/s, auc=0.9269, loss=0.5636]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.90batch/s, auc=0.9269, loss=0.5636]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.90batch/s, auc=0.9270, loss=0.4860]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.97batch/s, auc=0.9270, loss=0.4860]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.97batch/s, auc=0.9270, loss=0.6020]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.01batch/s, auc=0.9270, loss=0.6020]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.01batch/s, auc=0.9269, loss=0.5095]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.03batch/s, auc=0.9269, loss=0.5095]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.03batch/s, auc=0.9271, loss=0.4819]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.06batch/s, auc=0.9271, loss=0.4819]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.06batch/s, auc=0.9273, loss=0.4844]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.07batch/s, auc=0.9273, loss=0.4844]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.07batch/s, auc=0.9274, loss=0.4379]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9274, loss=0.4379]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9272, loss=0.6709]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.97batch/s, auc=0.9272, loss=0.6709]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.97batch/s, auc=0.9267, loss=1.0283]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 196/292 [00:32<00:16,  6.00batch/s, auc=0.9267, loss=1.0283]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 196/292 [00:32<00:16,  6.00batch/s, auc=0.9271, loss=0.4011]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 197/292 [00:32<00:15,  5.99batch/s, auc=0.9271, loss=0.4011]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 197/292 [00:32<00:15,  5.99batch/s, auc=0.9270, loss=0.4882]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.02batch/s, auc=0.9270, loss=0.4882]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.02batch/s, auc=0.9270, loss=0.6121]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.03batch/s, auc=0.9270, loss=0.6121]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.03batch/s, auc=0.9270, loss=0.5652]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9270, loss=0.5652]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9272, loss=0.4189]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.07batch/s, auc=0.9272, loss=0.4189]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.07batch/s, auc=0.9274, loss=0.4521]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9274, loss=0.4521]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9273, loss=0.6913]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.07batch/s, auc=0.9273, loss=0.6913]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.07batch/s, auc=0.9274, loss=0.4987]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.09batch/s, auc=0.9274, loss=0.4987]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.09batch/s, auc=0.9269, loss=0.9413]\u001b[A\n",
      "Training Epoch 13/25:  70%|███████   | 205/292 [00:33<00:14,  6.09batch/s, auc=0.9269, loss=0.9413]\u001b[A\n",
      "Training Epoch 13/25:  70%|███████   | 205/292 [00:34<00:14,  6.09batch/s, auc=0.9266, loss=0.7003]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 206/292 [00:34<00:14,  6.09batch/s, auc=0.9266, loss=0.7003]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 206/292 [00:34<00:14,  6.09batch/s, auc=0.9269, loss=0.3692]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 207/292 [00:34<00:14,  6.05batch/s, auc=0.9269, loss=0.3692]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 207/292 [00:34<00:14,  6.05batch/s, auc=0.9271, loss=0.4474]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 208/292 [00:34<00:13,  6.07batch/s, auc=0.9271, loss=0.4474]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 208/292 [00:34<00:13,  6.07batch/s, auc=0.9271, loss=0.4596]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.07batch/s, auc=0.9271, loss=0.4596]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.07batch/s, auc=0.9268, loss=0.9058]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 210/292 [00:34<00:13,  5.99batch/s, auc=0.9268, loss=0.9058]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 210/292 [00:34<00:13,  5.99batch/s, auc=0.9269, loss=0.4732]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 211/292 [00:34<00:13,  5.87batch/s, auc=0.9269, loss=0.4732]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.87batch/s, auc=0.9268, loss=0.6146]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.81batch/s, auc=0.9268, loss=0.6146]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.81batch/s, auc=0.9268, loss=0.6250]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.75batch/s, auc=0.9268, loss=0.6250]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.75batch/s, auc=0.9270, loss=0.5499]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.71batch/s, auc=0.9270, loss=0.5499]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.71batch/s, auc=0.9269, loss=0.5498]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.67batch/s, auc=0.9269, loss=0.5498]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.67batch/s, auc=0.9269, loss=0.5158]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.67batch/s, auc=0.9269, loss=0.5158]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.67batch/s, auc=0.9268, loss=0.5813]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 217/292 [00:35<00:13,  5.66batch/s, auc=0.9268, loss=0.5813]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.66batch/s, auc=0.9267, loss=0.7332]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.77batch/s, auc=0.9267, loss=0.7332]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.77batch/s, auc=0.9261, loss=1.2388]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.86batch/s, auc=0.9261, loss=1.2388]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.86batch/s, auc=0.9261, loss=0.5955]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.92batch/s, auc=0.9261, loss=0.5955]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.92batch/s, auc=0.9261, loss=0.4576]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.96batch/s, auc=0.9261, loss=0.4576]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.96batch/s, auc=0.9261, loss=0.6282]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.98batch/s, auc=0.9261, loss=0.6282]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.98batch/s, auc=0.9263, loss=0.3662]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▋  | 223/292 [00:36<00:11,  5.99batch/s, auc=0.9263, loss=0.3662]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.99batch/s, auc=0.9261, loss=0.9342]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.01batch/s, auc=0.9261, loss=0.9342]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.01batch/s, auc=0.9257, loss=0.9808]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.93batch/s, auc=0.9257, loss=0.9808]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.93batch/s, auc=0.9259, loss=0.4676]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.86batch/s, auc=0.9259, loss=0.4676]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.86batch/s, auc=0.9260, loss=0.4190]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.84batch/s, auc=0.9260, loss=0.4190]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.84batch/s, auc=0.9260, loss=0.5355]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.67batch/s, auc=0.9260, loss=0.5355]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.67batch/s, auc=0.9260, loss=0.5290]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.64batch/s, auc=0.9260, loss=0.5290]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.64batch/s, auc=0.9255, loss=0.9975]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.75batch/s, auc=0.9255, loss=0.9975]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.75batch/s, auc=0.9256, loss=0.4943]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.70batch/s, auc=0.9256, loss=0.4943]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.70batch/s, auc=0.9257, loss=0.4529]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.68batch/s, auc=0.9257, loss=0.4529]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.68batch/s, auc=0.9259, loss=0.4351]\u001b[A\n",
      "Training Epoch 13/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.65batch/s, auc=0.9259, loss=0.4351]\u001b[A\n",
      "Training Epoch 13/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.65batch/s, auc=0.9258, loss=0.5910]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 234/292 [00:38<00:10,  5.65batch/s, auc=0.9258, loss=0.5910]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 234/292 [00:39<00:10,  5.65batch/s, auc=0.9256, loss=0.7026]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 235/292 [00:39<00:10,  5.63batch/s, auc=0.9256, loss=0.7026]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 235/292 [00:39<00:10,  5.63batch/s, auc=0.9255, loss=0.6711]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 236/292 [00:39<00:09,  5.63batch/s, auc=0.9255, loss=0.6711]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 236/292 [00:39<00:09,  5.63batch/s, auc=0.9256, loss=0.4646]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 237/292 [00:39<00:09,  5.63batch/s, auc=0.9256, loss=0.4646]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 237/292 [00:39<00:09,  5.63batch/s, auc=0.9255, loss=0.5602]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.64batch/s, auc=0.9255, loss=0.5602]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.64batch/s, auc=0.9257, loss=0.4955]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.64batch/s, auc=0.9257, loss=0.4955]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.64batch/s, auc=0.9258, loss=0.4444]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.63batch/s, auc=0.9258, loss=0.4444]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.63batch/s, auc=0.9259, loss=0.4701]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.62batch/s, auc=0.9259, loss=0.4701]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.62batch/s, auc=0.9261, loss=0.4314]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.62batch/s, auc=0.9261, loss=0.4314]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.62batch/s, auc=0.9258, loss=0.8110]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.61batch/s, auc=0.9258, loss=0.8110]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.61batch/s, auc=0.9258, loss=0.5568]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.61batch/s, auc=0.9258, loss=0.5568]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.61batch/s, auc=0.9258, loss=0.5031]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.61batch/s, auc=0.9258, loss=0.5031]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.61batch/s, auc=0.9258, loss=0.5337]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.67batch/s, auc=0.9258, loss=0.5337]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.67batch/s, auc=0.9256, loss=0.8160]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9256, loss=0.8160]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9257, loss=0.4497]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.75batch/s, auc=0.9257, loss=0.4497]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.75batch/s, auc=0.9257, loss=0.6511]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.65batch/s, auc=0.9257, loss=0.6511]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.65batch/s, auc=0.9256, loss=0.8248]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.67batch/s, auc=0.9256, loss=0.8248]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.67batch/s, auc=0.9256, loss=0.5731]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.66batch/s, auc=0.9256, loss=0.5731]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.66batch/s, auc=0.9257, loss=0.4096]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.76batch/s, auc=0.9257, loss=0.4096]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.76batch/s, auc=0.9257, loss=0.5886]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.72batch/s, auc=0.9257, loss=0.5886]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.72batch/s, auc=0.9255, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.69batch/s, auc=0.9255, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.69batch/s, auc=0.9256, loss=0.4230]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.70batch/s, auc=0.9256, loss=0.4230]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.70batch/s, auc=0.9257, loss=0.6318]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.69batch/s, auc=0.9257, loss=0.6318]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.69batch/s, auc=0.9255, loss=0.7848]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.67batch/s, auc=0.9255, loss=0.7848]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.67batch/s, auc=0.9255, loss=0.5796]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.67batch/s, auc=0.9255, loss=0.5796]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.67batch/s, auc=0.9253, loss=0.8601]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.66batch/s, auc=0.9253, loss=0.8601]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.66batch/s, auc=0.9254, loss=0.4737]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9254, loss=0.4737]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9253, loss=0.6123]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.65batch/s, auc=0.9253, loss=0.6123]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.65batch/s, auc=0.9253, loss=0.5115]\u001b[A\n",
      "Training Epoch 13/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.65batch/s, auc=0.9253, loss=0.5115]\u001b[A\n",
      "Training Epoch 13/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.65batch/s, auc=0.9253, loss=0.5516]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 263/292 [00:44<00:05,  5.63batch/s, auc=0.9253, loss=0.5516]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 263/292 [00:44<00:05,  5.63batch/s, auc=0.9255, loss=0.4300]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9255, loss=0.4300]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 264/292 [00:44<00:04,  5.64batch/s, auc=0.9256, loss=0.5028]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 265/292 [00:44<00:04,  5.64batch/s, auc=0.9256, loss=0.5028]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 265/292 [00:44<00:04,  5.64batch/s, auc=0.9258, loss=0.4621]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 266/292 [00:44<00:04,  5.74batch/s, auc=0.9258, loss=0.4621]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 266/292 [00:44<00:04,  5.74batch/s, auc=0.9256, loss=0.7102]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.68batch/s, auc=0.9256, loss=0.7102]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.68batch/s, auc=0.9256, loss=0.6006]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.79batch/s, auc=0.9256, loss=0.6006]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.79batch/s, auc=0.9254, loss=0.5635]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.84batch/s, auc=0.9254, loss=0.5635]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.84batch/s, auc=0.9255, loss=0.5759]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.75batch/s, auc=0.9255, loss=0.5759]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.75batch/s, auc=0.9255, loss=0.4932]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.70batch/s, auc=0.9255, loss=0.4932]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.70batch/s, auc=0.9254, loss=0.7830]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.67batch/s, auc=0.9254, loss=0.7830]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.67batch/s, auc=0.9253, loss=0.5837]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.71batch/s, auc=0.9253, loss=0.5837]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.71batch/s, auc=0.9251, loss=0.5974]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.78batch/s, auc=0.9251, loss=0.5974]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.78batch/s, auc=0.9252, loss=0.4756]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.60batch/s, auc=0.9252, loss=0.4756]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.60batch/s, auc=0.9248, loss=1.2908]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.60batch/s, auc=0.9248, loss=1.2908]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.60batch/s, auc=0.9249, loss=0.4323]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.64batch/s, auc=0.9249, loss=0.4323]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.64batch/s, auc=0.9249, loss=0.5000]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.72batch/s, auc=0.9249, loss=0.5000]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.72batch/s, auc=0.9249, loss=0.5964]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.58batch/s, auc=0.9249, loss=0.5964]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.58batch/s, auc=0.9246, loss=1.1538]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9246, loss=1.1538]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9246, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9246, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9245, loss=0.6058]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.59batch/s, auc=0.9245, loss=0.6058]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.59batch/s, auc=0.9246, loss=0.4692]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.59batch/s, auc=0.9246, loss=0.4692]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.59batch/s, auc=0.9248, loss=0.4380]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.58batch/s, auc=0.9248, loss=0.4380]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.58batch/s, auc=0.9246, loss=0.8727]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.59batch/s, auc=0.9246, loss=0.8727]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9245, loss=0.9129]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9245, loss=0.9129]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9246, loss=0.4223]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.75batch/s, auc=0.9246, loss=0.4223]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.75batch/s, auc=0.9248, loss=0.3809]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.81batch/s, auc=0.9248, loss=0.3809]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.81batch/s, auc=0.9247, loss=0.6316]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.84batch/s, auc=0.9247, loss=0.6316]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.84batch/s, auc=0.9249, loss=0.3661]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.88batch/s, auc=0.9249, loss=0.3661]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.88batch/s, auc=0.9251, loss=0.5179]\u001b[A\n",
      "Training Epoch 13/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.90batch/s, auc=0.9251, loss=0.5179]\u001b[A\n",
      "Training Epoch 13/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.90batch/s, auc=0.9251, loss=0.5055]\u001b[A\n",
      "Training Epoch 13/25: 100%|██████████| 292/292 [00:49<00:00,  5.94batch/s, auc=0.9251, loss=0.5055]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25] Train Loss: 0.5882 | Train AUROC: 0.9251 Val Loss: 0.6119 | Val AUROC: 0.9132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|█████▏    | 13/25 [11:53<11:00, 55.00s/it]\n",
      "Training Epoch 14/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9407, loss=0.7218]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 1/292 [00:00<04:46,  1.02batch/s, auc=0.9407, loss=0.7218]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 1/292 [00:01<04:46,  1.02batch/s, auc=0.9480, loss=0.4774]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9480, loss=0.4774]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9657, loss=0.3411]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.9657, loss=0.3411]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.9610, loss=0.4692]\u001b[A\n",
      "Training Epoch 14/25:   1%|▏         | 4/292 [00:01<01:20,  3.58batch/s, auc=0.9610, loss=0.4692]\u001b[A\n",
      "Training Epoch 14/25:   1%|▏         | 4/292 [00:01<01:20,  3.58batch/s, auc=0.9534, loss=0.6066]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 5/292 [00:01<01:07,  4.23batch/s, auc=0.9534, loss=0.6066]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 5/292 [00:01<01:07,  4.23batch/s, auc=0.9560, loss=0.4726]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 6/292 [00:01<00:59,  4.77batch/s, auc=0.9560, loss=0.4726]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 6/292 [00:01<00:59,  4.77batch/s, auc=0.9534, loss=0.5414]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 7/292 [00:01<00:55,  5.09batch/s, auc=0.9534, loss=0.5414]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 7/292 [00:02<00:55,  5.09batch/s, auc=0.9444, loss=0.9059]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9444, loss=0.9059]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9454, loss=0.4033]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 9/292 [00:02<00:49,  5.67batch/s, auc=0.9454, loss=0.4033]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 9/292 [00:02<00:49,  5.67batch/s, auc=0.9462, loss=0.4303]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 10/292 [00:02<00:48,  5.82batch/s, auc=0.9462, loss=0.4303]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 10/292 [00:02<00:48,  5.82batch/s, auc=0.9498, loss=0.3313]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 11/292 [00:02<00:47,  5.98batch/s, auc=0.9498, loss=0.3313]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 11/292 [00:02<00:47,  5.98batch/s, auc=0.9442, loss=0.8340]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9442, loss=0.8340]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9413, loss=0.8544]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 13/292 [00:02<00:46,  5.97batch/s, auc=0.9413, loss=0.8544]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 13/292 [00:03<00:46,  5.97batch/s, auc=0.9417, loss=0.5191]\u001b[A\n",
      "Training Epoch 14/25:   5%|▍         | 14/292 [00:03<00:45,  6.08batch/s, auc=0.9417, loss=0.5191]\u001b[A\n",
      "Training Epoch 14/25:   5%|▍         | 14/292 [00:03<00:45,  6.08batch/s, auc=0.9425, loss=0.4113]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 15/292 [00:03<00:44,  6.17batch/s, auc=0.9425, loss=0.4113]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 15/292 [00:03<00:44,  6.17batch/s, auc=0.9427, loss=0.4727]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 16/292 [00:03<00:44,  6.22batch/s, auc=0.9427, loss=0.4727]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 16/292 [00:03<00:44,  6.22batch/s, auc=0.9423, loss=0.4851]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 17/292 [00:03<00:44,  6.21batch/s, auc=0.9423, loss=0.4851]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 17/292 [00:03<00:44,  6.21batch/s, auc=0.9411, loss=0.5276]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9411, loss=0.5276]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9375, loss=0.9386]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 19/292 [00:03<00:43,  6.28batch/s, auc=0.9375, loss=0.9386]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 19/292 [00:04<00:43,  6.28batch/s, auc=0.9375, loss=0.4289]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9375, loss=0.4289]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9340, loss=0.8077]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.9340, loss=0.8077]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.9347, loss=0.4812]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9347, loss=0.4812]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9336, loss=0.5906]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9336, loss=0.5906]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9327, loss=0.6116]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 24/292 [00:04<00:42,  6.35batch/s, auc=0.9327, loss=0.6116]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 24/292 [00:04<00:42,  6.35batch/s, auc=0.9314, loss=0.7073]\u001b[A\n",
      "Training Epoch 14/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9314, loss=0.7073]\u001b[A\n",
      "Training Epoch 14/25:   9%|▊         | 25/292 [00:04<00:42,  6.33batch/s, auc=0.9305, loss=0.5823]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 26/292 [00:04<00:41,  6.34batch/s, auc=0.9305, loss=0.5823]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.9302, loss=0.5453]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.9302, loss=0.5453]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 27/292 [00:05<00:41,  6.34batch/s, auc=0.9292, loss=0.5682]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.9292, loss=0.5682]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 28/292 [00:05<00:41,  6.36batch/s, auc=0.9284, loss=0.6768]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 29/292 [00:05<00:41,  6.37batch/s, auc=0.9284, loss=0.6768]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 29/292 [00:05<00:41,  6.37batch/s, auc=0.9322, loss=0.2159]\u001b[A\n",
      "Training Epoch 14/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.9322, loss=0.2159]\u001b[A\n",
      "Training Epoch 14/25:  10%|█         | 30/292 [00:05<00:41,  6.36batch/s, auc=0.9324, loss=0.5022]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 31/292 [00:05<00:40,  6.37batch/s, auc=0.9324, loss=0.5022]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 31/292 [00:05<00:40,  6.37batch/s, auc=0.9322, loss=0.5916]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 32/292 [00:05<00:40,  6.38batch/s, auc=0.9322, loss=0.5916]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 32/292 [00:06<00:40,  6.38batch/s, auc=0.9332, loss=0.4962]\u001b[A\n",
      "Training Epoch 14/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9332, loss=0.4962]\u001b[A\n",
      "Training Epoch 14/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9352, loss=0.3128]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9352, loss=0.3128]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9359, loss=0.4257]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 35/292 [00:06<00:40,  6.33batch/s, auc=0.9359, loss=0.4257]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 35/292 [00:06<00:40,  6.33batch/s, auc=0.9354, loss=0.6997]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9354, loss=0.6997]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9362, loss=0.3808]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.9362, loss=0.3808]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 37/292 [00:06<00:40,  6.34batch/s, auc=0.9374, loss=0.4167]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 38/292 [00:06<00:40,  6.35batch/s, auc=0.9374, loss=0.4167]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 38/292 [00:07<00:40,  6.35batch/s, auc=0.9384, loss=0.4330]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9384, loss=0.4330]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9363, loss=0.8905]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9363, loss=0.8905]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9354, loss=0.7594]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 41/292 [00:07<00:39,  6.28batch/s, auc=0.9354, loss=0.7594]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 41/292 [00:07<00:39,  6.28batch/s, auc=0.9352, loss=0.5446]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9352, loss=0.5446]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9363, loss=0.3994]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9363, loss=0.3994]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▍        | 43/292 [00:07<00:39,  6.31batch/s, auc=0.9359, loss=0.5897]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 44/292 [00:07<00:39,  6.27batch/s, auc=0.9359, loss=0.5897]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 44/292 [00:08<00:39,  6.27batch/s, auc=0.9366, loss=0.3653]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9366, loss=0.3653]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9369, loss=0.4469]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 46/292 [00:08<00:39,  6.29batch/s, auc=0.9369, loss=0.4469]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 46/292 [00:08<00:39,  6.29batch/s, auc=0.9371, loss=0.4370]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9371, loss=0.4370]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9356, loss=0.7434]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9356, loss=0.7434]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9361, loss=0.4435]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 49/292 [00:08<00:38,  6.30batch/s, auc=0.9361, loss=0.4435]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 49/292 [00:08<00:38,  6.30batch/s, auc=0.9371, loss=0.3801]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.9371, loss=0.3801]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.9359, loss=0.6643]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 51/292 [00:08<00:38,  6.31batch/s, auc=0.9359, loss=0.6643]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 51/292 [00:09<00:38,  6.31batch/s, auc=0.9351, loss=0.6738]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 52/292 [00:09<00:38,  6.31batch/s, auc=0.9351, loss=0.6738]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 52/292 [00:09<00:38,  6.31batch/s, auc=0.9351, loss=0.5721]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9351, loss=0.5721]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9343, loss=0.5596]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 54/292 [00:09<00:37,  6.31batch/s, auc=0.9343, loss=0.5596]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 54/292 [00:09<00:37,  6.31batch/s, auc=0.9330, loss=0.8864]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9330, loss=0.8864]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9328, loss=0.4352]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9328, loss=0.4352]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9323, loss=0.8757]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 57/292 [00:09<00:37,  6.24batch/s, auc=0.9323, loss=0.8757]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 57/292 [00:10<00:37,  6.24batch/s, auc=0.9329, loss=0.4817]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 58/292 [00:10<00:37,  6.26batch/s, auc=0.9329, loss=0.4817]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 58/292 [00:10<00:37,  6.26batch/s, auc=0.9332, loss=0.3990]\u001b[A\n",
      "Training Epoch 14/25:  20%|██        | 59/292 [00:10<00:37,  6.23batch/s, auc=0.9332, loss=0.3990]\u001b[A\n",
      "Training Epoch 14/25:  20%|██        | 59/292 [00:10<00:37,  6.23batch/s, auc=0.9333, loss=0.6572]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 60/292 [00:10<00:37,  6.26batch/s, auc=0.9333, loss=0.6572]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 60/292 [00:10<00:37,  6.26batch/s, auc=0.9331, loss=0.5091]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9331, loss=0.5091]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 61/292 [00:10<00:36,  6.27batch/s, auc=0.9319, loss=1.1416]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9319, loss=1.1416]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9316, loss=0.5410]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9316, loss=0.5410]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9314, loss=0.6329]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9314, loss=0.6329]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9318, loss=0.4017]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 65/292 [00:11<00:36,  6.29batch/s, auc=0.9318, loss=0.4017]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 65/292 [00:11<00:36,  6.29batch/s, auc=0.9322, loss=0.5637]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 66/292 [00:11<00:36,  6.22batch/s, auc=0.9322, loss=0.5637]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 66/292 [00:11<00:36,  6.22batch/s, auc=0.9317, loss=0.7964]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 67/292 [00:11<00:36,  6.21batch/s, auc=0.9317, loss=0.7964]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 67/292 [00:11<00:36,  6.21batch/s, auc=0.9319, loss=0.4307]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 68/292 [00:11<00:35,  6.23batch/s, auc=0.9319, loss=0.4307]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 68/292 [00:11<00:35,  6.23batch/s, auc=0.9325, loss=0.3538]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▎       | 69/292 [00:11<00:35,  6.24batch/s, auc=0.9325, loss=0.3538]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▎       | 69/292 [00:11<00:35,  6.24batch/s, auc=0.9325, loss=0.6654]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 70/292 [00:11<00:35,  6.26batch/s, auc=0.9325, loss=0.6654]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9323, loss=0.5701]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 71/292 [00:12<00:35,  6.22batch/s, auc=0.9323, loss=0.5701]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 71/292 [00:12<00:35,  6.22batch/s, auc=0.9330, loss=0.3610]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▍       | 72/292 [00:12<00:35,  6.23batch/s, auc=0.9330, loss=0.3610]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▍       | 72/292 [00:12<00:35,  6.23batch/s, auc=0.9322, loss=0.9849]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 73/292 [00:12<00:35,  6.22batch/s, auc=0.9322, loss=0.9849]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 73/292 [00:12<00:35,  6.22batch/s, auc=0.9325, loss=0.4892]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 74/292 [00:12<00:34,  6.23batch/s, auc=0.9325, loss=0.4892]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 74/292 [00:12<00:34,  6.23batch/s, auc=0.9321, loss=0.6019]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 75/292 [00:12<00:34,  6.23batch/s, auc=0.9321, loss=0.6019]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 75/292 [00:12<00:34,  6.23batch/s, auc=0.9324, loss=0.4991]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 76/292 [00:12<00:34,  6.24batch/s, auc=0.9324, loss=0.4991]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 76/292 [00:13<00:34,  6.24batch/s, auc=0.9314, loss=0.9219]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▋       | 77/292 [00:13<00:34,  6.25batch/s, auc=0.9314, loss=0.9219]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▋       | 77/292 [00:13<00:34,  6.25batch/s, auc=0.9316, loss=0.5509]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9316, loss=0.5509]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 78/292 [00:13<00:34,  6.25batch/s, auc=0.9323, loss=0.3539]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9323, loss=0.3539]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9334, loss=0.2839]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9334, loss=0.2839]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9339, loss=0.4246]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9339, loss=0.4246]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9339, loss=0.4894]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 82/292 [00:13<00:33,  6.24batch/s, auc=0.9339, loss=0.4894]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 82/292 [00:14<00:33,  6.24batch/s, auc=0.9338, loss=0.5045]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9338, loss=0.5045]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9335, loss=0.7396]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9335, loss=0.7396]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9338, loss=0.4880]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 85/292 [00:14<00:33,  6.20batch/s, auc=0.9338, loss=0.4880]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 85/292 [00:14<00:33,  6.20batch/s, auc=0.9340, loss=0.5051]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 86/292 [00:14<00:33,  6.21batch/s, auc=0.9340, loss=0.5051]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 86/292 [00:14<00:33,  6.21batch/s, auc=0.9336, loss=0.8770]\u001b[A\n",
      "Training Epoch 14/25:  30%|██▉       | 87/292 [00:14<00:32,  6.22batch/s, auc=0.9336, loss=0.8770]\u001b[A\n",
      "Training Epoch 14/25:  30%|██▉       | 87/292 [00:14<00:32,  6.22batch/s, auc=0.9341, loss=0.3995]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 88/292 [00:14<00:32,  6.23batch/s, auc=0.9341, loss=0.3995]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 88/292 [00:15<00:32,  6.23batch/s, auc=0.9339, loss=0.7333]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 89/292 [00:15<00:32,  6.21batch/s, auc=0.9339, loss=0.7333]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 89/292 [00:15<00:32,  6.21batch/s, auc=0.9340, loss=0.4836]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 90/292 [00:15<00:32,  6.22batch/s, auc=0.9340, loss=0.4836]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 90/292 [00:15<00:32,  6.22batch/s, auc=0.9330, loss=1.0024]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9330, loss=1.0024]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9328, loss=0.6576]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 92/292 [00:15<00:32,  6.22batch/s, auc=0.9328, loss=0.6576]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 92/292 [00:15<00:32,  6.22batch/s, auc=0.9333, loss=0.3274]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9333, loss=0.3274]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9336, loss=0.5154]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 94/292 [00:15<00:31,  6.22batch/s, auc=0.9336, loss=0.5154]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 94/292 [00:16<00:31,  6.22batch/s, auc=0.9337, loss=0.5487]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 95/292 [00:16<00:31,  6.21batch/s, auc=0.9337, loss=0.5487]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 95/292 [00:16<00:31,  6.21batch/s, auc=0.9331, loss=0.7524]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9331, loss=0.7524]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9322, loss=0.9749]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 97/292 [00:16<00:31,  6.19batch/s, auc=0.9322, loss=0.9749]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 97/292 [00:16<00:31,  6.19batch/s, auc=0.9326, loss=0.4853]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9326, loss=0.4853]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9330, loss=0.3923]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 99/292 [00:16<00:31,  6.20batch/s, auc=0.9330, loss=0.3923]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 99/292 [00:16<00:31,  6.20batch/s, auc=0.9336, loss=0.3357]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9336, loss=0.3357]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9330, loss=0.7719]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 101/292 [00:16<00:30,  6.21batch/s, auc=0.9330, loss=0.7719]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 101/292 [00:17<00:30,  6.21batch/s, auc=0.9330, loss=0.4657]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9330, loss=0.4657]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9334, loss=0.4418]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▌      | 103/292 [00:17<00:30,  6.14batch/s, auc=0.9334, loss=0.4418]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▌      | 103/292 [00:17<00:30,  6.14batch/s, auc=0.9337, loss=0.4049]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 104/292 [00:17<00:30,  6.16batch/s, auc=0.9337, loss=0.4049]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 104/292 [00:17<00:30,  6.16batch/s, auc=0.9339, loss=0.4411]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 105/292 [00:17<00:30,  6.16batch/s, auc=0.9339, loss=0.4411]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 105/292 [00:17<00:30,  6.16batch/s, auc=0.9337, loss=0.6415]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▋      | 106/292 [00:17<00:30,  6.18batch/s, auc=0.9337, loss=0.6415]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▋      | 106/292 [00:17<00:30,  6.18batch/s, auc=0.9332, loss=0.6985]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 107/292 [00:17<00:29,  6.18batch/s, auc=0.9332, loss=0.6985]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 107/292 [00:18<00:29,  6.18batch/s, auc=0.9316, loss=1.5837]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 108/292 [00:18<00:30,  6.13batch/s, auc=0.9316, loss=1.5837]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 108/292 [00:18<00:30,  6.13batch/s, auc=0.9314, loss=0.5143]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 109/292 [00:18<00:29,  6.14batch/s, auc=0.9314, loss=0.5143]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 109/292 [00:18<00:29,  6.14batch/s, auc=0.9314, loss=0.5577]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 110/292 [00:18<00:29,  6.16batch/s, auc=0.9314, loss=0.5577]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 110/292 [00:18<00:29,  6.16batch/s, auc=0.9311, loss=0.7304]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 111/292 [00:18<00:29,  6.16batch/s, auc=0.9311, loss=0.7304]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 111/292 [00:18<00:29,  6.16batch/s, auc=0.9313, loss=0.5011]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9313, loss=0.5011]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 112/292 [00:18<00:29,  6.17batch/s, auc=0.9310, loss=0.6498]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▊      | 113/292 [00:18<00:29,  6.17batch/s, auc=0.9310, loss=0.6498]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▊      | 113/292 [00:19<00:29,  6.17batch/s, auc=0.9312, loss=0.4467]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9312, loss=0.4467]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9310, loss=0.7165]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 115/292 [00:19<00:28,  6.15batch/s, auc=0.9310, loss=0.7165]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 115/292 [00:19<00:28,  6.15batch/s, auc=0.9312, loss=0.5917]\u001b[A\n",
      "Training Epoch 14/25:  40%|███▉      | 116/292 [00:19<00:28,  6.16batch/s, auc=0.9312, loss=0.5917]\u001b[A\n",
      "Training Epoch 14/25:  40%|███▉      | 116/292 [00:19<00:28,  6.16batch/s, auc=0.9305, loss=1.0694]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 117/292 [00:19<00:28,  6.16batch/s, auc=0.9305, loss=1.0694]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 117/292 [00:19<00:28,  6.16batch/s, auc=0.9302, loss=0.6086]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9302, loss=0.6086]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9302, loss=0.4828]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 119/292 [00:19<00:27,  6.19batch/s, auc=0.9302, loss=0.4828]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 119/292 [00:20<00:27,  6.19batch/s, auc=0.9300, loss=0.7406]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9300, loss=0.7406]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9302, loss=0.3741]\u001b[A\n",
      "Training Epoch 14/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9302, loss=0.3741]\u001b[A\n",
      "Training Epoch 14/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9302, loss=0.6757]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9302, loss=0.6757]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9304, loss=0.5398]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 123/292 [00:20<00:27,  6.12batch/s, auc=0.9304, loss=0.5398]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 123/292 [00:20<00:27,  6.12batch/s, auc=0.9301, loss=0.6010]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 124/292 [00:20<00:27,  6.15batch/s, auc=0.9301, loss=0.6010]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 124/292 [00:20<00:27,  6.15batch/s, auc=0.9301, loss=0.5843]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 125/292 [00:20<00:27,  6.16batch/s, auc=0.9301, loss=0.5843]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 125/292 [00:21<00:27,  6.16batch/s, auc=0.9299, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 126/292 [00:21<00:27,  6.14batch/s, auc=0.9299, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 126/292 [00:21<00:27,  6.14batch/s, auc=0.9300, loss=0.6446]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 127/292 [00:21<00:26,  6.16batch/s, auc=0.9300, loss=0.6446]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 127/292 [00:21<00:26,  6.16batch/s, auc=0.9298, loss=0.5571]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 128/292 [00:21<00:26,  6.17batch/s, auc=0.9298, loss=0.5571]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 128/292 [00:21<00:26,  6.17batch/s, auc=0.9298, loss=0.5473]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9298, loss=0.5473]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9299, loss=0.4867]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9299, loss=0.4867]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9297, loss=0.7058]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 131/292 [00:21<00:26,  6.19batch/s, auc=0.9297, loss=0.7058]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 131/292 [00:22<00:26,  6.19batch/s, auc=0.9296, loss=0.5306]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9296, loss=0.5306]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9294, loss=0.6792]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9294, loss=0.6792]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9297, loss=0.5042]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9297, loss=0.5042]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9295, loss=0.6037]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9295, loss=0.6037]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9291, loss=0.7144]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9291, loss=0.7144]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9291, loss=0.4795]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 137/292 [00:22<00:25,  6.08batch/s, auc=0.9291, loss=0.4795]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 137/292 [00:22<00:25,  6.08batch/s, auc=0.9286, loss=0.9273]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 138/292 [00:22<00:25,  6.09batch/s, auc=0.9286, loss=0.9273]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 138/292 [00:23<00:25,  6.09batch/s, auc=0.9286, loss=0.6239]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9286, loss=0.6239]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9288, loss=0.4201]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 140/292 [00:23<00:24,  6.13batch/s, auc=0.9288, loss=0.4201]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 140/292 [00:23<00:24,  6.13batch/s, auc=0.9284, loss=0.6793]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9284, loss=0.6793]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9286, loss=0.4438]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▊     | 142/292 [00:23<00:24,  6.14batch/s, auc=0.9286, loss=0.4438]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▊     | 142/292 [00:23<00:24,  6.14batch/s, auc=0.9288, loss=0.4595]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9288, loss=0.4595]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9284, loss=0.7814]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 144/292 [00:23<00:24,  6.14batch/s, auc=0.9284, loss=0.7814]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 144/292 [00:24<00:24,  6.14batch/s, auc=0.9281, loss=0.8941]\u001b[A\n",
      "Training Epoch 14/25:  50%|████▉     | 145/292 [00:24<00:23,  6.13batch/s, auc=0.9281, loss=0.8941]\u001b[A\n",
      "Training Epoch 14/25:  50%|████▉     | 145/292 [00:24<00:23,  6.13batch/s, auc=0.9282, loss=0.4849]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9282, loss=0.4849]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9283, loss=0.5305]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9283, loss=0.5305]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9279, loss=0.8199]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9279, loss=0.8199]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9279, loss=0.5611]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9279, loss=0.5611]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9282, loss=0.3448]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.14batch/s, auc=0.9282, loss=0.3448]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.14batch/s, auc=0.9284, loss=0.5295]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9284, loss=0.5295]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9286, loss=0.3991]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9286, loss=0.3991]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9288, loss=0.4566]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.13batch/s, auc=0.9288, loss=0.4566]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.13batch/s, auc=0.9289, loss=0.5174]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.12batch/s, auc=0.9289, loss=0.5174]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.12batch/s, auc=0.9289, loss=0.4617]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.02batch/s, auc=0.9289, loss=0.4617]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.02batch/s, auc=0.9284, loss=0.8568]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.05batch/s, auc=0.9284, loss=0.8568]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.05batch/s, auc=0.9288, loss=0.3411]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.07batch/s, auc=0.9288, loss=0.3411]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.07batch/s, auc=0.9288, loss=0.4978]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.11batch/s, auc=0.9288, loss=0.4978]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.11batch/s, auc=0.9286, loss=0.7357]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9286, loss=0.7357]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9284, loss=0.6509]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.11batch/s, auc=0.9284, loss=0.6509]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.11batch/s, auc=0.9283, loss=0.5985]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.08batch/s, auc=0.9283, loss=0.5985]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.08batch/s, auc=0.9281, loss=0.6970]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 162/292 [00:26<00:22,  5.88batch/s, auc=0.9281, loss=0.6970]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 162/292 [00:27<00:22,  5.88batch/s, auc=0.9284, loss=0.3692]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 163/292 [00:27<00:21,  5.93batch/s, auc=0.9284, loss=0.3692]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 163/292 [00:27<00:21,  5.93batch/s, auc=0.9285, loss=0.4202]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.81batch/s, auc=0.9285, loss=0.4202]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.81batch/s, auc=0.9287, loss=0.4458]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.78batch/s, auc=0.9287, loss=0.4458]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.78batch/s, auc=0.9287, loss=0.6786]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.86batch/s, auc=0.9287, loss=0.6786]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.86batch/s, auc=0.9285, loss=0.7207]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.93batch/s, auc=0.9285, loss=0.7207]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 167/292 [00:27<00:21,  5.93batch/s, auc=0.9283, loss=0.6602]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 168/292 [00:27<00:20,  5.98batch/s, auc=0.9283, loss=0.6602]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 168/292 [00:28<00:20,  5.98batch/s, auc=0.9283, loss=0.6339]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.00batch/s, auc=0.9283, loss=0.6339]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.00batch/s, auc=0.9282, loss=0.5031]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9282, loss=0.5031]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9283, loss=0.4507]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▊    | 171/292 [00:28<00:20,  6.04batch/s, auc=0.9283, loss=0.4507]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▊    | 171/292 [00:28<00:20,  6.04batch/s, auc=0.9285, loss=0.4192]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.06batch/s, auc=0.9285, loss=0.4192]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.06batch/s, auc=0.9285, loss=0.6249]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.06batch/s, auc=0.9285, loss=0.6249]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.06batch/s, auc=0.9285, loss=0.5433]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 174/292 [00:28<00:20,  5.84batch/s, auc=0.9285, loss=0.5433]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 174/292 [00:29<00:20,  5.84batch/s, auc=0.9285, loss=0.5633]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.78batch/s, auc=0.9285, loss=0.5633]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.78batch/s, auc=0.9285, loss=0.7505]\u001b[A\n",
      "Training Epoch 14/25:  60%|██████    | 176/292 [00:29<00:19,  5.87batch/s, auc=0.9285, loss=0.7505]\u001b[A\n",
      "Training Epoch 14/25:  60%|██████    | 176/292 [00:29<00:19,  5.87batch/s, auc=0.9286, loss=0.4972]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 177/292 [00:29<00:19,  5.81batch/s, auc=0.9286, loss=0.4972]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 177/292 [00:29<00:19,  5.81batch/s, auc=0.9288, loss=0.4128]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 178/292 [00:29<00:19,  5.78batch/s, auc=0.9288, loss=0.4128]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 178/292 [00:29<00:19,  5.78batch/s, auc=0.9292, loss=0.4111]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.87batch/s, auc=0.9292, loss=0.4111]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.87batch/s, auc=0.9295, loss=0.4521]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.85batch/s, auc=0.9295, loss=0.4521]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.85batch/s, auc=0.9292, loss=0.9051]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 181/292 [00:30<00:18,  5.95batch/s, auc=0.9292, loss=0.9051]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 181/292 [00:30<00:18,  5.95batch/s, auc=0.9292, loss=0.5121]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.99batch/s, auc=0.9292, loss=0.5121]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.99batch/s, auc=0.9291, loss=0.5972]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.01batch/s, auc=0.9291, loss=0.5972]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.01batch/s, auc=0.9294, loss=0.4390]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.03batch/s, auc=0.9294, loss=0.4390]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.03batch/s, auc=0.9293, loss=0.5196]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.04batch/s, auc=0.9293, loss=0.5196]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.04batch/s, auc=0.9290, loss=0.6188]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▎   | 186/292 [00:30<00:17,  5.90batch/s, auc=0.9290, loss=0.6188]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▎   | 186/292 [00:31<00:17,  5.90batch/s, auc=0.9288, loss=0.6276]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.83batch/s, auc=0.9288, loss=0.6276]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.83batch/s, auc=0.9283, loss=1.1912]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.79batch/s, auc=0.9283, loss=1.1912]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.79batch/s, auc=0.9284, loss=0.5253]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.86batch/s, auc=0.9284, loss=0.5253]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.86batch/s, auc=0.9286, loss=0.4959]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.80batch/s, auc=0.9286, loss=0.4959]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.80batch/s, auc=0.9286, loss=0.6036]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.84batch/s, auc=0.9286, loss=0.6036]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 191/292 [00:32<00:17,  5.84batch/s, auc=0.9286, loss=0.6777]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 192/292 [00:32<00:16,  5.90batch/s, auc=0.9286, loss=0.6777]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 192/292 [00:32<00:16,  5.90batch/s, auc=0.9286, loss=0.5808]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 193/292 [00:32<00:16,  5.84batch/s, auc=0.9286, loss=0.5808]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 193/292 [00:32<00:16,  5.84batch/s, auc=0.9289, loss=0.4496]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.69batch/s, auc=0.9289, loss=0.4496]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.69batch/s, auc=0.9287, loss=0.7592]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.74batch/s, auc=0.9287, loss=0.7592]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.74batch/s, auc=0.9286, loss=0.6826]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.84batch/s, auc=0.9286, loss=0.6826]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.84batch/s, auc=0.9287, loss=0.5006]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.89batch/s, auc=0.9287, loss=0.5006]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 197/292 [00:33<00:16,  5.89batch/s, auc=0.9286, loss=0.5946]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 198/292 [00:33<00:15,  5.92batch/s, auc=0.9286, loss=0.5946]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 198/292 [00:33<00:15,  5.92batch/s, auc=0.9286, loss=0.5778]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.96batch/s, auc=0.9286, loss=0.5778]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.96batch/s, auc=0.9287, loss=0.5749]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.98batch/s, auc=0.9287, loss=0.5749]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.98batch/s, auc=0.9287, loss=0.5930]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.98batch/s, auc=0.9287, loss=0.5930]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.98batch/s, auc=0.9286, loss=0.7605]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.01batch/s, auc=0.9286, loss=0.7605]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.01batch/s, auc=0.9284, loss=0.6573]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 203/292 [00:33<00:14,  5.98batch/s, auc=0.9284, loss=0.6573]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 203/292 [00:34<00:14,  5.98batch/s, auc=0.9283, loss=0.4891]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 204/292 [00:34<00:14,  5.88batch/s, auc=0.9283, loss=0.4891]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 204/292 [00:34<00:14,  5.88batch/s, auc=0.9280, loss=0.6042]\u001b[A\n",
      "Training Epoch 14/25:  70%|███████   | 205/292 [00:34<00:14,  5.93batch/s, auc=0.9280, loss=0.6042]\u001b[A\n",
      "Training Epoch 14/25:  70%|███████   | 205/292 [00:34<00:14,  5.93batch/s, auc=0.9281, loss=0.4769]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 206/292 [00:34<00:14,  5.96batch/s, auc=0.9281, loss=0.4769]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 206/292 [00:34<00:14,  5.96batch/s, auc=0.9280, loss=0.6628]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 207/292 [00:34<00:14,  5.86batch/s, auc=0.9280, loss=0.6628]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 207/292 [00:34<00:14,  5.86batch/s, auc=0.9280, loss=0.5302]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 208/292 [00:34<00:14,  5.91batch/s, auc=0.9280, loss=0.5302]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 208/292 [00:34<00:14,  5.91batch/s, auc=0.9279, loss=0.7638]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 209/292 [00:34<00:13,  5.96batch/s, auc=0.9279, loss=0.7638]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 209/292 [00:35<00:13,  5.96batch/s, auc=0.9282, loss=0.3305]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.78batch/s, auc=0.9282, loss=0.3305]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.78batch/s, auc=0.9285, loss=0.3563]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.74batch/s, auc=0.9285, loss=0.3563]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.74batch/s, auc=0.9284, loss=0.6011]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.72batch/s, auc=0.9284, loss=0.6011]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.72batch/s, auc=0.9284, loss=0.6227]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.70batch/s, auc=0.9284, loss=0.6227]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.70batch/s, auc=0.9282, loss=0.6744]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.70batch/s, auc=0.9282, loss=0.6744]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.70batch/s, auc=0.9282, loss=0.4637]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.69batch/s, auc=0.9282, loss=0.4637]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.69batch/s, auc=0.9282, loss=0.6349]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.78batch/s, auc=0.9282, loss=0.6349]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.78batch/s, auc=0.9283, loss=0.4735]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.79batch/s, auc=0.9283, loss=0.4735]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.79batch/s, auc=0.9283, loss=0.4286]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9283, loss=0.4286]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9283, loss=0.4297]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.69batch/s, auc=0.9283, loss=0.4297]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.69batch/s, auc=0.9280, loss=0.9713]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.68batch/s, auc=0.9280, loss=0.9713]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 220/292 [00:37<00:12,  5.68batch/s, auc=0.9278, loss=0.9025]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.62batch/s, auc=0.9278, loss=0.9025]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.62batch/s, auc=0.9280, loss=0.4014]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.62batch/s, auc=0.9280, loss=0.4014]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.62batch/s, auc=0.9281, loss=0.4885]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.73batch/s, auc=0.9281, loss=0.4885]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.73batch/s, auc=0.9280, loss=0.5029]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.63batch/s, auc=0.9280, loss=0.5029]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.63batch/s, auc=0.9277, loss=0.8335]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.74batch/s, auc=0.9277, loss=0.8335]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.74batch/s, auc=0.9278, loss=0.4735]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.70batch/s, auc=0.9278, loss=0.4735]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.70batch/s, auc=0.9279, loss=0.5020]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.69batch/s, auc=0.9279, loss=0.5020]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.69batch/s, auc=0.9280, loss=0.4828]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.68batch/s, auc=0.9280, loss=0.4828]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.68batch/s, auc=0.9279, loss=0.6230]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.66batch/s, auc=0.9279, loss=0.6230]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.66batch/s, auc=0.9279, loss=0.5368]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.76batch/s, auc=0.9279, loss=0.5368]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.76batch/s, auc=0.9280, loss=0.3575]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.65batch/s, auc=0.9280, loss=0.3575]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.65batch/s, auc=0.9280, loss=0.5719]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.75batch/s, auc=0.9280, loss=0.5719]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.75batch/s, auc=0.9277, loss=0.7215]\u001b[A\n",
      "Training Epoch 14/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.82batch/s, auc=0.9277, loss=0.7215]\u001b[A\n",
      "Training Epoch 14/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.82batch/s, auc=0.9279, loss=0.4097]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 234/292 [00:39<00:10,  5.63batch/s, auc=0.9279, loss=0.4097]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 234/292 [00:39<00:10,  5.63batch/s, auc=0.9278, loss=0.7137]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 235/292 [00:39<00:10,  5.63batch/s, auc=0.9278, loss=0.7137]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 235/292 [00:39<00:10,  5.63batch/s, auc=0.9279, loss=0.5257]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9279, loss=0.5257]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9280, loss=0.6630]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9280, loss=0.6630]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9277, loss=0.7260]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.62batch/s, auc=0.9277, loss=0.7260]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.62batch/s, auc=0.9280, loss=0.3325]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.71batch/s, auc=0.9280, loss=0.3325]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.71batch/s, auc=0.9281, loss=0.5077]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.80batch/s, auc=0.9281, loss=0.5077]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.80batch/s, auc=0.9282, loss=0.4517]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.71batch/s, auc=0.9282, loss=0.4517]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.71batch/s, auc=0.9280, loss=0.6129]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.68batch/s, auc=0.9280, loss=0.6129]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.68batch/s, auc=0.9281, loss=0.4316]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9281, loss=0.4316]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 243/292 [00:41<00:08,  5.63batch/s, auc=0.9280, loss=0.6575]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.61batch/s, auc=0.9280, loss=0.6575]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.61batch/s, auc=0.9279, loss=0.6598]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.73batch/s, auc=0.9279, loss=0.6598]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.73batch/s, auc=0.9278, loss=0.7298]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.70batch/s, auc=0.9278, loss=0.7298]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.70batch/s, auc=0.9277, loss=0.7373]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.67batch/s, auc=0.9277, loss=0.7373]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.67batch/s, auc=0.9277, loss=0.6279]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.66batch/s, auc=0.9277, loss=0.6279]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.66batch/s, auc=0.9279, loss=0.3942]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.71batch/s, auc=0.9279, loss=0.3942]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.71batch/s, auc=0.9274, loss=1.1597]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.70batch/s, auc=0.9274, loss=1.1597]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.70batch/s, auc=0.9271, loss=0.7054]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.67batch/s, auc=0.9271, loss=0.7054]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.67batch/s, auc=0.9272, loss=0.4823]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.58batch/s, auc=0.9272, loss=0.4823]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.58batch/s, auc=0.9272, loss=0.6108]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.64batch/s, auc=0.9272, loss=0.6108]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.64batch/s, auc=0.9269, loss=0.8997]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9269, loss=0.8997]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9266, loss=1.0960]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.63batch/s, auc=0.9266, loss=1.0960]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.63batch/s, auc=0.9267, loss=0.5889]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.59batch/s, auc=0.9267, loss=0.5889]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.59batch/s, auc=0.9266, loss=0.8093]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.58batch/s, auc=0.9266, loss=0.8093]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.58batch/s, auc=0.9269, loss=0.3829]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.57batch/s, auc=0.9269, loss=0.3829]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.57batch/s, auc=0.9269, loss=0.5422]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9269, loss=0.5422]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9271, loss=0.4103]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.62batch/s, auc=0.9271, loss=0.4103]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 260/292 [00:44<00:05,  5.62batch/s, auc=0.9272, loss=0.5043]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.59batch/s, auc=0.9272, loss=0.5043]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.59batch/s, auc=0.9270, loss=0.7792]\u001b[A\n",
      "Training Epoch 14/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.56batch/s, auc=0.9270, loss=0.7792]\u001b[A\n",
      "Training Epoch 14/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.56batch/s, auc=0.9270, loss=0.5439]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 263/292 [00:44<00:05,  5.54batch/s, auc=0.9270, loss=0.5439]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 263/292 [00:44<00:05,  5.54batch/s, auc=0.9270, loss=0.6012]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 264/292 [00:44<00:05,  5.54batch/s, auc=0.9270, loss=0.6012]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 264/292 [00:44<00:05,  5.54batch/s, auc=0.9270, loss=0.4593]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 265/292 [00:44<00:04,  5.51batch/s, auc=0.9270, loss=0.4593]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 265/292 [00:44<00:04,  5.51batch/s, auc=0.9270, loss=0.5902]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 266/292 [00:44<00:04,  5.53batch/s, auc=0.9270, loss=0.5902]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 266/292 [00:45<00:04,  5.53batch/s, auc=0.9271, loss=0.6136]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.54batch/s, auc=0.9271, loss=0.6136]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.54batch/s, auc=0.9269, loss=0.8350]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.52batch/s, auc=0.9269, loss=0.8350]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.52batch/s, auc=0.9269, loss=0.4771]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.52batch/s, auc=0.9269, loss=0.4771]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.52batch/s, auc=0.9271, loss=0.4209]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9271, loss=0.4209]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9271, loss=0.4884]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.52batch/s, auc=0.9271, loss=0.4884]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 271/292 [00:46<00:03,  5.52batch/s, auc=0.9269, loss=0.7222]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.55batch/s, auc=0.9269, loss=0.7222]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.55batch/s, auc=0.9269, loss=0.4902]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.54batch/s, auc=0.9269, loss=0.4902]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.54batch/s, auc=0.9270, loss=0.5455]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.54batch/s, auc=0.9270, loss=0.5455]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.54batch/s, auc=0.9270, loss=0.5168]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.54batch/s, auc=0.9270, loss=0.5168]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.54batch/s, auc=0.9270, loss=0.5457]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.52batch/s, auc=0.9270, loss=0.5457]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.52batch/s, auc=0.9270, loss=0.6539]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.59batch/s, auc=0.9270, loss=0.6539]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 277/292 [00:47<00:02,  5.59batch/s, auc=0.9270, loss=0.4759]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.56batch/s, auc=0.9270, loss=0.4759]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.56batch/s, auc=0.9270, loss=0.6526]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.52batch/s, auc=0.9270, loss=0.6526]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.52batch/s, auc=0.9269, loss=0.7161]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9269, loss=0.7161]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9269, loss=0.5195]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.56batch/s, auc=0.9269, loss=0.5195]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.56batch/s, auc=0.9271, loss=0.3770]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.56batch/s, auc=0.9271, loss=0.3770]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 282/292 [00:48<00:01,  5.56batch/s, auc=0.9271, loss=0.6172]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.66batch/s, auc=0.9271, loss=0.6172]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.66batch/s, auc=0.9272, loss=0.5517]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.58batch/s, auc=0.9272, loss=0.5517]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.58batch/s, auc=0.9270, loss=0.6753]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.57batch/s, auc=0.9270, loss=0.6753]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.57batch/s, auc=0.9269, loss=0.8557]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9269, loss=0.8557]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9266, loss=0.9007]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.73batch/s, auc=0.9266, loss=0.9007]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.73batch/s, auc=0.9266, loss=0.4683]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.77batch/s, auc=0.9266, loss=0.4683]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▊| 288/292 [00:49<00:00,  5.77batch/s, auc=0.9269, loss=0.3141]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.80batch/s, auc=0.9269, loss=0.3141]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.80batch/s, auc=0.9269, loss=0.4606]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.84batch/s, auc=0.9269, loss=0.4606]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.84batch/s, auc=0.9268, loss=0.5950]\u001b[A\n",
      "Training Epoch 14/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.82batch/s, auc=0.9268, loss=0.5950]\u001b[A\n",
      "Training Epoch 14/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.82batch/s, auc=0.9269, loss=0.3493]\u001b[A\n",
      "Training Epoch 14/25: 100%|██████████| 292/292 [00:49<00:00,  5.88batch/s, auc=0.9269, loss=0.3493]\u001b[A\n",
      "Epochs:  56%|█████▌    | 14/25 [12:48<10:05, 55.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25] Train Loss: 0.5833 | Train AUROC: 0.9269 Val Loss: 0.6540 | Val AUROC: 0.9010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 15/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9576, loss=0.4209]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 1/292 [00:01<04:59,  1.03s/batch, auc=0.9576, loss=0.4209]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 1/292 [00:01<04:59,  1.03s/batch, auc=0.9449, loss=0.5320]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 2/292 [00:01<02:29,  1.94batch/s, auc=0.9449, loss=0.5320]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 2/292 [00:01<02:29,  1.94batch/s, auc=0.9479, loss=0.4388]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 3/292 [00:01<01:43,  2.79batch/s, auc=0.9479, loss=0.4388]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 3/292 [00:01<01:43,  2.79batch/s, auc=0.9286, loss=0.8400]\u001b[A\n",
      "Training Epoch 15/25:   1%|▏         | 4/292 [00:01<01:20,  3.59batch/s, auc=0.9286, loss=0.8400]\u001b[A\n",
      "Training Epoch 15/25:   1%|▏         | 4/292 [00:01<01:20,  3.59batch/s, auc=0.9350, loss=0.4088]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 5/292 [00:01<01:07,  4.26batch/s, auc=0.9350, loss=0.4088]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 5/292 [00:01<01:07,  4.26batch/s, auc=0.9389, loss=0.4459]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 6/292 [00:01<01:00,  4.73batch/s, auc=0.9389, loss=0.4459]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 6/292 [00:02<01:00,  4.73batch/s, auc=0.9409, loss=0.5403]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 7/292 [00:02<00:56,  5.07batch/s, auc=0.9409, loss=0.5403]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 7/292 [00:02<00:56,  5.07batch/s, auc=0.9411, loss=0.6456]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 8/292 [00:02<00:52,  5.43batch/s, auc=0.9411, loss=0.6456]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 8/292 [00:02<00:52,  5.43batch/s, auc=0.9384, loss=0.6599]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 9/292 [00:02<00:49,  5.70batch/s, auc=0.9384, loss=0.6599]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 9/292 [00:02<00:49,  5.70batch/s, auc=0.9368, loss=0.5469]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 10/292 [00:02<00:48,  5.82batch/s, auc=0.9368, loss=0.5469]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 10/292 [00:02<00:48,  5.82batch/s, auc=0.9375, loss=0.3946]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 11/292 [00:02<00:46,  5.98batch/s, auc=0.9375, loss=0.3946]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 11/292 [00:02<00:46,  5.98batch/s, auc=0.9406, loss=0.3627]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 12/292 [00:02<00:45,  6.09batch/s, auc=0.9406, loss=0.3627]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 12/292 [00:02<00:45,  6.09batch/s, auc=0.9411, loss=0.4173]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 13/292 [00:02<00:45,  6.16batch/s, auc=0.9411, loss=0.4173]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 13/292 [00:03<00:45,  6.16batch/s, auc=0.9428, loss=0.4494]\u001b[A\n",
      "Training Epoch 15/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.9428, loss=0.4494]\u001b[A\n",
      "Training Epoch 15/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.9398, loss=0.6953]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 15/292 [00:03<00:44,  6.25batch/s, auc=0.9398, loss=0.6953]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 15/292 [00:03<00:44,  6.25batch/s, auc=0.9366, loss=0.6922]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 16/292 [00:03<00:43,  6.28batch/s, auc=0.9366, loss=0.6922]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 16/292 [00:03<00:43,  6.28batch/s, auc=0.9372, loss=0.4472]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.9372, loss=0.4472]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.9358, loss=0.6111]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 18/292 [00:03<00:43,  6.33batch/s, auc=0.9358, loss=0.6111]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 18/292 [00:03<00:43,  6.33batch/s, auc=0.9363, loss=0.4529]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 19/292 [00:03<00:43,  6.35batch/s, auc=0.9363, loss=0.4529]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 19/292 [00:04<00:43,  6.35batch/s, auc=0.9304, loss=1.1838]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 20/292 [00:04<00:42,  6.34batch/s, auc=0.9304, loss=1.1838]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 20/292 [00:04<00:42,  6.34batch/s, auc=0.9297, loss=0.5110]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 21/292 [00:04<00:43,  6.29batch/s, auc=0.9297, loss=0.5110]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 21/292 [00:04<00:43,  6.29batch/s, auc=0.9292, loss=0.4776]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 22/292 [00:04<00:43,  6.24batch/s, auc=0.9292, loss=0.4776]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 22/292 [00:04<00:43,  6.24batch/s, auc=0.9296, loss=0.4668]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 23/292 [00:04<00:42,  6.28batch/s, auc=0.9296, loss=0.4668]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 23/292 [00:04<00:42,  6.28batch/s, auc=0.9283, loss=0.7327]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9283, loss=0.7327]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9287, loss=0.4356]\u001b[A\n",
      "Training Epoch 15/25:   9%|▊         | 25/292 [00:04<00:42,  6.29batch/s, auc=0.9287, loss=0.4356]\u001b[A\n",
      "Training Epoch 15/25:   9%|▊         | 25/292 [00:05<00:42,  6.29batch/s, auc=0.9275, loss=0.5871]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 26/292 [00:05<00:42,  6.31batch/s, auc=0.9275, loss=0.5871]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 26/292 [00:05<00:42,  6.31batch/s, auc=0.9284, loss=0.5547]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9284, loss=0.5547]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9286, loss=0.4583]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9286, loss=0.4583]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 28/292 [00:05<00:41,  6.34batch/s, auc=0.9267, loss=0.6900]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 29/292 [00:05<00:41,  6.27batch/s, auc=0.9267, loss=0.6900]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 29/292 [00:05<00:41,  6.27batch/s, auc=0.9239, loss=0.9337]\u001b[A\n",
      "Training Epoch 15/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9239, loss=0.9337]\u001b[A\n",
      "Training Epoch 15/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9211, loss=0.9851]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 31/292 [00:05<00:41,  6.25batch/s, auc=0.9211, loss=0.9851]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 31/292 [00:05<00:41,  6.25batch/s, auc=0.9217, loss=0.5096]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 32/292 [00:05<00:41,  6.27batch/s, auc=0.9217, loss=0.5096]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 32/292 [00:06<00:41,  6.27batch/s, auc=0.9238, loss=0.4406]\u001b[A\n",
      "Training Epoch 15/25:  11%|█▏        | 33/292 [00:06<00:41,  6.29batch/s, auc=0.9238, loss=0.4406]\u001b[A\n",
      "Training Epoch 15/25:  11%|█▏        | 33/292 [00:06<00:41,  6.29batch/s, auc=0.9240, loss=0.4791]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9240, loss=0.4791]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9228, loss=0.5282]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9228, loss=0.5282]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9237, loss=0.5527]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9237, loss=0.5527]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9223, loss=0.8332]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9223, loss=0.8332]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9216, loss=0.5338]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 38/292 [00:06<00:40,  6.30batch/s, auc=0.9216, loss=0.5338]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 38/292 [00:07<00:40,  6.30batch/s, auc=0.9204, loss=0.7917]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.9204, loss=0.7917]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 39/292 [00:07<00:39,  6.33batch/s, auc=0.9214, loss=0.5351]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▎        | 40/292 [00:07<00:39,  6.33batch/s, auc=0.9214, loss=0.5351]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▎        | 40/292 [00:07<00:39,  6.33batch/s, auc=0.9225, loss=0.4857]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 41/292 [00:07<00:39,  6.34batch/s, auc=0.9225, loss=0.4857]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 41/292 [00:07<00:39,  6.34batch/s, auc=0.9220, loss=0.6081]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 42/292 [00:07<00:39,  6.34batch/s, auc=0.9220, loss=0.6081]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 42/292 [00:07<00:39,  6.34batch/s, auc=0.9213, loss=0.7010]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▍        | 43/292 [00:07<00:39,  6.33batch/s, auc=0.9213, loss=0.7010]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▍        | 43/292 [00:07<00:39,  6.33batch/s, auc=0.9219, loss=0.4629]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 44/292 [00:07<00:39,  6.27batch/s, auc=0.9219, loss=0.4629]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 44/292 [00:08<00:39,  6.27batch/s, auc=0.9226, loss=0.4996]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9226, loss=0.4996]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9228, loss=0.4304]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9228, loss=0.4304]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 46/292 [00:08<00:38,  6.31batch/s, auc=0.9226, loss=0.5723]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9226, loss=0.5723]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9217, loss=0.7696]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9217, loss=0.7696]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9225, loss=0.4132]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9225, loss=0.4132]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9215, loss=0.6270]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9215, loss=0.6270]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9204, loss=0.6748]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 51/292 [00:08<00:38,  6.31batch/s, auc=0.9204, loss=0.6748]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 51/292 [00:09<00:38,  6.31batch/s, auc=0.9189, loss=0.8069]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9189, loss=0.8069]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9198, loss=0.4991]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 53/292 [00:09<00:38,  6.27batch/s, auc=0.9198, loss=0.4991]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 53/292 [00:09<00:38,  6.27batch/s, auc=0.9204, loss=0.4758]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 54/292 [00:09<00:37,  6.28batch/s, auc=0.9204, loss=0.4758]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 54/292 [00:09<00:37,  6.28batch/s, auc=0.9208, loss=0.4767]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9208, loss=0.4767]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9215, loss=0.3704]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9215, loss=0.3704]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9218, loss=0.4157]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 57/292 [00:09<00:37,  6.31batch/s, auc=0.9218, loss=0.4157]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 57/292 [00:10<00:37,  6.31batch/s, auc=0.9223, loss=0.5230]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.9223, loss=0.5230]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 58/292 [00:10<00:37,  6.31batch/s, auc=0.9212, loss=0.8472]\u001b[A\n",
      "Training Epoch 15/25:  20%|██        | 59/292 [00:10<00:37,  6.26batch/s, auc=0.9212, loss=0.8472]\u001b[A\n",
      "Training Epoch 15/25:  20%|██        | 59/292 [00:10<00:37,  6.26batch/s, auc=0.9217, loss=0.4780]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 60/292 [00:10<00:37,  6.22batch/s, auc=0.9217, loss=0.4780]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 60/292 [00:10<00:37,  6.22batch/s, auc=0.9214, loss=0.8417]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 61/292 [00:10<00:37,  6.23batch/s, auc=0.9214, loss=0.8417]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 61/292 [00:10<00:37,  6.23batch/s, auc=0.9219, loss=0.3918]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 62/292 [00:10<00:36,  6.25batch/s, auc=0.9219, loss=0.3918]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 62/292 [00:10<00:36,  6.25batch/s, auc=0.9220, loss=0.6004]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9220, loss=0.6004]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9230, loss=0.5273]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 64/292 [00:11<00:36,  6.23batch/s, auc=0.9230, loss=0.5273]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 64/292 [00:11<00:36,  6.23batch/s, auc=0.9236, loss=0.5384]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 65/292 [00:11<00:36,  6.25batch/s, auc=0.9236, loss=0.5384]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 65/292 [00:11<00:36,  6.25batch/s, auc=0.9216, loss=1.1035]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 66/292 [00:11<00:36,  6.23batch/s, auc=0.9216, loss=1.1035]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 66/292 [00:11<00:36,  6.23batch/s, auc=0.9225, loss=0.3151]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 67/292 [00:11<00:35,  6.25batch/s, auc=0.9225, loss=0.3151]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 67/292 [00:11<00:35,  6.25batch/s, auc=0.9224, loss=0.5108]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 68/292 [00:11<00:35,  6.26batch/s, auc=0.9224, loss=0.5108]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 68/292 [00:11<00:35,  6.26batch/s, auc=0.9227, loss=0.5389]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▎       | 69/292 [00:11<00:35,  6.26batch/s, auc=0.9227, loss=0.5389]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▎       | 69/292 [00:12<00:35,  6.26batch/s, auc=0.9230, loss=0.5443]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9230, loss=0.5443]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9227, loss=0.6376]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9227, loss=0.6376]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9227, loss=0.4975]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9227, loss=0.4975]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9232, loss=0.4810]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 73/292 [00:12<00:35,  6.25batch/s, auc=0.9232, loss=0.4810]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 73/292 [00:12<00:35,  6.25batch/s, auc=0.9239, loss=0.4045]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9239, loss=0.4045]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9244, loss=0.3591]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9244, loss=0.3591]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9250, loss=0.4100]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 76/292 [00:12<00:34,  6.26batch/s, auc=0.9250, loss=0.4100]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 76/292 [00:13<00:34,  6.26batch/s, auc=0.9247, loss=0.7749]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9247, loss=0.7749]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9249, loss=0.5705]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9249, loss=0.5705]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 78/292 [00:13<00:34,  6.27batch/s, auc=0.9255, loss=0.3541]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9255, loss=0.3541]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9258, loss=0.4876]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 80/292 [00:13<00:34,  6.22batch/s, auc=0.9258, loss=0.4876]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 80/292 [00:13<00:34,  6.22batch/s, auc=0.9259, loss=0.5321]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 81/292 [00:13<00:33,  6.23batch/s, auc=0.9259, loss=0.5321]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 81/292 [00:13<00:33,  6.23batch/s, auc=0.9262, loss=0.4984]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 82/292 [00:13<00:33,  6.24batch/s, auc=0.9262, loss=0.4984]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 82/292 [00:14<00:33,  6.24batch/s, auc=0.9266, loss=0.4574]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9266, loss=0.4574]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 83/292 [00:14<00:33,  6.25batch/s, auc=0.9263, loss=0.7305]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9263, loss=0.7305]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9260, loss=0.7357]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9260, loss=0.7357]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9259, loss=0.5917]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 86/292 [00:14<00:32,  6.24batch/s, auc=0.9259, loss=0.5917]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 86/292 [00:14<00:32,  6.24batch/s, auc=0.9265, loss=0.4361]\u001b[A\n",
      "Training Epoch 15/25:  30%|██▉       | 87/292 [00:14<00:32,  6.25batch/s, auc=0.9265, loss=0.4361]\u001b[A\n",
      "Training Epoch 15/25:  30%|██▉       | 87/292 [00:14<00:32,  6.25batch/s, auc=0.9269, loss=0.4446]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 88/292 [00:14<00:32,  6.22batch/s, auc=0.9269, loss=0.4446]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 88/292 [00:15<00:32,  6.22batch/s, auc=0.9265, loss=0.7071]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 89/292 [00:15<00:32,  6.22batch/s, auc=0.9265, loss=0.7071]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 89/292 [00:15<00:32,  6.22batch/s, auc=0.9261, loss=0.7358]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 90/292 [00:15<00:32,  6.23batch/s, auc=0.9261, loss=0.7358]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 90/292 [00:15<00:32,  6.23batch/s, auc=0.9261, loss=0.6657]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9261, loss=0.6657]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9267, loss=0.3402]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9267, loss=0.3402]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9262, loss=0.7468]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9262, loss=0.7468]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9261, loss=0.5274]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 94/292 [00:15<00:31,  6.21batch/s, auc=0.9261, loss=0.5274]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 94/292 [00:16<00:31,  6.21batch/s, auc=0.9262, loss=0.5169]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 95/292 [00:16<00:31,  6.22batch/s, auc=0.9262, loss=0.5169]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 95/292 [00:16<00:31,  6.22batch/s, auc=0.9266, loss=0.4924]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9266, loss=0.4924]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9269, loss=0.4076]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9269, loss=0.4076]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9271, loss=0.4605]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9271, loss=0.4605]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9270, loss=0.7481]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9270, loss=0.7481]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9269, loss=0.8188]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9269, loss=0.8188]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9270, loss=0.5425]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 101/292 [00:16<00:30,  6.25batch/s, auc=0.9270, loss=0.5425]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 101/292 [00:17<00:30,  6.25batch/s, auc=0.9274, loss=0.4229]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 102/292 [00:17<00:30,  6.23batch/s, auc=0.9274, loss=0.4229]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 102/292 [00:17<00:30,  6.23batch/s, auc=0.9274, loss=0.5169]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▌      | 103/292 [00:17<00:30,  6.24batch/s, auc=0.9274, loss=0.5169]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▌      | 103/292 [00:17<00:30,  6.24batch/s, auc=0.9266, loss=1.1085]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9266, loss=1.1085]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 104/292 [00:17<00:30,  6.24batch/s, auc=0.9261, loss=0.8517]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 105/292 [00:17<00:29,  6.25batch/s, auc=0.9261, loss=0.8517]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 105/292 [00:17<00:29,  6.25batch/s, auc=0.9259, loss=0.6520]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▋      | 106/292 [00:17<00:30,  6.20batch/s, auc=0.9259, loss=0.6520]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▋      | 106/292 [00:17<00:30,  6.20batch/s, auc=0.9261, loss=0.4994]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 107/292 [00:17<00:29,  6.21batch/s, auc=0.9261, loss=0.4994]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 107/292 [00:18<00:29,  6.21batch/s, auc=0.9262, loss=0.5340]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9262, loss=0.5340]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9260, loss=0.6667]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9260, loss=0.6667]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9259, loss=0.4945]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 110/292 [00:18<00:29,  6.18batch/s, auc=0.9259, loss=0.4945]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 110/292 [00:18<00:29,  6.18batch/s, auc=0.9256, loss=0.8970]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9256, loss=0.8970]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9255, loss=0.5216]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9255, loss=0.5216]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9258, loss=0.6087]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▊      | 113/292 [00:18<00:28,  6.22batch/s, auc=0.9258, loss=0.6087]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9260, loss=0.3883]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9260, loss=0.3883]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9258, loss=0.6245]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9258, loss=0.6245]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9257, loss=0.7286]\u001b[A\n",
      "Training Epoch 15/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9257, loss=0.7286]\u001b[A\n",
      "Training Epoch 15/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9258, loss=0.5205]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9258, loss=0.5205]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9256, loss=0.6619]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 118/292 [00:19<00:28,  6.19batch/s, auc=0.9256, loss=0.6619]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 118/292 [00:19<00:28,  6.19batch/s, auc=0.9255, loss=0.6182]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 119/292 [00:19<00:27,  6.21batch/s, auc=0.9255, loss=0.6182]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 119/292 [00:20<00:27,  6.21batch/s, auc=0.9252, loss=0.6109]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 120/292 [00:20<00:27,  6.20batch/s, auc=0.9252, loss=0.6109]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 120/292 [00:20<00:27,  6.20batch/s, auc=0.9246, loss=0.7136]\u001b[A\n",
      "Training Epoch 15/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9246, loss=0.7136]\u001b[A\n",
      "Training Epoch 15/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9242, loss=0.6341]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 122/292 [00:20<00:27,  6.15batch/s, auc=0.9242, loss=0.6341]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 122/292 [00:20<00:27,  6.15batch/s, auc=0.9249, loss=0.3447]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9249, loss=0.3447]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9246, loss=0.7322]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9246, loss=0.7322]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9246, loss=0.4933]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9246, loss=0.4933]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 125/292 [00:21<00:26,  6.19batch/s, auc=0.9245, loss=0.5954]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9245, loss=0.5954]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.9273]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9239, loss=0.9273]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9242, loss=0.5394]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9242, loss=0.5394]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9245, loss=0.4013]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 129/292 [00:21<00:26,  6.18batch/s, auc=0.9245, loss=0.4013]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 129/292 [00:21<00:26,  6.18batch/s, auc=0.9247, loss=0.3844]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9247, loss=0.3844]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9240, loss=1.0737]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9240, loss=1.0737]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9240, loss=0.5372]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▌     | 132/292 [00:21<00:25,  6.18batch/s, auc=0.9240, loss=0.5372]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▌     | 132/292 [00:22<00:25,  6.18batch/s, auc=0.9242, loss=0.5007]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9242, loss=0.5007]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9238, loss=0.7559]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9238, loss=0.7559]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9231, loss=1.0504]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9231, loss=1.0504]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9232, loss=0.4565]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9232, loss=0.4565]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9233, loss=0.5606]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 137/292 [00:22<00:25,  6.18batch/s, auc=0.9233, loss=0.5606]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 137/292 [00:22<00:25,  6.18batch/s, auc=0.9234, loss=0.6477]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 138/292 [00:22<00:24,  6.17batch/s, auc=0.9234, loss=0.6477]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 138/292 [00:23<00:24,  6.17batch/s, auc=0.9235, loss=0.5756]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9235, loss=0.5756]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9232, loss=0.7634]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9232, loss=0.7634]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 140/292 [00:23<00:24,  6.17batch/s, auc=0.9235, loss=0.4305]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9235, loss=0.4305]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9229, loss=0.8952]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9229, loss=0.8952]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▊     | 142/292 [00:23<00:24,  6.17batch/s, auc=0.9232, loss=0.3746]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 143/292 [00:23<00:24,  6.17batch/s, auc=0.9232, loss=0.3746]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 143/292 [00:23<00:24,  6.17batch/s, auc=0.9233, loss=0.4850]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 144/292 [00:23<00:23,  6.18batch/s, auc=0.9233, loss=0.4850]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 144/292 [00:24<00:23,  6.18batch/s, auc=0.9234, loss=0.5025]\u001b[A\n",
      "Training Epoch 15/25:  50%|████▉     | 145/292 [00:24<00:23,  6.18batch/s, auc=0.9234, loss=0.5025]\u001b[A\n",
      "Training Epoch 15/25:  50%|████▉     | 145/292 [00:24<00:23,  6.18batch/s, auc=0.9236, loss=0.5039]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9236, loss=0.5039]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 146/292 [00:24<00:23,  6.17batch/s, auc=0.9238, loss=0.4619]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9238, loss=0.4619]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9236, loss=0.6094]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 148/292 [00:24<00:23,  6.19batch/s, auc=0.9236, loss=0.6094]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 148/292 [00:24<00:23,  6.19batch/s, auc=0.9237, loss=0.4700]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9237, loss=0.4700]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 149/292 [00:24<00:23,  6.16batch/s, auc=0.9238, loss=0.5646]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.16batch/s, auc=0.9238, loss=0.5646]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.16batch/s, auc=0.9237, loss=0.5571]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9237, loss=0.5571]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9234, loss=0.8579]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.16batch/s, auc=0.9234, loss=0.8579]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.16batch/s, auc=0.9237, loss=0.4588]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9237, loss=0.4588]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9238, loss=0.4549]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9238, loss=0.4549]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9240, loss=0.4162]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.11batch/s, auc=0.9240, loss=0.4162]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.11batch/s, auc=0.9242, loss=0.4109]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.12batch/s, auc=0.9242, loss=0.4109]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.12batch/s, auc=0.9242, loss=0.6247]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.12batch/s, auc=0.9242, loss=0.6247]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.12batch/s, auc=0.9239, loss=0.8436]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9239, loss=0.8436]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9243, loss=0.3809]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9243, loss=0.3809]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9243, loss=0.5365]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9243, loss=0.5365]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9243, loss=0.5894]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9243, loss=0.5894]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9246, loss=0.4356]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9246, loss=0.4356]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.13batch/s, auc=0.9244, loss=0.6330]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9244, loss=0.6330]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9246, loss=0.5108]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9246, loss=0.5108]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9247, loss=0.6162]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9247, loss=0.6162]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9250, loss=0.4091]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.12batch/s, auc=0.9250, loss=0.4091]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.12batch/s, auc=0.9251, loss=0.4975]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9251, loss=0.4975]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.13batch/s, auc=0.9252, loss=0.5318]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.14batch/s, auc=0.9252, loss=0.5318]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.14batch/s, auc=0.9255, loss=0.3767]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.14batch/s, auc=0.9255, loss=0.3767]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.14batch/s, auc=0.9259, loss=0.3948]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.13batch/s, auc=0.9259, loss=0.3948]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.13batch/s, auc=0.9260, loss=0.5795]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.14batch/s, auc=0.9260, loss=0.5795]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.14batch/s, auc=0.9260, loss=0.4685]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9260, loss=0.4685]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9259, loss=0.7861]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.13batch/s, auc=0.9259, loss=0.7861]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.13batch/s, auc=0.9262, loss=0.5255]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.12batch/s, auc=0.9262, loss=0.5255]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.12batch/s, auc=0.9264, loss=0.3906]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.14batch/s, auc=0.9264, loss=0.3906]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.14batch/s, auc=0.9265, loss=0.6383]\u001b[A\n",
      "Training Epoch 15/25:  60%|██████    | 176/292 [00:29<00:18,  6.14batch/s, auc=0.9265, loss=0.6383]\u001b[A\n",
      "Training Epoch 15/25:  60%|██████    | 176/292 [00:29<00:18,  6.14batch/s, auc=0.9264, loss=0.6586]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9264, loss=0.6586]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9265, loss=0.3665]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9265, loss=0.3665]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9265, loss=0.5050]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.13batch/s, auc=0.9265, loss=0.5050]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.13batch/s, auc=0.9267, loss=0.4692]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.13batch/s, auc=0.9267, loss=0.4692]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.13batch/s, auc=0.9265, loss=0.6428]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.12batch/s, auc=0.9265, loss=0.6428]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.12batch/s, auc=0.9264, loss=0.8478]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9264, loss=0.8478]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9264, loss=0.6091]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.14batch/s, auc=0.9264, loss=0.6091]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.14batch/s, auc=0.9261, loss=0.8486]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.13batch/s, auc=0.9261, loss=0.8486]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.13batch/s, auc=0.9259, loss=1.0156]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.13batch/s, auc=0.9259, loss=1.0156]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.13batch/s, auc=0.9259, loss=0.6059]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.14batch/s, auc=0.9259, loss=0.6059]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.14batch/s, auc=0.9259, loss=0.5350]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.12batch/s, auc=0.9259, loss=0.5350]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.12batch/s, auc=0.9262, loss=0.4092]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.07batch/s, auc=0.9262, loss=0.4092]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.07batch/s, auc=0.9261, loss=0.5631]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9261, loss=0.5631]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9262, loss=0.5186]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9262, loss=0.5186]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9262, loss=0.6387]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.11batch/s, auc=0.9262, loss=0.6387]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.11batch/s, auc=0.9259, loss=0.8926]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9259, loss=0.8926]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9260, loss=0.4351]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.09batch/s, auc=0.9260, loss=0.4351]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.09batch/s, auc=0.9261, loss=0.5295]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9261, loss=0.5295]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9262, loss=0.5388]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.05batch/s, auc=0.9262, loss=0.5388]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.05batch/s, auc=0.9263, loss=0.7012]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9263, loss=0.7012]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9263, loss=0.5580]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9263, loss=0.5580]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9264, loss=0.4392]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.06batch/s, auc=0.9264, loss=0.4392]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.06batch/s, auc=0.9265, loss=0.4870]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.07batch/s, auc=0.9265, loss=0.4870]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.07batch/s, auc=0.9265, loss=0.5384]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9265, loss=0.5384]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9266, loss=0.5189]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.08batch/s, auc=0.9266, loss=0.5189]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.08batch/s, auc=0.9267, loss=0.5804]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9267, loss=0.5804]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9269, loss=0.4407]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.09batch/s, auc=0.9269, loss=0.4407]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.09batch/s, auc=0.9272, loss=0.3950]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.08batch/s, auc=0.9272, loss=0.3950]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.08batch/s, auc=0.9273, loss=0.5875]\u001b[A\n",
      "Training Epoch 15/25:  70%|███████   | 205/292 [00:33<00:14,  6.07batch/s, auc=0.9273, loss=0.5875]\u001b[A\n",
      "Training Epoch 15/25:  70%|███████   | 205/292 [00:34<00:14,  6.07batch/s, auc=0.9272, loss=0.7422]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 206/292 [00:34<00:14,  6.05batch/s, auc=0.9272, loss=0.7422]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 206/292 [00:34<00:14,  6.05batch/s, auc=0.9270, loss=0.7760]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 207/292 [00:34<00:14,  6.06batch/s, auc=0.9270, loss=0.7760]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 207/292 [00:34<00:14,  6.06batch/s, auc=0.9271, loss=0.5390]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 208/292 [00:34<00:13,  6.06batch/s, auc=0.9271, loss=0.5390]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 208/292 [00:34<00:13,  6.06batch/s, auc=0.9270, loss=0.5888]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.05batch/s, auc=0.9270, loss=0.5888]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.05batch/s, auc=0.9273, loss=0.4407]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.05batch/s, auc=0.9273, loss=0.4407]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.05batch/s, auc=0.9276, loss=0.3839]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.07batch/s, auc=0.9276, loss=0.3839]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.07batch/s, auc=0.9276, loss=0.7174]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.07batch/s, auc=0.9276, loss=0.7174]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.07batch/s, auc=0.9275, loss=0.6156]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.07batch/s, auc=0.9275, loss=0.6156]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.07batch/s, auc=0.9276, loss=0.4255]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.06batch/s, auc=0.9276, loss=0.4255]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.06batch/s, auc=0.9277, loss=0.5138]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.05batch/s, auc=0.9277, loss=0.5138]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.05batch/s, auc=0.9276, loss=0.7026]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.06batch/s, auc=0.9276, loss=0.7026]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.06batch/s, auc=0.9274, loss=0.5843]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 217/292 [00:35<00:12,  6.07batch/s, auc=0.9274, loss=0.5843]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 217/292 [00:36<00:12,  6.07batch/s, auc=0.9274, loss=0.5269]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.06batch/s, auc=0.9274, loss=0.5269]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.06batch/s, auc=0.9276, loss=0.3804]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 219/292 [00:36<00:12,  6.06batch/s, auc=0.9276, loss=0.3804]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 219/292 [00:36<00:12,  6.06batch/s, auc=0.9278, loss=0.4483]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 220/292 [00:36<00:11,  6.07batch/s, auc=0.9278, loss=0.4483]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 220/292 [00:36<00:11,  6.07batch/s, auc=0.9279, loss=0.4175]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.07batch/s, auc=0.9279, loss=0.4175]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 221/292 [00:36<00:11,  6.07batch/s, auc=0.9280, loss=0.4215]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.07batch/s, auc=0.9280, loss=0.4215]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 222/292 [00:36<00:11,  6.07batch/s, auc=0.9280, loss=0.4508]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▋  | 223/292 [00:36<00:11,  6.05batch/s, auc=0.9280, loss=0.4508]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▋  | 223/292 [00:37<00:11,  6.05batch/s, auc=0.9281, loss=0.5127]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.05batch/s, auc=0.9281, loss=0.5127]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.05batch/s, auc=0.9281, loss=0.6739]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.05batch/s, auc=0.9281, loss=0.6739]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.05batch/s, auc=0.9283, loss=0.3702]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 226/292 [00:37<00:10,  6.04batch/s, auc=0.9283, loss=0.3702]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 226/292 [00:37<00:10,  6.04batch/s, auc=0.9281, loss=0.7826]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 227/292 [00:37<00:10,  6.05batch/s, auc=0.9281, loss=0.7826]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 227/292 [00:37<00:10,  6.05batch/s, auc=0.9282, loss=0.5403]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 228/292 [00:37<00:10,  6.05batch/s, auc=0.9282, loss=0.5403]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 228/292 [00:37<00:10,  6.05batch/s, auc=0.9285, loss=0.3343]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 229/292 [00:37<00:10,  6.05batch/s, auc=0.9285, loss=0.3343]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 229/292 [00:38<00:10,  6.05batch/s, auc=0.9287, loss=0.4858]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 230/292 [00:38<00:10,  6.05batch/s, auc=0.9287, loss=0.4858]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 230/292 [00:38<00:10,  6.05batch/s, auc=0.9288, loss=0.6552]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 231/292 [00:38<00:10,  6.06batch/s, auc=0.9288, loss=0.6552]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 231/292 [00:38<00:10,  6.06batch/s, auc=0.9287, loss=0.6711]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 232/292 [00:38<00:09,  6.06batch/s, auc=0.9287, loss=0.6711]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 232/292 [00:38<00:09,  6.06batch/s, auc=0.9289, loss=0.3650]\u001b[A\n",
      "Training Epoch 15/25:  80%|███████▉  | 233/292 [00:38<00:09,  6.05batch/s, auc=0.9289, loss=0.3650]\u001b[A\n",
      "Training Epoch 15/25:  80%|███████▉  | 233/292 [00:38<00:09,  6.05batch/s, auc=0.9286, loss=0.8806]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 234/292 [00:38<00:09,  6.03batch/s, auc=0.9286, loss=0.8806]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 234/292 [00:38<00:09,  6.03batch/s, auc=0.9287, loss=0.5001]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 235/292 [00:38<00:09,  5.93batch/s, auc=0.9287, loss=0.5001]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 235/292 [00:39<00:09,  5.93batch/s, auc=0.9285, loss=0.6367]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 236/292 [00:39<00:09,  5.85batch/s, auc=0.9285, loss=0.6367]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 236/292 [00:39<00:09,  5.85batch/s, auc=0.9286, loss=0.5272]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 237/292 [00:39<00:09,  5.79batch/s, auc=0.9286, loss=0.5272]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 237/292 [00:39<00:09,  5.79batch/s, auc=0.9284, loss=0.8685]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.74batch/s, auc=0.9284, loss=0.8685]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.74batch/s, auc=0.9281, loss=0.8111]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.70batch/s, auc=0.9281, loss=0.8111]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.70batch/s, auc=0.9279, loss=0.6692]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.68batch/s, auc=0.9279, loss=0.6692]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 240/292 [00:39<00:09,  5.68batch/s, auc=0.9281, loss=0.3931]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 241/292 [00:39<00:09,  5.67batch/s, auc=0.9281, loss=0.3931]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.67batch/s, auc=0.9283, loss=0.3966]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.65batch/s, auc=0.9283, loss=0.3966]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.65batch/s, auc=0.9282, loss=0.6039]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.66batch/s, auc=0.9282, loss=0.6039]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.66batch/s, auc=0.9281, loss=0.5929]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.65batch/s, auc=0.9281, loss=0.5929]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.65batch/s, auc=0.9281, loss=0.5861]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.64batch/s, auc=0.9281, loss=0.5861]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.64batch/s, auc=0.9278, loss=1.1014]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.64batch/s, auc=0.9278, loss=1.1014]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 246/292 [00:40<00:08,  5.64batch/s, auc=0.9276, loss=0.6392]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 247/292 [00:40<00:07,  5.64batch/s, auc=0.9276, loss=0.6392]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.64batch/s, auc=0.9275, loss=0.6537]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9275, loss=0.6537]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9272, loss=1.0361]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.65batch/s, auc=0.9272, loss=1.0361]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.65batch/s, auc=0.9272, loss=0.6726]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.64batch/s, auc=0.9272, loss=0.6726]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.64batch/s, auc=0.9273, loss=0.4716]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.63batch/s, auc=0.9273, loss=0.4716]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.63batch/s, auc=0.9274, loss=0.5034]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▋ | 252/292 [00:41<00:07,  5.63batch/s, auc=0.9274, loss=0.5034]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.63batch/s, auc=0.9275, loss=0.4676]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.74batch/s, auc=0.9275, loss=0.4676]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.74batch/s, auc=0.9275, loss=0.5112]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.68batch/s, auc=0.9275, loss=0.5112]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.68batch/s, auc=0.9277, loss=0.4363]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.66batch/s, auc=0.9277, loss=0.4363]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.66batch/s, auc=0.9276, loss=0.7794]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.76batch/s, auc=0.9276, loss=0.7794]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.76batch/s, auc=0.9278, loss=0.4483]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.66batch/s, auc=0.9278, loss=0.4483]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.66batch/s, auc=0.9279, loss=0.5094]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 258/292 [00:42<00:06,  5.62batch/s, auc=0.9279, loss=0.5094]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.62batch/s, auc=0.9276, loss=0.6686]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9276, loss=0.6686]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9276, loss=0.6006]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.58batch/s, auc=0.9276, loss=0.6006]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.58batch/s, auc=0.9277, loss=0.4245]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.59batch/s, auc=0.9277, loss=0.4245]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.59batch/s, auc=0.9279, loss=0.4158]\u001b[A\n",
      "Training Epoch 15/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9279, loss=0.4158]\u001b[A\n",
      "Training Epoch 15/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.60batch/s, auc=0.9281, loss=0.3726]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 263/292 [00:43<00:05,  5.59batch/s, auc=0.9281, loss=0.3726]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 263/292 [00:44<00:05,  5.59batch/s, auc=0.9282, loss=0.4709]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 264/292 [00:44<00:05,  5.56batch/s, auc=0.9282, loss=0.4709]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 264/292 [00:44<00:05,  5.56batch/s, auc=0.9281, loss=0.4981]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 265/292 [00:44<00:04,  5.57batch/s, auc=0.9281, loss=0.4981]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 265/292 [00:44<00:04,  5.57batch/s, auc=0.9283, loss=0.4665]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 266/292 [00:44<00:04,  5.58batch/s, auc=0.9283, loss=0.4665]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 266/292 [00:44<00:04,  5.58batch/s, auc=0.9282, loss=0.6022]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.58batch/s, auc=0.9282, loss=0.6022]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.58batch/s, auc=0.9283, loss=0.4619]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.58batch/s, auc=0.9283, loss=0.4619]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.58batch/s, auc=0.9284, loss=0.4696]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 269/292 [00:44<00:04,  5.60batch/s, auc=0.9284, loss=0.4696]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.60batch/s, auc=0.9284, loss=0.4752]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9284, loss=0.4752]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9284, loss=0.5689]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.68batch/s, auc=0.9284, loss=0.5689]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.68batch/s, auc=0.9285, loss=0.5640]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.56batch/s, auc=0.9285, loss=0.5640]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.56batch/s, auc=0.9285, loss=0.7419]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.58batch/s, auc=0.9285, loss=0.7419]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.58batch/s, auc=0.9286, loss=0.4897]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.68batch/s, auc=0.9286, loss=0.4897]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.68batch/s, auc=0.9287, loss=0.3859]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 275/292 [00:45<00:03,  5.65batch/s, auc=0.9287, loss=0.3859]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.65batch/s, auc=0.9286, loss=0.5829]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.62batch/s, auc=0.9286, loss=0.5829]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.62batch/s, auc=0.9287, loss=0.3882]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.61batch/s, auc=0.9287, loss=0.3882]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.61batch/s, auc=0.9286, loss=0.6950]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.56batch/s, auc=0.9286, loss=0.6950]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.56batch/s, auc=0.9287, loss=0.4244]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.59batch/s, auc=0.9287, loss=0.4244]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.59batch/s, auc=0.9287, loss=0.5787]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 280/292 [00:46<00:02,  5.58batch/s, auc=0.9287, loss=0.5787]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.58batch/s, auc=0.9289, loss=0.3562]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.58batch/s, auc=0.9289, loss=0.3562]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.58batch/s, auc=0.9291, loss=0.3580]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.68batch/s, auc=0.9291, loss=0.3580]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.68batch/s, auc=0.9295, loss=0.2812]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.56batch/s, auc=0.9295, loss=0.2812]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.56batch/s, auc=0.9293, loss=0.8430]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.57batch/s, auc=0.9293, loss=0.8430]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.57batch/s, auc=0.9294, loss=0.4884]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.61batch/s, auc=0.9294, loss=0.4884]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.61batch/s, auc=0.9292, loss=0.5190]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 286/292 [00:47<00:01,  5.73batch/s, auc=0.9292, loss=0.5190]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.73batch/s, auc=0.9291, loss=0.8306]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.9291, loss=0.8306]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.9290, loss=0.5197]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9290, loss=0.5197]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9289, loss=0.6816]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9289, loss=0.6816]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9292, loss=0.2999]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9292, loss=0.2999]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9294, loss=0.4249]\u001b[A\n",
      "Training Epoch 15/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.93batch/s, auc=0.9294, loss=0.4249]\u001b[A\n",
      "Training Epoch 15/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.93batch/s, auc=0.9294, loss=0.4266]\u001b[A\n",
      "Training Epoch 15/25: 100%|██████████| 292/292 [00:48<00:00,  5.96batch/s, auc=0.9294, loss=0.4266]\u001b[A\n",
      "Epochs:  60%|██████    | 15/25 [13:43<09:09, 54.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25] Train Loss: 0.5693 | Train AUROC: 0.9294 Val Loss: 0.6991 | Val AUROC: 0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 16/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.8937, loss=0.6152]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 1/292 [00:00<04:46,  1.02batch/s, auc=0.8937, loss=0.6152]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 1/292 [00:01<04:46,  1.02batch/s, auc=0.9245, loss=0.4813]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 2/292 [00:01<02:24,  2.01batch/s, auc=0.9245, loss=0.4813]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 2/292 [00:01<02:24,  2.01batch/s, auc=0.9407, loss=0.5914]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9407, loss=0.5914]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9422, loss=0.4747]\u001b[A\n",
      "Training Epoch 16/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9422, loss=0.4747]\u001b[A\n",
      "Training Epoch 16/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9399, loss=0.5684]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 5/292 [00:01<01:06,  4.31batch/s, auc=0.9399, loss=0.5684]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 5/292 [00:01<01:06,  4.31batch/s, auc=0.9383, loss=0.7138]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 6/292 [00:01<00:59,  4.84batch/s, auc=0.9383, loss=0.7138]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 6/292 [00:01<00:59,  4.84batch/s, auc=0.9360, loss=0.6031]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 7/292 [00:01<00:55,  5.10batch/s, auc=0.9360, loss=0.6031]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 7/292 [00:02<00:55,  5.10batch/s, auc=0.9362, loss=0.4654]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9362, loss=0.4654]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9413, loss=0.4155]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.9413, loss=0.4155]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.9425, loss=0.3743]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 10/292 [00:02<00:48,  5.87batch/s, auc=0.9425, loss=0.3743]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 10/292 [00:02<00:48,  5.87batch/s, auc=0.9435, loss=0.4546]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 11/292 [00:02<00:46,  6.00batch/s, auc=0.9435, loss=0.4546]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 11/292 [00:02<00:46,  6.00batch/s, auc=0.9431, loss=0.4922]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 12/292 [00:02<00:45,  6.11batch/s, auc=0.9431, loss=0.4922]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 12/292 [00:02<00:45,  6.11batch/s, auc=0.9371, loss=0.7032]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 13/292 [00:02<00:46,  6.05batch/s, auc=0.9371, loss=0.7032]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 13/292 [00:03<00:46,  6.05batch/s, auc=0.9348, loss=0.6262]\u001b[A\n",
      "Training Epoch 16/25:   5%|▍         | 14/292 [00:03<00:45,  6.11batch/s, auc=0.9348, loss=0.6262]\u001b[A\n",
      "Training Epoch 16/25:   5%|▍         | 14/292 [00:03<00:45,  6.11batch/s, auc=0.9351, loss=0.5151]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 15/292 [00:03<00:44,  6.16batch/s, auc=0.9351, loss=0.5151]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 15/292 [00:03<00:44,  6.16batch/s, auc=0.9348, loss=0.4715]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 16/292 [00:03<00:44,  6.21batch/s, auc=0.9348, loss=0.4715]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 16/292 [00:03<00:44,  6.21batch/s, auc=0.9339, loss=0.7441]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 17/292 [00:03<00:44,  6.19batch/s, auc=0.9339, loss=0.7441]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 17/292 [00:03<00:44,  6.19batch/s, auc=0.9330, loss=0.5179]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9330, loss=0.5179]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9320, loss=0.5404]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 19/292 [00:03<00:43,  6.24batch/s, auc=0.9320, loss=0.5404]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 19/292 [00:04<00:43,  6.24batch/s, auc=0.9304, loss=0.6649]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 20/292 [00:04<00:43,  6.28batch/s, auc=0.9304, loss=0.6649]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 20/292 [00:04<00:43,  6.28batch/s, auc=0.9297, loss=0.7134]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 21/292 [00:04<00:43,  6.30batch/s, auc=0.9297, loss=0.7134]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 21/292 [00:04<00:43,  6.30batch/s, auc=0.9302, loss=0.4793]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9302, loss=0.4793]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9295, loss=0.5013]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 23/292 [00:04<00:42,  6.28batch/s, auc=0.9295, loss=0.5013]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 23/292 [00:04<00:42,  6.28batch/s, auc=0.9254, loss=1.0484]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 24/292 [00:04<00:42,  6.30batch/s, auc=0.9254, loss=1.0484]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 24/292 [00:04<00:42,  6.30batch/s, auc=0.9273, loss=0.3294]\u001b[A\n",
      "Training Epoch 16/25:   9%|▊         | 25/292 [00:04<00:42,  6.25batch/s, auc=0.9273, loss=0.3294]\u001b[A\n",
      "Training Epoch 16/25:   9%|▊         | 25/292 [00:04<00:42,  6.25batch/s, auc=0.9284, loss=0.5077]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 26/292 [00:04<00:42,  6.28batch/s, auc=0.9284, loss=0.5077]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 26/292 [00:05<00:42,  6.28batch/s, auc=0.9283, loss=0.5675]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 27/292 [00:05<00:42,  6.26batch/s, auc=0.9283, loss=0.5675]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 27/292 [00:05<00:42,  6.26batch/s, auc=0.9281, loss=0.5590]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9281, loss=0.5590]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 28/292 [00:05<00:42,  6.28batch/s, auc=0.9285, loss=0.4213]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 29/292 [00:05<00:42,  6.26batch/s, auc=0.9285, loss=0.4213]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 29/292 [00:05<00:42,  6.26batch/s, auc=0.9295, loss=0.5027]\u001b[A\n",
      "Training Epoch 16/25:  10%|█         | 30/292 [00:05<00:41,  6.26batch/s, auc=0.9295, loss=0.5027]\u001b[A\n",
      "Training Epoch 16/25:  10%|█         | 30/292 [00:05<00:41,  6.26batch/s, auc=0.9294, loss=0.6307]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9294, loss=0.6307]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9299, loss=0.4735]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 32/292 [00:05<00:41,  6.28batch/s, auc=0.9299, loss=0.4735]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 32/292 [00:06<00:41,  6.28batch/s, auc=0.9299, loss=0.4351]\u001b[A\n",
      "Training Epoch 16/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9299, loss=0.4351]\u001b[A\n",
      "Training Epoch 16/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9309, loss=0.4043]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9309, loss=0.4043]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9314, loss=0.4824]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 35/292 [00:06<00:40,  6.31batch/s, auc=0.9314, loss=0.4824]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 35/292 [00:06<00:40,  6.31batch/s, auc=0.9325, loss=0.3764]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9325, loss=0.3764]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9331, loss=0.4916]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9331, loss=0.4916]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9339, loss=0.4314]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9339, loss=0.4314]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9341, loss=0.5017]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9341, loss=0.5017]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 39/292 [00:07<00:40,  6.28batch/s, auc=0.9338, loss=0.5355]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9338, loss=0.5355]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9335, loss=0.5440]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9335, loss=0.5440]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9346, loss=0.3644]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9346, loss=0.3644]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9342, loss=0.6483]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▍        | 43/292 [00:07<00:39,  6.28batch/s, auc=0.9342, loss=0.6483]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▍        | 43/292 [00:07<00:39,  6.28batch/s, auc=0.9343, loss=0.5275]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 44/292 [00:07<00:39,  6.21batch/s, auc=0.9343, loss=0.5275]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 44/292 [00:08<00:39,  6.21batch/s, auc=0.9335, loss=0.5384]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 45/292 [00:08<00:39,  6.25batch/s, auc=0.9335, loss=0.5384]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 45/292 [00:08<00:39,  6.25batch/s, auc=0.9325, loss=0.8123]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 46/292 [00:08<00:39,  6.27batch/s, auc=0.9325, loss=0.8123]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 46/292 [00:08<00:39,  6.27batch/s, auc=0.9340, loss=0.3305]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9340, loss=0.3305]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 47/292 [00:08<00:39,  6.28batch/s, auc=0.9342, loss=0.5375]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▋        | 48/292 [00:08<00:39,  6.24batch/s, auc=0.9342, loss=0.5375]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▋        | 48/292 [00:08<00:39,  6.24batch/s, auc=0.9341, loss=0.6412]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.9341, loss=0.6412]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 49/292 [00:08<00:38,  6.26batch/s, auc=0.9335, loss=0.7237]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9335, loss=0.7237]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9339, loss=0.4130]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 51/292 [00:08<00:38,  6.29batch/s, auc=0.9339, loss=0.4130]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9337, loss=0.7109]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9337, loss=0.7109]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9317, loss=1.1727]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 53/292 [00:09<00:38,  6.24batch/s, auc=0.9317, loss=1.1727]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 53/292 [00:09<00:38,  6.24batch/s, auc=0.9308, loss=0.5886]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9308, loss=0.5886]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 54/292 [00:09<00:38,  6.26batch/s, auc=0.9300, loss=0.6532]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9300, loss=0.6532]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9306, loss=0.3392]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 56/292 [00:09<00:37,  6.25batch/s, auc=0.9306, loss=0.3392]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 56/292 [00:09<00:37,  6.25batch/s, auc=0.9301, loss=0.5358]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 57/292 [00:09<00:37,  6.21batch/s, auc=0.9301, loss=0.5358]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 57/292 [00:10<00:37,  6.21batch/s, auc=0.9301, loss=0.5384]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 58/292 [00:10<00:38,  6.15batch/s, auc=0.9301, loss=0.5384]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 58/292 [00:10<00:38,  6.15batch/s, auc=0.9312, loss=0.3372]\u001b[A\n",
      "Training Epoch 16/25:  20%|██        | 59/292 [00:10<00:37,  6.20batch/s, auc=0.9312, loss=0.3372]\u001b[A\n",
      "Training Epoch 16/25:  20%|██        | 59/292 [00:10<00:37,  6.20batch/s, auc=0.9316, loss=0.5453]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 60/292 [00:10<00:37,  6.23batch/s, auc=0.9316, loss=0.5453]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 60/292 [00:10<00:37,  6.23batch/s, auc=0.9314, loss=0.4848]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 61/292 [00:10<00:37,  6.22batch/s, auc=0.9314, loss=0.4848]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 61/292 [00:10<00:37,  6.22batch/s, auc=0.9319, loss=0.4739]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 62/292 [00:10<00:36,  6.24batch/s, auc=0.9319, loss=0.4739]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 62/292 [00:10<00:36,  6.24batch/s, auc=0.9315, loss=0.6828]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 63/292 [00:10<00:36,  6.26batch/s, auc=0.9315, loss=0.6828]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 63/292 [00:11<00:36,  6.26batch/s, auc=0.9318, loss=0.4773]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9318, loss=0.4773]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9320, loss=0.5109]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9320, loss=0.5109]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9326, loss=0.4418]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 66/292 [00:11<00:35,  6.28batch/s, auc=0.9326, loss=0.4418]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 66/292 [00:11<00:35,  6.28batch/s, auc=0.9325, loss=0.6174]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9325, loss=0.6174]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9333, loss=0.4527]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9333, loss=0.4527]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9331, loss=0.6172]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▎       | 69/292 [00:11<00:35,  6.27batch/s, auc=0.9331, loss=0.6172]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▎       | 69/292 [00:12<00:35,  6.27batch/s, auc=0.9335, loss=0.5234]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 70/292 [00:12<00:35,  6.23batch/s, auc=0.9335, loss=0.5234]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 70/292 [00:12<00:35,  6.23batch/s, auc=0.9330, loss=0.5755]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 71/292 [00:12<00:35,  6.18batch/s, auc=0.9330, loss=0.5755]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 71/292 [00:12<00:35,  6.18batch/s, auc=0.9328, loss=0.5631]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▍       | 72/292 [00:12<00:35,  6.19batch/s, auc=0.9328, loss=0.5631]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▍       | 72/292 [00:12<00:35,  6.19batch/s, auc=0.9325, loss=0.5233]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 73/292 [00:12<00:35,  6.21batch/s, auc=0.9325, loss=0.5233]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 73/292 [00:12<00:35,  6.21batch/s, auc=0.9327, loss=0.4832]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 74/292 [00:12<00:34,  6.23batch/s, auc=0.9327, loss=0.4832]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 74/292 [00:12<00:34,  6.23batch/s, auc=0.9326, loss=0.4838]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 75/292 [00:12<00:34,  6.25batch/s, auc=0.9326, loss=0.4838]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 75/292 [00:12<00:34,  6.25batch/s, auc=0.9320, loss=0.6102]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 76/292 [00:12<00:34,  6.27batch/s, auc=0.9320, loss=0.6102]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 76/292 [00:13<00:34,  6.27batch/s, auc=0.9328, loss=0.3401]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▋       | 77/292 [00:13<00:34,  6.28batch/s, auc=0.9328, loss=0.3401]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▋       | 77/292 [00:13<00:34,  6.28batch/s, auc=0.9324, loss=0.8835]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9324, loss=0.8835]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9328, loss=0.4024]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9328, loss=0.4024]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9329, loss=0.4855]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9329, loss=0.4855]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9338, loss=0.2834]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 81/292 [00:13<00:33,  6.24batch/s, auc=0.9338, loss=0.2834]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 81/292 [00:13<00:33,  6.24batch/s, auc=0.9343, loss=0.4330]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 82/292 [00:13<00:33,  6.25batch/s, auc=0.9343, loss=0.4330]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 82/292 [00:14<00:33,  6.25batch/s, auc=0.9345, loss=0.4251]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 83/292 [00:14<00:33,  6.23batch/s, auc=0.9345, loss=0.4251]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 83/292 [00:14<00:33,  6.23batch/s, auc=0.9341, loss=0.6388]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 84/292 [00:14<00:33,  6.23batch/s, auc=0.9341, loss=0.6388]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 84/292 [00:14<00:33,  6.23batch/s, auc=0.9339, loss=0.5534]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9339, loss=0.5534]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9340, loss=0.4387]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 86/292 [00:14<00:33,  6.23batch/s, auc=0.9340, loss=0.4387]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 86/292 [00:14<00:33,  6.23batch/s, auc=0.9343, loss=0.4874]\u001b[A\n",
      "Training Epoch 16/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9343, loss=0.4874]\u001b[A\n",
      "Training Epoch 16/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9346, loss=0.3893]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 88/292 [00:14<00:32,  6.23batch/s, auc=0.9346, loss=0.3893]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 88/292 [00:15<00:32,  6.23batch/s, auc=0.9348, loss=0.3814]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 89/292 [00:15<00:32,  6.24batch/s, auc=0.9348, loss=0.3814]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 89/292 [00:15<00:32,  6.24batch/s, auc=0.9343, loss=0.6495]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9343, loss=0.6495]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9343, loss=0.4777]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 91/292 [00:15<00:32,  6.23batch/s, auc=0.9343, loss=0.4777]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 91/292 [00:15<00:32,  6.23batch/s, auc=0.9347, loss=0.2944]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9347, loss=0.2944]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 92/292 [00:15<00:32,  6.23batch/s, auc=0.9341, loss=0.7308]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 93/292 [00:15<00:32,  6.16batch/s, auc=0.9341, loss=0.7308]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 93/292 [00:15<00:32,  6.16batch/s, auc=0.9336, loss=0.6885]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 94/292 [00:15<00:32,  6.15batch/s, auc=0.9336, loss=0.6885]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 94/292 [00:16<00:32,  6.15batch/s, auc=0.9334, loss=0.6357]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 95/292 [00:16<00:31,  6.17batch/s, auc=0.9334, loss=0.6357]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 95/292 [00:16<00:31,  6.17batch/s, auc=0.9330, loss=0.6365]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 96/292 [00:16<00:31,  6.18batch/s, auc=0.9330, loss=0.6365]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 96/292 [00:16<00:31,  6.18batch/s, auc=0.9334, loss=0.4116]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 97/292 [00:16<00:31,  6.20batch/s, auc=0.9334, loss=0.4116]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 97/292 [00:16<00:31,  6.20batch/s, auc=0.9336, loss=0.5099]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9336, loss=0.5099]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9337, loss=0.4236]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 99/292 [00:16<00:31,  6.20batch/s, auc=0.9337, loss=0.4236]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 99/292 [00:16<00:31,  6.20batch/s, auc=0.9340, loss=0.3945]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 100/292 [00:16<00:30,  6.20batch/s, auc=0.9340, loss=0.3945]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 100/292 [00:16<00:30,  6.20batch/s, auc=0.9341, loss=0.4194]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 101/292 [00:16<00:30,  6.21batch/s, auc=0.9341, loss=0.4194]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 101/292 [00:17<00:30,  6.21batch/s, auc=0.9337, loss=0.7144]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9337, loss=0.7144]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9338, loss=0.4311]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▌      | 103/292 [00:17<00:30,  6.22batch/s, auc=0.9338, loss=0.4311]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▌      | 103/292 [00:17<00:30,  6.22batch/s, auc=0.9341, loss=0.4845]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9341, loss=0.4845]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9342, loss=0.4904]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9342, loss=0.4904]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9345, loss=0.4269]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9345, loss=0.4269]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9348, loss=0.4107]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 107/292 [00:17<00:29,  6.21batch/s, auc=0.9348, loss=0.4107]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 107/292 [00:18<00:29,  6.21batch/s, auc=0.9343, loss=0.8161]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9343, loss=0.8161]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9347, loss=0.3218]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 109/292 [00:18<00:29,  6.19batch/s, auc=0.9347, loss=0.3218]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 109/292 [00:18<00:29,  6.19batch/s, auc=0.9345, loss=0.6588]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9345, loss=0.6588]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9348, loss=0.4835]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9348, loss=0.4835]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9350, loss=0.4112]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 112/292 [00:18<00:28,  6.21batch/s, auc=0.9350, loss=0.4112]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 112/292 [00:18<00:28,  6.21batch/s, auc=0.9350, loss=0.4927]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▊      | 113/292 [00:18<00:28,  6.21batch/s, auc=0.9350, loss=0.4927]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▊      | 113/292 [00:19<00:28,  6.21batch/s, auc=0.9347, loss=0.5416]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 114/292 [00:19<00:28,  6.21batch/s, auc=0.9347, loss=0.5416]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 114/292 [00:19<00:28,  6.21batch/s, auc=0.9348, loss=0.4523]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 115/292 [00:19<00:28,  6.21batch/s, auc=0.9348, loss=0.4523]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 115/292 [00:19<00:28,  6.21batch/s, auc=0.9349, loss=0.4225]\u001b[A\n",
      "Training Epoch 16/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9349, loss=0.4225]\u001b[A\n",
      "Training Epoch 16/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9346, loss=0.7418]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9346, loss=0.7418]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 117/292 [00:19<00:28,  6.19batch/s, auc=0.9347, loss=0.6067]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9347, loss=0.6067]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 118/292 [00:19<00:28,  6.20batch/s, auc=0.9338, loss=1.1154]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 119/292 [00:19<00:27,  6.20batch/s, auc=0.9338, loss=1.1154]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 119/292 [00:20<00:27,  6.20batch/s, auc=0.9341, loss=0.3987]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9341, loss=0.3987]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9339, loss=0.5772]\u001b[A\n",
      "Training Epoch 16/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9339, loss=0.5772]\u001b[A\n",
      "Training Epoch 16/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9338, loss=0.5179]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 122/292 [00:20<00:27,  6.19batch/s, auc=0.9338, loss=0.5179]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 122/292 [00:20<00:27,  6.19batch/s, auc=0.9338, loss=0.4540]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 123/292 [00:20<00:27,  6.19batch/s, auc=0.9338, loss=0.4540]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 123/292 [00:20<00:27,  6.19batch/s, auc=0.9340, loss=0.4763]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 124/292 [00:20<00:27,  6.19batch/s, auc=0.9340, loss=0.4763]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 124/292 [00:20<00:27,  6.19batch/s, auc=0.9337, loss=0.9105]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 125/292 [00:20<00:26,  6.20batch/s, auc=0.9337, loss=0.9105]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 125/292 [00:21<00:26,  6.20batch/s, auc=0.9333, loss=0.8092]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9333, loss=0.8092]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9335, loss=0.4790]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9335, loss=0.4790]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9338, loss=0.3766]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9338, loss=0.3766]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9342, loss=0.4141]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9342, loss=0.4141]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9342, loss=0.4851]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9342, loss=0.4851]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9344, loss=0.5518]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 131/292 [00:21<00:26,  6.16batch/s, auc=0.9344, loss=0.5518]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 131/292 [00:22<00:26,  6.16batch/s, auc=0.9341, loss=0.6566]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9341, loss=0.6566]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9342, loss=0.5230]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9342, loss=0.5230]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9343, loss=0.5343]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9343, loss=0.5343]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9343, loss=0.6159]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 135/292 [00:22<00:25,  6.13batch/s, auc=0.9343, loss=0.6159]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 135/292 [00:22<00:25,  6.13batch/s, auc=0.9346, loss=0.3399]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 136/292 [00:22<00:25,  6.14batch/s, auc=0.9346, loss=0.3399]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 136/292 [00:22<00:25,  6.14batch/s, auc=0.9342, loss=1.0182]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9342, loss=1.0182]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9343, loss=0.5722]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 138/292 [00:22<00:25,  6.14batch/s, auc=0.9343, loss=0.5722]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 138/292 [00:23<00:25,  6.14batch/s, auc=0.9338, loss=0.8523]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9338, loss=0.8523]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 139/292 [00:23<00:25,  6.11batch/s, auc=0.9340, loss=0.6080]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 140/292 [00:23<00:24,  6.12batch/s, auc=0.9340, loss=0.6080]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 140/292 [00:23<00:24,  6.12batch/s, auc=0.9339, loss=0.5684]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9339, loss=0.5684]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 141/292 [00:23<00:24,  6.12batch/s, auc=0.9336, loss=0.8607]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▊     | 142/292 [00:23<00:24,  6.14batch/s, auc=0.9336, loss=0.8607]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▊     | 142/292 [00:23<00:24,  6.14batch/s, auc=0.9339, loss=0.4160]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9339, loss=0.4160]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9338, loss=0.5169]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 144/292 [00:23<00:24,  6.14batch/s, auc=0.9338, loss=0.5169]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 144/292 [00:24<00:24,  6.14batch/s, auc=0.9341, loss=0.3998]\u001b[A\n",
      "Training Epoch 16/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9341, loss=0.3998]\u001b[A\n",
      "Training Epoch 16/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9341, loss=0.4618]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9341, loss=0.4618]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9340, loss=0.6444]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9340, loss=0.6444]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9342, loss=0.4370]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9342, loss=0.4370]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9340, loss=0.7076]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9340, loss=0.7076]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9336, loss=0.8579]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.15batch/s, auc=0.9336, loss=0.8579]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.15batch/s, auc=0.9335, loss=0.6269]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.15batch/s, auc=0.9335, loss=0.6269]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.15batch/s, auc=0.9334, loss=0.6302]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.15batch/s, auc=0.9334, loss=0.6302]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.15batch/s, auc=0.9333, loss=0.7865]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.13batch/s, auc=0.9333, loss=0.7865]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.13batch/s, auc=0.9332, loss=0.7074]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9332, loss=0.7074]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9331, loss=0.6170]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9331, loss=0.6170]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9332, loss=0.6080]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.07batch/s, auc=0.9332, loss=0.6080]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.07batch/s, auc=0.9331, loss=0.5334]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.08batch/s, auc=0.9331, loss=0.5334]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.08batch/s, auc=0.9333, loss=0.4819]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.09batch/s, auc=0.9333, loss=0.4819]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.09batch/s, auc=0.9332, loss=0.4867]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.07batch/s, auc=0.9332, loss=0.4867]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.07batch/s, auc=0.9334, loss=0.4458]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.09batch/s, auc=0.9334, loss=0.4458]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.09batch/s, auc=0.9332, loss=0.8324]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.08batch/s, auc=0.9332, loss=0.8324]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.08batch/s, auc=0.9334, loss=0.4374]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.11batch/s, auc=0.9334, loss=0.4374]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.11batch/s, auc=0.9335, loss=0.4929]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9335, loss=0.4929]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9333, loss=0.7372]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9333, loss=0.7372]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9336, loss=0.4145]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9336, loss=0.4145]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9335, loss=0.5920]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.15batch/s, auc=0.9335, loss=0.5920]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.15batch/s, auc=0.9337, loss=0.4298]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.9337, loss=0.4298]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.9341, loss=0.3055]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.14batch/s, auc=0.9341, loss=0.3055]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.14batch/s, auc=0.9340, loss=0.5758]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9340, loss=0.5758]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9339, loss=0.6980]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.15batch/s, auc=0.9339, loss=0.6980]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.15batch/s, auc=0.9339, loss=0.5067]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.16batch/s, auc=0.9339, loss=0.5067]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.16batch/s, auc=0.9341, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.17batch/s, auc=0.9341, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.17batch/s, auc=0.9343, loss=0.4756]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.16batch/s, auc=0.9343, loss=0.4756]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.16batch/s, auc=0.9341, loss=0.7142]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.15batch/s, auc=0.9341, loss=0.7142]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.15batch/s, auc=0.9340, loss=0.6327]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.09batch/s, auc=0.9340, loss=0.6327]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.09batch/s, auc=0.9339, loss=0.5604]\u001b[A\n",
      "Training Epoch 16/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.9339, loss=0.5604]\u001b[A\n",
      "Training Epoch 16/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.9337, loss=0.5197]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9337, loss=0.5197]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 177/292 [00:29<00:18,  6.09batch/s, auc=0.9336, loss=0.5926]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 178/292 [00:29<00:18,  6.09batch/s, auc=0.9336, loss=0.5926]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 178/292 [00:29<00:18,  6.09batch/s, auc=0.9335, loss=0.5131]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.08batch/s, auc=0.9335, loss=0.5131]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.08batch/s, auc=0.9326, loss=1.7556]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.10batch/s, auc=0.9326, loss=1.7556]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 180/292 [00:30<00:18,  6.10batch/s, auc=0.9324, loss=0.5055]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9324, loss=0.5055]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9326, loss=0.3593]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.11batch/s, auc=0.9326, loss=0.3593]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.11batch/s, auc=0.9329, loss=0.3568]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.10batch/s, auc=0.9329, loss=0.3568]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.10batch/s, auc=0.9323, loss=1.2705]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9323, loss=1.2705]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9320, loss=0.9327]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9320, loss=0.9327]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9321, loss=0.5019]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.14batch/s, auc=0.9321, loss=0.5019]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.14batch/s, auc=0.9320, loss=0.4788]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.14batch/s, auc=0.9320, loss=0.4788]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.14batch/s, auc=0.9322, loss=0.4670]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 188/292 [00:31<00:16,  6.14batch/s, auc=0.9322, loss=0.4670]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 188/292 [00:31<00:16,  6.14batch/s, auc=0.9325, loss=0.3637]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.13batch/s, auc=0.9325, loss=0.3637]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.13batch/s, auc=0.9324, loss=0.6209]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.13batch/s, auc=0.9324, loss=0.6209]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.13batch/s, auc=0.9324, loss=0.5768]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.12batch/s, auc=0.9324, loss=0.5768]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.12batch/s, auc=0.9323, loss=0.5004]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.11batch/s, auc=0.9323, loss=0.5004]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.11batch/s, auc=0.9322, loss=0.6945]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.08batch/s, auc=0.9322, loss=0.6945]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.9324, loss=0.4057]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9324, loss=0.4057]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9322, loss=0.7829]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.07batch/s, auc=0.9322, loss=0.7829]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.07batch/s, auc=0.9323, loss=0.4419]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9323, loss=0.4419]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.07batch/s, auc=0.9323, loss=0.5823]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.06batch/s, auc=0.9323, loss=0.5823]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.06batch/s, auc=0.9325, loss=0.4820]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 198/292 [00:32<00:15,  5.98batch/s, auc=0.9325, loss=0.4820]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 198/292 [00:32<00:15,  5.98batch/s, auc=0.9326, loss=0.5435]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 199/292 [00:32<00:15,  5.89batch/s, auc=0.9326, loss=0.5435]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.89batch/s, auc=0.9324, loss=0.6423]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.81batch/s, auc=0.9324, loss=0.6423]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.81batch/s, auc=0.9321, loss=0.8476]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.88batch/s, auc=0.9321, loss=0.8476]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.88batch/s, auc=0.9321, loss=0.5085]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.83batch/s, auc=0.9321, loss=0.5085]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.83batch/s, auc=0.9321, loss=0.5369]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.72batch/s, auc=0.9321, loss=0.5369]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.72batch/s, auc=0.9323, loss=0.4068]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.82batch/s, auc=0.9323, loss=0.4068]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.82batch/s, auc=0.9323, loss=0.5376]\u001b[A\n",
      "Training Epoch 16/25:  70%|███████   | 205/292 [00:34<00:15,  5.76batch/s, auc=0.9323, loss=0.5376]\u001b[A\n",
      "Training Epoch 16/25:  70%|███████   | 205/292 [00:34<00:15,  5.76batch/s, auc=0.9324, loss=0.5445]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 206/292 [00:34<00:14,  5.85batch/s, auc=0.9324, loss=0.5445]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 206/292 [00:34<00:14,  5.85batch/s, auc=0.9324, loss=0.4755]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 207/292 [00:34<00:14,  5.79batch/s, auc=0.9324, loss=0.4755]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 207/292 [00:34<00:14,  5.79batch/s, auc=0.9325, loss=0.5365]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 208/292 [00:34<00:14,  5.84batch/s, auc=0.9325, loss=0.5365]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 208/292 [00:34<00:14,  5.84batch/s, auc=0.9325, loss=0.4302]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.69batch/s, auc=0.9325, loss=0.4302]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.69batch/s, auc=0.9325, loss=0.5578]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.79batch/s, auc=0.9325, loss=0.5578]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.79batch/s, auc=0.9326, loss=0.5023]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.75batch/s, auc=0.9326, loss=0.5023]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.75batch/s, auc=0.9325, loss=0.5693]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.72batch/s, auc=0.9325, loss=0.5693]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.72batch/s, auc=0.9324, loss=0.7539]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.70batch/s, auc=0.9324, loss=0.7539]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.70batch/s, auc=0.9327, loss=0.3273]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.68batch/s, auc=0.9327, loss=0.3273]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.68batch/s, auc=0.9326, loss=0.6648]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.77batch/s, auc=0.9326, loss=0.6648]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.77batch/s, auc=0.9327, loss=0.5713]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.73batch/s, auc=0.9327, loss=0.5713]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.73batch/s, auc=0.9325, loss=0.6223]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.70batch/s, auc=0.9325, loss=0.6223]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.70batch/s, auc=0.9324, loss=0.7117]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.70batch/s, auc=0.9324, loss=0.7117]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.70batch/s, auc=0.9325, loss=0.3649]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9325, loss=0.3649]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9324, loss=0.5587]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.63batch/s, auc=0.9324, loss=0.5587]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.63batch/s, auc=0.9326, loss=0.4931]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.61batch/s, auc=0.9326, loss=0.4931]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.61batch/s, auc=0.9328, loss=0.4422]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.62batch/s, auc=0.9328, loss=0.4422]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.62batch/s, auc=0.9327, loss=0.7045]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.65batch/s, auc=0.9327, loss=0.7045]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.65batch/s, auc=0.9328, loss=0.4529]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.65batch/s, auc=0.9328, loss=0.4529]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.65batch/s, auc=0.9324, loss=1.0013]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.64batch/s, auc=0.9324, loss=1.0013]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.64batch/s, auc=0.9322, loss=0.6027]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.63batch/s, auc=0.9322, loss=0.6027]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.63batch/s, auc=0.9324, loss=0.3212]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.64batch/s, auc=0.9324, loss=0.3212]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.64batch/s, auc=0.9324, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.65batch/s, auc=0.9324, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.65batch/s, auc=0.9323, loss=0.6916]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.77batch/s, auc=0.9323, loss=0.6916]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.77batch/s, auc=0.9322, loss=0.5549]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.84batch/s, auc=0.9322, loss=0.5549]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.84batch/s, auc=0.9321, loss=0.6913]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.87batch/s, auc=0.9321, loss=0.6913]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.87batch/s, auc=0.9321, loss=0.5249]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.90batch/s, auc=0.9321, loss=0.5249]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.90batch/s, auc=0.9322, loss=0.3963]\u001b[A\n",
      "Training Epoch 16/25:  80%|███████▉  | 233/292 [00:38<00:09,  5.92batch/s, auc=0.9322, loss=0.3963]\u001b[A\n",
      "Training Epoch 16/25:  80%|███████▉  | 233/292 [00:39<00:09,  5.92batch/s, auc=0.9325, loss=0.3195]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 234/292 [00:39<00:09,  5.94batch/s, auc=0.9325, loss=0.3195]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 234/292 [00:39<00:09,  5.94batch/s, auc=0.9325, loss=0.6795]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 235/292 [00:39<00:09,  5.83batch/s, auc=0.9325, loss=0.6795]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 235/292 [00:39<00:09,  5.83batch/s, auc=0.9326, loss=0.5393]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 236/292 [00:39<00:09,  5.87batch/s, auc=0.9326, loss=0.5393]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 236/292 [00:39<00:09,  5.87batch/s, auc=0.9324, loss=0.8039]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 237/292 [00:39<00:09,  5.67batch/s, auc=0.9324, loss=0.8039]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 237/292 [00:39<00:09,  5.67batch/s, auc=0.9325, loss=0.4741]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.65batch/s, auc=0.9325, loss=0.4741]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.65batch/s, auc=0.9326, loss=0.4570]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.72batch/s, auc=0.9326, loss=0.4570]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.72batch/s, auc=0.9326, loss=0.5508]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.68batch/s, auc=0.9326, loss=0.5508]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.68batch/s, auc=0.9325, loss=0.4662]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9325, loss=0.4662]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9326, loss=0.4582]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.65batch/s, auc=0.9326, loss=0.4582]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.65batch/s, auc=0.9324, loss=0.9825]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.65batch/s, auc=0.9324, loss=0.9825]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.65batch/s, auc=0.9325, loss=0.4252]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.64batch/s, auc=0.9325, loss=0.4252]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.64batch/s, auc=0.9325, loss=0.4766]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.63batch/s, auc=0.9325, loss=0.4766]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.63batch/s, auc=0.9326, loss=0.3860]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.63batch/s, auc=0.9326, loss=0.3860]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.63batch/s, auc=0.9324, loss=0.6513]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9324, loss=0.6513]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9323, loss=0.7416]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.60batch/s, auc=0.9323, loss=0.7416]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.60batch/s, auc=0.9323, loss=0.6128]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.59batch/s, auc=0.9323, loss=0.6128]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.59batch/s, auc=0.9324, loss=0.4861]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.62batch/s, auc=0.9324, loss=0.4861]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.62batch/s, auc=0.9322, loss=0.6009]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.61batch/s, auc=0.9322, loss=0.6009]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.61batch/s, auc=0.9321, loss=0.6694]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9321, loss=0.6694]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.62batch/s, auc=0.9323, loss=0.3957]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.62batch/s, auc=0.9323, loss=0.3957]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.62batch/s, auc=0.9322, loss=0.6719]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.72batch/s, auc=0.9322, loss=0.6719]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.72batch/s, auc=0.9321, loss=0.6587]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.81batch/s, auc=0.9321, loss=0.6587]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.81batch/s, auc=0.9320, loss=0.5872]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.87batch/s, auc=0.9320, loss=0.5872]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.87batch/s, auc=0.9320, loss=0.7195]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 257/292 [00:43<00:05,  5.89batch/s, auc=0.9320, loss=0.7195]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 257/292 [00:43<00:05,  5.89batch/s, auc=0.9318, loss=1.0438]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.81batch/s, auc=0.9318, loss=1.0438]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.81batch/s, auc=0.9318, loss=0.4757]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.73batch/s, auc=0.9318, loss=0.4757]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.73batch/s, auc=0.9315, loss=0.9727]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.70batch/s, auc=0.9315, loss=0.9727]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.70batch/s, auc=0.9314, loss=0.8140]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.75batch/s, auc=0.9314, loss=0.8140]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.75batch/s, auc=0.9314, loss=0.6241]\u001b[A\n",
      "Training Epoch 16/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.81batch/s, auc=0.9314, loss=0.6241]\u001b[A\n",
      "Training Epoch 16/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.81batch/s, auc=0.9315, loss=0.4432]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 263/292 [00:44<00:05,  5.74batch/s, auc=0.9315, loss=0.4432]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 263/292 [00:44<00:05,  5.74batch/s, auc=0.9315, loss=0.4927]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 264/292 [00:44<00:04,  5.83batch/s, auc=0.9315, loss=0.4927]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 264/292 [00:44<00:04,  5.83batch/s, auc=0.9316, loss=0.6167]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 265/292 [00:44<00:04,  5.76batch/s, auc=0.9316, loss=0.6167]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 265/292 [00:44<00:04,  5.76batch/s, auc=0.9315, loss=0.5201]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 266/292 [00:44<00:04,  5.82batch/s, auc=0.9315, loss=0.5201]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 266/292 [00:44<00:04,  5.82batch/s, auc=0.9314, loss=0.5877]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.73batch/s, auc=0.9314, loss=0.5877]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.73batch/s, auc=0.9313, loss=0.5518]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.79batch/s, auc=0.9313, loss=0.5518]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.79batch/s, auc=0.9312, loss=0.5550]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.82batch/s, auc=0.9312, loss=0.5550]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 269/292 [00:45<00:03,  5.82batch/s, auc=0.9313, loss=0.5124]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9313, loss=0.5124]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.63batch/s, auc=0.9314, loss=0.4315]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.61batch/s, auc=0.9314, loss=0.4315]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.61batch/s, auc=0.9316, loss=0.3604]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.60batch/s, auc=0.9316, loss=0.3604]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.60batch/s, auc=0.9317, loss=0.3956]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.59batch/s, auc=0.9317, loss=0.3956]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.59batch/s, auc=0.9319, loss=0.3243]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.68batch/s, auc=0.9319, loss=0.3243]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.68batch/s, auc=0.9319, loss=0.5023]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.46batch/s, auc=0.9319, loss=0.5023]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.46batch/s, auc=0.9321, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.48batch/s, auc=0.9321, loss=0.4187]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.48batch/s, auc=0.9320, loss=0.7823]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.51batch/s, auc=0.9320, loss=0.7823]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.51batch/s, auc=0.9318, loss=0.7309]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.52batch/s, auc=0.9318, loss=0.7309]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.52batch/s, auc=0.9316, loss=0.8733]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.51batch/s, auc=0.9316, loss=0.8733]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.51batch/s, auc=0.9317, loss=0.4210]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.48batch/s, auc=0.9317, loss=0.4210]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.48batch/s, auc=0.9317, loss=0.6983]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9317, loss=0.6983]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9319, loss=0.3965]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9319, loss=0.3965]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.58batch/s, auc=0.9319, loss=0.5679]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.57batch/s, auc=0.9319, loss=0.5679]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.57batch/s, auc=0.9316, loss=0.7788]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9316, loss=0.7788]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.55batch/s, auc=0.9318, loss=0.3413]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.57batch/s, auc=0.9318, loss=0.3413]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.57batch/s, auc=0.9320, loss=0.3532]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9320, loss=0.3532]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9319, loss=0.6930]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.79batch/s, auc=0.9319, loss=0.6930]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.79batch/s, auc=0.9320, loss=0.4011]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9320, loss=0.4011]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9320, loss=0.3675]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.84batch/s, auc=0.9320, loss=0.3675]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.84batch/s, auc=0.9321, loss=0.4036]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.87batch/s, auc=0.9321, loss=0.4036]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.87batch/s, auc=0.9321, loss=0.4907]\u001b[A\n",
      "Training Epoch 16/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.87batch/s, auc=0.9321, loss=0.4907]\u001b[A\n",
      "Training Epoch 16/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.87batch/s, auc=0.9320, loss=0.6265]\u001b[A\n",
      "Training Epoch 16/25: 100%|██████████| 292/292 [00:49<00:00,  5.92batch/s, auc=0.9320, loss=0.6265]\u001b[A\n",
      "Epochs:  64%|██████▍   | 16/25 [14:38<08:14, 54.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25] Train Loss: 0.5592 | Train AUROC: 0.9320 Val Loss: 0.6539 | Val AUROC: 0.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 17/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9636, loss=0.4330]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 1/292 [00:00<04:43,  1.03batch/s, auc=0.9636, loss=0.4330]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 1/292 [00:01<04:43,  1.03batch/s, auc=0.9430, loss=0.8328]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.9430, loss=0.8328]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.9389, loss=0.4647]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 3/292 [00:01<01:40,  2.87batch/s, auc=0.9389, loss=0.4647]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 3/292 [00:01<01:40,  2.87batch/s, auc=0.9437, loss=0.3737]\u001b[A\n",
      "Training Epoch 17/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9437, loss=0.3737]\u001b[A\n",
      "Training Epoch 17/25:   1%|▏         | 4/292 [00:01<01:19,  3.62batch/s, auc=0.9427, loss=0.5799]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9427, loss=0.5799]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 5/292 [00:01<01:06,  4.29batch/s, auc=0.9510, loss=0.3650]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 6/292 [00:01<01:00,  4.74batch/s, auc=0.9510, loss=0.3650]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 6/292 [00:01<01:00,  4.74batch/s, auc=0.9488, loss=0.5001]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 7/292 [00:01<00:55,  5.10batch/s, auc=0.9488, loss=0.5001]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 7/292 [00:02<00:55,  5.10batch/s, auc=0.9525, loss=0.3635]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 8/292 [00:02<00:52,  5.45batch/s, auc=0.9525, loss=0.3635]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 8/292 [00:02<00:52,  5.45batch/s, auc=0.9505, loss=0.5083]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 9/292 [00:02<00:49,  5.71batch/s, auc=0.9505, loss=0.5083]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 9/292 [00:02<00:49,  5.71batch/s, auc=0.9499, loss=0.5458]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 10/292 [00:02<00:47,  5.89batch/s, auc=0.9499, loss=0.5458]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 10/292 [00:02<00:47,  5.89batch/s, auc=0.9503, loss=0.3437]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 11/292 [00:02<00:46,  6.03batch/s, auc=0.9503, loss=0.3437]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 11/292 [00:02<00:46,  6.03batch/s, auc=0.9476, loss=0.7365]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 12/292 [00:02<00:45,  6.13batch/s, auc=0.9476, loss=0.7365]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 12/292 [00:02<00:45,  6.13batch/s, auc=0.9480, loss=0.5559]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 13/292 [00:02<00:45,  6.20batch/s, auc=0.9480, loss=0.5559]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 13/292 [00:03<00:45,  6.20batch/s, auc=0.9418, loss=0.8725]\u001b[A\n",
      "Training Epoch 17/25:   5%|▍         | 14/292 [00:03<00:45,  6.18batch/s, auc=0.9418, loss=0.8725]\u001b[A\n",
      "Training Epoch 17/25:   5%|▍         | 14/292 [00:03<00:45,  6.18batch/s, auc=0.9405, loss=0.4229]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9405, loss=0.4229]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 15/292 [00:03<00:44,  6.23batch/s, auc=0.9394, loss=0.5634]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9394, loss=0.5634]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 16/292 [00:03<00:44,  6.27batch/s, auc=0.9335, loss=1.0298]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 17/292 [00:03<00:43,  6.30batch/s, auc=0.9335, loss=1.0298]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 17/292 [00:03<00:43,  6.30batch/s, auc=0.9337, loss=0.3489]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 18/292 [00:03<00:43,  6.32batch/s, auc=0.9337, loss=0.3489]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 18/292 [00:03<00:43,  6.32batch/s, auc=0.9331, loss=0.5611]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 19/292 [00:03<00:43,  6.30batch/s, auc=0.9331, loss=0.5611]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 19/292 [00:04<00:43,  6.30batch/s, auc=0.9308, loss=0.8677]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9308, loss=0.8677]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9317, loss=0.4875]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 21/292 [00:04<00:43,  6.28batch/s, auc=0.9317, loss=0.4875]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 21/292 [00:04<00:43,  6.28batch/s, auc=0.9330, loss=0.5466]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.9330, loss=0.5466]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 22/292 [00:04<00:42,  6.30batch/s, auc=0.9322, loss=0.5171]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9322, loss=0.5171]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9322, loss=0.6995]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.9322, loss=0.6995]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 24/292 [00:04<00:42,  6.33batch/s, auc=0.9331, loss=0.5019]\u001b[A\n",
      "Training Epoch 17/25:   9%|▊         | 25/292 [00:04<00:42,  6.29batch/s, auc=0.9331, loss=0.5019]\u001b[A\n",
      "Training Epoch 17/25:   9%|▊         | 25/292 [00:04<00:42,  6.29batch/s, auc=0.9316, loss=0.8283]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 26/292 [00:04<00:43,  6.12batch/s, auc=0.9316, loss=0.8283]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 26/292 [00:05<00:43,  6.12batch/s, auc=0.9318, loss=0.5536]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 27/292 [00:05<00:42,  6.19batch/s, auc=0.9318, loss=0.5536]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 27/292 [00:05<00:42,  6.19batch/s, auc=0.9319, loss=0.6359]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 28/292 [00:05<00:42,  6.24batch/s, auc=0.9319, loss=0.6359]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 28/292 [00:05<00:42,  6.24batch/s, auc=0.9316, loss=0.5638]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 29/292 [00:05<00:42,  6.22batch/s, auc=0.9316, loss=0.5638]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 29/292 [00:05<00:42,  6.22batch/s, auc=0.9312, loss=0.6552]\u001b[A\n",
      "Training Epoch 17/25:  10%|█         | 30/292 [00:05<00:41,  6.25batch/s, auc=0.9312, loss=0.6552]\u001b[A\n",
      "Training Epoch 17/25:  10%|█         | 30/292 [00:05<00:41,  6.25batch/s, auc=0.9304, loss=0.5442]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9304, loss=0.5442]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9303, loss=0.5594]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 32/292 [00:05<00:41,  6.30batch/s, auc=0.9303, loss=0.5594]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 32/292 [00:06<00:41,  6.30batch/s, auc=0.9302, loss=0.4427]\u001b[A\n",
      "Training Epoch 17/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9302, loss=0.4427]\u001b[A\n",
      "Training Epoch 17/25:  11%|█▏        | 33/292 [00:06<00:41,  6.31batch/s, auc=0.9299, loss=0.5831]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9299, loss=0.5831]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9316, loss=0.4080]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 35/292 [00:06<00:40,  6.27batch/s, auc=0.9316, loss=0.4080]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 35/292 [00:06<00:40,  6.27batch/s, auc=0.9319, loss=0.4836]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9319, loss=0.4836]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9323, loss=0.5315]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9323, loss=0.5315]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9324, loss=0.4670]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9324, loss=0.4670]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9333, loss=0.4117]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 39/292 [00:07<00:40,  6.32batch/s, auc=0.9333, loss=0.4117]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 39/292 [00:07<00:40,  6.32batch/s, auc=0.9334, loss=0.4287]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.9334, loss=0.4287]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▎        | 40/292 [00:07<00:39,  6.32batch/s, auc=0.9331, loss=0.5808]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9331, loss=0.5808]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9338, loss=0.4120]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9338, loss=0.4120]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9344, loss=0.4491]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9344, loss=0.4491]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9344, loss=0.6494]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9344, loss=0.6494]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 44/292 [00:07<00:39,  6.29batch/s, auc=0.9353, loss=0.5291]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 45/292 [00:07<00:39,  6.30batch/s, auc=0.9353, loss=0.5291]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9357, loss=0.4606]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9357, loss=0.4606]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9357, loss=0.4593]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9357, loss=0.4593]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9363, loss=0.4294]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9363, loss=0.4294]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9368, loss=0.4691]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.9368, loss=0.4691]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 49/292 [00:08<00:38,  6.28batch/s, auc=0.9366, loss=0.4933]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9366, loss=0.4933]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9367, loss=0.4686]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 51/292 [00:08<00:38,  6.28batch/s, auc=0.9367, loss=0.4686]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 51/292 [00:09<00:38,  6.28batch/s, auc=0.9364, loss=0.6127]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 52/292 [00:09<00:38,  6.24batch/s, auc=0.9364, loss=0.6127]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 52/292 [00:09<00:38,  6.24batch/s, auc=0.9363, loss=0.5883]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 53/292 [00:09<00:38,  6.26batch/s, auc=0.9363, loss=0.5883]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 53/292 [00:09<00:38,  6.26batch/s, auc=0.9363, loss=0.5090]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9363, loss=0.5090]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9371, loss=0.3879]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 55/292 [00:09<00:37,  6.26batch/s, auc=0.9371, loss=0.3879]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 55/292 [00:09<00:37,  6.26batch/s, auc=0.9366, loss=0.7751]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 56/292 [00:09<00:37,  6.26batch/s, auc=0.9366, loss=0.7751]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 56/292 [00:09<00:37,  6.26batch/s, auc=0.9359, loss=0.6452]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 57/292 [00:09<00:37,  6.24batch/s, auc=0.9359, loss=0.6452]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 57/292 [00:10<00:37,  6.24batch/s, auc=0.9363, loss=0.4656]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 58/292 [00:10<00:37,  6.26batch/s, auc=0.9363, loss=0.4656]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 58/292 [00:10<00:37,  6.26batch/s, auc=0.9370, loss=0.3487]\u001b[A\n",
      "Training Epoch 17/25:  20%|██        | 59/292 [00:10<00:37,  6.22batch/s, auc=0.9370, loss=0.3487]\u001b[A\n",
      "Training Epoch 17/25:  20%|██        | 59/292 [00:10<00:37,  6.22batch/s, auc=0.9377, loss=0.3807]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 60/292 [00:10<00:37,  6.25batch/s, auc=0.9377, loss=0.3807]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 60/292 [00:10<00:37,  6.25batch/s, auc=0.9378, loss=0.4659]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 61/292 [00:10<00:36,  6.26batch/s, auc=0.9378, loss=0.4659]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 61/292 [00:10<00:36,  6.26batch/s, auc=0.9381, loss=0.4918]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9381, loss=0.4918]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 62/292 [00:10<00:36,  6.27batch/s, auc=0.9374, loss=0.7097]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9374, loss=0.7097]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9370, loss=0.5887]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9370, loss=0.5887]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9367, loss=0.5804]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 65/292 [00:11<00:36,  6.25batch/s, auc=0.9367, loss=0.5804]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 65/292 [00:11<00:36,  6.25batch/s, auc=0.9363, loss=0.5508]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9363, loss=0.5508]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9345, loss=1.3630]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9345, loss=1.3630]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 67/292 [00:11<00:35,  6.27batch/s, auc=0.9342, loss=0.5117]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9342, loss=0.5117]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9344, loss=0.4814]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9344, loss=0.4814]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9342, loss=0.5329]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 70/292 [00:11<00:35,  6.27batch/s, auc=0.9342, loss=0.5329]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9344, loss=0.4144]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9344, loss=0.4144]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9346, loss=0.3923]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▍       | 72/292 [00:12<00:35,  6.25batch/s, auc=0.9346, loss=0.3923]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▍       | 72/292 [00:12<00:35,  6.25batch/s, auc=0.9344, loss=0.5783]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9344, loss=0.5783]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9347, loss=0.3990]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 74/292 [00:12<00:35,  6.17batch/s, auc=0.9347, loss=0.3990]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 74/292 [00:12<00:35,  6.17batch/s, auc=0.9341, loss=0.8360]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 75/292 [00:12<00:34,  6.20batch/s, auc=0.9341, loss=0.8360]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 75/292 [00:12<00:34,  6.20batch/s, auc=0.9340, loss=0.6479]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 76/292 [00:12<00:34,  6.21batch/s, auc=0.9340, loss=0.6479]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 76/292 [00:13<00:34,  6.21batch/s, auc=0.9344, loss=0.5853]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▋       | 77/292 [00:13<00:34,  6.22batch/s, auc=0.9344, loss=0.5853]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▋       | 77/292 [00:13<00:34,  6.22batch/s, auc=0.9352, loss=0.4228]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 78/292 [00:13<00:34,  6.23batch/s, auc=0.9352, loss=0.4228]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 78/292 [00:13<00:34,  6.23batch/s, auc=0.9342, loss=0.8430]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9342, loss=0.8430]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9348, loss=0.3795]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9348, loss=0.3795]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9354, loss=0.3725]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9354, loss=0.3725]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9356, loss=0.4391]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 82/292 [00:13<00:33,  6.22batch/s, auc=0.9356, loss=0.4391]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 82/292 [00:14<00:33,  6.22batch/s, auc=0.9360, loss=0.3954]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9360, loss=0.3954]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9361, loss=0.4048]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9361, loss=0.4048]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 84/292 [00:14<00:33,  6.25batch/s, auc=0.9359, loss=0.5630]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9359, loss=0.5630]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 85/292 [00:14<00:33,  6.26batch/s, auc=0.9362, loss=0.4390]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 86/292 [00:14<00:32,  6.25batch/s, auc=0.9362, loss=0.4390]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 86/292 [00:14<00:32,  6.25batch/s, auc=0.9361, loss=0.5159]\u001b[A\n",
      "Training Epoch 17/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9361, loss=0.5159]\u001b[A\n",
      "Training Epoch 17/25:  30%|██▉       | 87/292 [00:14<00:32,  6.26batch/s, auc=0.9364, loss=0.3920]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 88/292 [00:14<00:32,  6.25batch/s, auc=0.9364, loss=0.3920]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 88/292 [00:15<00:32,  6.25batch/s, auc=0.9369, loss=0.3217]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 89/292 [00:15<00:32,  6.24batch/s, auc=0.9369, loss=0.3217]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 89/292 [00:15<00:32,  6.24batch/s, auc=0.9364, loss=0.8715]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9364, loss=0.8715]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9363, loss=0.4706]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9363, loss=0.4706]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9358, loss=0.7288]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9358, loss=0.7288]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 92/292 [00:15<00:31,  6.26batch/s, auc=0.9356, loss=0.6070]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9356, loss=0.6070]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 93/292 [00:15<00:31,  6.26batch/s, auc=0.9358, loss=0.3808]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 94/292 [00:15<00:31,  6.26batch/s, auc=0.9358, loss=0.3808]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 94/292 [00:15<00:31,  6.26batch/s, auc=0.9358, loss=0.4589]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 95/292 [00:15<00:31,  6.25batch/s, auc=0.9358, loss=0.4589]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 95/292 [00:16<00:31,  6.25batch/s, auc=0.9356, loss=0.7365]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9356, loss=0.7365]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 96/292 [00:16<00:31,  6.25batch/s, auc=0.9352, loss=0.7087]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9352, loss=0.7087]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9352, loss=0.6445]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9352, loss=0.6445]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9351, loss=0.4711]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 99/292 [00:16<00:30,  6.25batch/s, auc=0.9351, loss=0.4711]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 99/292 [00:16<00:30,  6.25batch/s, auc=0.9344, loss=0.9312]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 100/292 [00:16<00:30,  6.25batch/s, auc=0.9344, loss=0.9312]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 100/292 [00:16<00:30,  6.25batch/s, auc=0.9348, loss=0.3189]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 101/292 [00:16<00:31,  6.14batch/s, auc=0.9348, loss=0.3189]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 101/292 [00:17<00:31,  6.14batch/s, auc=0.9350, loss=0.4002]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 102/292 [00:17<00:30,  6.17batch/s, auc=0.9350, loss=0.4002]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 102/292 [00:17<00:30,  6.17batch/s, auc=0.9347, loss=0.7509]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▌      | 103/292 [00:17<00:30,  6.18batch/s, auc=0.9347, loss=0.7509]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▌      | 103/292 [00:17<00:30,  6.18batch/s, auc=0.9346, loss=0.6871]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 104/292 [00:17<00:30,  6.20batch/s, auc=0.9346, loss=0.6871]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 104/292 [00:17<00:30,  6.20batch/s, auc=0.9348, loss=0.5997]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9348, loss=0.5997]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9346, loss=0.5896]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9346, loss=0.5896]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9348, loss=0.4605]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9348, loss=0.4605]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9349, loss=0.4889]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9349, loss=0.4889]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9352, loss=0.4413]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9352, loss=0.4413]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9350, loss=0.5573]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 110/292 [00:18<00:29,  6.22batch/s, auc=0.9350, loss=0.5573]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 110/292 [00:18<00:29,  6.22batch/s, auc=0.9352, loss=0.3768]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 111/292 [00:18<00:29,  6.23batch/s, auc=0.9352, loss=0.3768]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 111/292 [00:18<00:29,  6.23batch/s, auc=0.9351, loss=0.5657]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9351, loss=0.5657]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 112/292 [00:18<00:28,  6.23batch/s, auc=0.9353, loss=0.4740]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▊      | 113/292 [00:18<00:28,  6.22batch/s, auc=0.9353, loss=0.4740]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9353, loss=0.5124]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9353, loss=0.5124]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9349, loss=0.5952]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 115/292 [00:19<00:28,  6.21batch/s, auc=0.9349, loss=0.5952]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 115/292 [00:19<00:28,  6.21batch/s, auc=0.9347, loss=0.6031]\u001b[A\n",
      "Training Epoch 17/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9347, loss=0.6031]\u001b[A\n",
      "Training Epoch 17/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9350, loss=0.4856]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9350, loss=0.4856]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9345, loss=0.8063]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9345, loss=0.8063]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9348, loss=0.4940]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 119/292 [00:19<00:28,  6.17batch/s, auc=0.9348, loss=0.4940]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 119/292 [00:20<00:28,  6.17batch/s, auc=0.9347, loss=0.4856]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9347, loss=0.4856]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9346, loss=0.5184]\u001b[A\n",
      "Training Epoch 17/25:  41%|████▏     | 121/292 [00:20<00:27,  6.17batch/s, auc=0.9346, loss=0.5184]\u001b[A\n",
      "Training Epoch 17/25:  41%|████▏     | 121/292 [00:20<00:27,  6.17batch/s, auc=0.9348, loss=0.4429]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9348, loss=0.4429]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9344, loss=0.9151]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 123/292 [00:20<00:27,  6.16batch/s, auc=0.9344, loss=0.9151]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 123/292 [00:20<00:27,  6.16batch/s, auc=0.9347, loss=0.3699]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 124/292 [00:20<00:27,  6.17batch/s, auc=0.9347, loss=0.3699]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 124/292 [00:20<00:27,  6.17batch/s, auc=0.9350, loss=0.3933]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9350, loss=0.3933]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 125/292 [00:20<00:26,  6.19batch/s, auc=0.9354, loss=0.3675]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 126/292 [00:20<00:26,  6.19batch/s, auc=0.9354, loss=0.3675]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9352, loss=0.6330]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9352, loss=0.6330]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9354, loss=0.4545]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 128/292 [00:21<00:26,  6.20batch/s, auc=0.9354, loss=0.4545]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 128/292 [00:21<00:26,  6.20batch/s, auc=0.9354, loss=0.5949]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 129/292 [00:21<00:26,  6.20batch/s, auc=0.9354, loss=0.5949]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 129/292 [00:21<00:26,  6.20batch/s, auc=0.9352, loss=0.6751]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9352, loss=0.6751]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9348, loss=0.7472]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9348, loss=0.7472]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9348, loss=0.4487]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▌     | 132/292 [00:21<00:25,  6.20batch/s, auc=0.9348, loss=0.4487]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9345, loss=0.7315]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9345, loss=0.7315]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9341, loss=0.7904]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 134/292 [00:22<00:25,  6.14batch/s, auc=0.9341, loss=0.7904]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 134/292 [00:22<00:25,  6.14batch/s, auc=0.9344, loss=0.3681]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 135/292 [00:22<00:25,  6.14batch/s, auc=0.9344, loss=0.3681]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 135/292 [00:22<00:25,  6.14batch/s, auc=0.9340, loss=0.6978]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 136/292 [00:22<00:25,  6.15batch/s, auc=0.9340, loss=0.6978]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 136/292 [00:22<00:25,  6.15batch/s, auc=0.9341, loss=0.4262]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9341, loss=0.4262]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 137/292 [00:22<00:25,  6.14batch/s, auc=0.9339, loss=0.8080]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 138/292 [00:22<00:25,  6.12batch/s, auc=0.9339, loss=0.8080]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 138/292 [00:23<00:25,  6.12batch/s, auc=0.9341, loss=0.5178]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 139/292 [00:23<00:24,  6.14batch/s, auc=0.9341, loss=0.5178]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 139/292 [00:23<00:24,  6.14batch/s, auc=0.9342, loss=0.4735]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 140/292 [00:23<00:24,  6.15batch/s, auc=0.9342, loss=0.4735]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 140/292 [00:23<00:24,  6.15batch/s, auc=0.9341, loss=0.4931]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9341, loss=0.4931]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9341, loss=0.5319]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9341, loss=0.5319]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9339, loss=0.6569]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 143/292 [00:23<00:24,  6.13batch/s, auc=0.9339, loss=0.6569]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 143/292 [00:23<00:24,  6.13batch/s, auc=0.9339, loss=0.5484]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 144/292 [00:23<00:24,  6.12batch/s, auc=0.9339, loss=0.5484]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 144/292 [00:24<00:24,  6.12batch/s, auc=0.9339, loss=0.5855]\u001b[A\n",
      "Training Epoch 17/25:  50%|████▉     | 145/292 [00:24<00:23,  6.13batch/s, auc=0.9339, loss=0.5855]\u001b[A\n",
      "Training Epoch 17/25:  50%|████▉     | 145/292 [00:24<00:23,  6.13batch/s, auc=0.9338, loss=0.7092]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 146/292 [00:24<00:23,  6.12batch/s, auc=0.9338, loss=0.7092]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 146/292 [00:24<00:23,  6.12batch/s, auc=0.9335, loss=0.7308]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 147/292 [00:24<00:23,  6.11batch/s, auc=0.9335, loss=0.7308]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 147/292 [00:24<00:23,  6.11batch/s, auc=0.9337, loss=0.4571]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 148/292 [00:24<00:23,  6.11batch/s, auc=0.9337, loss=0.4571]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 148/292 [00:24<00:23,  6.11batch/s, auc=0.9337, loss=0.5051]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 149/292 [00:24<00:23,  6.12batch/s, auc=0.9337, loss=0.5051]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 149/292 [00:24<00:23,  6.12batch/s, auc=0.9335, loss=0.5858]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.13batch/s, auc=0.9335, loss=0.5858]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.13batch/s, auc=0.9334, loss=0.7183]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9334, loss=0.7183]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.14batch/s, auc=0.9336, loss=0.4033]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9336, loss=0.4033]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9339, loss=0.3707]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9339, loss=0.3707]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9342, loss=0.4029]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9342, loss=0.4029]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.15batch/s, auc=0.9343, loss=0.4644]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9343, loss=0.4644]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9344, loss=0.4064]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9344, loss=0.4064]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.14batch/s, auc=0.9347, loss=0.3843]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9347, loss=0.3843]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9351, loss=0.3247]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9351, loss=0.3247]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9350, loss=0.5044]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9350, loss=0.5044]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9352, loss=0.4699]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.12batch/s, auc=0.9352, loss=0.4699]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.12batch/s, auc=0.9350, loss=0.6612]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9350, loss=0.6612]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9351, loss=0.4847]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9351, loss=0.4847]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.13batch/s, auc=0.9350, loss=0.6632]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9350, loss=0.6632]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9347, loss=0.9805]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.11batch/s, auc=0.9347, loss=0.9805]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.11batch/s, auc=0.9345, loss=0.5635]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9345, loss=0.5635]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.12batch/s, auc=0.9348, loss=0.3805]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9348, loss=0.3805]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9349, loss=0.4894]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9349, loss=0.4894]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9350, loss=0.4258]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9350, loss=0.4258]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9352, loss=0.3949]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 169/292 [00:27<00:20,  6.11batch/s, auc=0.9352, loss=0.3949]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.11batch/s, auc=0.9352, loss=0.5228]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9352, loss=0.5228]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.11batch/s, auc=0.9351, loss=0.6719]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.10batch/s, auc=0.9351, loss=0.6719]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.10batch/s, auc=0.9350, loss=0.8158]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9350, loss=0.8158]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9353, loss=0.3529]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.00batch/s, auc=0.9353, loss=0.3529]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.00batch/s, auc=0.9345, loss=1.5659]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.04batch/s, auc=0.9345, loss=1.5659]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.04batch/s, auc=0.9347, loss=0.3465]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.07batch/s, auc=0.9347, loss=0.3465]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.07batch/s, auc=0.9349, loss=0.4374]\u001b[A\n",
      "Training Epoch 17/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.9349, loss=0.4374]\u001b[A\n",
      "Training Epoch 17/25:  60%|██████    | 176/292 [00:29<00:19,  6.08batch/s, auc=0.9350, loss=0.4234]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 177/292 [00:29<00:18,  6.10batch/s, auc=0.9350, loss=0.4234]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 177/292 [00:29<00:18,  6.10batch/s, auc=0.9350, loss=0.5506]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9350, loss=0.5506]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9347, loss=0.7243]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9347, loss=0.7243]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9350, loss=0.3570]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.10batch/s, auc=0.9350, loss=0.3570]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.10batch/s, auc=0.9350, loss=0.5002]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.12batch/s, auc=0.9350, loss=0.5002]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.12batch/s, auc=0.9349, loss=0.5636]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9349, loss=0.5636]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9349, loss=0.4654]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.12batch/s, auc=0.9349, loss=0.4654]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.12batch/s, auc=0.9349, loss=0.5350]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9349, loss=0.5350]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9347, loss=0.5346]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9347, loss=0.5346]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9347, loss=0.4953]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.12batch/s, auc=0.9347, loss=0.4953]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.12batch/s, auc=0.9344, loss=0.7103]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.11batch/s, auc=0.9344, loss=0.7103]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.11batch/s, auc=0.9342, loss=0.6447]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9342, loss=0.6447]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9342, loss=0.5402]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.11batch/s, auc=0.9342, loss=0.5402]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.11batch/s, auc=0.9343, loss=0.4468]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9343, loss=0.4468]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9345, loss=0.3976]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9345, loss=0.3976]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9346, loss=0.3991]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.07batch/s, auc=0.9346, loss=0.3991]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.07batch/s, auc=0.9345, loss=0.6941]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.03batch/s, auc=0.9345, loss=0.6941]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.03batch/s, auc=0.9347, loss=0.3598]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.04batch/s, auc=0.9347, loss=0.3598]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.04batch/s, auc=0.9344, loss=0.8207]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.93batch/s, auc=0.9344, loss=0.8207]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.93batch/s, auc=0.9342, loss=0.7988]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.82batch/s, auc=0.9342, loss=0.7988]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.82batch/s, auc=0.9342, loss=0.5399]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.78batch/s, auc=0.9342, loss=0.5399]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.78batch/s, auc=0.9342, loss=0.4287]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.75batch/s, auc=0.9342, loss=0.4287]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.75batch/s, auc=0.9343, loss=0.3779]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 199/292 [00:32<00:15,  5.85batch/s, auc=0.9343, loss=0.3779]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.85batch/s, auc=0.9344, loss=0.5667]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.79batch/s, auc=0.9344, loss=0.5667]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.79batch/s, auc=0.9344, loss=0.4671]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.89batch/s, auc=0.9344, loss=0.4671]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.89batch/s, auc=0.9343, loss=0.5644]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.82batch/s, auc=0.9343, loss=0.5644]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.82batch/s, auc=0.9342, loss=0.6669]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.87batch/s, auc=0.9342, loss=0.6669]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.87batch/s, auc=0.9342, loss=0.6204]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.70batch/s, auc=0.9342, loss=0.6204]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.70batch/s, auc=0.9341, loss=0.6415]\u001b[A\n",
      "Training Epoch 17/25:  70%|███████   | 205/292 [00:34<00:15,  5.70batch/s, auc=0.9341, loss=0.6415]\u001b[A\n",
      "Training Epoch 17/25:  70%|███████   | 205/292 [00:34<00:15,  5.70batch/s, auc=0.9337, loss=1.2172]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 206/292 [00:34<00:15,  5.71batch/s, auc=0.9337, loss=1.2172]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 206/292 [00:34<00:15,  5.71batch/s, auc=0.9336, loss=0.7849]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9336, loss=0.7849]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9336, loss=0.5599]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 208/292 [00:34<00:14,  5.64batch/s, auc=0.9336, loss=0.5599]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 208/292 [00:34<00:14,  5.64batch/s, auc=0.9337, loss=0.5034]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.65batch/s, auc=0.9337, loss=0.5034]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.65batch/s, auc=0.9335, loss=0.6438]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.65batch/s, auc=0.9335, loss=0.6438]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.65batch/s, auc=0.9335, loss=0.5668]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.63batch/s, auc=0.9335, loss=0.5668]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.63batch/s, auc=0.9334, loss=0.5864]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.64batch/s, auc=0.9334, loss=0.5864]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.64batch/s, auc=0.9334, loss=0.5688]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 213/292 [00:35<00:14,  5.64batch/s, auc=0.9334, loss=0.5688]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 213/292 [00:35<00:14,  5.64batch/s, auc=0.9334, loss=0.6778]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.75batch/s, auc=0.9334, loss=0.6778]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.75batch/s, auc=0.9332, loss=0.6742]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.74batch/s, auc=0.9332, loss=0.6742]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.74batch/s, auc=0.9331, loss=0.6592]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.70batch/s, auc=0.9331, loss=0.6592]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.70batch/s, auc=0.9328, loss=0.9563]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.80batch/s, auc=0.9328, loss=0.9563]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.80batch/s, auc=0.9328, loss=0.5111]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.87batch/s, auc=0.9328, loss=0.5111]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.87batch/s, auc=0.9327, loss=0.5669]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.84batch/s, auc=0.9327, loss=0.5669]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.84batch/s, auc=0.9327, loss=0.6295]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.75batch/s, auc=0.9327, loss=0.6295]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.75batch/s, auc=0.9326, loss=0.5850]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.74batch/s, auc=0.9326, loss=0.5850]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.74batch/s, auc=0.9328, loss=0.5480]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.72batch/s, auc=0.9328, loss=0.5480]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.72batch/s, auc=0.9327, loss=0.6551]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.70batch/s, auc=0.9327, loss=0.6551]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.70batch/s, auc=0.9326, loss=0.5940]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.73batch/s, auc=0.9326, loss=0.5940]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.73batch/s, auc=0.9328, loss=0.4472]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.70batch/s, auc=0.9328, loss=0.4472]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.70batch/s, auc=0.9328, loss=0.5490]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.78batch/s, auc=0.9328, loss=0.5490]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.78batch/s, auc=0.9328, loss=0.4783]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.71batch/s, auc=0.9328, loss=0.4783]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.71batch/s, auc=0.9328, loss=0.5751]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.68batch/s, auc=0.9328, loss=0.5751]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.68batch/s, auc=0.9327, loss=0.6256]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.67batch/s, auc=0.9327, loss=0.6256]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.67batch/s, auc=0.9327, loss=0.5740]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.61batch/s, auc=0.9327, loss=0.5740]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.61batch/s, auc=0.9326, loss=0.6218]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.62batch/s, auc=0.9326, loss=0.6218]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.62batch/s, auc=0.9326, loss=0.5165]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9326, loss=0.5165]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9325, loss=0.5769]\u001b[A\n",
      "Training Epoch 17/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.62batch/s, auc=0.9325, loss=0.5769]\u001b[A\n",
      "Training Epoch 17/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.62batch/s, auc=0.9326, loss=0.4866]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 234/292 [00:39<00:10,  5.63batch/s, auc=0.9326, loss=0.4866]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 234/292 [00:39<00:10,  5.63batch/s, auc=0.9327, loss=0.3889]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 235/292 [00:39<00:10,  5.62batch/s, auc=0.9327, loss=0.3889]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 235/292 [00:39<00:10,  5.62batch/s, auc=0.9327, loss=0.4997]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9327, loss=0.4997]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9326, loss=0.6438]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9326, loss=0.6438]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 237/292 [00:39<00:09,  5.62batch/s, auc=0.9326, loss=0.3685]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.62batch/s, auc=0.9326, loss=0.3685]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.62batch/s, auc=0.9327, loss=0.5817]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.61batch/s, auc=0.9327, loss=0.5817]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.61batch/s, auc=0.9328, loss=0.4110]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.61batch/s, auc=0.9328, loss=0.4110]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.61batch/s, auc=0.9329, loss=0.4698]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.64batch/s, auc=0.9329, loss=0.4698]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.64batch/s, auc=0.9329, loss=0.5755]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.63batch/s, auc=0.9329, loss=0.5755]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.63batch/s, auc=0.9330, loss=0.5208]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9330, loss=0.5208]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9330, loss=0.4704]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9330, loss=0.4704]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.62batch/s, auc=0.9329, loss=0.5576]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.63batch/s, auc=0.9329, loss=0.5576]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.63batch/s, auc=0.9329, loss=0.6349]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9329, loss=0.6349]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.62batch/s, auc=0.9326, loss=0.7927]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.62batch/s, auc=0.9326, loss=0.7927]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.62batch/s, auc=0.9326, loss=0.6060]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.62batch/s, auc=0.9326, loss=0.6060]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.62batch/s, auc=0.9325, loss=0.5870]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.62batch/s, auc=0.9325, loss=0.5870]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.62batch/s, auc=0.9326, loss=0.4913]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.62batch/s, auc=0.9326, loss=0.4913]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.62batch/s, auc=0.9325, loss=0.6926]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9325, loss=0.6926]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9323, loss=0.8598]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.61batch/s, auc=0.9323, loss=0.8598]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.61batch/s, auc=0.9322, loss=0.7865]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.62batch/s, auc=0.9322, loss=0.7865]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.62batch/s, auc=0.9323, loss=0.4110]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.61batch/s, auc=0.9323, loss=0.4110]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.61batch/s, auc=0.9325, loss=0.3536]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.60batch/s, auc=0.9325, loss=0.3536]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.60batch/s, auc=0.9326, loss=0.4425]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.60batch/s, auc=0.9326, loss=0.4425]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.60batch/s, auc=0.9324, loss=0.7469]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.59batch/s, auc=0.9324, loss=0.7469]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.59batch/s, auc=0.9322, loss=0.9178]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.58batch/s, auc=0.9322, loss=0.9178]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.58batch/s, auc=0.9322, loss=0.5492]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.59batch/s, auc=0.9322, loss=0.5492]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.59batch/s, auc=0.9322, loss=0.5627]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.60batch/s, auc=0.9322, loss=0.5627]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.60batch/s, auc=0.9321, loss=0.6497]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.64batch/s, auc=0.9321, loss=0.6497]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.64batch/s, auc=0.9322, loss=0.4364]\u001b[A\n",
      "Training Epoch 17/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.61batch/s, auc=0.9322, loss=0.4364]\u001b[A\n",
      "Training Epoch 17/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.61batch/s, auc=0.9321, loss=0.5385]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 263/292 [00:44<00:05,  5.60batch/s, auc=0.9321, loss=0.5385]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 263/292 [00:44<00:05,  5.60batch/s, auc=0.9320, loss=0.7997]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 264/292 [00:44<00:05,  5.60batch/s, auc=0.9320, loss=0.7997]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 264/292 [00:44<00:05,  5.60batch/s, auc=0.9320, loss=0.6259]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 265/292 [00:44<00:04,  5.59batch/s, auc=0.9320, loss=0.6259]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 265/292 [00:44<00:04,  5.59batch/s, auc=0.9321, loss=0.5490]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 266/292 [00:44<00:04,  5.58batch/s, auc=0.9321, loss=0.5490]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 266/292 [00:45<00:04,  5.58batch/s, auc=0.9321, loss=0.8067]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.58batch/s, auc=0.9321, loss=0.8067]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.58batch/s, auc=0.9323, loss=0.3132]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.61batch/s, auc=0.9323, loss=0.3132]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.61batch/s, auc=0.9323, loss=0.5539]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.62batch/s, auc=0.9323, loss=0.5539]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.62batch/s, auc=0.9324, loss=0.3431]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9324, loss=0.3431]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9325, loss=0.4045]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9325, loss=0.4045]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9326, loss=0.4325]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.70batch/s, auc=0.9326, loss=0.4325]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.70batch/s, auc=0.9326, loss=0.5801]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.56batch/s, auc=0.9326, loss=0.5801]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.56batch/s, auc=0.9326, loss=0.4672]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.55batch/s, auc=0.9326, loss=0.4672]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.55batch/s, auc=0.9324, loss=0.8542]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9324, loss=0.8542]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.55batch/s, auc=0.9325, loss=0.4755]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.56batch/s, auc=0.9325, loss=0.4755]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.56batch/s, auc=0.9325, loss=0.5279]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.57batch/s, auc=0.9325, loss=0.5279]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.57batch/s, auc=0.9325, loss=0.5757]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.55batch/s, auc=0.9325, loss=0.5757]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.55batch/s, auc=0.9326, loss=0.4314]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.56batch/s, auc=0.9326, loss=0.4314]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.56batch/s, auc=0.9326, loss=0.5046]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9326, loss=0.5046]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.54batch/s, auc=0.9325, loss=0.7071]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.59batch/s, auc=0.9325, loss=0.7071]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.59batch/s, auc=0.9326, loss=0.4600]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.57batch/s, auc=0.9326, loss=0.4600]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.57batch/s, auc=0.9326, loss=0.5293]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.65batch/s, auc=0.9326, loss=0.5293]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.65batch/s, auc=0.9327, loss=0.3591]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.59batch/s, auc=0.9327, loss=0.3591]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.59batch/s, auc=0.9329, loss=0.4076]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9329, loss=0.4076]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9328, loss=0.5428]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9328, loss=0.5428]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9329, loss=0.4283]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9329, loss=0.4283]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9329, loss=0.4612]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9329, loss=0.4612]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9330, loss=0.3605]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9330, loss=0.3605]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9330, loss=0.4421]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9330, loss=0.4421]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9331, loss=0.4320]\u001b[A\n",
      "Training Epoch 17/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.90batch/s, auc=0.9331, loss=0.4320]\u001b[A\n",
      "Training Epoch 17/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.90batch/s, auc=0.9332, loss=0.5955]\u001b[A\n",
      "Training Epoch 17/25: 100%|██████████| 292/292 [00:49<00:00,  5.91batch/s, auc=0.9332, loss=0.5955]\u001b[A\n",
      "Epochs:  68%|██████▊   | 17/25 [15:33<07:19, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25] Train Loss: 0.5556 | Train AUROC: 0.9332 Val Loss: 0.6400 | Val AUROC: 0.9116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 18/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.8667, loss=0.8866]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 1/292 [00:00<04:43,  1.03batch/s, auc=0.8667, loss=0.8866]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 1/292 [00:01<04:43,  1.03batch/s, auc=0.8901, loss=0.7081]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.8901, loss=0.7081]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 2/292 [00:01<02:22,  2.03batch/s, auc=0.9159, loss=0.3176]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 3/292 [00:01<01:38,  2.95batch/s, auc=0.9159, loss=0.3176]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 3/292 [00:01<01:38,  2.95batch/s, auc=0.9246, loss=0.4441]\u001b[A\n",
      "Training Epoch 18/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.9246, loss=0.4441]\u001b[A\n",
      "Training Epoch 18/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.9318, loss=0.4531]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 5/292 [00:01<01:07,  4.27batch/s, auc=0.9318, loss=0.4531]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 5/292 [00:01<01:07,  4.27batch/s, auc=0.9361, loss=0.5307]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 6/292 [00:01<00:59,  4.82batch/s, auc=0.9361, loss=0.5307]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 6/292 [00:01<00:59,  4.82batch/s, auc=0.9364, loss=0.5071]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 7/292 [00:01<00:54,  5.24batch/s, auc=0.9364, loss=0.5071]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 7/292 [00:02<00:54,  5.24batch/s, auc=0.9354, loss=0.6005]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 8/292 [00:02<00:51,  5.56batch/s, auc=0.9354, loss=0.6005]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 8/292 [00:02<00:51,  5.56batch/s, auc=0.9413, loss=0.4288]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 9/292 [00:02<00:48,  5.79batch/s, auc=0.9413, loss=0.4288]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 9/292 [00:02<00:48,  5.79batch/s, auc=0.9383, loss=0.6477]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 10/292 [00:02<00:47,  5.95batch/s, auc=0.9383, loss=0.6477]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 10/292 [00:02<00:47,  5.95batch/s, auc=0.9393, loss=0.4557]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 11/292 [00:02<00:46,  6.07batch/s, auc=0.9393, loss=0.4557]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 11/292 [00:02<00:46,  6.07batch/s, auc=0.9403, loss=0.4962]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 12/292 [00:02<00:45,  6.17batch/s, auc=0.9403, loss=0.4962]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 12/292 [00:02<00:45,  6.17batch/s, auc=0.9412, loss=0.3632]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 13/292 [00:02<00:45,  6.10batch/s, auc=0.9412, loss=0.3632]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 13/292 [00:03<00:45,  6.10batch/s, auc=0.9359, loss=0.8950]\u001b[A\n",
      "Training Epoch 18/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9359, loss=0.8950]\u001b[A\n",
      "Training Epoch 18/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9375, loss=0.4528]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.9375, loss=0.4528]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.9391, loss=0.4522]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 16/292 [00:03<00:45,  6.08batch/s, auc=0.9391, loss=0.4522]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 16/292 [00:03<00:45,  6.08batch/s, auc=0.9395, loss=0.4693]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 17/292 [00:03<00:44,  6.17batch/s, auc=0.9395, loss=0.4693]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 17/292 [00:03<00:44,  6.17batch/s, auc=0.9395, loss=0.4544]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 18/292 [00:03<00:44,  6.19batch/s, auc=0.9395, loss=0.4544]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 18/292 [00:03<00:44,  6.19batch/s, auc=0.9407, loss=0.5054]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 19/292 [00:03<00:43,  6.24batch/s, auc=0.9407, loss=0.5054]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 19/292 [00:04<00:43,  6.24batch/s, auc=0.9402, loss=0.5749]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9402, loss=0.5749]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9382, loss=0.6785]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 21/292 [00:04<00:43,  6.30batch/s, auc=0.9382, loss=0.6785]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 21/292 [00:04<00:43,  6.30batch/s, auc=0.9372, loss=0.5396]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9372, loss=0.5396]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9370, loss=0.4462]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9370, loss=0.4462]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 23/292 [00:04<00:42,  6.33batch/s, auc=0.9364, loss=0.6092]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9364, loss=0.6092]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9382, loss=0.3309]\u001b[A\n",
      "Training Epoch 18/25:   9%|▊         | 25/292 [00:04<00:42,  6.34batch/s, auc=0.9382, loss=0.3309]\u001b[A\n",
      "Training Epoch 18/25:   9%|▊         | 25/292 [00:04<00:42,  6.34batch/s, auc=0.9388, loss=0.5606]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 26/292 [00:04<00:41,  6.34batch/s, auc=0.9388, loss=0.5606]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 26/292 [00:05<00:41,  6.34batch/s, auc=0.9386, loss=0.5525]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9386, loss=0.5525]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 27/292 [00:05<00:41,  6.35batch/s, auc=0.9382, loss=0.4849]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.9382, loss=0.4849]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 28/292 [00:05<00:41,  6.35batch/s, auc=0.9376, loss=0.5450]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9376, loss=0.5450]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9357, loss=0.8398]\u001b[A\n",
      "Training Epoch 18/25:  10%|█         | 30/292 [00:05<00:41,  6.32batch/s, auc=0.9357, loss=0.8398]\u001b[A\n",
      "Training Epoch 18/25:  10%|█         | 30/292 [00:05<00:41,  6.32batch/s, auc=0.9325, loss=0.8866]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 31/292 [00:05<00:41,  6.33batch/s, auc=0.9325, loss=0.8866]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 31/292 [00:05<00:41,  6.33batch/s, auc=0.9333, loss=0.3098]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 32/292 [00:05<00:41,  6.28batch/s, auc=0.9333, loss=0.3098]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 32/292 [00:06<00:41,  6.28batch/s, auc=0.9321, loss=0.6753]\u001b[A\n",
      "Training Epoch 18/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9321, loss=0.6753]\u001b[A\n",
      "Training Epoch 18/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9303, loss=0.7930]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9303, loss=0.7930]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9315, loss=0.4324]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9315, loss=0.4324]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 35/292 [00:06<00:40,  6.32batch/s, auc=0.9316, loss=0.6005]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9316, loss=0.6005]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9319, loss=0.6013]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 37/292 [00:06<00:40,  6.32batch/s, auc=0.9319, loss=0.6013]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 37/292 [00:06<00:40,  6.32batch/s, auc=0.9329, loss=0.3916]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 38/292 [00:06<00:40,  6.33batch/s, auc=0.9329, loss=0.3916]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 38/292 [00:07<00:40,  6.33batch/s, auc=0.9337, loss=0.3935]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 39/292 [00:07<00:40,  6.29batch/s, auc=0.9337, loss=0.3935]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 39/292 [00:07<00:40,  6.29batch/s, auc=0.9340, loss=0.4823]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▎        | 40/292 [00:07<00:40,  6.25batch/s, auc=0.9340, loss=0.4823]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▎        | 40/292 [00:07<00:40,  6.25batch/s, auc=0.9318, loss=0.9250]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9318, loss=0.9250]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9313, loss=0.5898]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9313, loss=0.5898]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 42/292 [00:07<00:39,  6.28batch/s, auc=0.9319, loss=0.5518]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9319, loss=0.5518]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9325, loss=0.4045]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 44/292 [00:07<00:39,  6.30batch/s, auc=0.9325, loss=0.4045]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 44/292 [00:07<00:39,  6.30batch/s, auc=0.9324, loss=0.4925]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 45/292 [00:07<00:39,  6.31batch/s, auc=0.9324, loss=0.4925]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9308, loss=0.7917]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9308, loss=0.7917]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9306, loss=0.5604]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9306, loss=0.5604]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9320, loss=0.3685]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9320, loss=0.3685]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9316, loss=0.5713]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9316, loss=0.5713]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9318, loss=0.4765]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 50/292 [00:08<00:38,  6.27batch/s, auc=0.9318, loss=0.4765]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 50/292 [00:08<00:38,  6.27batch/s, auc=0.9314, loss=0.5544]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 51/292 [00:08<00:38,  6.29batch/s, auc=0.9314, loss=0.5544]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9315, loss=0.4859]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9315, loss=0.4859]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9312, loss=0.5703]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9312, loss=0.5703]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 53/292 [00:09<00:37,  6.30batch/s, auc=0.9317, loss=0.4163]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 54/292 [00:09<00:38,  6.25batch/s, auc=0.9317, loss=0.4163]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 54/292 [00:09<00:38,  6.25batch/s, auc=0.9323, loss=0.3597]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 55/292 [00:09<00:38,  6.16batch/s, auc=0.9323, loss=0.3597]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 55/292 [00:09<00:38,  6.16batch/s, auc=0.9317, loss=0.7793]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 56/292 [00:09<00:38,  6.20batch/s, auc=0.9317, loss=0.7793]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 56/292 [00:09<00:38,  6.20batch/s, auc=0.9318, loss=0.5491]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 57/292 [00:09<00:37,  6.24batch/s, auc=0.9318, loss=0.5491]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 57/292 [00:10<00:37,  6.24batch/s, auc=0.9313, loss=0.6225]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 58/292 [00:10<00:37,  6.23batch/s, auc=0.9313, loss=0.6225]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 58/292 [00:10<00:37,  6.23batch/s, auc=0.9314, loss=0.4230]\u001b[A\n",
      "Training Epoch 18/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9314, loss=0.4230]\u001b[A\n",
      "Training Epoch 18/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9313, loss=0.4569]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 60/292 [00:10<00:36,  6.27batch/s, auc=0.9313, loss=0.4569]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 60/292 [00:10<00:36,  6.27batch/s, auc=0.9311, loss=0.5410]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 61/292 [00:10<00:36,  6.29batch/s, auc=0.9311, loss=0.5410]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 61/292 [00:10<00:36,  6.29batch/s, auc=0.9299, loss=1.0738]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 62/292 [00:10<00:36,  6.30batch/s, auc=0.9299, loss=1.0738]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 62/292 [00:10<00:36,  6.30batch/s, auc=0.9300, loss=0.4061]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9300, loss=0.4061]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9306, loss=0.4969]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 64/292 [00:11<00:36,  6.29batch/s, auc=0.9306, loss=0.4969]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 64/292 [00:11<00:36,  6.29batch/s, auc=0.9306, loss=0.5301]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 65/292 [00:11<00:36,  6.29batch/s, auc=0.9306, loss=0.5301]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 65/292 [00:11<00:36,  6.29batch/s, auc=0.9314, loss=0.3470]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 66/292 [00:11<00:35,  6.28batch/s, auc=0.9314, loss=0.3470]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 66/292 [00:11<00:35,  6.28batch/s, auc=0.9315, loss=0.3960]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9315, loss=0.3960]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9310, loss=0.6753]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9310, loss=0.6753]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9311, loss=0.4482]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▎       | 69/292 [00:11<00:35,  6.26batch/s, auc=0.9311, loss=0.4482]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▎       | 69/292 [00:11<00:35,  6.26batch/s, auc=0.9313, loss=0.3905]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 70/292 [00:11<00:35,  6.26batch/s, auc=0.9313, loss=0.3905]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9313, loss=0.4678]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9313, loss=0.4678]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9311, loss=0.5408]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9311, loss=0.5408]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9313, loss=0.6673]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 73/292 [00:12<00:34,  6.29batch/s, auc=0.9313, loss=0.6673]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 73/292 [00:12<00:34,  6.29batch/s, auc=0.9317, loss=0.4598]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9317, loss=0.4598]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 74/292 [00:12<00:34,  6.29batch/s, auc=0.9317, loss=0.4886]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9317, loss=0.4886]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9316, loss=0.6579]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 76/292 [00:12<00:34,  6.28batch/s, auc=0.9316, loss=0.6579]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 76/292 [00:13<00:34,  6.28batch/s, auc=0.9322, loss=0.4113]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▋       | 77/292 [00:13<00:34,  6.30batch/s, auc=0.9322, loss=0.4113]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▋       | 77/292 [00:13<00:34,  6.30batch/s, auc=0.9320, loss=0.5844]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 78/292 [00:13<00:33,  6.31batch/s, auc=0.9320, loss=0.5844]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 78/292 [00:13<00:33,  6.31batch/s, auc=0.9317, loss=0.6489]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 79/292 [00:13<00:33,  6.32batch/s, auc=0.9317, loss=0.6489]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 79/292 [00:13<00:33,  6.32batch/s, auc=0.9318, loss=0.5887]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 80/292 [00:13<00:33,  6.32batch/s, auc=0.9318, loss=0.5887]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 80/292 [00:13<00:33,  6.32batch/s, auc=0.9324, loss=0.4064]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 81/292 [00:13<00:33,  6.31batch/s, auc=0.9324, loss=0.4064]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 81/292 [00:13<00:33,  6.31batch/s, auc=0.9317, loss=0.7024]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 82/292 [00:13<00:33,  6.26batch/s, auc=0.9317, loss=0.7024]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 82/292 [00:14<00:33,  6.26batch/s, auc=0.9319, loss=0.4856]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 83/292 [00:14<00:33,  6.28batch/s, auc=0.9319, loss=0.4856]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 83/292 [00:14<00:33,  6.28batch/s, auc=0.9325, loss=0.3298]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9325, loss=0.3298]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 84/292 [00:14<00:33,  6.28batch/s, auc=0.9328, loss=0.5231]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 85/292 [00:14<00:32,  6.28batch/s, auc=0.9328, loss=0.5231]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 85/292 [00:14<00:32,  6.28batch/s, auc=0.9329, loss=0.4502]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.9329, loss=0.4502]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 86/292 [00:14<00:32,  6.29batch/s, auc=0.9332, loss=0.4203]\u001b[A\n",
      "Training Epoch 18/25:  30%|██▉       | 87/292 [00:14<00:32,  6.24batch/s, auc=0.9332, loss=0.4203]\u001b[A\n",
      "Training Epoch 18/25:  30%|██▉       | 87/292 [00:14<00:32,  6.24batch/s, auc=0.9335, loss=0.4455]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 88/292 [00:14<00:32,  6.26batch/s, auc=0.9335, loss=0.4455]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 88/292 [00:14<00:32,  6.26batch/s, auc=0.9332, loss=0.5590]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 89/292 [00:14<00:32,  6.26batch/s, auc=0.9332, loss=0.5590]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 89/292 [00:15<00:32,  6.26batch/s, auc=0.9333, loss=0.4619]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 90/292 [00:15<00:32,  6.27batch/s, auc=0.9333, loss=0.4619]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 90/292 [00:15<00:32,  6.27batch/s, auc=0.9330, loss=0.7383]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 91/292 [00:15<00:31,  6.29batch/s, auc=0.9330, loss=0.7383]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 91/292 [00:15<00:31,  6.29batch/s, auc=0.9329, loss=0.6629]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 92/292 [00:15<00:31,  6.29batch/s, auc=0.9329, loss=0.6629]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 92/292 [00:15<00:31,  6.29batch/s, auc=0.9326, loss=0.5050]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 93/292 [00:15<00:31,  6.29batch/s, auc=0.9326, loss=0.5050]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 93/292 [00:15<00:31,  6.29batch/s, auc=0.9323, loss=0.7020]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 94/292 [00:15<00:31,  6.30batch/s, auc=0.9323, loss=0.7020]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 94/292 [00:15<00:31,  6.30batch/s, auc=0.9318, loss=0.8132]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 95/292 [00:15<00:31,  6.29batch/s, auc=0.9318, loss=0.8132]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 95/292 [00:16<00:31,  6.29batch/s, auc=0.9319, loss=0.4748]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9319, loss=0.4748]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9313, loss=0.8310]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9313, loss=0.8310]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9314, loss=0.5118]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9314, loss=0.5118]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9316, loss=0.3598]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 99/292 [00:16<00:31,  6.22batch/s, auc=0.9316, loss=0.3598]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 99/292 [00:16<00:31,  6.22batch/s, auc=0.9316, loss=0.5881]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 100/292 [00:16<00:30,  6.22batch/s, auc=0.9316, loss=0.5881]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 100/292 [00:16<00:30,  6.22batch/s, auc=0.9316, loss=0.4636]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 101/292 [00:16<00:30,  6.24batch/s, auc=0.9316, loss=0.4636]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 101/292 [00:17<00:30,  6.24batch/s, auc=0.9316, loss=0.4606]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 102/292 [00:17<00:30,  6.24batch/s, auc=0.9316, loss=0.4606]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 102/292 [00:17<00:30,  6.24batch/s, auc=0.9319, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▌      | 103/292 [00:17<00:30,  6.25batch/s, auc=0.9319, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▌      | 103/292 [00:17<00:30,  6.25batch/s, auc=0.9312, loss=0.7359]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 104/292 [00:17<00:30,  6.22batch/s, auc=0.9312, loss=0.7359]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 104/292 [00:17<00:30,  6.22batch/s, auc=0.9313, loss=0.4893]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9313, loss=0.4893]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 105/292 [00:17<00:29,  6.24batch/s, auc=0.9313, loss=0.4897]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9313, loss=0.4897]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9320, loss=0.3023]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9320, loss=0.3023]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9324, loss=0.4608]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9324, loss=0.4608]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9319, loss=0.6821]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9319, loss=0.6821]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 109/292 [00:18<00:29,  6.24batch/s, auc=0.9320, loss=0.4492]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 110/292 [00:18<00:29,  6.21batch/s, auc=0.9320, loss=0.4492]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 110/292 [00:18<00:29,  6.21batch/s, auc=0.9323, loss=0.4502]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 111/292 [00:18<00:29,  6.22batch/s, auc=0.9323, loss=0.4502]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 111/292 [00:18<00:29,  6.22batch/s, auc=0.9326, loss=0.4744]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9326, loss=0.4744]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9325, loss=0.4946]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▊      | 113/292 [00:18<00:28,  6.24batch/s, auc=0.9325, loss=0.4946]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▊      | 113/292 [00:18<00:28,  6.24batch/s, auc=0.9330, loss=0.3465]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 114/292 [00:18<00:28,  6.22batch/s, auc=0.9330, loss=0.3465]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9331, loss=0.5346]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9331, loss=0.5346]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9331, loss=0.4445]\u001b[A\n",
      "Training Epoch 18/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9331, loss=0.4445]\u001b[A\n",
      "Training Epoch 18/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9328, loss=0.7256]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 117/292 [00:19<00:28,  6.23batch/s, auc=0.9328, loss=0.7256]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 117/292 [00:19<00:28,  6.23batch/s, auc=0.9323, loss=0.8170]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9323, loss=0.8170]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9322, loss=0.5018]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9322, loss=0.5018]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9322, loss=0.5223]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 120/292 [00:19<00:27,  6.21batch/s, auc=0.9322, loss=0.5223]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9320, loss=0.7825]\u001b[A\n",
      "Training Epoch 18/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9320, loss=0.7825]\u001b[A\n",
      "Training Epoch 18/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9322, loss=0.4693]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9322, loss=0.4693]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9325, loss=0.4252]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 123/292 [00:20<00:27,  6.21batch/s, auc=0.9325, loss=0.4252]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 123/292 [00:20<00:27,  6.21batch/s, auc=0.9326, loss=0.4862]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9326, loss=0.4862]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 124/292 [00:20<00:27,  6.20batch/s, auc=0.9327, loss=0.4900]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9327, loss=0.4900]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9330, loss=0.4329]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 126/292 [00:20<00:26,  6.16batch/s, auc=0.9330, loss=0.4329]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 126/292 [00:21<00:26,  6.16batch/s, auc=0.9327, loss=0.6794]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9327, loss=0.6794]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9331, loss=0.3747]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 128/292 [00:21<00:26,  6.17batch/s, auc=0.9331, loss=0.3747]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 128/292 [00:21<00:26,  6.17batch/s, auc=0.9336, loss=0.3091]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9336, loss=0.3091]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 129/292 [00:21<00:26,  6.19batch/s, auc=0.9337, loss=0.4695]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9337, loss=0.4695]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9337, loss=0.6979]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9337, loss=0.6979]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9335, loss=0.8964]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▌     | 132/292 [00:21<00:25,  6.19batch/s, auc=0.9335, loss=0.8964]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▌     | 132/292 [00:22<00:25,  6.19batch/s, auc=0.9333, loss=0.6793]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 133/292 [00:22<00:25,  6.20batch/s, auc=0.9333, loss=0.6793]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 133/292 [00:22<00:25,  6.20batch/s, auc=0.9330, loss=0.6615]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 134/292 [00:22<00:25,  6.20batch/s, auc=0.9330, loss=0.6615]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 134/292 [00:22<00:25,  6.20batch/s, auc=0.9328, loss=0.7540]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 135/292 [00:22<00:25,  6.20batch/s, auc=0.9328, loss=0.7540]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 135/292 [00:22<00:25,  6.20batch/s, auc=0.9329, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9329, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9328, loss=0.4994]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 137/292 [00:22<00:25,  6.20batch/s, auc=0.9328, loss=0.4994]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 137/292 [00:22<00:25,  6.20batch/s, auc=0.9328, loss=0.4133]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 138/292 [00:22<00:24,  6.20batch/s, auc=0.9328, loss=0.4133]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 138/292 [00:23<00:24,  6.20batch/s, auc=0.9325, loss=0.8925]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 139/292 [00:23<00:24,  6.20batch/s, auc=0.9325, loss=0.8925]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 139/292 [00:23<00:24,  6.20batch/s, auc=0.9323, loss=0.5172]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 140/292 [00:23<00:24,  6.19batch/s, auc=0.9323, loss=0.5172]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 140/292 [00:23<00:24,  6.19batch/s, auc=0.9323, loss=0.5548]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9323, loss=0.5548]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 141/292 [00:23<00:24,  6.19batch/s, auc=0.9323, loss=0.5571]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▊     | 142/292 [00:23<00:24,  6.19batch/s, auc=0.9323, loss=0.5571]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▊     | 142/292 [00:23<00:24,  6.19batch/s, auc=0.9324, loss=0.6711]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9324, loss=0.6711]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9327, loss=0.3854]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 144/292 [00:23<00:23,  6.17batch/s, auc=0.9327, loss=0.3854]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 144/292 [00:23<00:23,  6.17batch/s, auc=0.9326, loss=0.6692]\u001b[A\n",
      "Training Epoch 18/25:  50%|████▉     | 145/292 [00:23<00:23,  6.17batch/s, auc=0.9326, loss=0.6692]\u001b[A\n",
      "Training Epoch 18/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9328, loss=0.4792]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9328, loss=0.4792]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9325, loss=0.7631]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9325, loss=0.7631]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9323, loss=0.7730]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9323, loss=0.7730]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9324, loss=0.4954]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9324, loss=0.4954]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9325, loss=0.5773]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.14batch/s, auc=0.9325, loss=0.5773]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.14batch/s, auc=0.9323, loss=0.5845]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 151/292 [00:24<00:22,  6.15batch/s, auc=0.9323, loss=0.5845]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.15batch/s, auc=0.9326, loss=0.4292]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9326, loss=0.4292]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9326, loss=0.6296]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9326, loss=0.6296]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9329, loss=0.4856]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9329, loss=0.4856]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9329, loss=0.4935]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9329, loss=0.4935]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.14batch/s, auc=0.9328, loss=0.5760]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9328, loss=0.5760]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9329, loss=0.5225]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 157/292 [00:25<00:21,  6.14batch/s, auc=0.9329, loss=0.5225]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.14batch/s, auc=0.9325, loss=0.7654]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.14batch/s, auc=0.9325, loss=0.7654]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.14batch/s, auc=0.9325, loss=0.5921]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.14batch/s, auc=0.9325, loss=0.5921]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.14batch/s, auc=0.9326, loss=0.4622]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9326, loss=0.4622]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.13batch/s, auc=0.9326, loss=0.5829]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9326, loss=0.5829]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9330, loss=0.4075]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9330, loss=0.4075]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9331, loss=0.4296]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 163/292 [00:26<00:21,  6.14batch/s, auc=0.9331, loss=0.4296]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9334, loss=0.3657]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9334, loss=0.3657]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9335, loss=0.4968]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.05batch/s, auc=0.9335, loss=0.4968]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.05batch/s, auc=0.9340, loss=0.2831]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.08batch/s, auc=0.9340, loss=0.2831]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.08batch/s, auc=0.9341, loss=0.4768]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.10batch/s, auc=0.9341, loss=0.4768]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.10batch/s, auc=0.9343, loss=0.4185]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.09batch/s, auc=0.9343, loss=0.4185]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.09batch/s, auc=0.9345, loss=0.4714]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 169/292 [00:27<00:20,  5.98batch/s, auc=0.9345, loss=0.4714]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.98batch/s, auc=0.9345, loss=0.5191]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9345, loss=0.5191]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9345, loss=0.3777]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.05batch/s, auc=0.9345, loss=0.3777]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.05batch/s, auc=0.9347, loss=0.4540]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.84batch/s, auc=0.9347, loss=0.4540]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.84batch/s, auc=0.9350, loss=0.4516]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.93batch/s, auc=0.9350, loss=0.4516]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.93batch/s, auc=0.9348, loss=0.7407]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 174/292 [00:28<00:20,  5.79batch/s, auc=0.9348, loss=0.7407]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 174/292 [00:28<00:20,  5.79batch/s, auc=0.9346, loss=0.9579]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 175/292 [00:28<00:19,  5.88batch/s, auc=0.9346, loss=0.9579]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 175/292 [00:29<00:19,  5.88batch/s, auc=0.9344, loss=0.4805]\u001b[A\n",
      "Training Epoch 18/25:  60%|██████    | 176/292 [00:29<00:20,  5.77batch/s, auc=0.9344, loss=0.4805]\u001b[A\n",
      "Training Epoch 18/25:  60%|██████    | 176/292 [00:29<00:20,  5.77batch/s, auc=0.9338, loss=1.1115]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 177/292 [00:29<00:19,  5.89batch/s, auc=0.9338, loss=1.1115]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 177/292 [00:29<00:19,  5.89batch/s, auc=0.9339, loss=0.4218]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 178/292 [00:29<00:19,  5.85batch/s, auc=0.9339, loss=0.4218]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 178/292 [00:29<00:19,  5.85batch/s, auc=0.9338, loss=0.5504]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.92batch/s, auc=0.9338, loss=0.5504]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.92batch/s, auc=0.9339, loss=0.4938]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.75batch/s, auc=0.9339, loss=0.4938]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.75batch/s, auc=0.9339, loss=0.4062]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 181/292 [00:29<00:19,  5.76batch/s, auc=0.9339, loss=0.4062]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.76batch/s, auc=0.9340, loss=0.4632]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.75batch/s, auc=0.9340, loss=0.4632]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.75batch/s, auc=0.9341, loss=0.3821]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.85batch/s, auc=0.9341, loss=0.3821]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.85batch/s, auc=0.9342, loss=0.4808]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.77batch/s, auc=0.9342, loss=0.4808]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.77batch/s, auc=0.9340, loss=0.6064]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.74batch/s, auc=0.9340, loss=0.6064]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.74batch/s, auc=0.9341, loss=0.4794]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▎   | 186/292 [00:30<00:18,  5.73batch/s, auc=0.9341, loss=0.4794]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▎   | 186/292 [00:31<00:18,  5.73batch/s, auc=0.9344, loss=0.2936]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.70batch/s, auc=0.9344, loss=0.2936]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.70batch/s, auc=0.9342, loss=0.7027]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.69batch/s, auc=0.9342, loss=0.7027]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.69batch/s, auc=0.9344, loss=0.3840]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▍   | 189/292 [00:31<00:18,  5.70batch/s, auc=0.9344, loss=0.3840]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▍   | 189/292 [00:31<00:18,  5.70batch/s, auc=0.9343, loss=0.6516]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.71batch/s, auc=0.9343, loss=0.6516]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.71batch/s, auc=0.9345, loss=0.4473]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.71batch/s, auc=0.9345, loss=0.4473]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.71batch/s, auc=0.9342, loss=0.7883]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.70batch/s, auc=0.9342, loss=0.7883]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.70batch/s, auc=0.9344, loss=0.3178]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.72batch/s, auc=0.9344, loss=0.3178]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.72batch/s, auc=0.9345, loss=0.5345]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.66batch/s, auc=0.9345, loss=0.5345]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.66batch/s, auc=0.9345, loss=0.5152]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 195/292 [00:32<00:17,  5.67batch/s, auc=0.9345, loss=0.5152]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 195/292 [00:32<00:17,  5.67batch/s, auc=0.9345, loss=0.5292]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.68batch/s, auc=0.9345, loss=0.5292]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.68batch/s, auc=0.9346, loss=0.4814]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.71batch/s, auc=0.9346, loss=0.4814]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.71batch/s, auc=0.9349, loss=0.3816]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.70batch/s, auc=0.9349, loss=0.3816]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.70batch/s, auc=0.9348, loss=0.5097]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.69batch/s, auc=0.9348, loss=0.5097]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.69batch/s, auc=0.9348, loss=0.5708]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.69batch/s, auc=0.9348, loss=0.5708]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.69batch/s, auc=0.9343, loss=0.9144]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.71batch/s, auc=0.9343, loss=0.9144]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.71batch/s, auc=0.9340, loss=1.1652]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.69batch/s, auc=0.9340, loss=1.1652]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.69batch/s, auc=0.9344, loss=0.2888]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.68batch/s, auc=0.9344, loss=0.2888]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.68batch/s, auc=0.9342, loss=0.6299]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9342, loss=0.6299]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9342, loss=0.5061]\u001b[A\n",
      "Training Epoch 18/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9342, loss=0.5061]\u001b[A\n",
      "Training Epoch 18/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9341, loss=0.6639]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 206/292 [00:34<00:15,  5.68batch/s, auc=0.9341, loss=0.6639]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 206/292 [00:34<00:15,  5.68batch/s, auc=0.9341, loss=0.5646]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9341, loss=0.5646]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9337, loss=1.0402]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9337, loss=1.0402]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9337, loss=0.5053]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.67batch/s, auc=0.9337, loss=0.5053]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 209/292 [00:35<00:14,  5.67batch/s, auc=0.9336, loss=0.5598]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.79batch/s, auc=0.9336, loss=0.5598]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.79batch/s, auc=0.9340, loss=0.2934]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.87batch/s, auc=0.9340, loss=0.2934]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.87batch/s, auc=0.9342, loss=0.3498]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.80batch/s, auc=0.9342, loss=0.3498]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.80batch/s, auc=0.9342, loss=0.6677]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.78batch/s, auc=0.9342, loss=0.6677]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.78batch/s, auc=0.9343, loss=0.4573]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.75batch/s, auc=0.9343, loss=0.4573]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.75batch/s, auc=0.9341, loss=0.6787]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.73batch/s, auc=0.9341, loss=0.6787]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.73batch/s, auc=0.9342, loss=0.4001]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.71batch/s, auc=0.9342, loss=0.4001]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.71batch/s, auc=0.9344, loss=0.4569]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.76batch/s, auc=0.9344, loss=0.4569]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.76batch/s, auc=0.9347, loss=0.3771]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.73batch/s, auc=0.9347, loss=0.3771]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.73batch/s, auc=0.9346, loss=0.5645]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.71batch/s, auc=0.9346, loss=0.5645]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.71batch/s, auc=0.9346, loss=0.5074]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.70batch/s, auc=0.9346, loss=0.5074]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.70batch/s, auc=0.9348, loss=0.5507]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.70batch/s, auc=0.9348, loss=0.5507]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.70batch/s, auc=0.9349, loss=0.4256]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.69batch/s, auc=0.9349, loss=0.4256]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.69batch/s, auc=0.9349, loss=0.5835]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.68batch/s, auc=0.9349, loss=0.5835]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.68batch/s, auc=0.9351, loss=0.5295]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.79batch/s, auc=0.9351, loss=0.5295]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.79batch/s, auc=0.9350, loss=0.5308]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.74batch/s, auc=0.9350, loss=0.5308]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.74batch/s, auc=0.9347, loss=0.9615]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.81batch/s, auc=0.9347, loss=0.9615]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.81batch/s, auc=0.9348, loss=0.4162]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.63batch/s, auc=0.9348, loss=0.4162]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.63batch/s, auc=0.9347, loss=0.6067]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.63batch/s, auc=0.9347, loss=0.6067]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.63batch/s, auc=0.9346, loss=0.6153]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.75batch/s, auc=0.9346, loss=0.6153]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.75batch/s, auc=0.9345, loss=0.5458]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.84batch/s, auc=0.9345, loss=0.5458]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.84batch/s, auc=0.9346, loss=0.4450]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.89batch/s, auc=0.9346, loss=0.4450]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.89batch/s, auc=0.9345, loss=0.5354]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.92batch/s, auc=0.9345, loss=0.5354]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.92batch/s, auc=0.9344, loss=0.5641]\u001b[A\n",
      "Training Epoch 18/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.75batch/s, auc=0.9344, loss=0.5641]\u001b[A\n",
      "Training Epoch 18/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.75batch/s, auc=0.9343, loss=0.5887]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 234/292 [00:39<00:09,  5.83batch/s, auc=0.9343, loss=0.5887]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 234/292 [00:39<00:09,  5.83batch/s, auc=0.9339, loss=0.9506]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 235/292 [00:39<00:10,  5.68batch/s, auc=0.9339, loss=0.9506]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 235/292 [00:39<00:10,  5.68batch/s, auc=0.9339, loss=0.4437]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 236/292 [00:39<00:09,  5.68batch/s, auc=0.9339, loss=0.4437]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 236/292 [00:39<00:09,  5.68batch/s, auc=0.9340, loss=0.5536]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 237/292 [00:39<00:09,  5.78batch/s, auc=0.9340, loss=0.5536]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 237/292 [00:39<00:09,  5.78batch/s, auc=0.9339, loss=0.7367]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.87batch/s, auc=0.9339, loss=0.7367]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.87batch/s, auc=0.9339, loss=0.4525]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 239/292 [00:40<00:08,  5.92batch/s, auc=0.9339, loss=0.4525]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 239/292 [00:40<00:08,  5.92batch/s, auc=0.9339, loss=0.6621]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.92batch/s, auc=0.9339, loss=0.6621]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.92batch/s, auc=0.9338, loss=0.5620]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.83batch/s, auc=0.9338, loss=0.5620]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.83batch/s, auc=0.9336, loss=0.9356]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.76batch/s, auc=0.9336, loss=0.9356]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.76batch/s, auc=0.9333, loss=0.8675]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.71batch/s, auc=0.9333, loss=0.8675]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.71batch/s, auc=0.9332, loss=0.6680]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.68batch/s, auc=0.9332, loss=0.6680]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.68batch/s, auc=0.9333, loss=0.4327]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.69batch/s, auc=0.9333, loss=0.4327]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.69batch/s, auc=0.9334, loss=0.4826]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.64batch/s, auc=0.9334, loss=0.4826]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.64batch/s, auc=0.9332, loss=0.7963]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9332, loss=0.7963]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.65batch/s, auc=0.9331, loss=0.7102]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9331, loss=0.7102]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9330, loss=0.5222]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.68batch/s, auc=0.9330, loss=0.5222]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.68batch/s, auc=0.9332, loss=0.4326]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.67batch/s, auc=0.9332, loss=0.4326]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.67batch/s, auc=0.9332, loss=0.5986]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.66batch/s, auc=0.9332, loss=0.5986]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.66batch/s, auc=0.9333, loss=0.4444]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.65batch/s, auc=0.9333, loss=0.4444]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.65batch/s, auc=0.9331, loss=0.7062]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9331, loss=0.7062]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.65batch/s, auc=0.9330, loss=0.6319]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9330, loss=0.6319]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.63batch/s, auc=0.9330, loss=0.4653]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.62batch/s, auc=0.9330, loss=0.4653]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.62batch/s, auc=0.9332, loss=0.3325]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.59batch/s, auc=0.9332, loss=0.3325]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.59batch/s, auc=0.9332, loss=0.5442]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.66batch/s, auc=0.9332, loss=0.5442]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.66batch/s, auc=0.9332, loss=0.4558]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.75batch/s, auc=0.9332, loss=0.4558]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.75batch/s, auc=0.9331, loss=0.4563]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9331, loss=0.4563]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9333, loss=0.4321]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9333, loss=0.4321]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9334, loss=0.4548]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.73batch/s, auc=0.9334, loss=0.4548]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.73batch/s, auc=0.9332, loss=0.6494]\u001b[A\n",
      "Training Epoch 18/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.69batch/s, auc=0.9332, loss=0.6494]\u001b[A\n",
      "Training Epoch 18/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.69batch/s, auc=0.9333, loss=0.5575]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 263/292 [00:44<00:05,  5.64batch/s, auc=0.9333, loss=0.5575]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 263/292 [00:44<00:05,  5.64batch/s, auc=0.9332, loss=0.5266]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 264/292 [00:44<00:04,  5.62batch/s, auc=0.9332, loss=0.5266]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 264/292 [00:44<00:04,  5.62batch/s, auc=0.9334, loss=0.4028]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 265/292 [00:44<00:04,  5.62batch/s, auc=0.9334, loss=0.4028]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 265/292 [00:44<00:04,  5.62batch/s, auc=0.9334, loss=0.5663]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 266/292 [00:44<00:04,  5.61batch/s, auc=0.9334, loss=0.5663]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 266/292 [00:45<00:04,  5.61batch/s, auc=0.9333, loss=0.5717]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.59batch/s, auc=0.9333, loss=0.5717]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.59batch/s, auc=0.9332, loss=0.5853]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9332, loss=0.5853]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.60batch/s, auc=0.9332, loss=0.5989]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9332, loss=0.5989]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9332, loss=0.4840]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.66batch/s, auc=0.9332, loss=0.4840]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.66batch/s, auc=0.9333, loss=0.5612]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9333, loss=0.5612]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9333, loss=0.6812]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.63batch/s, auc=0.9333, loss=0.6812]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.63batch/s, auc=0.9334, loss=0.4490]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.64batch/s, auc=0.9334, loss=0.4490]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.64batch/s, auc=0.9334, loss=0.4508]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.76batch/s, auc=0.9334, loss=0.4508]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.76batch/s, auc=0.9331, loss=0.7000]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.70batch/s, auc=0.9331, loss=0.7000]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.70batch/s, auc=0.9334, loss=0.2872]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.76batch/s, auc=0.9334, loss=0.2872]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.76batch/s, auc=0.9334, loss=0.4767]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.69batch/s, auc=0.9334, loss=0.4767]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.69batch/s, auc=0.9337, loss=0.2969]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.77batch/s, auc=0.9337, loss=0.2969]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.77batch/s, auc=0.9335, loss=0.9082]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.61batch/s, auc=0.9335, loss=0.9082]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.61batch/s, auc=0.9336, loss=0.4385]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.71batch/s, auc=0.9336, loss=0.4385]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.71batch/s, auc=0.9335, loss=0.5479]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.68batch/s, auc=0.9335, loss=0.5479]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.68batch/s, auc=0.9335, loss=0.5094]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.77batch/s, auc=0.9335, loss=0.5094]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.77batch/s, auc=0.9335, loss=0.6227]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.70batch/s, auc=0.9335, loss=0.6227]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.70batch/s, auc=0.9334, loss=0.4761]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.77batch/s, auc=0.9334, loss=0.4761]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.77batch/s, auc=0.9335, loss=0.4441]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9335, loss=0.4441]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9335, loss=0.5558]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9335, loss=0.5558]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.70batch/s, auc=0.9336, loss=0.5426]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9336, loss=0.5426]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9334, loss=0.7452]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9334, loss=0.7452]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9335, loss=0.4732]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9335, loss=0.4732]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9331, loss=1.2792]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9331, loss=1.2792]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9331, loss=0.6066]\u001b[A\n",
      "Training Epoch 18/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.88batch/s, auc=0.9331, loss=0.6066]\u001b[A\n",
      "Training Epoch 18/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.88batch/s, auc=0.9331, loss=0.5529]\u001b[A\n",
      "Training Epoch 18/25: 100%|██████████| 292/292 [00:49<00:00,  5.91batch/s, auc=0.9331, loss=0.5529]\u001b[A\n",
      "Epochs:  72%|███████▏  | 18/25 [16:28<06:24, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25] Train Loss: 0.5515 | Train AUROC: 0.9331 Val Loss: 0.6442 | Val AUROC: 0.9101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 19/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9198, loss=0.5656]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 1/292 [00:00<04:50,  1.00batch/s, auc=0.9198, loss=0.5656]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 1/292 [00:01<04:50,  1.00batch/s, auc=0.9434, loss=0.3580]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 2/292 [00:01<02:25,  1.99batch/s, auc=0.9434, loss=0.3580]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 2/292 [00:01<02:25,  1.99batch/s, auc=0.9581, loss=0.3460]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 3/292 [00:01<01:39,  2.90batch/s, auc=0.9581, loss=0.3460]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 3/292 [00:01<01:39,  2.90batch/s, auc=0.9541, loss=0.5059]\u001b[A\n",
      "Training Epoch 19/25:   1%|▏         | 4/292 [00:01<01:20,  3.56batch/s, auc=0.9541, loss=0.5059]\u001b[A\n",
      "Training Epoch 19/25:   1%|▏         | 4/292 [00:01<01:20,  3.56batch/s, auc=0.9399, loss=0.7764]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 5/292 [00:01<01:07,  4.23batch/s, auc=0.9399, loss=0.7764]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 5/292 [00:01<01:07,  4.23batch/s, auc=0.9355, loss=0.7032]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 6/292 [00:01<01:00,  4.76batch/s, auc=0.9355, loss=0.7032]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 6/292 [00:01<01:00,  4.76batch/s, auc=0.9404, loss=0.4869]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 7/292 [00:01<00:57,  5.00batch/s, auc=0.9404, loss=0.4869]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 7/292 [00:02<00:57,  5.00batch/s, auc=0.9366, loss=0.7764]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 8/292 [00:02<00:52,  5.37batch/s, auc=0.9366, loss=0.7764]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 8/292 [00:02<00:52,  5.37batch/s, auc=0.9415, loss=0.4037]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 9/292 [00:02<00:50,  5.64batch/s, auc=0.9415, loss=0.4037]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 9/292 [00:02<00:50,  5.64batch/s, auc=0.9471, loss=0.2728]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9471, loss=0.2728]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9420, loss=0.6086]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 11/292 [00:02<00:47,  5.89batch/s, auc=0.9420, loss=0.6086]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 11/292 [00:02<00:47,  5.89batch/s, auc=0.9384, loss=1.0285]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 12/292 [00:02<00:46,  6.03batch/s, auc=0.9384, loss=1.0285]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 12/292 [00:02<00:46,  6.03batch/s, auc=0.9364, loss=0.4684]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 13/292 [00:02<00:45,  6.13batch/s, auc=0.9364, loss=0.4684]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 13/292 [00:03<00:45,  6.13batch/s, auc=0.9382, loss=0.4069]\u001b[A\n",
      "Training Epoch 19/25:   5%|▍         | 14/292 [00:03<00:45,  6.16batch/s, auc=0.9382, loss=0.4069]\u001b[A\n",
      "Training Epoch 19/25:   5%|▍         | 14/292 [00:03<00:45,  6.16batch/s, auc=0.9398, loss=0.4430]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9398, loss=0.4430]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9405, loss=0.5247]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 16/292 [00:03<00:44,  6.23batch/s, auc=0.9405, loss=0.5247]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 16/292 [00:03<00:44,  6.23batch/s, auc=0.9416, loss=0.6359]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9416, loss=0.6359]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9421, loss=0.4449]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9421, loss=0.4449]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9408, loss=0.7660]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 19/292 [00:03<00:43,  6.26batch/s, auc=0.9408, loss=0.7660]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 19/292 [00:04<00:43,  6.26batch/s, auc=0.9412, loss=0.5916]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 20/292 [00:04<00:43,  6.23batch/s, auc=0.9412, loss=0.5916]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 20/292 [00:04<00:43,  6.23batch/s, auc=0.9407, loss=0.5686]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9407, loss=0.5686]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 21/292 [00:04<00:43,  6.27batch/s, auc=0.9410, loss=0.4536]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 22/292 [00:04<00:42,  6.28batch/s, auc=0.9410, loss=0.4536]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 22/292 [00:04<00:42,  6.28batch/s, auc=0.9415, loss=0.4289]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 23/292 [00:04<00:42,  6.29batch/s, auc=0.9415, loss=0.4289]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 23/292 [00:04<00:42,  6.29batch/s, auc=0.9411, loss=0.6022]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9411, loss=0.6022]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9399, loss=0.6650]\u001b[A\n",
      "Training Epoch 19/25:   9%|▊         | 25/292 [00:04<00:42,  6.30batch/s, auc=0.9399, loss=0.6650]\u001b[A\n",
      "Training Epoch 19/25:   9%|▊         | 25/292 [00:05<00:42,  6.30batch/s, auc=0.9398, loss=0.5162]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 26/292 [00:05<00:42,  6.32batch/s, auc=0.9398, loss=0.5162]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 26/292 [00:05<00:42,  6.32batch/s, auc=0.9414, loss=0.3671]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.9414, loss=0.3671]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 27/292 [00:05<00:41,  6.33batch/s, auc=0.9397, loss=0.7200]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 28/292 [00:05<00:42,  6.26batch/s, auc=0.9397, loss=0.7200]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 28/292 [00:05<00:42,  6.26batch/s, auc=0.9396, loss=0.5871]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 29/292 [00:05<00:41,  6.29batch/s, auc=0.9396, loss=0.5871]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 29/292 [00:05<00:41,  6.29batch/s, auc=0.9389, loss=0.6698]\u001b[A\n",
      "Training Epoch 19/25:  10%|█         | 30/292 [00:05<00:41,  6.31batch/s, auc=0.9389, loss=0.6698]\u001b[A\n",
      "Training Epoch 19/25:  10%|█         | 30/292 [00:05<00:41,  6.31batch/s, auc=0.9391, loss=0.5137]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9391, loss=0.5137]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9393, loss=0.4846]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 32/292 [00:05<00:41,  6.31batch/s, auc=0.9393, loss=0.4846]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 32/292 [00:06<00:41,  6.31batch/s, auc=0.9385, loss=0.5601]\u001b[A\n",
      "Training Epoch 19/25:  11%|█▏        | 33/292 [00:06<00:41,  6.25batch/s, auc=0.9385, loss=0.5601]\u001b[A\n",
      "Training Epoch 19/25:  11%|█▏        | 33/292 [00:06<00:41,  6.25batch/s, auc=0.9387, loss=0.4374]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 34/292 [00:06<00:41,  6.28batch/s, auc=0.9387, loss=0.4374]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 34/292 [00:06<00:41,  6.28batch/s, auc=0.9379, loss=0.8162]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9379, loss=0.8162]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 35/292 [00:06<00:40,  6.30batch/s, auc=0.9376, loss=0.6384]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9376, loss=0.6384]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 36/292 [00:06<00:40,  6.31batch/s, auc=0.9381, loss=0.4428]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9381, loss=0.4428]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9386, loss=0.4481]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9386, loss=0.4481]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9386, loss=0.5785]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9386, loss=0.5785]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9381, loss=0.7157]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▎        | 40/292 [00:07<00:39,  6.31batch/s, auc=0.9381, loss=0.7157]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▎        | 40/292 [00:07<00:39,  6.31batch/s, auc=0.9385, loss=0.4418]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 41/292 [00:07<00:40,  6.22batch/s, auc=0.9385, loss=0.4418]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 41/292 [00:07<00:40,  6.22batch/s, auc=0.9378, loss=0.6711]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9378, loss=0.6711]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9378, loss=0.5347]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▍        | 43/292 [00:07<00:39,  6.24batch/s, auc=0.9378, loss=0.5347]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▍        | 43/292 [00:07<00:39,  6.24batch/s, auc=0.9370, loss=0.9015]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 44/292 [00:07<00:39,  6.26batch/s, auc=0.9370, loss=0.9015]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 44/292 [00:08<00:39,  6.26batch/s, auc=0.9365, loss=0.7150]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9365, loss=0.7150]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9372, loss=0.3973]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9372, loss=0.3973]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9371, loss=0.5253]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9371, loss=0.5253]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 47/292 [00:08<00:38,  6.29batch/s, auc=0.9380, loss=0.3757]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▋        | 48/292 [00:08<00:38,  6.30batch/s, auc=0.9380, loss=0.3757]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▋        | 48/292 [00:08<00:38,  6.30batch/s, auc=0.9391, loss=0.3064]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9391, loss=0.3064]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 49/292 [00:08<00:38,  6.31batch/s, auc=0.9386, loss=0.5711]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 50/292 [00:08<00:38,  6.31batch/s, auc=0.9386, loss=0.5711]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 50/292 [00:08<00:38,  6.31batch/s, auc=0.9400, loss=0.2840]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 51/292 [00:08<00:38,  6.30batch/s, auc=0.9400, loss=0.2840]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 51/292 [00:09<00:38,  6.30batch/s, auc=0.9398, loss=0.5517]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9398, loss=0.5517]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9390, loss=0.6833]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 53/292 [00:09<00:38,  6.27batch/s, auc=0.9390, loss=0.6833]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 53/292 [00:09<00:38,  6.27batch/s, auc=0.9394, loss=0.4665]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9394, loss=0.4665]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9394, loss=0.4571]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9394, loss=0.4571]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 55/292 [00:09<00:37,  6.27batch/s, auc=0.9403, loss=0.3161]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 56/292 [00:09<00:37,  6.27batch/s, auc=0.9403, loss=0.3161]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 56/292 [00:09<00:37,  6.27batch/s, auc=0.9405, loss=0.5163]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 57/292 [00:09<00:37,  6.27batch/s, auc=0.9405, loss=0.5163]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 57/292 [00:10<00:37,  6.27batch/s, auc=0.9404, loss=0.4621]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9404, loss=0.4621]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9399, loss=0.8690]\u001b[A\n",
      "Training Epoch 19/25:  20%|██        | 59/292 [00:10<00:37,  6.27batch/s, auc=0.9399, loss=0.8690]\u001b[A\n",
      "Training Epoch 19/25:  20%|██        | 59/292 [00:10<00:37,  6.27batch/s, auc=0.9397, loss=0.6213]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9397, loss=0.6213]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9391, loss=0.6786]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9391, loss=0.6786]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9397, loss=0.3335]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9397, loss=0.3335]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9397, loss=0.5282]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 63/292 [00:10<00:36,  6.27batch/s, auc=0.9397, loss=0.5282]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 63/292 [00:11<00:36,  6.27batch/s, auc=0.9407, loss=0.3095]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 64/292 [00:11<00:36,  6.27batch/s, auc=0.9407, loss=0.3095]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 64/292 [00:11<00:36,  6.27batch/s, auc=0.9406, loss=0.5727]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9406, loss=0.5727]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9398, loss=0.7449]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9398, loss=0.7449]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 66/292 [00:11<00:36,  6.28batch/s, auc=0.9393, loss=0.6796]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9393, loss=0.6796]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9394, loss=0.4627]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9394, loss=0.4627]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9401, loss=0.3639]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▎       | 69/292 [00:11<00:35,  6.28batch/s, auc=0.9401, loss=0.3639]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▎       | 69/292 [00:12<00:35,  6.28batch/s, auc=0.9393, loss=0.8337]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9393, loss=0.8337]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9386, loss=0.6437]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9386, loss=0.6437]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9381, loss=0.7192]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9381, loss=0.7192]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9379, loss=0.5613]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9379, loss=0.5613]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9377, loss=0.5614]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 74/292 [00:12<00:34,  6.26batch/s, auc=0.9377, loss=0.5614]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 74/292 [00:12<00:34,  6.26batch/s, auc=0.9378, loss=0.5918]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 75/292 [00:12<00:34,  6.26batch/s, auc=0.9378, loss=0.5918]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 75/292 [00:12<00:34,  6.26batch/s, auc=0.9379, loss=0.4816]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 76/292 [00:12<00:34,  6.25batch/s, auc=0.9379, loss=0.4816]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 76/292 [00:13<00:34,  6.25batch/s, auc=0.9374, loss=0.5922]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9374, loss=0.5922]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9382, loss=0.4053]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9382, loss=0.4053]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9381, loss=0.4556]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 79/292 [00:13<00:34,  6.24batch/s, auc=0.9381, loss=0.4556]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 79/292 [00:13<00:34,  6.24batch/s, auc=0.9387, loss=0.3062]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9387, loss=0.3062]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 80/292 [00:13<00:33,  6.24batch/s, auc=0.9387, loss=0.5748]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9387, loss=0.5748]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9387, loss=0.5286]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 82/292 [00:13<00:33,  6.25batch/s, auc=0.9387, loss=0.5286]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 82/292 [00:14<00:33,  6.25batch/s, auc=0.9388, loss=0.4454]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9388, loss=0.4454]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9393, loss=0.3512]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9393, loss=0.3512]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9397, loss=0.3775]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9397, loss=0.3775]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9401, loss=0.4087]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 86/292 [00:14<00:33,  6.18batch/s, auc=0.9401, loss=0.4087]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 86/292 [00:14<00:33,  6.18batch/s, auc=0.9405, loss=0.3669]\u001b[A\n",
      "Training Epoch 19/25:  30%|██▉       | 87/292 [00:14<00:33,  6.19batch/s, auc=0.9405, loss=0.3669]\u001b[A\n",
      "Training Epoch 19/25:  30%|██▉       | 87/292 [00:14<00:33,  6.19batch/s, auc=0.9409, loss=0.3502]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 88/292 [00:14<00:32,  6.20batch/s, auc=0.9409, loss=0.3502]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 88/292 [00:15<00:32,  6.20batch/s, auc=0.9406, loss=0.6528]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 89/292 [00:15<00:32,  6.21batch/s, auc=0.9406, loss=0.6528]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 89/292 [00:15<00:32,  6.21batch/s, auc=0.9412, loss=0.4007]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 90/292 [00:15<00:32,  6.20batch/s, auc=0.9412, loss=0.4007]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 90/292 [00:15<00:32,  6.20batch/s, auc=0.9409, loss=0.6506]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 91/292 [00:15<00:32,  6.21batch/s, auc=0.9409, loss=0.6506]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 91/292 [00:15<00:32,  6.21batch/s, auc=0.9406, loss=0.5305]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 92/292 [00:15<00:32,  6.21batch/s, auc=0.9406, loss=0.5305]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 92/292 [00:15<00:32,  6.21batch/s, auc=0.9409, loss=0.3907]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 93/292 [00:15<00:31,  6.23batch/s, auc=0.9409, loss=0.3907]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 93/292 [00:15<00:31,  6.23batch/s, auc=0.9413, loss=0.4620]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 94/292 [00:15<00:31,  6.23batch/s, auc=0.9413, loss=0.4620]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 94/292 [00:16<00:31,  6.23batch/s, auc=0.9414, loss=0.4772]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 95/292 [00:16<00:31,  6.23batch/s, auc=0.9414, loss=0.4772]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 95/292 [00:16<00:31,  6.23batch/s, auc=0.9406, loss=0.8932]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9406, loss=0.8932]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9411, loss=0.3818]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9411, loss=0.3818]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9408, loss=0.5636]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▎      | 98/292 [00:16<00:31,  6.19batch/s, auc=0.9408, loss=0.5636]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▎      | 98/292 [00:16<00:31,  6.19batch/s, auc=0.9406, loss=0.5164]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9406, loss=0.5164]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9402, loss=0.7867]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9402, loss=0.7867]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9400, loss=0.4723]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 101/292 [00:16<00:30,  6.22batch/s, auc=0.9400, loss=0.4723]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 101/292 [00:17<00:30,  6.22batch/s, auc=0.9399, loss=0.5094]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9399, loss=0.5094]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 102/292 [00:17<00:30,  6.22batch/s, auc=0.9397, loss=0.5975]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▌      | 103/292 [00:17<00:30,  6.21batch/s, auc=0.9397, loss=0.5975]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▌      | 103/292 [00:17<00:30,  6.21batch/s, auc=0.9401, loss=0.3273]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9401, loss=0.3273]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9397, loss=0.8034]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9397, loss=0.8034]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9395, loss=0.6572]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9395, loss=0.6572]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9395, loss=0.5445]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9395, loss=0.5445]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9396, loss=0.4260]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9396, loss=0.4260]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9396, loss=0.4748]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9396, loss=0.4748]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9397, loss=0.4313]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 110/292 [00:18<00:29,  6.17batch/s, auc=0.9397, loss=0.4313]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 110/292 [00:18<00:29,  6.17batch/s, auc=0.9400, loss=0.4580]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 111/292 [00:18<00:29,  6.19batch/s, auc=0.9400, loss=0.4580]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 111/292 [00:18<00:29,  6.19batch/s, auc=0.9402, loss=0.3469]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 112/292 [00:18<00:29,  6.20batch/s, auc=0.9402, loss=0.3469]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 112/292 [00:18<00:29,  6.20batch/s, auc=0.9402, loss=0.5824]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▊      | 113/292 [00:18<00:28,  6.21batch/s, auc=0.9402, loss=0.5824]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▊      | 113/292 [00:19<00:28,  6.21batch/s, auc=0.9402, loss=0.6024]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9402, loss=0.6024]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9404, loss=0.3580]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 115/292 [00:19<00:28,  6.23batch/s, auc=0.9404, loss=0.3580]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 115/292 [00:19<00:28,  6.23batch/s, auc=0.9404, loss=0.4385]\u001b[A\n",
      "Training Epoch 19/25:  40%|███▉      | 116/292 [00:19<00:28,  6.23batch/s, auc=0.9404, loss=0.4385]\u001b[A\n",
      "Training Epoch 19/25:  40%|███▉      | 116/292 [00:19<00:28,  6.23batch/s, auc=0.9405, loss=0.3666]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9405, loss=0.3666]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9400, loss=0.7455]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9400, loss=0.7455]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9399, loss=0.4418]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 119/292 [00:19<00:27,  6.21batch/s, auc=0.9399, loss=0.4418]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 119/292 [00:20<00:27,  6.21batch/s, auc=0.9398, loss=0.5661]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9398, loss=0.5661]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9393, loss=0.8507]\u001b[A\n",
      "Training Epoch 19/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9393, loss=0.8507]\u001b[A\n",
      "Training Epoch 19/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9389, loss=0.7785]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9389, loss=0.7785]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9391, loss=0.4045]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9391, loss=0.4045]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9390, loss=0.5784]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9390, loss=0.5784]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9393, loss=0.4302]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9393, loss=0.4302]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 125/292 [00:21<00:26,  6.21batch/s, auc=0.9393, loss=0.5073]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 126/292 [00:21<00:26,  6.21batch/s, auc=0.9393, loss=0.5073]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 126/292 [00:21<00:26,  6.21batch/s, auc=0.9393, loss=0.5086]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 127/292 [00:21<00:26,  6.22batch/s, auc=0.9393, loss=0.5086]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 127/292 [00:21<00:26,  6.22batch/s, auc=0.9394, loss=0.4765]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9394, loss=0.4765]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9395, loss=0.4231]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 129/292 [00:21<00:26,  6.21batch/s, auc=0.9395, loss=0.4231]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 129/292 [00:21<00:26,  6.21batch/s, auc=0.9395, loss=0.5010]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9395, loss=0.5010]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 130/292 [00:21<00:26,  6.20batch/s, auc=0.9389, loss=1.1274]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9389, loss=1.1274]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9386, loss=0.7149]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▌     | 132/292 [00:21<00:25,  6.18batch/s, auc=0.9386, loss=0.7149]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▌     | 132/292 [00:22<00:25,  6.18batch/s, auc=0.9387, loss=0.6739]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 133/292 [00:22<00:25,  6.18batch/s, auc=0.9387, loss=0.6739]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 133/292 [00:22<00:25,  6.18batch/s, auc=0.9387, loss=0.5745]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9387, loss=0.5745]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 134/292 [00:22<00:25,  6.18batch/s, auc=0.9385, loss=0.5398]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9385, loss=0.5398]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9380, loss=0.8145]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9380, loss=0.8145]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9381, loss=0.4578]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9381, loss=0.4578]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9382, loss=0.5061]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 138/292 [00:22<00:25,  6.16batch/s, auc=0.9382, loss=0.5061]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 138/292 [00:23<00:25,  6.16batch/s, auc=0.9379, loss=0.7485]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 139/292 [00:23<00:24,  6.16batch/s, auc=0.9379, loss=0.7485]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 139/292 [00:23<00:24,  6.16batch/s, auc=0.9381, loss=0.4228]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 140/292 [00:23<00:24,  6.15batch/s, auc=0.9381, loss=0.4228]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 140/292 [00:23<00:24,  6.15batch/s, auc=0.9378, loss=0.7140]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9378, loss=0.7140]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9374, loss=0.6684]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9374, loss=0.6684]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▊     | 142/292 [00:23<00:24,  6.15batch/s, auc=0.9376, loss=0.3942]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9376, loss=0.3942]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9376, loss=0.5084]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 144/292 [00:23<00:24,  6.14batch/s, auc=0.9376, loss=0.5084]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 144/292 [00:24<00:24,  6.14batch/s, auc=0.9374, loss=0.7105]\u001b[A\n",
      "Training Epoch 19/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9374, loss=0.7105]\u001b[A\n",
      "Training Epoch 19/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9372, loss=0.5976]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9372, loss=0.5976]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 146/292 [00:24<00:23,  6.15batch/s, auc=0.9368, loss=0.8497]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9368, loss=0.8497]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9369, loss=0.5162]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9369, loss=0.5162]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9369, loss=0.4565]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9369, loss=0.4565]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9370, loss=0.5531]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.15batch/s, auc=0.9370, loss=0.5531]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.15batch/s, auc=0.9371, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.16batch/s, auc=0.9371, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.16batch/s, auc=0.9371, loss=0.5547]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9371, loss=0.5547]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.14batch/s, auc=0.9372, loss=0.4392]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9372, loss=0.4392]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.14batch/s, auc=0.9374, loss=0.4164]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9374, loss=0.4164]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.14batch/s, auc=0.9375, loss=0.4430]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9375, loss=0.4430]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.15batch/s, auc=0.9374, loss=0.4897]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.12batch/s, auc=0.9374, loss=0.4897]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.12batch/s, auc=0.9377, loss=0.3449]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.13batch/s, auc=0.9377, loss=0.3449]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.13batch/s, auc=0.9377, loss=0.5886]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9377, loss=0.5886]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9377, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9377, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.12batch/s, auc=0.9377, loss=0.5573]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.11batch/s, auc=0.9377, loss=0.5573]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.11batch/s, auc=0.9377, loss=0.4699]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9377, loss=0.4699]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9378, loss=0.5290]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9378, loss=0.5290]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.13batch/s, auc=0.9377, loss=0.6725]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.12batch/s, auc=0.9377, loss=0.6725]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.12batch/s, auc=0.9377, loss=0.6214]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9377, loss=0.6214]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9377, loss=0.4052]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9377, loss=0.4052]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9376, loss=0.5835]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9376, loss=0.5835]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9373, loss=0.8758]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9373, loss=0.8758]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9375, loss=0.4154]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.09batch/s, auc=0.9375, loss=0.4154]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.09batch/s, auc=0.9377, loss=0.3427]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.99batch/s, auc=0.9377, loss=0.3427]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 169/292 [00:28<00:20,  5.99batch/s, auc=0.9379, loss=0.3938]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9379, loss=0.3938]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.02batch/s, auc=0.9380, loss=0.4739]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▊    | 171/292 [00:28<00:20,  5.82batch/s, auc=0.9380, loss=0.4739]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▊    | 171/292 [00:28<00:20,  5.82batch/s, auc=0.9381, loss=0.5584]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.79batch/s, auc=0.9381, loss=0.5584]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.79batch/s, auc=0.9380, loss=0.4841]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.77batch/s, auc=0.9380, loss=0.4841]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 173/292 [00:28<00:20,  5.77batch/s, auc=0.9381, loss=0.3586]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 174/292 [00:28<00:20,  5.85batch/s, auc=0.9381, loss=0.3586]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 174/292 [00:29<00:20,  5.85batch/s, auc=0.9379, loss=0.5479]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.70batch/s, auc=0.9379, loss=0.5479]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 175/292 [00:29<00:20,  5.70batch/s, auc=0.9374, loss=0.8795]\u001b[A\n",
      "Training Epoch 19/25:  60%|██████    | 176/292 [00:29<00:20,  5.74batch/s, auc=0.9374, loss=0.8795]\u001b[A\n",
      "Training Epoch 19/25:  60%|██████    | 176/292 [00:29<00:20,  5.74batch/s, auc=0.9375, loss=0.6313]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 177/292 [00:29<00:20,  5.73batch/s, auc=0.9375, loss=0.6313]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 177/292 [00:29<00:20,  5.73batch/s, auc=0.9377, loss=0.3339]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 178/292 [00:29<00:19,  5.71batch/s, auc=0.9377, loss=0.3339]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 178/292 [00:29<00:19,  5.71batch/s, auc=0.9381, loss=0.2523]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.71batch/s, auc=0.9381, loss=0.2523]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.71batch/s, auc=0.9377, loss=0.7781]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.72batch/s, auc=0.9377, loss=0.7781]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.72batch/s, auc=0.9376, loss=0.5973]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.83batch/s, auc=0.9376, loss=0.5973]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.83batch/s, auc=0.9377, loss=0.3769]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.69batch/s, auc=0.9377, loss=0.3769]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.69batch/s, auc=0.9379, loss=0.3801]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.81batch/s, auc=0.9379, loss=0.3801]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.81batch/s, auc=0.9380, loss=0.3955]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.77batch/s, auc=0.9380, loss=0.3955]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.77batch/s, auc=0.9377, loss=0.9543]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.88batch/s, auc=0.9377, loss=0.9543]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 185/292 [00:30<00:18,  5.88batch/s, auc=0.9379, loss=0.3950]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▎   | 186/292 [00:30<00:18,  5.78batch/s, auc=0.9379, loss=0.3950]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▎   | 186/292 [00:31<00:18,  5.78batch/s, auc=0.9380, loss=0.5105]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 187/292 [00:31<00:17,  5.87batch/s, auc=0.9380, loss=0.5105]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 187/292 [00:31<00:17,  5.87batch/s, auc=0.9382, loss=0.3745]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.83batch/s, auc=0.9382, loss=0.3745]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 188/292 [00:31<00:17,  5.83batch/s, auc=0.9382, loss=0.4760]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.92batch/s, auc=0.9382, loss=0.4760]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.92batch/s, auc=0.9381, loss=0.5674]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.97batch/s, auc=0.9381, loss=0.5674]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 190/292 [00:31<00:17,  5.97batch/s, auc=0.9380, loss=0.5084]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 191/292 [00:31<00:16,  5.97batch/s, auc=0.9380, loss=0.5084]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 191/292 [00:31<00:16,  5.97batch/s, auc=0.9378, loss=0.5963]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 192/292 [00:31<00:16,  5.99batch/s, auc=0.9378, loss=0.5963]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 192/292 [00:32<00:16,  5.99batch/s, auc=0.9376, loss=0.5821]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.02batch/s, auc=0.9376, loss=0.5821]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.02batch/s, auc=0.9375, loss=0.5059]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.87batch/s, auc=0.9375, loss=0.5059]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.87batch/s, auc=0.9377, loss=0.3325]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.92batch/s, auc=0.9377, loss=0.3325]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.92batch/s, auc=0.9380, loss=0.3206]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.75batch/s, auc=0.9380, loss=0.3206]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.75batch/s, auc=0.9382, loss=0.4093]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.86batch/s, auc=0.9382, loss=0.4093]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 197/292 [00:33<00:16,  5.86batch/s, auc=0.9380, loss=0.6618]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 198/292 [00:33<00:15,  5.92batch/s, auc=0.9380, loss=0.6618]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 198/292 [00:33<00:15,  5.92batch/s, auc=0.9380, loss=0.4851]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.95batch/s, auc=0.9380, loss=0.4851]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 199/292 [00:33<00:15,  5.95batch/s, auc=0.9382, loss=0.3305]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.85batch/s, auc=0.9382, loss=0.3305]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 200/292 [00:33<00:15,  5.85batch/s, auc=0.9382, loss=0.4792]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.90batch/s, auc=0.9382, loss=0.4792]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.90batch/s, auc=0.9385, loss=0.2880]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.70batch/s, auc=0.9385, loss=0.2880]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.70batch/s, auc=0.9387, loss=0.3333]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.70batch/s, auc=0.9387, loss=0.3333]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.70batch/s, auc=0.9388, loss=0.4046]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.80batch/s, auc=0.9388, loss=0.4046]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.80batch/s, auc=0.9385, loss=1.0554]\u001b[A\n",
      "Training Epoch 19/25:  70%|███████   | 205/292 [00:34<00:15,  5.63batch/s, auc=0.9385, loss=1.0554]\u001b[A\n",
      "Training Epoch 19/25:  70%|███████   | 205/292 [00:34<00:15,  5.63batch/s, auc=0.9385, loss=0.4568]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 206/292 [00:34<00:15,  5.62batch/s, auc=0.9385, loss=0.4568]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 206/292 [00:34<00:15,  5.62batch/s, auc=0.9379, loss=1.3103]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 207/292 [00:34<00:15,  5.65batch/s, auc=0.9379, loss=1.3103]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 207/292 [00:34<00:15,  5.65batch/s, auc=0.9380, loss=0.3595]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 208/292 [00:34<00:14,  5.68batch/s, auc=0.9380, loss=0.3595]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 208/292 [00:34<00:14,  5.68batch/s, auc=0.9378, loss=0.6837]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.80batch/s, auc=0.9378, loss=0.6837]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 209/292 [00:35<00:14,  5.80batch/s, auc=0.9379, loss=0.4080]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.71batch/s, auc=0.9379, loss=0.4080]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.71batch/s, auc=0.9380, loss=0.3356]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.81batch/s, auc=0.9380, loss=0.3356]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.81batch/s, auc=0.9381, loss=0.3923]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.75batch/s, auc=0.9381, loss=0.3923]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.75batch/s, auc=0.9383, loss=0.3139]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.84batch/s, auc=0.9383, loss=0.3139]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.84batch/s, auc=0.9383, loss=0.6398]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.78batch/s, auc=0.9383, loss=0.6398]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.78batch/s, auc=0.9383, loss=0.4114]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.73batch/s, auc=0.9383, loss=0.4114]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.73batch/s, auc=0.9380, loss=0.8949]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.71batch/s, auc=0.9380, loss=0.8949]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.71batch/s, auc=0.9382, loss=0.3389]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.79batch/s, auc=0.9382, loss=0.3389]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.79batch/s, auc=0.9381, loss=0.5841]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9381, loss=0.5841]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9382, loss=0.5138]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.68batch/s, auc=0.9382, loss=0.5138]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.68batch/s, auc=0.9382, loss=0.4552]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.73batch/s, auc=0.9382, loss=0.4552]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 220/292 [00:37<00:12,  5.73batch/s, auc=0.9382, loss=0.4678]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.81batch/s, auc=0.9382, loss=0.4678]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.81batch/s, auc=0.9383, loss=0.5938]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.85batch/s, auc=0.9383, loss=0.5938]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.85batch/s, auc=0.9385, loss=0.3759]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.91batch/s, auc=0.9385, loss=0.3759]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.91batch/s, auc=0.9384, loss=0.5692]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.82batch/s, auc=0.9384, loss=0.5692]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.82batch/s, auc=0.9384, loss=0.6149]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.75batch/s, auc=0.9384, loss=0.6149]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.75batch/s, auc=0.9385, loss=0.3904]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.70batch/s, auc=0.9385, loss=0.3904]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.70batch/s, auc=0.9385, loss=0.5037]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.68batch/s, auc=0.9385, loss=0.5037]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.68batch/s, auc=0.9386, loss=0.4326]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.72batch/s, auc=0.9386, loss=0.4326]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.72batch/s, auc=0.9389, loss=0.3445]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.66batch/s, auc=0.9389, loss=0.3445]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.66batch/s, auc=0.9388, loss=0.5450]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.63batch/s, auc=0.9388, loss=0.5450]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 230/292 [00:38<00:11,  5.63batch/s, auc=0.9385, loss=0.8927]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.64batch/s, auc=0.9385, loss=0.8927]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.64batch/s, auc=0.9386, loss=0.4011]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.62batch/s, auc=0.9386, loss=0.4011]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.62batch/s, auc=0.9386, loss=0.4629]\u001b[A\n",
      "Training Epoch 19/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.62batch/s, auc=0.9386, loss=0.4629]\u001b[A\n",
      "Training Epoch 19/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.62batch/s, auc=0.9386, loss=0.4547]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 234/292 [00:39<00:10,  5.62batch/s, auc=0.9386, loss=0.4547]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 234/292 [00:39<00:10,  5.62batch/s, auc=0.9384, loss=0.6610]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 235/292 [00:39<00:10,  5.60batch/s, auc=0.9384, loss=0.6610]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 235/292 [00:39<00:10,  5.60batch/s, auc=0.9384, loss=0.5018]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 236/292 [00:39<00:09,  5.65batch/s, auc=0.9384, loss=0.5018]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 236/292 [00:39<00:09,  5.65batch/s, auc=0.9384, loss=0.5946]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 237/292 [00:39<00:09,  5.64batch/s, auc=0.9384, loss=0.5946]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 237/292 [00:40<00:09,  5.64batch/s, auc=0.9383, loss=0.7609]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.60batch/s, auc=0.9383, loss=0.7609]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.60batch/s, auc=0.9382, loss=0.7655]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.58batch/s, auc=0.9382, loss=0.7655]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.58batch/s, auc=0.9382, loss=0.6121]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.59batch/s, auc=0.9382, loss=0.6121]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.59batch/s, auc=0.9380, loss=0.9416]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.60batch/s, auc=0.9380, loss=0.9416]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.60batch/s, auc=0.9381, loss=0.4741]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.60batch/s, auc=0.9381, loss=0.4741]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.60batch/s, auc=0.9381, loss=0.5073]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.60batch/s, auc=0.9381, loss=0.5073]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 243/292 [00:41<00:08,  5.60batch/s, auc=0.9380, loss=0.7063]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.60batch/s, auc=0.9380, loss=0.7063]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.60batch/s, auc=0.9379, loss=0.5847]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.62batch/s, auc=0.9379, loss=0.5847]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.62batch/s, auc=0.9381, loss=0.3835]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.61batch/s, auc=0.9381, loss=0.3835]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.61batch/s, auc=0.9381, loss=0.5380]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.61batch/s, auc=0.9381, loss=0.5380]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.61batch/s, auc=0.9380, loss=0.6460]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.62batch/s, auc=0.9380, loss=0.6460]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.62batch/s, auc=0.9379, loss=0.6219]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.62batch/s, auc=0.9379, loss=0.6219]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.62batch/s, auc=0.9378, loss=0.6635]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.62batch/s, auc=0.9378, loss=0.6635]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.62batch/s, auc=0.9379, loss=0.4648]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9379, loss=0.4648]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.62batch/s, auc=0.9378, loss=0.5473]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.63batch/s, auc=0.9378, loss=0.5473]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.63batch/s, auc=0.9378, loss=0.4839]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.64batch/s, auc=0.9378, loss=0.4839]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.64batch/s, auc=0.9379, loss=0.4953]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.62batch/s, auc=0.9379, loss=0.4953]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 254/292 [00:43<00:06,  5.62batch/s, auc=0.9380, loss=0.3944]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.61batch/s, auc=0.9380, loss=0.3944]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.61batch/s, auc=0.9379, loss=0.5899]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.64batch/s, auc=0.9379, loss=0.5899]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.64batch/s, auc=0.9378, loss=0.5599]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.74batch/s, auc=0.9378, loss=0.5599]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.74batch/s, auc=0.9378, loss=0.6769]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.71batch/s, auc=0.9378, loss=0.6769]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.71batch/s, auc=0.9378, loss=0.5397]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9378, loss=0.5397]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.68batch/s, auc=0.9377, loss=0.5544]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.65batch/s, auc=0.9377, loss=0.5544]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 260/292 [00:44<00:05,  5.65batch/s, auc=0.9377, loss=0.4859]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.64batch/s, auc=0.9377, loss=0.4859]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.64batch/s, auc=0.9379, loss=0.3299]\u001b[A\n",
      "Training Epoch 19/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.63batch/s, auc=0.9379, loss=0.3299]\u001b[A\n",
      "Training Epoch 19/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.63batch/s, auc=0.9380, loss=0.4651]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 263/292 [00:44<00:05,  5.60batch/s, auc=0.9380, loss=0.4651]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 263/292 [00:44<00:05,  5.60batch/s, auc=0.9380, loss=0.4979]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.9380, loss=0.4979]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 264/292 [00:44<00:04,  5.70batch/s, auc=0.9379, loss=0.5743]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 265/292 [00:44<00:04,  5.67batch/s, auc=0.9379, loss=0.5743]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 265/292 [00:44<00:04,  5.67batch/s, auc=0.9379, loss=0.5333]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 266/292 [00:44<00:04,  5.75batch/s, auc=0.9379, loss=0.5333]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 266/292 [00:45<00:04,  5.75batch/s, auc=0.9381, loss=0.3489]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.60batch/s, auc=0.9381, loss=0.3489]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.60batch/s, auc=0.9380, loss=0.5422]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.59batch/s, auc=0.9380, loss=0.5422]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.59batch/s, auc=0.9378, loss=0.7327]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.59batch/s, auc=0.9378, loss=0.7327]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.59batch/s, auc=0.9376, loss=0.5923]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9376, loss=0.5923]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9375, loss=0.7648]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.58batch/s, auc=0.9375, loss=0.7648]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 271/292 [00:46<00:03,  5.58batch/s, auc=0.9374, loss=0.6114]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.60batch/s, auc=0.9374, loss=0.6114]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.60batch/s, auc=0.9374, loss=0.4938]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.61batch/s, auc=0.9374, loss=0.4938]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.61batch/s, auc=0.9374, loss=0.5912]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.60batch/s, auc=0.9374, loss=0.5912]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.60batch/s, auc=0.9376, loss=0.2872]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.59batch/s, auc=0.9376, loss=0.2872]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.59batch/s, auc=0.9376, loss=0.3970]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.61batch/s, auc=0.9376, loss=0.3970]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.61batch/s, auc=0.9377, loss=0.4695]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.59batch/s, auc=0.9377, loss=0.4695]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 277/292 [00:47<00:02,  5.59batch/s, auc=0.9376, loss=0.4828]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.60batch/s, auc=0.9376, loss=0.4828]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.60batch/s, auc=0.9376, loss=0.7782]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.57batch/s, auc=0.9376, loss=0.7782]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.57batch/s, auc=0.9376, loss=0.4861]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.55batch/s, auc=0.9376, loss=0.4861]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.55batch/s, auc=0.9376, loss=0.5546]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.55batch/s, auc=0.9376, loss=0.5546]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.55batch/s, auc=0.9376, loss=0.4570]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.55batch/s, auc=0.9376, loss=0.4570]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 282/292 [00:48<00:01,  5.55batch/s, auc=0.9375, loss=0.7102]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.56batch/s, auc=0.9375, loss=0.7102]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.56batch/s, auc=0.9376, loss=0.3279]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.59batch/s, auc=0.9376, loss=0.3279]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.59batch/s, auc=0.9374, loss=0.8075]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.55batch/s, auc=0.9374, loss=0.8075]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.55batch/s, auc=0.9375, loss=0.4095]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9375, loss=0.4095]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.66batch/s, auc=0.9377, loss=0.3425]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.74batch/s, auc=0.9377, loss=0.3425]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.74batch/s, auc=0.9378, loss=0.3742]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.81batch/s, auc=0.9378, loss=0.3742]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▊| 288/292 [00:49<00:00,  5.81batch/s, auc=0.9378, loss=0.3848]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9378, loss=0.3848]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9378, loss=0.4375]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.90batch/s, auc=0.9378, loss=0.4375]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.90batch/s, auc=0.9378, loss=0.5779]\u001b[A\n",
      "Training Epoch 19/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9378, loss=0.5779]\u001b[A\n",
      "Training Epoch 19/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9379, loss=0.3646]\u001b[A\n",
      "Training Epoch 19/25: 100%|██████████| 292/292 [00:49<00:00,  5.89batch/s, auc=0.9379, loss=0.3646]\u001b[A\n",
      "Epochs:  76%|███████▌  | 19/25 [17:23<05:30, 55.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25] Train Loss: 0.5361 | Train AUROC: 0.9379 Val Loss: 0.6528 | Val AUROC: 0.9089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 20/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9222, loss=0.6098]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 1/292 [00:00<04:43,  1.02batch/s, auc=0.9222, loss=0.6098]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 1/292 [00:01<04:43,  1.02batch/s, auc=0.9232, loss=0.6195]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 2/292 [00:01<02:23,  2.03batch/s, auc=0.9232, loss=0.6195]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 2/292 [00:01<02:23,  2.03batch/s, auc=0.9347, loss=0.4217]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 3/292 [00:01<01:40,  2.86batch/s, auc=0.9347, loss=0.4217]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 3/292 [00:01<01:40,  2.86batch/s, auc=0.9372, loss=0.5542]\u001b[A\n",
      "Training Epoch 20/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9372, loss=0.5542]\u001b[A\n",
      "Training Epoch 20/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9278, loss=0.8460]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 5/292 [00:01<01:06,  4.32batch/s, auc=0.9278, loss=0.8460]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 5/292 [00:01<01:06,  4.32batch/s, auc=0.9280, loss=0.4707]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 6/292 [00:01<00:58,  4.85batch/s, auc=0.9280, loss=0.4707]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 6/292 [00:01<00:58,  4.85batch/s, auc=0.9166, loss=1.0172]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 7/292 [00:01<00:56,  5.06batch/s, auc=0.9166, loss=1.0172]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 7/292 [00:02<00:56,  5.06batch/s, auc=0.9197, loss=0.3829]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.9197, loss=0.3829]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 8/292 [00:02<00:52,  5.42batch/s, auc=0.9224, loss=0.4832]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.9224, loss=0.4832]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 9/292 [00:02<00:49,  5.69batch/s, auc=0.9214, loss=0.6230]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 10/292 [00:02<00:48,  5.77batch/s, auc=0.9214, loss=0.6230]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 10/292 [00:02<00:48,  5.77batch/s, auc=0.9279, loss=0.3238]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 11/292 [00:02<00:47,  5.95batch/s, auc=0.9279, loss=0.3238]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 11/292 [00:02<00:47,  5.95batch/s, auc=0.9277, loss=0.6583]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9277, loss=0.6583]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9306, loss=0.7856]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 13/292 [00:02<00:45,  6.18batch/s, auc=0.9306, loss=0.7856]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 13/292 [00:03<00:45,  6.18batch/s, auc=0.9287, loss=0.5486]\u001b[A\n",
      "Training Epoch 20/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9287, loss=0.5486]\u001b[A\n",
      "Training Epoch 20/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9302, loss=0.4434]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.9302, loss=0.4434]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 15/292 [00:03<00:44,  6.24batch/s, auc=0.9301, loss=0.4774]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 16/292 [00:03<00:44,  6.17batch/s, auc=0.9301, loss=0.4774]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 16/292 [00:03<00:44,  6.17batch/s, auc=0.9324, loss=0.5123]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 17/292 [00:03<00:44,  6.24batch/s, auc=0.9324, loss=0.5123]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 17/292 [00:03<00:44,  6.24batch/s, auc=0.9316, loss=0.5725]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 18/292 [00:03<00:43,  6.28batch/s, auc=0.9316, loss=0.5725]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 18/292 [00:03<00:43,  6.28batch/s, auc=0.9321, loss=0.6296]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 19/292 [00:03<00:43,  6.30batch/s, auc=0.9321, loss=0.6296]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 19/292 [00:04<00:43,  6.30batch/s, auc=0.9329, loss=0.3626]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9329, loss=0.3626]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9319, loss=0.4758]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9319, loss=0.4758]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 21/292 [00:04<00:42,  6.32batch/s, auc=0.9343, loss=0.4754]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 22/292 [00:04<00:42,  6.29batch/s, auc=0.9343, loss=0.4754]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 22/292 [00:04<00:42,  6.29batch/s, auc=0.9324, loss=0.6456]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9324, loss=0.6456]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 23/292 [00:04<00:42,  6.32batch/s, auc=0.9330, loss=0.3678]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9330, loss=0.3678]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 24/292 [00:04<00:42,  6.34batch/s, auc=0.9343, loss=0.3542]\u001b[A\n",
      "Training Epoch 20/25:   9%|▊         | 25/292 [00:04<00:42,  6.36batch/s, auc=0.9343, loss=0.3542]\u001b[A\n",
      "Training Epoch 20/25:   9%|▊         | 25/292 [00:04<00:42,  6.36batch/s, auc=0.9348, loss=0.4325]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 26/292 [00:04<00:41,  6.36batch/s, auc=0.9348, loss=0.4325]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 26/292 [00:05<00:41,  6.36batch/s, auc=0.9362, loss=0.3268]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 27/292 [00:05<00:41,  6.36batch/s, auc=0.9362, loss=0.3268]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 27/292 [00:05<00:41,  6.36batch/s, auc=0.9369, loss=0.4536]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9369, loss=0.4536]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9357, loss=0.6623]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9357, loss=0.6623]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 29/292 [00:05<00:41,  6.34batch/s, auc=0.9360, loss=0.4865]\u001b[A\n",
      "Training Epoch 20/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9360, loss=0.4865]\u001b[A\n",
      "Training Epoch 20/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9348, loss=0.7972]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9348, loss=0.7972]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9338, loss=0.9011]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 32/292 [00:05<00:41,  6.34batch/s, auc=0.9338, loss=0.9011]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 32/292 [00:06<00:41,  6.34batch/s, auc=0.9355, loss=0.2836]\u001b[A\n",
      "Training Epoch 20/25:  11%|█▏        | 33/292 [00:06<00:40,  6.35batch/s, auc=0.9355, loss=0.2836]\u001b[A\n",
      "Training Epoch 20/25:  11%|█▏        | 33/292 [00:06<00:40,  6.35batch/s, auc=0.9355, loss=0.5476]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9355, loss=0.5476]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9368, loss=0.3784]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9368, loss=0.3784]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9372, loss=0.5528]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9372, loss=0.5528]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9381, loss=0.3497]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9381, loss=0.3497]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9377, loss=0.5212]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 38/292 [00:06<00:40,  6.32batch/s, auc=0.9377, loss=0.5212]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 38/292 [00:07<00:40,  6.32batch/s, auc=0.9378, loss=0.6726]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 39/292 [00:07<00:40,  6.25batch/s, auc=0.9378, loss=0.6726]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 39/292 [00:07<00:40,  6.25batch/s, auc=0.9375, loss=0.6040]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▎        | 40/292 [00:07<00:40,  6.27batch/s, auc=0.9375, loss=0.6040]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▎        | 40/292 [00:07<00:40,  6.27batch/s, auc=0.9382, loss=0.4004]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9382, loss=0.4004]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 41/292 [00:07<00:39,  6.29batch/s, auc=0.9381, loss=0.5727]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 42/292 [00:07<00:40,  6.25batch/s, auc=0.9381, loss=0.5727]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 42/292 [00:07<00:40,  6.25batch/s, auc=0.9402, loss=0.2774]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9402, loss=0.2774]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9409, loss=0.3612]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 44/292 [00:07<00:39,  6.27batch/s, auc=0.9409, loss=0.3612]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 44/292 [00:07<00:39,  6.27batch/s, auc=0.9406, loss=0.5224]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 45/292 [00:07<00:39,  6.25batch/s, auc=0.9406, loss=0.5224]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 45/292 [00:08<00:39,  6.25batch/s, auc=0.9412, loss=0.4421]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9412, loss=0.4421]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9417, loss=0.3544]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9417, loss=0.3544]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9429, loss=0.3533]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9429, loss=0.3533]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9418, loss=0.6687]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9418, loss=0.6687]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9417, loss=0.5055]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.9417, loss=0.5055]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 50/292 [00:08<00:38,  6.30batch/s, auc=0.9407, loss=0.5188]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 51/292 [00:08<00:38,  6.31batch/s, auc=0.9407, loss=0.5188]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 51/292 [00:09<00:38,  6.31batch/s, auc=0.9416, loss=0.4433]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9416, loss=0.4433]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 52/292 [00:09<00:37,  6.32batch/s, auc=0.9407, loss=0.5600]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 53/292 [00:09<00:37,  6.31batch/s, auc=0.9407, loss=0.5600]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 53/292 [00:09<00:37,  6.31batch/s, auc=0.9407, loss=0.5078]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 54/292 [00:09<00:37,  6.30batch/s, auc=0.9407, loss=0.5078]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 54/292 [00:09<00:37,  6.30batch/s, auc=0.9409, loss=0.3972]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9409, loss=0.3972]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9418, loss=0.3547]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9418, loss=0.3547]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9414, loss=0.4501]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 57/292 [00:09<00:37,  6.30batch/s, auc=0.9414, loss=0.4501]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 57/292 [00:10<00:37,  6.30batch/s, auc=0.9410, loss=0.5394]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9410, loss=0.5394]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9411, loss=0.4800]\u001b[A\n",
      "Training Epoch 20/25:  20%|██        | 59/292 [00:10<00:37,  6.30batch/s, auc=0.9411, loss=0.4800]\u001b[A\n",
      "Training Epoch 20/25:  20%|██        | 59/292 [00:10<00:37,  6.30batch/s, auc=0.9416, loss=0.4427]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9416, loss=0.4427]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9424, loss=0.2889]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9424, loss=0.2889]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9423, loss=0.5694]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9423, loss=0.5694]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9422, loss=0.4999]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 63/292 [00:10<00:36,  6.27batch/s, auc=0.9422, loss=0.4999]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 63/292 [00:11<00:36,  6.27batch/s, auc=0.9425, loss=0.3776]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9425, loss=0.3776]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9424, loss=0.4299]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9424, loss=0.4299]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 65/292 [00:11<00:36,  6.28batch/s, auc=0.9424, loss=0.4618]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 66/292 [00:11<00:36,  6.22batch/s, auc=0.9424, loss=0.4618]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 66/292 [00:11<00:36,  6.22batch/s, auc=0.9426, loss=0.4321]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 67/292 [00:11<00:36,  6.25batch/s, auc=0.9426, loss=0.4321]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 67/292 [00:11<00:36,  6.25batch/s, auc=0.9415, loss=1.0260]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 68/292 [00:11<00:35,  6.25batch/s, auc=0.9415, loss=1.0260]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 68/292 [00:11<00:35,  6.25batch/s, auc=0.9415, loss=0.5613]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▎       | 69/292 [00:11<00:36,  6.17batch/s, auc=0.9415, loss=0.5613]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▎       | 69/292 [00:11<00:36,  6.17batch/s, auc=0.9422, loss=0.3104]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 70/292 [00:11<00:35,  6.17batch/s, auc=0.9422, loss=0.3104]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 70/292 [00:12<00:35,  6.17batch/s, auc=0.9421, loss=0.5191]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 71/292 [00:12<00:35,  6.20batch/s, auc=0.9421, loss=0.5191]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 71/292 [00:12<00:35,  6.20batch/s, auc=0.9426, loss=0.2902]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▍       | 72/292 [00:12<00:35,  6.22batch/s, auc=0.9426, loss=0.2902]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▍       | 72/292 [00:12<00:35,  6.22batch/s, auc=0.9429, loss=0.4006]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 73/292 [00:12<00:35,  6.23batch/s, auc=0.9429, loss=0.4006]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 73/292 [00:12<00:35,  6.23batch/s, auc=0.9434, loss=0.3593]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 74/292 [00:12<00:34,  6.24batch/s, auc=0.9434, loss=0.3593]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 74/292 [00:12<00:34,  6.24batch/s, auc=0.9430, loss=0.6166]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 75/292 [00:12<00:34,  6.24batch/s, auc=0.9430, loss=0.6166]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 75/292 [00:12<00:34,  6.24batch/s, auc=0.9433, loss=0.4023]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 76/292 [00:12<00:34,  6.23batch/s, auc=0.9433, loss=0.4023]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 76/292 [00:13<00:34,  6.23batch/s, auc=0.9437, loss=0.3727]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9437, loss=0.3727]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▋       | 77/292 [00:13<00:34,  6.24batch/s, auc=0.9434, loss=0.5009]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 78/292 [00:13<00:34,  6.24batch/s, auc=0.9434, loss=0.5009]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 78/292 [00:13<00:34,  6.24batch/s, auc=0.9434, loss=0.4163]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9434, loss=0.4163]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 79/292 [00:13<00:34,  6.25batch/s, auc=0.9433, loss=0.6610]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9433, loss=0.6610]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 80/292 [00:13<00:33,  6.25batch/s, auc=0.9442, loss=0.2271]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 81/292 [00:13<00:33,  6.23batch/s, auc=0.9442, loss=0.2271]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 81/292 [00:13<00:33,  6.23batch/s, auc=0.9441, loss=0.5391]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 82/292 [00:13<00:33,  6.24batch/s, auc=0.9441, loss=0.5391]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 82/292 [00:14<00:33,  6.24batch/s, auc=0.9441, loss=0.4533]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9441, loss=0.4533]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9444, loss=0.3541]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 84/292 [00:14<00:33,  6.21batch/s, auc=0.9444, loss=0.3541]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 84/292 [00:14<00:33,  6.21batch/s, auc=0.9447, loss=0.3661]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 85/292 [00:14<00:33,  6.23batch/s, auc=0.9447, loss=0.3661]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 85/292 [00:14<00:33,  6.23batch/s, auc=0.9445, loss=0.5631]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 86/292 [00:14<00:33,  6.24batch/s, auc=0.9445, loss=0.5631]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 86/292 [00:14<00:33,  6.24batch/s, auc=0.9444, loss=0.6109]\u001b[A\n",
      "Training Epoch 20/25:  30%|██▉       | 87/292 [00:14<00:32,  6.25batch/s, auc=0.9444, loss=0.6109]\u001b[A\n",
      "Training Epoch 20/25:  30%|██▉       | 87/292 [00:14<00:32,  6.25batch/s, auc=0.9443, loss=0.4950]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 88/292 [00:14<00:32,  6.25batch/s, auc=0.9443, loss=0.4950]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 88/292 [00:15<00:32,  6.25batch/s, auc=0.9441, loss=0.6780]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9441, loss=0.6780]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9449, loss=0.2154]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9449, loss=0.2154]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 90/292 [00:15<00:32,  6.25batch/s, auc=0.9451, loss=0.4009]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9451, loss=0.4009]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 91/292 [00:15<00:32,  6.24batch/s, auc=0.9454, loss=0.5409]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 92/292 [00:15<00:32,  6.21batch/s, auc=0.9454, loss=0.5409]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 92/292 [00:15<00:32,  6.21batch/s, auc=0.9453, loss=0.5085]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9453, loss=0.5085]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 93/292 [00:15<00:31,  6.22batch/s, auc=0.9452, loss=0.4622]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9452, loss=0.4622]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 94/292 [00:15<00:31,  6.24batch/s, auc=0.9456, loss=0.3251]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 95/292 [00:15<00:31,  6.24batch/s, auc=0.9456, loss=0.3251]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 95/292 [00:16<00:31,  6.24batch/s, auc=0.9459, loss=0.3564]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9459, loss=0.3564]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9453, loss=0.8213]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9453, loss=0.8213]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9453, loss=0.4074]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9453, loss=0.4074]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9455, loss=0.3358]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9455, loss=0.3358]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9453, loss=0.4363]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9453, loss=0.4363]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 100/292 [00:16<00:30,  6.21batch/s, auc=0.9458, loss=0.2814]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 101/292 [00:16<00:30,  6.21batch/s, auc=0.9458, loss=0.2814]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 101/292 [00:17<00:30,  6.21batch/s, auc=0.9455, loss=0.7236]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9455, loss=0.7236]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 102/292 [00:17<00:30,  6.21batch/s, auc=0.9461, loss=0.2563]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▌      | 103/292 [00:17<00:30,  6.22batch/s, auc=0.9461, loss=0.2563]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▌      | 103/292 [00:17<00:30,  6.22batch/s, auc=0.9462, loss=0.3462]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9462, loss=0.3462]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9459, loss=0.5519]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9459, loss=0.5519]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 105/292 [00:17<00:30,  6.22batch/s, auc=0.9458, loss=0.4509]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9458, loss=0.4509]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9455, loss=0.6511]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9455, loss=0.6511]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9457, loss=0.3544]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9457, loss=0.3544]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9460, loss=0.2955]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9460, loss=0.2955]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 109/292 [00:18<00:29,  6.21batch/s, auc=0.9451, loss=1.0083]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 110/292 [00:18<00:29,  6.21batch/s, auc=0.9451, loss=1.0083]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 110/292 [00:18<00:29,  6.21batch/s, auc=0.9451, loss=0.5016]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9451, loss=0.5016]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9446, loss=0.6726]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 112/292 [00:18<00:29,  6.20batch/s, auc=0.9446, loss=0.6726]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 112/292 [00:18<00:29,  6.20batch/s, auc=0.9440, loss=0.7669]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▊      | 113/292 [00:18<00:28,  6.19batch/s, auc=0.9440, loss=0.7669]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▊      | 113/292 [00:19<00:28,  6.19batch/s, auc=0.9438, loss=0.4984]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 114/292 [00:19<00:28,  6.17batch/s, auc=0.9438, loss=0.4984]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 114/292 [00:19<00:28,  6.17batch/s, auc=0.9437, loss=0.6511]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 115/292 [00:19<00:28,  6.19batch/s, auc=0.9437, loss=0.6511]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 115/292 [00:19<00:28,  6.19batch/s, auc=0.9436, loss=0.4528]\u001b[A\n",
      "Training Epoch 20/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9436, loss=0.4528]\u001b[A\n",
      "Training Epoch 20/25:  40%|███▉      | 116/292 [00:19<00:28,  6.21batch/s, auc=0.9430, loss=1.0114]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 117/292 [00:19<00:28,  6.23batch/s, auc=0.9430, loss=1.0114]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 117/292 [00:19<00:28,  6.23batch/s, auc=0.9431, loss=0.4057]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 118/292 [00:19<00:27,  6.23batch/s, auc=0.9431, loss=0.4057]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 118/292 [00:19<00:27,  6.23batch/s, auc=0.9429, loss=0.5062]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 119/292 [00:19<00:28,  6.18batch/s, auc=0.9429, loss=0.5062]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 119/292 [00:20<00:28,  6.18batch/s, auc=0.9426, loss=0.6566]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9426, loss=0.6566]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 120/292 [00:20<00:27,  6.19batch/s, auc=0.9426, loss=0.5152]\u001b[A\n",
      "Training Epoch 20/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9426, loss=0.5152]\u001b[A\n",
      "Training Epoch 20/25:  41%|████▏     | 121/292 [00:20<00:27,  6.20batch/s, auc=0.9423, loss=0.7213]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9423, loss=0.7213]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 122/292 [00:20<00:27,  6.21batch/s, auc=0.9425, loss=0.3706]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9425, loss=0.3706]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 123/292 [00:20<00:27,  6.22batch/s, auc=0.9424, loss=0.5991]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 124/292 [00:20<00:27,  6.22batch/s, auc=0.9424, loss=0.5991]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 124/292 [00:20<00:27,  6.22batch/s, auc=0.9422, loss=0.7906]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 125/292 [00:20<00:27,  6.12batch/s, auc=0.9422, loss=0.7906]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 125/292 [00:20<00:27,  6.12batch/s, auc=0.9421, loss=0.5393]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 126/292 [00:20<00:27,  6.08batch/s, auc=0.9421, loss=0.5393]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 126/292 [00:21<00:27,  6.08batch/s, auc=0.9421, loss=0.4513]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 127/292 [00:21<00:26,  6.12batch/s, auc=0.9421, loss=0.4513]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 127/292 [00:21<00:26,  6.12batch/s, auc=0.9421, loss=0.5209]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 128/292 [00:21<00:26,  6.15batch/s, auc=0.9421, loss=0.5209]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 128/292 [00:21<00:26,  6.15batch/s, auc=0.9415, loss=0.9005]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 129/292 [00:21<00:26,  6.16batch/s, auc=0.9415, loss=0.9005]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 129/292 [00:21<00:26,  6.16batch/s, auc=0.9416, loss=0.5303]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 130/292 [00:21<00:26,  6.10batch/s, auc=0.9416, loss=0.5303]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 130/292 [00:21<00:26,  6.10batch/s, auc=0.9415, loss=0.5276]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 131/292 [00:21<00:26,  6.13batch/s, auc=0.9415, loss=0.5276]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 131/292 [00:21<00:26,  6.13batch/s, auc=0.9413, loss=0.5884]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▌     | 132/292 [00:21<00:26,  6.14batch/s, auc=0.9413, loss=0.5884]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▌     | 132/292 [00:22<00:26,  6.14batch/s, auc=0.9414, loss=0.4811]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 133/292 [00:22<00:25,  6.15batch/s, auc=0.9414, loss=0.4811]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 133/292 [00:22<00:25,  6.15batch/s, auc=0.9416, loss=0.3381]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 134/292 [00:22<00:25,  6.15batch/s, auc=0.9416, loss=0.3381]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 134/292 [00:22<00:25,  6.15batch/s, auc=0.9417, loss=0.6417]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9417, loss=0.6417]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9417, loss=0.5066]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9417, loss=0.5066]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9413, loss=0.9912]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 137/292 [00:22<00:25,  6.16batch/s, auc=0.9413, loss=0.9912]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 137/292 [00:22<00:25,  6.16batch/s, auc=0.9413, loss=0.5036]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 138/292 [00:22<00:25,  6.16batch/s, auc=0.9413, loss=0.5036]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 138/292 [00:23<00:25,  6.16batch/s, auc=0.9414, loss=0.4512]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 139/292 [00:23<00:24,  6.14batch/s, auc=0.9414, loss=0.4512]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 139/292 [00:23<00:24,  6.14batch/s, auc=0.9416, loss=0.3800]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 140/292 [00:23<00:24,  6.14batch/s, auc=0.9416, loss=0.3800]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 140/292 [00:23<00:24,  6.14batch/s, auc=0.9419, loss=0.3527]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 141/292 [00:23<00:24,  6.14batch/s, auc=0.9419, loss=0.3527]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 141/292 [00:23<00:24,  6.14batch/s, auc=0.9418, loss=0.5509]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▊     | 142/292 [00:23<00:24,  6.13batch/s, auc=0.9418, loss=0.5509]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▊     | 142/292 [00:23<00:24,  6.13batch/s, auc=0.9418, loss=0.5324]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9418, loss=0.5324]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9417, loss=0.5748]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 144/292 [00:23<00:24,  6.14batch/s, auc=0.9417, loss=0.5748]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 144/292 [00:24<00:24,  6.14batch/s, auc=0.9413, loss=0.7364]\u001b[A\n",
      "Training Epoch 20/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9413, loss=0.7364]\u001b[A\n",
      "Training Epoch 20/25:  50%|████▉     | 145/292 [00:24<00:23,  6.15batch/s, auc=0.9416, loss=0.4247]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 146/292 [00:24<00:23,  6.14batch/s, auc=0.9416, loss=0.4247]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 146/292 [00:24<00:23,  6.14batch/s, auc=0.9413, loss=0.5695]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9413, loss=0.5695]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 147/292 [00:24<00:23,  6.16batch/s, auc=0.9411, loss=0.5333]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9411, loss=0.5333]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 148/292 [00:24<00:23,  6.16batch/s, auc=0.9412, loss=0.4298]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 149/292 [00:24<00:23,  6.17batch/s, auc=0.9412, loss=0.4298]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 149/292 [00:24<00:23,  6.17batch/s, auc=0.9410, loss=0.6887]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.16batch/s, auc=0.9410, loss=0.6887]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.16batch/s, auc=0.9410, loss=0.5476]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9410, loss=0.5476]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9411, loss=0.4336]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9411, loss=0.4336]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9407, loss=0.8637]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9407, loss=0.8637]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9408, loss=0.3945]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9408, loss=0.3945]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9412, loss=0.3480]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9412, loss=0.3480]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.16batch/s, auc=0.9413, loss=0.3221]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.16batch/s, auc=0.9413, loss=0.3221]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.16batch/s, auc=0.9415, loss=0.3696]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.02batch/s, auc=0.9415, loss=0.3696]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.02batch/s, auc=0.9417, loss=0.4124]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 158/292 [00:26<00:22,  5.92batch/s, auc=0.9417, loss=0.4124]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 158/292 [00:26<00:22,  5.92batch/s, auc=0.9418, loss=0.4393]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 159/292 [00:26<00:22,  5.82batch/s, auc=0.9418, loss=0.4393]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 159/292 [00:26<00:22,  5.82batch/s, auc=0.9417, loss=0.6297]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▍    | 160/292 [00:26<00:22,  5.82batch/s, auc=0.9417, loss=0.6297]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▍    | 160/292 [00:26<00:22,  5.82batch/s, auc=0.9419, loss=0.3809]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 161/292 [00:26<00:22,  5.80batch/s, auc=0.9419, loss=0.3809]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 161/292 [00:26<00:22,  5.80batch/s, auc=0.9418, loss=0.5450]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 162/292 [00:26<00:22,  5.77batch/s, auc=0.9418, loss=0.5450]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 162/292 [00:27<00:22,  5.77batch/s, auc=0.9418, loss=0.5365]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 163/292 [00:27<00:22,  5.75batch/s, auc=0.9418, loss=0.5365]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 163/292 [00:27<00:22,  5.75batch/s, auc=0.9416, loss=0.5577]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.69batch/s, auc=0.9416, loss=0.5577]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 164/292 [00:27<00:22,  5.69batch/s, auc=0.9418, loss=0.4407]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.83batch/s, auc=0.9418, loss=0.4407]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.83batch/s, auc=0.9419, loss=0.4266]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.92batch/s, auc=0.9419, loss=0.4266]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.92batch/s, auc=0.9421, loss=0.4260]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 167/292 [00:27<00:20,  5.99batch/s, auc=0.9421, loss=0.4260]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 167/292 [00:27<00:20,  5.99batch/s, auc=0.9419, loss=0.6185]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.02batch/s, auc=0.9419, loss=0.6185]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.02batch/s, auc=0.9417, loss=0.5347]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.07batch/s, auc=0.9417, loss=0.5347]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.07batch/s, auc=0.9415, loss=0.6279]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.09batch/s, auc=0.9415, loss=0.6279]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 170/292 [00:28<00:20,  6.09batch/s, auc=0.9417, loss=0.4144]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.06batch/s, auc=0.9417, loss=0.4144]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.06batch/s, auc=0.9415, loss=0.4934]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9415, loss=0.4934]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.07batch/s, auc=0.9414, loss=0.4630]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.10batch/s, auc=0.9414, loss=0.4630]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.10batch/s, auc=0.9414, loss=0.5557]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.10batch/s, auc=0.9414, loss=0.5557]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.10batch/s, auc=0.9412, loss=0.5198]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.11batch/s, auc=0.9412, loss=0.5198]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.11batch/s, auc=0.9412, loss=0.4482]\u001b[A\n",
      "Training Epoch 20/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9412, loss=0.4482]\u001b[A\n",
      "Training Epoch 20/25:  60%|██████    | 176/292 [00:29<00:19,  6.10batch/s, auc=0.9412, loss=0.4493]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 177/292 [00:29<00:18,  6.12batch/s, auc=0.9412, loss=0.4493]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 177/292 [00:29<00:18,  6.12batch/s, auc=0.9412, loss=0.5651]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9412, loss=0.5651]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9413, loss=0.3935]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.12batch/s, auc=0.9413, loss=0.3935]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.12batch/s, auc=0.9412, loss=0.6589]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.12batch/s, auc=0.9412, loss=0.6589]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 180/292 [00:30<00:18,  6.12batch/s, auc=0.9414, loss=0.3873]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9414, loss=0.3873]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9416, loss=0.3502]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.11batch/s, auc=0.9416, loss=0.3502]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.11batch/s, auc=0.9418, loss=0.3553]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9418, loss=0.3553]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.11batch/s, auc=0.9416, loss=0.6790]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9416, loss=0.6790]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9415, loss=0.7396]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9415, loss=0.7396]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.12batch/s, auc=0.9414, loss=0.6236]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.13batch/s, auc=0.9414, loss=0.6236]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▎   | 186/292 [00:31<00:17,  6.13batch/s, auc=0.9414, loss=0.4381]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.12batch/s, auc=0.9414, loss=0.4381]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.12batch/s, auc=0.9415, loss=0.6440]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9415, loss=0.6440]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.11batch/s, auc=0.9416, loss=0.4659]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9416, loss=0.4659]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9415, loss=0.5915]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.10batch/s, auc=0.9415, loss=0.5915]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.10batch/s, auc=0.9415, loss=0.5089]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9415, loss=0.5089]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9416, loss=0.3813]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.07batch/s, auc=0.9416, loss=0.3813]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 192/292 [00:32<00:16,  6.07batch/s, auc=0.9415, loss=0.5831]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.9415, loss=0.5831]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.08batch/s, auc=0.9414, loss=0.5665]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9414, loss=0.5665]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.08batch/s, auc=0.9413, loss=0.6602]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9413, loss=0.6602]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.08batch/s, auc=0.9408, loss=1.0055]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.06batch/s, auc=0.9408, loss=1.0055]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.06batch/s, auc=0.9404, loss=0.9376]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9404, loss=0.9376]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.07batch/s, auc=0.9405, loss=0.5196]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.05batch/s, auc=0.9405, loss=0.5196]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 198/292 [00:33<00:15,  6.05batch/s, auc=0.9403, loss=0.6550]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.06batch/s, auc=0.9403, loss=0.6550]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.06batch/s, auc=0.9403, loss=0.5608]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9403, loss=0.5608]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.06batch/s, auc=0.9399, loss=0.9352]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.99batch/s, auc=0.9399, loss=0.9352]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.99batch/s, auc=0.9399, loss=0.4088]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.03batch/s, auc=0.9399, loss=0.4088]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.03batch/s, auc=0.9398, loss=0.6904]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.05batch/s, auc=0.9398, loss=0.6904]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.05batch/s, auc=0.9399, loss=0.4715]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.06batch/s, auc=0.9399, loss=0.4715]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.06batch/s, auc=0.9400, loss=0.4225]\u001b[A\n",
      "Training Epoch 20/25:  70%|███████   | 205/292 [00:33<00:14,  6.06batch/s, auc=0.9400, loss=0.4225]\u001b[A\n",
      "Training Epoch 20/25:  70%|███████   | 205/292 [00:34<00:14,  6.06batch/s, auc=0.9402, loss=0.4281]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 206/292 [00:34<00:14,  6.08batch/s, auc=0.9402, loss=0.4281]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 206/292 [00:34<00:14,  6.08batch/s, auc=0.9400, loss=0.5771]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 207/292 [00:34<00:13,  6.07batch/s, auc=0.9400, loss=0.5771]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 207/292 [00:34<00:13,  6.07batch/s, auc=0.9400, loss=0.4155]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 208/292 [00:34<00:13,  6.07batch/s, auc=0.9400, loss=0.4155]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 208/292 [00:34<00:13,  6.07batch/s, auc=0.9399, loss=0.5470]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.07batch/s, auc=0.9399, loss=0.5470]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.07batch/s, auc=0.9399, loss=0.5474]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.06batch/s, auc=0.9399, loss=0.5474]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.06batch/s, auc=0.9398, loss=0.5547]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 211/292 [00:34<00:13,  6.07batch/s, auc=0.9398, loss=0.5547]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.07batch/s, auc=0.9400, loss=0.3397]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.05batch/s, auc=0.9400, loss=0.3397]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 212/292 [00:35<00:13,  6.05batch/s, auc=0.9399, loss=0.5381]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.03batch/s, auc=0.9399, loss=0.5381]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.03batch/s, auc=0.9396, loss=0.7845]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.05batch/s, auc=0.9396, loss=0.7845]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 214/292 [00:35<00:12,  6.05batch/s, auc=0.9397, loss=0.4205]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.03batch/s, auc=0.9397, loss=0.4205]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▎  | 215/292 [00:35<00:12,  6.03batch/s, auc=0.9398, loss=0.4338]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.03batch/s, auc=0.9398, loss=0.4338]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 216/292 [00:35<00:12,  6.03batch/s, auc=0.9398, loss=0.7940]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 217/292 [00:35<00:12,  5.98batch/s, auc=0.9398, loss=0.7940]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 217/292 [00:36<00:12,  5.98batch/s, auc=0.9398, loss=0.4834]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.01batch/s, auc=0.9398, loss=0.4834]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▍  | 218/292 [00:36<00:12,  6.01batch/s, auc=0.9396, loss=0.7621]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.79batch/s, auc=0.9396, loss=0.7621]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.79batch/s, auc=0.9395, loss=0.5144]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.71batch/s, auc=0.9395, loss=0.5144]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.71batch/s, auc=0.9396, loss=0.4387]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.70batch/s, auc=0.9396, loss=0.4387]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.70batch/s, auc=0.9395, loss=0.6334]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.70batch/s, auc=0.9395, loss=0.6334]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.70batch/s, auc=0.9395, loss=0.6133]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.67batch/s, auc=0.9395, loss=0.6133]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.67batch/s, auc=0.9392, loss=0.6543]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.67batch/s, auc=0.9392, loss=0.6543]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.67batch/s, auc=0.9389, loss=0.7182]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.64batch/s, auc=0.9389, loss=0.7182]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.64batch/s, auc=0.9385, loss=1.1966]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.64batch/s, auc=0.9385, loss=1.1966]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.64batch/s, auc=0.9386, loss=0.4728]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.64batch/s, auc=0.9386, loss=0.4728]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.64batch/s, auc=0.9386, loss=0.4701]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.67batch/s, auc=0.9386, loss=0.4701]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.67batch/s, auc=0.9387, loss=0.5469]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.76batch/s, auc=0.9387, loss=0.5469]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.76batch/s, auc=0.9385, loss=0.8792]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.74batch/s, auc=0.9385, loss=0.8792]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.74batch/s, auc=0.9384, loss=0.4981]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.69batch/s, auc=0.9384, loss=0.4981]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.69batch/s, auc=0.9383, loss=0.6945]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.68batch/s, auc=0.9383, loss=0.6945]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.68batch/s, auc=0.9381, loss=0.6303]\u001b[A\n",
      "Training Epoch 20/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.65batch/s, auc=0.9381, loss=0.6303]\u001b[A\n",
      "Training Epoch 20/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.65batch/s, auc=0.9380, loss=0.6101]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 234/292 [00:38<00:10,  5.65batch/s, auc=0.9380, loss=0.6101]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 234/292 [00:39<00:10,  5.65batch/s, auc=0.9380, loss=0.4454]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 235/292 [00:39<00:09,  5.75batch/s, auc=0.9380, loss=0.4454]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 235/292 [00:39<00:09,  5.75batch/s, auc=0.9380, loss=0.5457]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 236/292 [00:39<00:09,  5.69batch/s, auc=0.9380, loss=0.5457]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 236/292 [00:39<00:09,  5.69batch/s, auc=0.9380, loss=0.5338]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 237/292 [00:39<00:09,  5.78batch/s, auc=0.9380, loss=0.5338]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 237/292 [00:39<00:09,  5.78batch/s, auc=0.9378, loss=0.5401]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.85batch/s, auc=0.9378, loss=0.5401]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.85batch/s, auc=0.9378, loss=0.4984]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.85batch/s, auc=0.9378, loss=0.4984]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.85batch/s, auc=0.9377, loss=0.7316]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.67batch/s, auc=0.9377, loss=0.7316]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.67batch/s, auc=0.9377, loss=0.5639]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.65batch/s, auc=0.9377, loss=0.5639]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.65batch/s, auc=0.9376, loss=0.5481]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.69batch/s, auc=0.9376, loss=0.5481]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.69batch/s, auc=0.9378, loss=0.3638]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9378, loss=0.3638]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9379, loss=0.4302]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.9379, loss=0.4302]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.73batch/s, auc=0.9377, loss=0.6154]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.81batch/s, auc=0.9377, loss=0.6154]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.81batch/s, auc=0.9379, loss=0.4258]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.86batch/s, auc=0.9379, loss=0.4258]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.86batch/s, auc=0.9380, loss=0.4358]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.91batch/s, auc=0.9380, loss=0.4358]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.91batch/s, auc=0.9382, loss=0.3654]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.82batch/s, auc=0.9382, loss=0.3654]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.82batch/s, auc=0.9381, loss=0.5355]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.89batch/s, auc=0.9381, loss=0.5355]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.89batch/s, auc=0.9384, loss=0.3194]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.94batch/s, auc=0.9384, loss=0.3194]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.94batch/s, auc=0.9384, loss=0.4210]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 251/292 [00:41<00:06,  5.98batch/s, auc=0.9384, loss=0.4210]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 251/292 [00:42<00:06,  5.98batch/s, auc=0.9386, loss=0.4110]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.87batch/s, auc=0.9386, loss=0.4110]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.87batch/s, auc=0.9387, loss=0.4645]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.91batch/s, auc=0.9387, loss=0.4645]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.91batch/s, auc=0.9388, loss=0.4265]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.73batch/s, auc=0.9388, loss=0.4265]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.73batch/s, auc=0.9389, loss=0.4015]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.56batch/s, auc=0.9389, loss=0.4015]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.56batch/s, auc=0.9389, loss=0.5752]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.62batch/s, auc=0.9389, loss=0.5752]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.62batch/s, auc=0.9389, loss=0.4322]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.60batch/s, auc=0.9389, loss=0.4322]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.60batch/s, auc=0.9388, loss=0.5322]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9388, loss=0.5322]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9388, loss=0.4672]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.71batch/s, auc=0.9388, loss=0.4672]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.71batch/s, auc=0.9389, loss=0.5780]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.52batch/s, auc=0.9389, loss=0.5780]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.52batch/s, auc=0.9388, loss=0.6436]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.64batch/s, auc=0.9388, loss=0.6436]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.64batch/s, auc=0.9389, loss=0.3981]\u001b[A\n",
      "Training Epoch 20/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.62batch/s, auc=0.9389, loss=0.3981]\u001b[A\n",
      "Training Epoch 20/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.62batch/s, auc=0.9388, loss=0.6472]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 263/292 [00:44<00:05,  5.73batch/s, auc=0.9388, loss=0.6472]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 263/292 [00:44<00:05,  5.73batch/s, auc=0.9390, loss=0.3330]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 264/292 [00:44<00:04,  5.78batch/s, auc=0.9390, loss=0.3330]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 264/292 [00:44<00:04,  5.78batch/s, auc=0.9389, loss=0.6257]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 265/292 [00:44<00:04,  5.73batch/s, auc=0.9389, loss=0.6257]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 265/292 [00:44<00:04,  5.73batch/s, auc=0.9387, loss=0.8003]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 266/292 [00:44<00:04,  5.68batch/s, auc=0.9387, loss=0.8003]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 266/292 [00:44<00:04,  5.68batch/s, auc=0.9389, loss=0.3556]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.74batch/s, auc=0.9389, loss=0.3556]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.74batch/s, auc=0.9388, loss=0.4850]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.58batch/s, auc=0.9388, loss=0.4850]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.58batch/s, auc=0.9389, loss=0.5376]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.57batch/s, auc=0.9389, loss=0.5376]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.57batch/s, auc=0.9389, loss=0.4364]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.54batch/s, auc=0.9389, loss=0.4364]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.54batch/s, auc=0.9388, loss=0.6442]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.51batch/s, auc=0.9388, loss=0.6442]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.51batch/s, auc=0.9389, loss=0.4145]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.52batch/s, auc=0.9389, loss=0.4145]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.52batch/s, auc=0.9390, loss=0.3834]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.60batch/s, auc=0.9390, loss=0.3834]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.60batch/s, auc=0.9391, loss=0.3404]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 274/292 [00:45<00:03,  5.59batch/s, auc=0.9391, loss=0.3404]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.59batch/s, auc=0.9389, loss=0.8863]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.69batch/s, auc=0.9389, loss=0.8863]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.69batch/s, auc=0.9390, loss=0.4919]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.58batch/s, auc=0.9390, loss=0.4919]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.58batch/s, auc=0.9392, loss=0.4785]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.69batch/s, auc=0.9392, loss=0.4785]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.69batch/s, auc=0.9391, loss=0.5258]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.66batch/s, auc=0.9391, loss=0.5258]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.66batch/s, auc=0.9392, loss=0.3939]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.62batch/s, auc=0.9392, loss=0.3939]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.62batch/s, auc=0.9393, loss=0.4035]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.64batch/s, auc=0.9393, loss=0.4035]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.64batch/s, auc=0.9392, loss=0.5797]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.62batch/s, auc=0.9392, loss=0.5797]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.62batch/s, auc=0.9393, loss=0.3788]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.61batch/s, auc=0.9393, loss=0.3788]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.61batch/s, auc=0.9394, loss=0.4230]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.58batch/s, auc=0.9394, loss=0.4230]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.58batch/s, auc=0.9395, loss=0.4418]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.68batch/s, auc=0.9395, loss=0.4418]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.68batch/s, auc=0.9396, loss=0.3828]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.66batch/s, auc=0.9396, loss=0.3828]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.66batch/s, auc=0.9396, loss=0.5047]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.77batch/s, auc=0.9396, loss=0.5047]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.77batch/s, auc=0.9398, loss=0.3297]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.9398, loss=0.3297]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.83batch/s, auc=0.9398, loss=0.4584]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9398, loss=0.4584]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.86batch/s, auc=0.9397, loss=0.5877]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9397, loss=0.5877]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.89batch/s, auc=0.9398, loss=0.3278]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9398, loss=0.3278]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.91batch/s, auc=0.9398, loss=0.5250]\u001b[A\n",
      "Training Epoch 20/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.91batch/s, auc=0.9398, loss=0.5250]\u001b[A\n",
      "Training Epoch 20/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9395, loss=1.7292]\u001b[A\n",
      "Training Epoch 20/25: 100%|██████████| 292/292 [00:49<00:00,  5.94batch/s, auc=0.9395, loss=1.7292]\u001b[A\n",
      "Epochs:  80%|████████  | 20/25 [18:18<04:34, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25] Train Loss: 0.5249 | Train AUROC: 0.9395 Val Loss: 0.6689 | Val AUROC: 0.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 21/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9508, loss=0.4348]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 1/292 [00:00<04:44,  1.02batch/s, auc=0.9508, loss=0.4348]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 1/292 [00:01<04:44,  1.02batch/s, auc=0.9260, loss=0.6625]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.9260, loss=0.6625]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.9488, loss=0.3644]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 3/292 [00:01<01:39,  2.89batch/s, auc=0.9488, loss=0.3644]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 3/292 [00:01<01:39,  2.89batch/s, auc=0.9461, loss=0.4965]\u001b[A\n",
      "Training Epoch 21/25:   1%|▏         | 4/292 [00:01<01:21,  3.52batch/s, auc=0.9461, loss=0.4965]\u001b[A\n",
      "Training Epoch 21/25:   1%|▏         | 4/292 [00:01<01:21,  3.52batch/s, auc=0.9381, loss=0.6319]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 5/292 [00:01<01:08,  4.20batch/s, auc=0.9381, loss=0.6319]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 5/292 [00:01<01:08,  4.20batch/s, auc=0.9488, loss=0.3142]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 6/292 [00:01<01:02,  4.56batch/s, auc=0.9488, loss=0.3142]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 6/292 [00:01<01:02,  4.56batch/s, auc=0.9496, loss=0.4748]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 7/292 [00:02<00:57,  4.93batch/s, auc=0.9496, loss=0.4748]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 7/292 [00:02<00:57,  4.93batch/s, auc=0.9530, loss=0.3560]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 8/292 [00:02<00:53,  5.31batch/s, auc=0.9530, loss=0.3560]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 8/292 [00:02<00:53,  5.31batch/s, auc=0.9542, loss=0.4129]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 9/292 [00:02<00:50,  5.58batch/s, auc=0.9542, loss=0.4129]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 9/292 [00:02<00:50,  5.58batch/s, auc=0.9564, loss=0.3694]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 10/292 [00:02<00:51,  5.43batch/s, auc=0.9564, loss=0.3694]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 10/292 [00:02<00:51,  5.43batch/s, auc=0.9577, loss=0.3509]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 11/292 [00:02<00:49,  5.68batch/s, auc=0.9577, loss=0.3509]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 11/292 [00:02<00:49,  5.68batch/s, auc=0.9510, loss=0.8956]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 12/292 [00:02<00:47,  5.88batch/s, auc=0.9510, loss=0.8956]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 12/292 [00:02<00:47,  5.88batch/s, auc=0.9458, loss=0.7887]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 13/292 [00:02<00:46,  5.99batch/s, auc=0.9458, loss=0.7887]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 13/292 [00:03<00:46,  5.99batch/s, auc=0.9466, loss=0.3942]\u001b[A\n",
      "Training Epoch 21/25:   5%|▍         | 14/292 [00:03<00:45,  6.10batch/s, auc=0.9466, loss=0.3942]\u001b[A\n",
      "Training Epoch 21/25:   5%|▍         | 14/292 [00:03<00:45,  6.10batch/s, auc=0.9440, loss=0.7337]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9440, loss=0.7337]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9454, loss=0.4702]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 16/292 [00:03<00:44,  6.24batch/s, auc=0.9454, loss=0.4702]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 16/292 [00:03<00:44,  6.24batch/s, auc=0.9490, loss=0.2819]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9490, loss=0.2819]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 17/292 [00:03<00:43,  6.27batch/s, auc=0.9504, loss=0.3229]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9504, loss=0.3229]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9507, loss=0.4148]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 19/292 [00:03<00:43,  6.31batch/s, auc=0.9507, loss=0.4148]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 19/292 [00:04<00:43,  6.31batch/s, auc=0.9501, loss=0.5922]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9501, loss=0.5922]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9511, loss=0.3375]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.9511, loss=0.3375]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 21/292 [00:04<00:42,  6.33batch/s, auc=0.9503, loss=0.6527]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 22/292 [00:04<00:42,  6.34batch/s, auc=0.9503, loss=0.6527]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 22/292 [00:04<00:42,  6.34batch/s, auc=0.9502, loss=0.4516]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.9502, loss=0.4516]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 23/292 [00:04<00:42,  6.35batch/s, auc=0.9504, loss=0.4426]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9504, loss=0.4426]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 24/292 [00:04<00:42,  6.31batch/s, auc=0.9495, loss=0.4909]\u001b[A\n",
      "Training Epoch 21/25:   9%|▊         | 25/292 [00:04<00:42,  6.32batch/s, auc=0.9495, loss=0.4909]\u001b[A\n",
      "Training Epoch 21/25:   9%|▊         | 25/292 [00:05<00:42,  6.32batch/s, auc=0.9487, loss=0.5562]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 26/292 [00:05<00:42,  6.30batch/s, auc=0.9487, loss=0.5562]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 26/292 [00:05<00:42,  6.30batch/s, auc=0.9498, loss=0.3232]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9498, loss=0.3232]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 27/292 [00:05<00:41,  6.32batch/s, auc=0.9497, loss=0.4440]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9497, loss=0.4440]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 28/292 [00:05<00:41,  6.32batch/s, auc=0.9481, loss=0.6402]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 29/292 [00:05<00:41,  6.33batch/s, auc=0.9481, loss=0.6402]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 29/292 [00:05<00:41,  6.33batch/s, auc=0.9483, loss=0.3933]\u001b[A\n",
      "Training Epoch 21/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9483, loss=0.3933]\u001b[A\n",
      "Training Epoch 21/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9475, loss=0.5618]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9475, loss=0.5618]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 31/292 [00:05<00:41,  6.34batch/s, auc=0.9472, loss=0.5279]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 32/292 [00:05<00:41,  6.33batch/s, auc=0.9472, loss=0.5279]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 32/292 [00:06<00:41,  6.33batch/s, auc=0.9469, loss=0.4687]\u001b[A\n",
      "Training Epoch 21/25:  11%|█▏        | 33/292 [00:06<00:40,  6.34batch/s, auc=0.9469, loss=0.4687]\u001b[A\n",
      "Training Epoch 21/25:  11%|█▏        | 33/292 [00:06<00:40,  6.34batch/s, auc=0.9468, loss=0.4389]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9468, loss=0.4389]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 34/292 [00:06<00:40,  6.34batch/s, auc=0.9455, loss=0.6705]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9455, loss=0.6705]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 35/292 [00:06<00:40,  6.34batch/s, auc=0.9458, loss=0.3949]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9458, loss=0.3949]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 36/292 [00:06<00:40,  6.34batch/s, auc=0.9455, loss=0.5407]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9455, loss=0.5407]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 37/292 [00:06<00:40,  6.31batch/s, auc=0.9463, loss=0.3737]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 38/292 [00:06<00:40,  6.32batch/s, auc=0.9463, loss=0.3737]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 38/292 [00:07<00:40,  6.32batch/s, auc=0.9461, loss=0.4574]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9461, loss=0.4574]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9471, loss=0.3820]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▎        | 40/292 [00:07<00:39,  6.30batch/s, auc=0.9471, loss=0.3820]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▎        | 40/292 [00:07<00:39,  6.30batch/s, auc=0.9475, loss=0.4606]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 41/292 [00:07<00:39,  6.31batch/s, auc=0.9475, loss=0.4606]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 41/292 [00:07<00:39,  6.31batch/s, auc=0.9470, loss=0.5415]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9470, loss=0.5415]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9469, loss=0.4759]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9469, loss=0.4759]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9475, loss=0.3495]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 44/292 [00:07<00:39,  6.26batch/s, auc=0.9475, loss=0.3495]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 44/292 [00:08<00:39,  6.26batch/s, auc=0.9473, loss=0.4733]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 45/292 [00:08<00:39,  6.24batch/s, auc=0.9473, loss=0.4733]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 45/292 [00:08<00:39,  6.24batch/s, auc=0.9468, loss=0.7029]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.9468, loss=0.7029]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 46/292 [00:08<00:39,  6.25batch/s, auc=0.9469, loss=0.4358]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9469, loss=0.4358]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 47/292 [00:08<00:39,  6.25batch/s, auc=0.9463, loss=0.5957]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9463, loss=0.5957]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▋        | 48/292 [00:08<00:38,  6.27batch/s, auc=0.9454, loss=0.6914]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 49/292 [00:08<00:38,  6.25batch/s, auc=0.9454, loss=0.6914]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 49/292 [00:08<00:38,  6.25batch/s, auc=0.9468, loss=0.2701]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 50/292 [00:08<00:38,  6.23batch/s, auc=0.9468, loss=0.2701]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 50/292 [00:09<00:38,  6.23batch/s, auc=0.9457, loss=0.7226]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 51/292 [00:09<00:38,  6.25batch/s, auc=0.9457, loss=0.7226]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 51/292 [00:09<00:38,  6.25batch/s, auc=0.9449, loss=0.6842]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 52/292 [00:09<00:38,  6.25batch/s, auc=0.9449, loss=0.6842]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 52/292 [00:09<00:38,  6.25batch/s, auc=0.9454, loss=0.3910]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 53/292 [00:09<00:38,  6.23batch/s, auc=0.9454, loss=0.3910]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 53/292 [00:09<00:38,  6.23batch/s, auc=0.9460, loss=0.3238]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 54/292 [00:09<00:38,  6.24batch/s, auc=0.9460, loss=0.3238]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 54/292 [00:09<00:38,  6.24batch/s, auc=0.9462, loss=0.4118]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 55/292 [00:09<00:37,  6.25batch/s, auc=0.9462, loss=0.4118]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 55/292 [00:09<00:37,  6.25batch/s, auc=0.9464, loss=0.3974]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 56/292 [00:09<00:37,  6.22batch/s, auc=0.9464, loss=0.3974]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 56/292 [00:09<00:37,  6.22batch/s, auc=0.9461, loss=0.5151]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 57/292 [00:09<00:37,  6.24batch/s, auc=0.9461, loss=0.5151]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 57/292 [00:10<00:37,  6.24batch/s, auc=0.9463, loss=0.3863]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 58/292 [00:10<00:37,  6.25batch/s, auc=0.9463, loss=0.3863]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 58/292 [00:10<00:37,  6.25batch/s, auc=0.9461, loss=0.6046]\u001b[A\n",
      "Training Epoch 21/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9461, loss=0.6046]\u001b[A\n",
      "Training Epoch 21/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9456, loss=0.5422]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 60/292 [00:10<00:37,  6.26batch/s, auc=0.9456, loss=0.5422]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 60/292 [00:10<00:37,  6.26batch/s, auc=0.9461, loss=0.3627]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 61/292 [00:10<00:36,  6.26batch/s, auc=0.9461, loss=0.3627]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 61/292 [00:10<00:36,  6.26batch/s, auc=0.9457, loss=0.5380]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 62/292 [00:10<00:36,  6.26batch/s, auc=0.9457, loss=0.5380]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 62/292 [00:10<00:36,  6.26batch/s, auc=0.9465, loss=0.3290]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 63/292 [00:10<00:37,  6.18batch/s, auc=0.9465, loss=0.3290]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 63/292 [00:11<00:37,  6.18batch/s, auc=0.9467, loss=0.4814]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 64/292 [00:11<00:36,  6.21batch/s, auc=0.9467, loss=0.4814]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 64/292 [00:11<00:36,  6.21batch/s, auc=0.9459, loss=0.9016]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 65/292 [00:11<00:36,  6.24batch/s, auc=0.9459, loss=0.9016]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 65/292 [00:11<00:36,  6.24batch/s, auc=0.9458, loss=0.4846]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9458, loss=0.4846]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 66/292 [00:11<00:36,  6.26batch/s, auc=0.9457, loss=0.4648]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9457, loss=0.4648]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 67/292 [00:11<00:35,  6.28batch/s, auc=0.9458, loss=0.3471]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9458, loss=0.3471]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9456, loss=0.4524]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▎       | 69/292 [00:11<00:35,  6.27batch/s, auc=0.9456, loss=0.4524]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▎       | 69/292 [00:12<00:35,  6.27batch/s, auc=0.9447, loss=0.8445]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9447, loss=0.8445]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 70/292 [00:12<00:35,  6.27batch/s, auc=0.9449, loss=0.3847]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9449, loss=0.3847]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 71/292 [00:12<00:35,  6.27batch/s, auc=0.9450, loss=0.3728]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9450, loss=0.3728]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▍       | 72/292 [00:12<00:35,  6.27batch/s, auc=0.9451, loss=0.3963]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9451, loss=0.3963]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9453, loss=0.3843]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9453, loss=0.3843]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9452, loss=0.6132]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 75/292 [00:12<00:34,  6.27batch/s, auc=0.9452, loss=0.6132]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 75/292 [00:13<00:34,  6.27batch/s, auc=0.9451, loss=0.4703]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 76/292 [00:13<00:35,  6.15batch/s, auc=0.9451, loss=0.4703]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 76/292 [00:13<00:35,  6.15batch/s, auc=0.9451, loss=0.5090]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9451, loss=0.5090]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9445, loss=0.8727]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 78/292 [00:13<00:35,  6.09batch/s, auc=0.9445, loss=0.8727]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 78/292 [00:13<00:35,  6.09batch/s, auc=0.9441, loss=0.6913]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 79/292 [00:13<00:34,  6.12batch/s, auc=0.9441, loss=0.6913]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 79/292 [00:13<00:34,  6.12batch/s, auc=0.9432, loss=0.8714]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 80/292 [00:13<00:34,  6.16batch/s, auc=0.9432, loss=0.8714]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 80/292 [00:13<00:34,  6.16batch/s, auc=0.9425, loss=0.8474]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 81/292 [00:13<00:34,  6.18batch/s, auc=0.9425, loss=0.8474]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 81/292 [00:13<00:34,  6.18batch/s, auc=0.9424, loss=0.5316]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 82/292 [00:13<00:33,  6.20batch/s, auc=0.9424, loss=0.5316]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 82/292 [00:14<00:33,  6.20batch/s, auc=0.9430, loss=0.3482]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9430, loss=0.3482]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9433, loss=0.4140]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9433, loss=0.4140]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9438, loss=0.3828]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 85/292 [00:14<00:33,  6.18batch/s, auc=0.9438, loss=0.3828]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 85/292 [00:14<00:33,  6.18batch/s, auc=0.9444, loss=0.3426]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 86/292 [00:14<00:33,  6.20batch/s, auc=0.9444, loss=0.3426]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 86/292 [00:14<00:33,  6.20batch/s, auc=0.9448, loss=0.3913]\u001b[A\n",
      "Training Epoch 21/25:  30%|██▉       | 87/292 [00:14<00:32,  6.21batch/s, auc=0.9448, loss=0.3913]\u001b[A\n",
      "Training Epoch 21/25:  30%|██▉       | 87/292 [00:14<00:32,  6.21batch/s, auc=0.9448, loss=0.5014]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 88/292 [00:14<00:33,  6.08batch/s, auc=0.9448, loss=0.5014]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 88/292 [00:15<00:33,  6.08batch/s, auc=0.9445, loss=0.5788]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 89/292 [00:15<00:33,  6.13batch/s, auc=0.9445, loss=0.5788]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 89/292 [00:15<00:33,  6.13batch/s, auc=0.9447, loss=0.5027]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 90/292 [00:15<00:33,  6.10batch/s, auc=0.9447, loss=0.5027]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 90/292 [00:15<00:33,  6.10batch/s, auc=0.9443, loss=0.6551]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 91/292 [00:15<00:32,  6.13batch/s, auc=0.9443, loss=0.6551]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 91/292 [00:15<00:32,  6.13batch/s, auc=0.9441, loss=0.5467]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 92/292 [00:15<00:32,  6.13batch/s, auc=0.9441, loss=0.5467]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 92/292 [00:15<00:32,  6.13batch/s, auc=0.9439, loss=0.5643]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 93/292 [00:15<00:32,  6.15batch/s, auc=0.9439, loss=0.5643]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 93/292 [00:15<00:32,  6.15batch/s, auc=0.9431, loss=0.7696]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 94/292 [00:15<00:32,  6.17batch/s, auc=0.9431, loss=0.7696]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 94/292 [00:16<00:32,  6.17batch/s, auc=0.9434, loss=0.4471]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 95/292 [00:16<00:31,  6.17batch/s, auc=0.9434, loss=0.4471]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 95/292 [00:16<00:31,  6.17batch/s, auc=0.9431, loss=0.6892]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 96/292 [00:16<00:31,  6.18batch/s, auc=0.9431, loss=0.6892]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 96/292 [00:16<00:31,  6.18batch/s, auc=0.9426, loss=0.7821]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 97/292 [00:16<00:31,  6.20batch/s, auc=0.9426, loss=0.7821]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 97/292 [00:16<00:31,  6.20batch/s, auc=0.9427, loss=0.4871]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▎      | 98/292 [00:16<00:31,  6.15batch/s, auc=0.9427, loss=0.4871]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▎      | 98/292 [00:16<00:31,  6.15batch/s, auc=0.9424, loss=0.4967]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 99/292 [00:16<00:31,  6.17batch/s, auc=0.9424, loss=0.4967]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 99/292 [00:16<00:31,  6.17batch/s, auc=0.9423, loss=0.5003]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 100/292 [00:16<00:31,  6.17batch/s, auc=0.9423, loss=0.5003]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 100/292 [00:17<00:31,  6.17batch/s, auc=0.9423, loss=0.4629]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 101/292 [00:17<00:30,  6.18batch/s, auc=0.9423, loss=0.4629]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 101/292 [00:17<00:30,  6.18batch/s, auc=0.9423, loss=0.5652]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 102/292 [00:17<00:30,  6.20batch/s, auc=0.9423, loss=0.5652]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 102/292 [00:17<00:30,  6.20batch/s, auc=0.9426, loss=0.5001]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▌      | 103/292 [00:17<00:30,  6.16batch/s, auc=0.9426, loss=0.5001]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▌      | 103/292 [00:17<00:30,  6.16batch/s, auc=0.9423, loss=0.5798]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9423, loss=0.5798]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 104/292 [00:17<00:30,  6.19batch/s, auc=0.9425, loss=0.4772]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 105/292 [00:17<00:30,  6.20batch/s, auc=0.9425, loss=0.4772]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 105/292 [00:17<00:30,  6.20batch/s, auc=0.9425, loss=0.5086]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▋      | 106/292 [00:17<00:29,  6.21batch/s, auc=0.9425, loss=0.5086]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▋      | 106/292 [00:18<00:29,  6.21batch/s, auc=0.9421, loss=0.5487]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 107/292 [00:18<00:29,  6.21batch/s, auc=0.9421, loss=0.5487]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 107/292 [00:18<00:29,  6.21batch/s, auc=0.9422, loss=0.4890]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9422, loss=0.4890]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 108/292 [00:18<00:29,  6.20batch/s, auc=0.9417, loss=0.6211]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9417, loss=0.6211]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9417, loss=0.4601]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9417, loss=0.4601]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9416, loss=0.5422]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9416, loss=0.5422]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9414, loss=0.5177]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 112/292 [00:18<00:28,  6.21batch/s, auc=0.9414, loss=0.5177]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 112/292 [00:19<00:28,  6.21batch/s, auc=0.9414, loss=0.4993]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9414, loss=0.4993]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9413, loss=0.4691]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 114/292 [00:19<00:28,  6.20batch/s, auc=0.9413, loss=0.4691]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 114/292 [00:19<00:28,  6.20batch/s, auc=0.9412, loss=0.5723]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 115/292 [00:19<00:29,  6.09batch/s, auc=0.9412, loss=0.5723]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 115/292 [00:19<00:29,  6.09batch/s, auc=0.9415, loss=0.3845]\u001b[A\n",
      "Training Epoch 21/25:  40%|███▉      | 116/292 [00:19<00:28,  6.12batch/s, auc=0.9415, loss=0.3845]\u001b[A\n",
      "Training Epoch 21/25:  40%|███▉      | 116/292 [00:19<00:28,  6.12batch/s, auc=0.9412, loss=0.6607]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 117/292 [00:19<00:28,  6.15batch/s, auc=0.9412, loss=0.6607]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 117/292 [00:19<00:28,  6.15batch/s, auc=0.9409, loss=0.5324]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 118/292 [00:19<00:28,  6.17batch/s, auc=0.9409, loss=0.5324]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 118/292 [00:19<00:28,  6.17batch/s, auc=0.9412, loss=0.4427]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 119/292 [00:19<00:27,  6.18batch/s, auc=0.9412, loss=0.4427]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 119/292 [00:20<00:27,  6.18batch/s, auc=0.9414, loss=0.3980]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 120/292 [00:20<00:27,  6.18batch/s, auc=0.9414, loss=0.3980]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 120/292 [00:20<00:27,  6.18batch/s, auc=0.9417, loss=0.4565]\u001b[A\n",
      "Training Epoch 21/25:  41%|████▏     | 121/292 [00:20<00:27,  6.16batch/s, auc=0.9417, loss=0.4565]\u001b[A\n",
      "Training Epoch 21/25:  41%|████▏     | 121/292 [00:20<00:27,  6.16batch/s, auc=0.9415, loss=0.5328]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 122/292 [00:20<00:27,  6.17batch/s, auc=0.9415, loss=0.5328]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 122/292 [00:20<00:27,  6.17batch/s, auc=0.9414, loss=0.7751]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 123/292 [00:20<00:27,  6.18batch/s, auc=0.9414, loss=0.7751]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 123/292 [00:20<00:27,  6.18batch/s, auc=0.9413, loss=0.4940]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9413, loss=0.4940]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9410, loss=0.7651]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 125/292 [00:20<00:27,  6.18batch/s, auc=0.9410, loss=0.7651]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 125/292 [00:21<00:27,  6.18batch/s, auc=0.9414, loss=0.3352]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9414, loss=0.3352]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 126/292 [00:21<00:26,  6.19batch/s, auc=0.9414, loss=0.5361]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9414, loss=0.5361]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 127/292 [00:21<00:26,  6.19batch/s, auc=0.9416, loss=0.3674]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9416, loss=0.3674]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 128/292 [00:21<00:26,  6.19batch/s, auc=0.9414, loss=0.4914]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 129/292 [00:21<00:26,  6.13batch/s, auc=0.9414, loss=0.4914]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 129/292 [00:21<00:26,  6.13batch/s, auc=0.9414, loss=0.4545]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 130/292 [00:21<00:26,  6.16batch/s, auc=0.9414, loss=0.4545]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 130/292 [00:21<00:26,  6.16batch/s, auc=0.9413, loss=0.5962]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 131/292 [00:21<00:26,  6.17batch/s, auc=0.9413, loss=0.5962]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 131/292 [00:22<00:26,  6.17batch/s, auc=0.9415, loss=0.4488]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9415, loss=0.4488]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▌     | 132/292 [00:22<00:25,  6.16batch/s, auc=0.9416, loss=0.5324]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9416, loss=0.5324]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 133/292 [00:22<00:25,  6.16batch/s, auc=0.9415, loss=0.4292]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9415, loss=0.4292]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 134/292 [00:22<00:25,  6.17batch/s, auc=0.9414, loss=0.6747]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9414, loss=0.6747]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9412, loss=0.5077]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9412, loss=0.5077]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9410, loss=0.5682]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 137/292 [00:22<00:25,  6.12batch/s, auc=0.9410, loss=0.5682]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 137/292 [00:23<00:25,  6.12batch/s, auc=0.9411, loss=0.5882]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 138/292 [00:23<00:25,  6.15batch/s, auc=0.9411, loss=0.5882]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 138/292 [00:23<00:25,  6.15batch/s, auc=0.9413, loss=0.3560]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 139/292 [00:23<00:24,  6.16batch/s, auc=0.9413, loss=0.3560]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 139/292 [00:23<00:24,  6.16batch/s, auc=0.9411, loss=0.6261]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9411, loss=0.6261]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 140/292 [00:23<00:24,  6.16batch/s, auc=0.9410, loss=0.5339]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9410, loss=0.5339]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 141/292 [00:23<00:24,  6.16batch/s, auc=0.9407, loss=0.6386]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▊     | 142/292 [00:23<00:24,  6.16batch/s, auc=0.9407, loss=0.6386]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▊     | 142/292 [00:23<00:24,  6.16batch/s, auc=0.9408, loss=0.3899]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 143/292 [00:23<00:24,  6.15batch/s, auc=0.9408, loss=0.3899]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 143/292 [00:24<00:24,  6.15batch/s, auc=0.9410, loss=0.4282]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 144/292 [00:24<00:24,  6.15batch/s, auc=0.9410, loss=0.4282]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 144/292 [00:24<00:24,  6.15batch/s, auc=0.9411, loss=0.4653]\u001b[A\n",
      "Training Epoch 21/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9411, loss=0.4653]\u001b[A\n",
      "Training Epoch 21/25:  50%|████▉     | 145/292 [00:24<00:23,  6.14batch/s, auc=0.9414, loss=0.4092]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9414, loss=0.4092]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9414, loss=0.6084]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 147/292 [00:24<00:23,  6.12batch/s, auc=0.9414, loss=0.6084]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 147/292 [00:24<00:23,  6.12batch/s, auc=0.9413, loss=0.5218]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9413, loss=0.5218]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 148/292 [00:24<00:23,  6.14batch/s, auc=0.9413, loss=0.3908]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 149/292 [00:24<00:23,  6.14batch/s, auc=0.9413, loss=0.3908]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 149/292 [00:25<00:23,  6.14batch/s, auc=0.9411, loss=0.6648]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.15batch/s, auc=0.9411, loss=0.6648]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.15batch/s, auc=0.9413, loss=0.4491]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9413, loss=0.4491]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9411, loss=0.6912]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9411, loss=0.6912]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.17batch/s, auc=0.9413, loss=0.3326]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9413, loss=0.3326]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.17batch/s, auc=0.9414, loss=0.4285]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9414, loss=0.4285]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.17batch/s, auc=0.9412, loss=0.5145]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.17batch/s, auc=0.9412, loss=0.5145]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.17batch/s, auc=0.9416, loss=0.2722]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.18batch/s, auc=0.9416, loss=0.2722]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.18batch/s, auc=0.9412, loss=0.8904]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.18batch/s, auc=0.9412, loss=0.8904]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.18batch/s, auc=0.9416, loss=0.2602]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.17batch/s, auc=0.9416, loss=0.2602]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.17batch/s, auc=0.9418, loss=0.3525]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.14batch/s, auc=0.9418, loss=0.3525]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.14batch/s, auc=0.9420, loss=0.3770]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.14batch/s, auc=0.9420, loss=0.3770]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.14batch/s, auc=0.9421, loss=0.4228]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9421, loss=0.4228]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9420, loss=0.5431]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.13batch/s, auc=0.9420, loss=0.5431]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.13batch/s, auc=0.9423, loss=0.3299]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9423, loss=0.3299]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9423, loss=0.5244]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9423, loss=0.5244]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.14batch/s, auc=0.9420, loss=0.7093]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9420, loss=0.7093]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.14batch/s, auc=0.9418, loss=0.8691]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.14batch/s, auc=0.9418, loss=0.8691]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.14batch/s, auc=0.9419, loss=0.4200]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.9419, loss=0.4200]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.15batch/s, auc=0.9421, loss=0.4051]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.09batch/s, auc=0.9421, loss=0.4051]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.09batch/s, auc=0.9422, loss=0.3315]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.09batch/s, auc=0.9422, loss=0.3315]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.09batch/s, auc=0.9419, loss=0.9138]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.10batch/s, auc=0.9419, loss=0.9138]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.10batch/s, auc=0.9417, loss=0.6080]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9417, loss=0.6080]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9416, loss=0.5730]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.09batch/s, auc=0.9416, loss=0.5730]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.09batch/s, auc=0.9414, loss=0.5155]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.11batch/s, auc=0.9414, loss=0.5155]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.11batch/s, auc=0.9410, loss=0.7219]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.12batch/s, auc=0.9410, loss=0.7219]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.12batch/s, auc=0.9413, loss=0.3499]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.12batch/s, auc=0.9413, loss=0.3499]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.12batch/s, auc=0.9410, loss=0.6001]\u001b[A\n",
      "Training Epoch 21/25:  60%|██████    | 176/292 [00:29<00:18,  6.13batch/s, auc=0.9410, loss=0.6001]\u001b[A\n",
      "Training Epoch 21/25:  60%|██████    | 176/292 [00:29<00:18,  6.13batch/s, auc=0.9408, loss=0.6404]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9408, loss=0.6404]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9407, loss=0.6545]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9407, loss=0.6545]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 178/292 [00:29<00:18,  6.11batch/s, auc=0.9406, loss=0.7461]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9406, loss=0.7461]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9406, loss=0.4734]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.12batch/s, auc=0.9406, loss=0.4734]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 180/292 [00:30<00:18,  6.12batch/s, auc=0.9406, loss=0.5523]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9406, loss=0.5523]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.11batch/s, auc=0.9407, loss=0.4226]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9407, loss=0.4226]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.10batch/s, auc=0.9409, loss=0.4198]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.07batch/s, auc=0.9409, loss=0.4198]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.07batch/s, auc=0.9411, loss=0.3637]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.04batch/s, auc=0.9411, loss=0.3637]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.04batch/s, auc=0.9411, loss=0.5545]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9411, loss=0.5545]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9411, loss=0.4985]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.08batch/s, auc=0.9411, loss=0.4985]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▎   | 186/292 [00:31<00:17,  6.08batch/s, auc=0.9412, loss=0.4106]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.07batch/s, auc=0.9412, loss=0.4106]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.07batch/s, auc=0.9415, loss=0.3807]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.09batch/s, auc=0.9415, loss=0.3807]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.09batch/s, auc=0.9416, loss=0.4428]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.09batch/s, auc=0.9416, loss=0.4428]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.09batch/s, auc=0.9414, loss=0.6855]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.09batch/s, auc=0.9414, loss=0.6855]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.09batch/s, auc=0.9415, loss=0.4561]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.10batch/s, auc=0.9415, loss=0.4561]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.10batch/s, auc=0.9414, loss=0.6720]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9414, loss=0.6720]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 192/292 [00:32<00:16,  6.09batch/s, auc=0.9414, loss=0.4073]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.10batch/s, auc=0.9414, loss=0.4073]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.10batch/s, auc=0.9414, loss=0.6090]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9414, loss=0.6090]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.09batch/s, auc=0.9414, loss=0.5148]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.09batch/s, auc=0.9414, loss=0.5148]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.09batch/s, auc=0.9415, loss=0.4053]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.09batch/s, auc=0.9415, loss=0.4053]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.09batch/s, auc=0.9416, loss=0.4093]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.08batch/s, auc=0.9416, loss=0.4093]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.08batch/s, auc=0.9416, loss=0.4724]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.09batch/s, auc=0.9416, loss=0.4724]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 198/292 [00:33<00:15,  6.09batch/s, auc=0.9413, loss=0.5780]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.09batch/s, auc=0.9413, loss=0.5780]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.09batch/s, auc=0.9412, loss=0.4838]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9412, loss=0.4838]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9410, loss=0.6189]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.08batch/s, auc=0.9410, loss=0.6189]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.08batch/s, auc=0.9412, loss=0.4466]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9412, loss=0.4466]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.08batch/s, auc=0.9408, loss=0.7449]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.08batch/s, auc=0.9408, loss=0.7449]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 203/292 [00:33<00:14,  6.08batch/s, auc=0.9409, loss=0.4182]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 204/292 [00:33<00:14,  6.07batch/s, auc=0.9409, loss=0.4182]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 204/292 [00:34<00:14,  6.07batch/s, auc=0.9409, loss=0.5910]\u001b[A\n",
      "Training Epoch 21/25:  70%|███████   | 205/292 [00:34<00:14,  6.07batch/s, auc=0.9409, loss=0.5910]\u001b[A\n",
      "Training Epoch 21/25:  70%|███████   | 205/292 [00:34<00:14,  6.07batch/s, auc=0.9411, loss=0.3313]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 206/292 [00:34<00:14,  6.07batch/s, auc=0.9411, loss=0.3313]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 206/292 [00:34<00:14,  6.07batch/s, auc=0.9411, loss=0.4225]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 207/292 [00:34<00:14,  6.07batch/s, auc=0.9411, loss=0.4225]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 207/292 [00:34<00:14,  6.07batch/s, auc=0.9411, loss=0.4677]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 208/292 [00:34<00:13,  6.05batch/s, auc=0.9411, loss=0.4677]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 208/292 [00:34<00:13,  6.05batch/s, auc=0.9413, loss=0.3271]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.05batch/s, auc=0.9413, loss=0.3271]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 209/292 [00:34<00:13,  6.05batch/s, auc=0.9413, loss=0.4332]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 210/292 [00:34<00:13,  6.05batch/s, auc=0.9413, loss=0.4332]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 210/292 [00:35<00:13,  6.05batch/s, auc=0.9410, loss=0.8198]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.05batch/s, auc=0.9410, loss=0.8198]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 211/292 [00:35<00:13,  6.05batch/s, auc=0.9410, loss=0.6616]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.95batch/s, auc=0.9410, loss=0.6616]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.95batch/s, auc=0.9410, loss=0.4495]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.85batch/s, auc=0.9410, loss=0.4495]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.85batch/s, auc=0.9409, loss=0.5888]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.78batch/s, auc=0.9409, loss=0.5888]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.78batch/s, auc=0.9410, loss=0.4441]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.9410, loss=0.4441]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.9412, loss=0.3424]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.72batch/s, auc=0.9412, loss=0.3424]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.72batch/s, auc=0.9410, loss=0.8163]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.69batch/s, auc=0.9410, loss=0.8163]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.69batch/s, auc=0.9411, loss=0.3868]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.67batch/s, auc=0.9411, loss=0.3868]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.67batch/s, auc=0.9411, loss=0.4557]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9411, loss=0.4557]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.70batch/s, auc=0.9409, loss=0.7763]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.80batch/s, auc=0.9409, loss=0.7763]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.80batch/s, auc=0.9406, loss=0.7951]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.65batch/s, auc=0.9406, loss=0.7951]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.65batch/s, auc=0.9407, loss=0.4003]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.76batch/s, auc=0.9407, loss=0.4003]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.76batch/s, auc=0.9407, loss=0.4799]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.84batch/s, auc=0.9407, loss=0.4799]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.84batch/s, auc=0.9407, loss=0.6983]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.89batch/s, auc=0.9407, loss=0.6983]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.89batch/s, auc=0.9405, loss=0.8219]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.92batch/s, auc=0.9405, loss=0.8219]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.92batch/s, auc=0.9404, loss=0.7667]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.96batch/s, auc=0.9404, loss=0.7667]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.96batch/s, auc=0.9405, loss=0.4789]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.99batch/s, auc=0.9405, loss=0.4789]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.99batch/s, auc=0.9405, loss=0.6068]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 228/292 [00:37<00:10,  5.99batch/s, auc=0.9405, loss=0.6068]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.99batch/s, auc=0.9403, loss=0.8138]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.98batch/s, auc=0.9403, loss=0.8138]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.98batch/s, auc=0.9405, loss=0.4142]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.78batch/s, auc=0.9405, loss=0.4142]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.78batch/s, auc=0.9405, loss=0.4553]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.85batch/s, auc=0.9405, loss=0.4553]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.85batch/s, auc=0.9407, loss=0.4283]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.79batch/s, auc=0.9407, loss=0.4283]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.79batch/s, auc=0.9407, loss=0.4886]\u001b[A\n",
      "Training Epoch 21/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.88batch/s, auc=0.9407, loss=0.4886]\u001b[A\n",
      "Training Epoch 21/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.88batch/s, auc=0.9408, loss=0.3989]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 234/292 [00:38<00:09,  5.92batch/s, auc=0.9408, loss=0.3989]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 234/292 [00:39<00:09,  5.92batch/s, auc=0.9409, loss=0.3604]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9409, loss=0.3604]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9408, loss=0.5387]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 236/292 [00:39<00:09,  5.77batch/s, auc=0.9408, loss=0.5387]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 236/292 [00:39<00:09,  5.77batch/s, auc=0.9404, loss=1.2781]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 237/292 [00:39<00:09,  5.70batch/s, auc=0.9404, loss=1.2781]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 237/292 [00:39<00:09,  5.70batch/s, auc=0.9402, loss=0.8477]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.67batch/s, auc=0.9402, loss=0.8477]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.67batch/s, auc=0.9401, loss=0.5657]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.67batch/s, auc=0.9401, loss=0.5657]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.67batch/s, auc=0.9396, loss=0.8757]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.66batch/s, auc=0.9396, loss=0.8757]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.66batch/s, auc=0.9396, loss=0.4920]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9396, loss=0.4920]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9394, loss=0.6366]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.64batch/s, auc=0.9394, loss=0.6366]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.64batch/s, auc=0.9394, loss=0.4815]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9394, loss=0.4815]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.63batch/s, auc=0.9397, loss=0.3257]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9397, loss=0.3257]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.62batch/s, auc=0.9397, loss=0.4890]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.62batch/s, auc=0.9397, loss=0.4890]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.62batch/s, auc=0.9397, loss=0.4345]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.58batch/s, auc=0.9397, loss=0.4345]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.58batch/s, auc=0.9395, loss=0.7103]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.60batch/s, auc=0.9395, loss=0.7103]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 247/292 [00:41<00:08,  5.60batch/s, auc=0.9394, loss=0.5558]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.61batch/s, auc=0.9394, loss=0.5558]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.61batch/s, auc=0.9393, loss=0.8191]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.61batch/s, auc=0.9393, loss=0.8191]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.61batch/s, auc=0.9394, loss=0.3553]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.54batch/s, auc=0.9394, loss=0.3553]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.54batch/s, auc=0.9394, loss=0.5761]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.58batch/s, auc=0.9394, loss=0.5761]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.58batch/s, auc=0.9394, loss=0.5111]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.59batch/s, auc=0.9394, loss=0.5111]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.59batch/s, auc=0.9394, loss=0.4754]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.60batch/s, auc=0.9394, loss=0.4754]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.60batch/s, auc=0.9394, loss=0.5898]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.61batch/s, auc=0.9394, loss=0.5898]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.61batch/s, auc=0.9394, loss=0.6222]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.63batch/s, auc=0.9394, loss=0.6222]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.63batch/s, auc=0.9393, loss=0.6913]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.58batch/s, auc=0.9393, loss=0.6913]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.58batch/s, auc=0.9394, loss=0.5000]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.59batch/s, auc=0.9394, loss=0.5000]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.59batch/s, auc=0.9393, loss=0.5792]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.58batch/s, auc=0.9393, loss=0.5792]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.58batch/s, auc=0.9392, loss=0.8232]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9392, loss=0.8232]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.57batch/s, auc=0.9392, loss=0.4821]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.57batch/s, auc=0.9392, loss=0.4821]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.57batch/s, auc=0.9393, loss=0.3940]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.58batch/s, auc=0.9393, loss=0.3940]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.58batch/s, auc=0.9392, loss=0.6173]\u001b[A\n",
      "Training Epoch 21/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.59batch/s, auc=0.9392, loss=0.6173]\u001b[A\n",
      "Training Epoch 21/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.59batch/s, auc=0.9391, loss=0.6401]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 263/292 [00:44<00:05,  5.63batch/s, auc=0.9391, loss=0.6401]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 263/292 [00:44<00:05,  5.63batch/s, auc=0.9390, loss=0.7547]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 264/292 [00:44<00:05,  5.55batch/s, auc=0.9390, loss=0.7547]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 264/292 [00:44<00:05,  5.55batch/s, auc=0.9391, loss=0.4523]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 265/292 [00:44<00:04,  5.48batch/s, auc=0.9391, loss=0.4523]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 265/292 [00:44<00:04,  5.48batch/s, auc=0.9392, loss=0.4203]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 266/292 [00:44<00:04,  5.46batch/s, auc=0.9392, loss=0.4203]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 266/292 [00:44<00:04,  5.46batch/s, auc=0.9391, loss=0.5679]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.53batch/s, auc=0.9391, loss=0.5679]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.53batch/s, auc=0.9391, loss=0.6000]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.66batch/s, auc=0.9391, loss=0.6000]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.66batch/s, auc=0.9391, loss=0.5720]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.64batch/s, auc=0.9391, loss=0.5720]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.64batch/s, auc=0.9392, loss=0.4797]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9392, loss=0.4797]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.61batch/s, auc=0.9391, loss=0.9868]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9391, loss=0.9868]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.60batch/s, auc=0.9389, loss=0.5730]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.58batch/s, auc=0.9389, loss=0.5730]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.58batch/s, auc=0.9389, loss=0.4833]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.43batch/s, auc=0.9389, loss=0.4833]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.43batch/s, auc=0.9388, loss=0.5752]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.56batch/s, auc=0.9388, loss=0.5752]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.56batch/s, auc=0.9388, loss=0.6488]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.50batch/s, auc=0.9388, loss=0.6488]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.50batch/s, auc=0.9388, loss=0.4927]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.49batch/s, auc=0.9388, loss=0.4927]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.49batch/s, auc=0.9388, loss=0.4369]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.48batch/s, auc=0.9388, loss=0.4369]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.48batch/s, auc=0.9387, loss=0.5992]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.51batch/s, auc=0.9387, loss=0.5992]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.51batch/s, auc=0.9385, loss=0.9081]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.52batch/s, auc=0.9385, loss=0.9081]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.52batch/s, auc=0.9384, loss=0.5297]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.64batch/s, auc=0.9384, loss=0.5297]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.64batch/s, auc=0.9384, loss=0.3528]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9384, loss=0.3528]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.61batch/s, auc=0.9383, loss=0.6495]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.70batch/s, auc=0.9383, loss=0.6495]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.70batch/s, auc=0.9384, loss=0.3967]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.55batch/s, auc=0.9384, loss=0.3967]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.55batch/s, auc=0.9381, loss=0.8548]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.67batch/s, auc=0.9381, loss=0.8548]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.67batch/s, auc=0.9379, loss=1.0916]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.61batch/s, auc=0.9379, loss=1.0916]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.61batch/s, auc=0.9379, loss=0.5015]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.73batch/s, auc=0.9379, loss=0.5015]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.73batch/s, auc=0.9379, loss=0.4326]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9379, loss=0.4326]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.81batch/s, auc=0.9378, loss=0.5578]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.85batch/s, auc=0.9378, loss=0.5578]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.85batch/s, auc=0.9378, loss=0.6363]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9378, loss=0.6363]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9377, loss=0.6472]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.88batch/s, auc=0.9377, loss=0.6472]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9377, loss=0.4818]\u001b[A\n",
      "Training Epoch 21/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.89batch/s, auc=0.9377, loss=0.4818]\u001b[A\n",
      "Training Epoch 21/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.89batch/s, auc=0.9377, loss=0.5464]\u001b[A\n",
      "Training Epoch 21/25: 100%|██████████| 292/292 [00:49<00:00,  5.92batch/s, auc=0.9377, loss=0.5464]\u001b[A\n",
      "Epochs:  84%|████████▍ | 21/25 [19:13<03:40, 55.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25] Train Loss: 0.5322 | Train AUROC: 0.9377 Val Loss: 0.6482 | Val AUROC: 0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 22/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9389, loss=0.5793]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 1/292 [00:01<04:52,  1.00s/batch, auc=0.9389, loss=0.5793]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 1/292 [00:01<04:52,  1.00s/batch, auc=0.9528, loss=0.3424]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9528, loss=0.3424]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9473, loss=0.4818]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9473, loss=0.4818]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 3/292 [00:01<01:40,  2.88batch/s, auc=0.9475, loss=0.4513]\u001b[A\n",
      "Training Epoch 22/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9475, loss=0.4513]\u001b[A\n",
      "Training Epoch 22/25:   1%|▏         | 4/292 [00:01<01:18,  3.65batch/s, auc=0.9516, loss=0.3732]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 5/292 [00:01<01:06,  4.30batch/s, auc=0.9516, loss=0.3732]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 5/292 [00:01<01:06,  4.30batch/s, auc=0.9412, loss=0.6661]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 6/292 [00:01<00:59,  4.83batch/s, auc=0.9412, loss=0.6661]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 6/292 [00:01<00:59,  4.83batch/s, auc=0.9402, loss=0.4886]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 7/292 [00:01<00:55,  5.15batch/s, auc=0.9402, loss=0.4886]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 7/292 [00:02<00:55,  5.15batch/s, auc=0.9440, loss=0.5372]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 8/292 [00:02<00:51,  5.46batch/s, auc=0.9440, loss=0.5372]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 8/292 [00:02<00:51,  5.46batch/s, auc=0.9468, loss=0.4836]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 9/292 [00:02<00:49,  5.72batch/s, auc=0.9468, loss=0.4836]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 9/292 [00:02<00:49,  5.72batch/s, auc=0.9481, loss=0.5741]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 10/292 [00:02<00:47,  5.89batch/s, auc=0.9481, loss=0.5741]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 10/292 [00:02<00:47,  5.89batch/s, auc=0.9478, loss=0.5028]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9478, loss=0.5028]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 11/292 [00:02<00:47,  5.97batch/s, auc=0.9458, loss=0.4659]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9458, loss=0.4659]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 12/292 [00:02<00:46,  6.08batch/s, auc=0.9465, loss=0.3956]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 13/292 [00:02<00:45,  6.16batch/s, auc=0.9465, loss=0.3956]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 13/292 [00:03<00:45,  6.16batch/s, auc=0.9452, loss=0.4102]\u001b[A\n",
      "Training Epoch 22/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.9452, loss=0.4102]\u001b[A\n",
      "Training Epoch 22/25:   5%|▍         | 14/292 [00:03<00:44,  6.21batch/s, auc=0.9478, loss=0.3452]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 15/292 [00:03<00:44,  6.26batch/s, auc=0.9478, loss=0.3452]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 15/292 [00:03<00:44,  6.26batch/s, auc=0.9486, loss=0.4339]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 16/292 [00:03<00:43,  6.29batch/s, auc=0.9486, loss=0.4339]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 16/292 [00:03<00:43,  6.29batch/s, auc=0.9461, loss=0.6093]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.9461, loss=0.6093]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 17/292 [00:03<00:43,  6.31batch/s, auc=0.9473, loss=0.3280]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9473, loss=0.3280]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9472, loss=0.4878]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 19/292 [00:03<00:43,  6.28batch/s, auc=0.9472, loss=0.4878]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 19/292 [00:04<00:43,  6.28batch/s, auc=0.9465, loss=0.5562]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9465, loss=0.5562]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 20/292 [00:04<00:43,  6.26batch/s, auc=0.9412, loss=1.0931]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 21/292 [00:04<00:43,  6.26batch/s, auc=0.9412, loss=1.0931]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 21/292 [00:04<00:43,  6.26batch/s, auc=0.9427, loss=0.4194]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 22/292 [00:04<00:42,  6.28batch/s, auc=0.9427, loss=0.4194]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 22/292 [00:04<00:42,  6.28batch/s, auc=0.9414, loss=0.5683]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9414, loss=0.5683]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9407, loss=0.5362]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 24/292 [00:04<00:42,  6.28batch/s, auc=0.9407, loss=0.5362]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 24/292 [00:04<00:42,  6.28batch/s, auc=0.9426, loss=0.3196]\u001b[A\n",
      "Training Epoch 22/25:   9%|▊         | 25/292 [00:04<00:42,  6.26batch/s, auc=0.9426, loss=0.3196]\u001b[A\n",
      "Training Epoch 22/25:   9%|▊         | 25/292 [00:04<00:42,  6.26batch/s, auc=0.9418, loss=0.6513]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 26/292 [00:04<00:42,  6.25batch/s, auc=0.9418, loss=0.6513]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 26/292 [00:05<00:42,  6.25batch/s, auc=0.9436, loss=0.5190]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 27/292 [00:05<00:42,  6.21batch/s, auc=0.9436, loss=0.5190]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 27/292 [00:05<00:42,  6.21batch/s, auc=0.9434, loss=0.4890]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 28/292 [00:05<00:42,  6.23batch/s, auc=0.9434, loss=0.4890]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 28/292 [00:05<00:42,  6.23batch/s, auc=0.9422, loss=0.6403]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 29/292 [00:05<00:42,  6.26batch/s, auc=0.9422, loss=0.6403]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 29/292 [00:05<00:42,  6.26batch/s, auc=0.9424, loss=0.4046]\u001b[A\n",
      "Training Epoch 22/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9424, loss=0.4046]\u001b[A\n",
      "Training Epoch 22/25:  10%|█         | 30/292 [00:05<00:41,  6.29batch/s, auc=0.9407, loss=0.6083]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 31/292 [00:05<00:41,  6.31batch/s, auc=0.9407, loss=0.6083]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 31/292 [00:05<00:41,  6.31batch/s, auc=0.9402, loss=0.5529]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 32/292 [00:05<00:41,  6.26batch/s, auc=0.9402, loss=0.5529]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 32/292 [00:06<00:41,  6.26batch/s, auc=0.9409, loss=0.4284]\u001b[A\n",
      "Training Epoch 22/25:  11%|█▏        | 33/292 [00:06<00:41,  6.28batch/s, auc=0.9409, loss=0.4284]\u001b[A\n",
      "Training Epoch 22/25:  11%|█▏        | 33/292 [00:06<00:41,  6.28batch/s, auc=0.9419, loss=0.3756]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9419, loss=0.3756]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 34/292 [00:06<00:40,  6.31batch/s, auc=0.9429, loss=0.4599]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 35/292 [00:06<00:41,  6.26batch/s, auc=0.9429, loss=0.4599]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 35/292 [00:06<00:41,  6.26batch/s, auc=0.9428, loss=0.3939]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 36/292 [00:06<00:40,  6.27batch/s, auc=0.9428, loss=0.3939]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 36/292 [00:06<00:40,  6.27batch/s, auc=0.9429, loss=0.4791]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9429, loss=0.4791]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 37/292 [00:06<00:40,  6.30batch/s, auc=0.9420, loss=0.5067]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9420, loss=0.5067]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9419, loss=0.5348]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 39/292 [00:07<00:40,  6.32batch/s, auc=0.9419, loss=0.5348]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 39/292 [00:07<00:40,  6.32batch/s, auc=0.9417, loss=0.5677]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9417, loss=0.5677]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▎        | 40/292 [00:07<00:40,  6.26batch/s, auc=0.9422, loss=0.3911]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 41/292 [00:07<00:39,  6.28batch/s, auc=0.9422, loss=0.3911]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 41/292 [00:07<00:39,  6.28batch/s, auc=0.9425, loss=0.4462]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9425, loss=0.4462]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 42/292 [00:07<00:39,  6.30batch/s, auc=0.9423, loss=0.4314]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9423, loss=0.4314]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▍        | 43/292 [00:07<00:39,  6.30batch/s, auc=0.9393, loss=1.1566]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.9393, loss=1.1566]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.9396, loss=0.3678]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9396, loss=0.3678]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 45/292 [00:08<00:39,  6.30batch/s, auc=0.9386, loss=0.7330]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9386, loss=0.7330]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 46/292 [00:08<00:39,  6.30batch/s, auc=0.9387, loss=0.5255]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 47/292 [00:08<00:38,  6.31batch/s, auc=0.9387, loss=0.5255]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 47/292 [00:08<00:38,  6.31batch/s, auc=0.9394, loss=0.3793]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▋        | 48/292 [00:08<00:39,  6.22batch/s, auc=0.9394, loss=0.3793]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▋        | 48/292 [00:08<00:39,  6.22batch/s, auc=0.9394, loss=0.6933]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 49/292 [00:08<00:39,  6.23batch/s, auc=0.9394, loss=0.6933]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 49/292 [00:08<00:39,  6.23batch/s, auc=0.9402, loss=0.4056]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 50/292 [00:08<00:38,  6.26batch/s, auc=0.9402, loss=0.4056]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 50/292 [00:08<00:38,  6.26batch/s, auc=0.9403, loss=0.5343]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 51/292 [00:08<00:38,  6.27batch/s, auc=0.9403, loss=0.5343]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 51/292 [00:09<00:38,  6.27batch/s, auc=0.9396, loss=0.5535]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9396, loss=0.5535]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9400, loss=0.4733]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.9400, loss=0.4733]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 53/292 [00:09<00:37,  6.32batch/s, auc=0.9407, loss=0.3502]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.9407, loss=0.3502]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 54/292 [00:09<00:37,  6.32batch/s, auc=0.9404, loss=0.5011]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 55/292 [00:09<00:37,  6.32batch/s, auc=0.9404, loss=0.5011]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 55/292 [00:09<00:37,  6.32batch/s, auc=0.9411, loss=0.3619]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9411, loss=0.3619]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 56/292 [00:09<00:37,  6.31batch/s, auc=0.9408, loss=0.4685]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 57/292 [00:09<00:37,  6.32batch/s, auc=0.9408, loss=0.4685]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 57/292 [00:10<00:37,  6.32batch/s, auc=0.9412, loss=0.3898]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9412, loss=0.3898]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 58/292 [00:10<00:37,  6.30batch/s, auc=0.9415, loss=0.3593]\u001b[A\n",
      "Training Epoch 22/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9415, loss=0.3593]\u001b[A\n",
      "Training Epoch 22/25:  20%|██        | 59/292 [00:10<00:37,  6.25batch/s, auc=0.9416, loss=0.4155]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 60/292 [00:10<00:37,  6.27batch/s, auc=0.9416, loss=0.4155]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 60/292 [00:10<00:37,  6.27batch/s, auc=0.9413, loss=0.5267]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9413, loss=0.5267]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9415, loss=0.4796]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9415, loss=0.4796]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 62/292 [00:10<00:36,  6.28batch/s, auc=0.9417, loss=0.4497]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9417, loss=0.4497]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9415, loss=0.4142]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.9415, loss=0.4142]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.9414, loss=0.4731]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 65/292 [00:11<00:36,  6.26batch/s, auc=0.9414, loss=0.4731]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 65/292 [00:11<00:36,  6.26batch/s, auc=0.9422, loss=0.2666]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 66/292 [00:11<00:36,  6.19batch/s, auc=0.9422, loss=0.2666]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 66/292 [00:11<00:36,  6.19batch/s, auc=0.9419, loss=0.7090]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 67/292 [00:11<00:36,  6.22batch/s, auc=0.9419, loss=0.7090]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 67/292 [00:11<00:36,  6.22batch/s, auc=0.9417, loss=0.4751]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 68/292 [00:11<00:36,  6.21batch/s, auc=0.9417, loss=0.4751]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 68/292 [00:11<00:36,  6.21batch/s, auc=0.9418, loss=0.4898]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▎       | 69/292 [00:11<00:35,  6.24batch/s, auc=0.9418, loss=0.4898]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▎       | 69/292 [00:11<00:35,  6.24batch/s, auc=0.9420, loss=0.4848]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 70/292 [00:11<00:35,  6.25batch/s, auc=0.9420, loss=0.4848]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 70/292 [00:12<00:35,  6.25batch/s, auc=0.9417, loss=0.5879]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 71/292 [00:12<00:35,  6.24batch/s, auc=0.9417, loss=0.5879]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 71/292 [00:12<00:35,  6.24batch/s, auc=0.9419, loss=0.3694]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▍       | 72/292 [00:12<00:35,  6.24batch/s, auc=0.9419, loss=0.3694]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▍       | 72/292 [00:12<00:35,  6.24batch/s, auc=0.9413, loss=0.7656]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 73/292 [00:12<00:35,  6.25batch/s, auc=0.9413, loss=0.7656]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 73/292 [00:12<00:35,  6.25batch/s, auc=0.9415, loss=0.4308]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 74/292 [00:12<00:34,  6.26batch/s, auc=0.9415, loss=0.4308]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 74/292 [00:12<00:34,  6.26batch/s, auc=0.9419, loss=0.3590]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 75/292 [00:12<00:34,  6.22batch/s, auc=0.9419, loss=0.3590]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 75/292 [00:12<00:34,  6.22batch/s, auc=0.9419, loss=0.5588]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 76/292 [00:12<00:34,  6.24batch/s, auc=0.9419, loss=0.5588]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 76/292 [00:13<00:34,  6.24batch/s, auc=0.9426, loss=0.2814]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9426, loss=0.2814]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▋       | 77/292 [00:13<00:34,  6.19batch/s, auc=0.9432, loss=0.3690]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 78/292 [00:13<00:34,  6.21batch/s, auc=0.9432, loss=0.3690]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 78/292 [00:13<00:34,  6.21batch/s, auc=0.9433, loss=0.4126]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 79/292 [00:13<00:34,  6.23batch/s, auc=0.9433, loss=0.4126]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 79/292 [00:13<00:34,  6.23batch/s, auc=0.9432, loss=0.4517]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 80/292 [00:13<00:34,  6.23batch/s, auc=0.9432, loss=0.4517]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 80/292 [00:13<00:34,  6.23batch/s, auc=0.9439, loss=0.4033]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9439, loss=0.4033]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 81/292 [00:13<00:33,  6.25batch/s, auc=0.9438, loss=0.4137]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 82/292 [00:13<00:33,  6.26batch/s, auc=0.9438, loss=0.4137]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 82/292 [00:14<00:33,  6.26batch/s, auc=0.9439, loss=0.4380]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9439, loss=0.4380]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9440, loss=0.3742]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9440, loss=0.3742]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9436, loss=0.7444]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9436, loss=0.7444]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9442, loss=0.3283]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 86/292 [00:14<00:33,  6.23batch/s, auc=0.9442, loss=0.3283]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 86/292 [00:14<00:33,  6.23batch/s, auc=0.9444, loss=0.3841]\u001b[A\n",
      "Training Epoch 22/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9444, loss=0.3841]\u001b[A\n",
      "Training Epoch 22/25:  30%|██▉       | 87/292 [00:14<00:32,  6.23batch/s, auc=0.9441, loss=0.4883]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 88/292 [00:14<00:32,  6.22batch/s, auc=0.9441, loss=0.4883]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 88/292 [00:15<00:32,  6.22batch/s, auc=0.9438, loss=0.5716]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 89/292 [00:15<00:32,  6.23batch/s, auc=0.9438, loss=0.5716]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 89/292 [00:15<00:32,  6.23batch/s, auc=0.9428, loss=1.0752]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9428, loss=1.0752]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 90/292 [00:15<00:32,  6.24batch/s, auc=0.9425, loss=0.5383]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 91/292 [00:15<00:32,  6.19batch/s, auc=0.9425, loss=0.5383]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 91/292 [00:15<00:32,  6.19batch/s, auc=0.9421, loss=0.4808]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 92/292 [00:15<00:32,  6.19batch/s, auc=0.9421, loss=0.4808]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 92/292 [00:15<00:32,  6.19batch/s, auc=0.9422, loss=0.4848]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9422, loss=0.4848]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9423, loss=0.4349]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 94/292 [00:15<00:31,  6.22batch/s, auc=0.9423, loss=0.4349]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 94/292 [00:16<00:31,  6.22batch/s, auc=0.9423, loss=0.4318]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 95/292 [00:16<00:31,  6.21batch/s, auc=0.9423, loss=0.4318]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 95/292 [00:16<00:31,  6.21batch/s, auc=0.9423, loss=0.5157]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9423, loss=0.5157]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 96/292 [00:16<00:31,  6.22batch/s, auc=0.9425, loss=0.3939]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9425, loss=0.3939]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 97/292 [00:16<00:31,  6.22batch/s, auc=0.9423, loss=0.4234]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▎      | 98/292 [00:16<00:31,  6.22batch/s, auc=0.9423, loss=0.4234]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▎      | 98/292 [00:16<00:31,  6.22batch/s, auc=0.9429, loss=0.2543]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9429, loss=0.2543]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9427, loss=0.5101]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 100/292 [00:16<00:30,  6.20batch/s, auc=0.9427, loss=0.5101]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 100/292 [00:16<00:30,  6.20batch/s, auc=0.9429, loss=0.4582]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 101/292 [00:16<00:30,  6.21batch/s, auc=0.9429, loss=0.4582]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 101/292 [00:17<00:30,  6.21batch/s, auc=0.9423, loss=0.7957]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 102/292 [00:17<00:30,  6.15batch/s, auc=0.9423, loss=0.7957]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 102/292 [00:17<00:30,  6.15batch/s, auc=0.9421, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▌      | 103/292 [00:17<00:30,  6.17batch/s, auc=0.9421, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▌      | 103/292 [00:17<00:30,  6.17batch/s, auc=0.9419, loss=0.6309]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 104/292 [00:17<00:30,  6.18batch/s, auc=0.9419, loss=0.6309]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 104/292 [00:17<00:30,  6.18batch/s, auc=0.9418, loss=0.6503]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9418, loss=0.6503]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9421, loss=0.3422]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▋      | 106/292 [00:17<00:29,  6.20batch/s, auc=0.9421, loss=0.3422]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▋      | 106/292 [00:17<00:29,  6.20batch/s, auc=0.9423, loss=0.5351]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 107/292 [00:17<00:29,  6.21batch/s, auc=0.9423, loss=0.5351]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 107/292 [00:18<00:29,  6.21batch/s, auc=0.9422, loss=0.6097]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 108/292 [00:18<00:29,  6.17batch/s, auc=0.9422, loss=0.6097]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 108/292 [00:18<00:29,  6.17batch/s, auc=0.9426, loss=0.2802]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 109/292 [00:18<00:29,  6.18batch/s, auc=0.9426, loss=0.2802]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 109/292 [00:18<00:29,  6.18batch/s, auc=0.9424, loss=0.6540]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9424, loss=0.6540]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9426, loss=0.4708]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9426, loss=0.4708]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 111/292 [00:18<00:29,  6.20batch/s, auc=0.9424, loss=0.4660]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9424, loss=0.4660]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9421, loss=0.5745]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▊      | 113/292 [00:18<00:28,  6.22batch/s, auc=0.9421, loss=0.5745]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9423, loss=0.4460]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9423, loss=0.4460]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 114/292 [00:19<00:28,  6.22batch/s, auc=0.9428, loss=0.3769]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9428, loss=0.3769]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 115/292 [00:19<00:28,  6.22batch/s, auc=0.9427, loss=0.5344]\u001b[A\n",
      "Training Epoch 22/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9427, loss=0.5344]\u001b[A\n",
      "Training Epoch 22/25:  40%|███▉      | 116/292 [00:19<00:28,  6.22batch/s, auc=0.9434, loss=0.2519]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9434, loss=0.2519]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 117/292 [00:19<00:28,  6.22batch/s, auc=0.9431, loss=0.8389]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9431, loss=0.8389]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 118/292 [00:19<00:27,  6.22batch/s, auc=0.9434, loss=0.4015]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 119/292 [00:19<00:27,  6.22batch/s, auc=0.9434, loss=0.4015]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 119/292 [00:20<00:27,  6.22batch/s, auc=0.9431, loss=0.5999]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 120/292 [00:20<00:27,  6.22batch/s, auc=0.9431, loss=0.5999]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 120/292 [00:20<00:27,  6.22batch/s, auc=0.9430, loss=0.5698]\u001b[A\n",
      "Training Epoch 22/25:  41%|████▏     | 121/292 [00:20<00:27,  6.17batch/s, auc=0.9430, loss=0.5698]\u001b[A\n",
      "Training Epoch 22/25:  41%|████▏     | 121/292 [00:20<00:27,  6.17batch/s, auc=0.9428, loss=0.5483]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9428, loss=0.5483]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9427, loss=0.4744]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 123/292 [00:20<00:27,  6.13batch/s, auc=0.9427, loss=0.4744]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 123/292 [00:20<00:27,  6.13batch/s, auc=0.9427, loss=0.5011]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 124/292 [00:20<00:27,  6.15batch/s, auc=0.9427, loss=0.5011]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 124/292 [00:20<00:27,  6.15batch/s, auc=0.9427, loss=0.5158]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 125/292 [00:20<00:27,  6.17batch/s, auc=0.9427, loss=0.5158]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 125/292 [00:21<00:27,  6.17batch/s, auc=0.9428, loss=0.4300]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 126/292 [00:21<00:26,  6.17batch/s, auc=0.9428, loss=0.4300]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 126/292 [00:21<00:26,  6.17batch/s, auc=0.9428, loss=0.5905]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 127/292 [00:21<00:26,  6.18batch/s, auc=0.9428, loss=0.5905]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 127/292 [00:21<00:26,  6.18batch/s, auc=0.9425, loss=0.6370]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9425, loss=0.6370]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9426, loss=0.6181]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 129/292 [00:21<00:26,  6.17batch/s, auc=0.9426, loss=0.6181]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 129/292 [00:21<00:26,  6.17batch/s, auc=0.9428, loss=0.4540]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 130/292 [00:21<00:26,  6.17batch/s, auc=0.9428, loss=0.4540]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 130/292 [00:21<00:26,  6.17batch/s, auc=0.9429, loss=0.4466]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 131/292 [00:21<00:26,  6.17batch/s, auc=0.9429, loss=0.4466]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 131/292 [00:21<00:26,  6.17batch/s, auc=0.9435, loss=0.3038]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▌     | 132/292 [00:21<00:25,  6.18batch/s, auc=0.9435, loss=0.3038]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▌     | 132/292 [00:22<00:25,  6.18batch/s, auc=0.9434, loss=0.6545]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9434, loss=0.6545]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 133/292 [00:22<00:25,  6.19batch/s, auc=0.9435, loss=0.3941]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9435, loss=0.3941]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9435, loss=0.5797]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9435, loss=0.5797]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 135/292 [00:22<00:25,  6.18batch/s, auc=0.9436, loss=0.4025]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9436, loss=0.4025]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 136/292 [00:22<00:25,  6.18batch/s, auc=0.9437, loss=0.4386]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9437, loss=0.4386]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 137/292 [00:22<00:25,  6.17batch/s, auc=0.9438, loss=0.4442]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 138/292 [00:22<00:24,  6.18batch/s, auc=0.9438, loss=0.4442]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 138/292 [00:23<00:24,  6.18batch/s, auc=0.9437, loss=0.5357]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 139/292 [00:23<00:24,  6.17batch/s, auc=0.9437, loss=0.5357]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 139/292 [00:23<00:24,  6.17batch/s, auc=0.9440, loss=0.2998]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9440, loss=0.2998]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9441, loss=0.5831]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 141/292 [00:23<00:24,  6.18batch/s, auc=0.9441, loss=0.5831]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 141/292 [00:23<00:24,  6.18batch/s, auc=0.9440, loss=0.4950]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▊     | 142/292 [00:23<00:24,  6.18batch/s, auc=0.9440, loss=0.4950]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▊     | 142/292 [00:23<00:24,  6.18batch/s, auc=0.9440, loss=0.4558]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9440, loss=0.4558]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9436, loss=0.5936]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 144/292 [00:23<00:23,  6.18batch/s, auc=0.9436, loss=0.5936]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 144/292 [00:24<00:23,  6.18batch/s, auc=0.9435, loss=0.4670]\u001b[A\n",
      "Training Epoch 22/25:  50%|████▉     | 145/292 [00:24<00:23,  6.18batch/s, auc=0.9435, loss=0.4670]\u001b[A\n",
      "Training Epoch 22/25:  50%|████▉     | 145/292 [00:24<00:23,  6.18batch/s, auc=0.9438, loss=0.3273]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9438, loss=0.3273]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 146/292 [00:24<00:23,  6.16batch/s, auc=0.9437, loss=0.6910]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9437, loss=0.6910]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 147/292 [00:24<00:23,  6.15batch/s, auc=0.9437, loss=0.4747]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9437, loss=0.4747]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 148/292 [00:24<00:23,  6.15batch/s, auc=0.9435, loss=0.5999]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9435, loss=0.5999]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 149/292 [00:24<00:23,  6.15batch/s, auc=0.9439, loss=0.2666]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.14batch/s, auc=0.9439, loss=0.2666]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.14batch/s, auc=0.9438, loss=0.5891]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.13batch/s, auc=0.9438, loss=0.5891]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 151/292 [00:25<00:23,  6.13batch/s, auc=0.9439, loss=0.4756]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.4756]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.5290]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.12batch/s, auc=0.9439, loss=0.5290]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.12batch/s, auc=0.9440, loss=0.3608]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.13batch/s, auc=0.9440, loss=0.3608]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.4626]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.4626]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.3700]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.13batch/s, auc=0.9439, loss=0.3700]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.13batch/s, auc=0.9436, loss=0.6743]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.14batch/s, auc=0.9436, loss=0.6743]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.14batch/s, auc=0.9437, loss=0.3913]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9437, loss=0.3913]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.13batch/s, auc=0.9437, loss=0.3818]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.11batch/s, auc=0.9437, loss=0.3818]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.11batch/s, auc=0.9437, loss=0.6759]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.12batch/s, auc=0.9437, loss=0.6759]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.12batch/s, auc=0.9440, loss=0.2831]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9440, loss=0.2831]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.12batch/s, auc=0.9439, loss=0.4661]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.12batch/s, auc=0.9439, loss=0.4661]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.12batch/s, auc=0.9440, loss=0.5387]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9440, loss=0.5387]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9439, loss=0.5253]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9439, loss=0.5253]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.12batch/s, auc=0.9439, loss=0.6028]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9439, loss=0.6028]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9438, loss=0.5828]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9438, loss=0.5828]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.13batch/s, auc=0.9441, loss=0.2464]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9441, loss=0.2464]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9441, loss=0.4456]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.13batch/s, auc=0.9441, loss=0.4456]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.13batch/s, auc=0.9439, loss=0.8841]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9439, loss=0.8841]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.13batch/s, auc=0.9440, loss=0.4231]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9440, loss=0.4231]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9440, loss=0.6138]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9440, loss=0.6138]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9438, loss=0.7321]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9438, loss=0.7321]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.14batch/s, auc=0.9437, loss=0.4864]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.14batch/s, auc=0.9437, loss=0.4864]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.14batch/s, auc=0.9439, loss=0.2942]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.13batch/s, auc=0.9439, loss=0.2942]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.13batch/s, auc=0.9440, loss=0.4595]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 175/292 [00:28<00:19,  6.13batch/s, auc=0.9440, loss=0.4595]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.13batch/s, auc=0.9442, loss=0.3953]\u001b[A\n",
      "Training Epoch 22/25:  60%|██████    | 176/292 [00:29<00:18,  6.13batch/s, auc=0.9442, loss=0.3953]\u001b[A\n",
      "Training Epoch 22/25:  60%|██████    | 176/292 [00:29<00:18,  6.13batch/s, auc=0.9437, loss=0.9520]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 177/292 [00:29<00:18,  6.13batch/s, auc=0.9437, loss=0.9520]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 177/292 [00:29<00:18,  6.13batch/s, auc=0.9435, loss=0.6723]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9435, loss=0.6723]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 178/292 [00:29<00:18,  6.12batch/s, auc=0.9439, loss=0.2481]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9439, loss=0.2481]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.11batch/s, auc=0.9438, loss=0.4505]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.05batch/s, auc=0.9438, loss=0.4505]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.05batch/s, auc=0.9434, loss=0.9353]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.07batch/s, auc=0.9434, loss=0.9353]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.07batch/s, auc=0.9434, loss=0.4314]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.08batch/s, auc=0.9434, loss=0.4314]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 182/292 [00:30<00:18,  6.08batch/s, auc=0.9431, loss=0.7111]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.06batch/s, auc=0.9431, loss=0.7111]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.06batch/s, auc=0.9431, loss=0.4773]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.06batch/s, auc=0.9431, loss=0.4773]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.06batch/s, auc=0.9431, loss=0.4370]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.07batch/s, auc=0.9431, loss=0.4370]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.07batch/s, auc=0.9432, loss=0.4242]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.08batch/s, auc=0.9432, loss=0.4242]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.08batch/s, auc=0.9431, loss=0.5404]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.09batch/s, auc=0.9431, loss=0.5404]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.09batch/s, auc=0.9431, loss=0.5908]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9431, loss=0.5908]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9429, loss=0.8987]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9429, loss=0.8987]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9426, loss=0.6941]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9426, loss=0.6941]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.11batch/s, auc=0.9425, loss=0.5231]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9425, loss=0.5231]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.08batch/s, auc=0.9425, loss=0.4186]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9425, loss=0.4186]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9428, loss=0.3165]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.10batch/s, auc=0.9428, loss=0.3165]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.10batch/s, auc=0.9430, loss=0.4357]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.12batch/s, auc=0.9430, loss=0.4357]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.12batch/s, auc=0.9433, loss=0.3141]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.11batch/s, auc=0.9433, loss=0.3141]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 195/292 [00:32<00:15,  6.11batch/s, auc=0.9433, loss=0.5314]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.11batch/s, auc=0.9433, loss=0.5314]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 196/292 [00:32<00:15,  6.11batch/s, auc=0.9434, loss=0.4693]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.10batch/s, auc=0.9434, loss=0.4693]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 197/292 [00:32<00:15,  6.10batch/s, auc=0.9435, loss=0.4260]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.09batch/s, auc=0.9435, loss=0.4260]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 198/292 [00:32<00:15,  6.09batch/s, auc=0.9436, loss=0.4465]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 199/292 [00:32<00:15,  6.07batch/s, auc=0.9436, loss=0.4465]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 199/292 [00:33<00:15,  6.07batch/s, auc=0.9438, loss=0.3411]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9438, loss=0.3411]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 200/292 [00:33<00:15,  6.08batch/s, auc=0.9439, loss=0.4726]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.07batch/s, auc=0.9439, loss=0.4726]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 201/292 [00:33<00:14,  6.07batch/s, auc=0.9440, loss=0.3842]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.06batch/s, auc=0.9440, loss=0.3842]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 202/292 [00:33<00:14,  6.06batch/s, auc=0.9442, loss=0.3478]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.93batch/s, auc=0.9442, loss=0.3478]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.93batch/s, auc=0.9443, loss=0.3960]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.72batch/s, auc=0.9443, loss=0.3960]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.72batch/s, auc=0.9444, loss=0.3841]\u001b[A\n",
      "Training Epoch 22/25:  70%|███████   | 205/292 [00:33<00:15,  5.71batch/s, auc=0.9444, loss=0.3841]\u001b[A\n",
      "Training Epoch 22/25:  70%|███████   | 205/292 [00:34<00:15,  5.71batch/s, auc=0.9442, loss=0.6175]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 206/292 [00:34<00:14,  5.80batch/s, auc=0.9442, loss=0.6175]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 206/292 [00:34<00:14,  5.80batch/s, auc=0.9439, loss=0.8395]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 207/292 [00:34<00:14,  5.84batch/s, auc=0.9439, loss=0.8395]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 207/292 [00:34<00:14,  5.84batch/s, auc=0.9438, loss=0.4681]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 208/292 [00:34<00:14,  5.90batch/s, auc=0.9438, loss=0.4681]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 208/292 [00:34<00:14,  5.90batch/s, auc=0.9437, loss=0.4988]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 209/292 [00:34<00:13,  5.95batch/s, auc=0.9437, loss=0.4988]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 209/292 [00:34<00:13,  5.95batch/s, auc=0.9439, loss=0.3319]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 210/292 [00:34<00:13,  5.96batch/s, auc=0.9439, loss=0.3319]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 210/292 [00:34<00:13,  5.96batch/s, auc=0.9439, loss=0.4166]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 211/292 [00:34<00:13,  5.96batch/s, auc=0.9439, loss=0.4166]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.96batch/s, auc=0.9440, loss=0.3719]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.98batch/s, auc=0.9440, loss=0.3719]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.98batch/s, auc=0.9437, loss=0.8912]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.00batch/s, auc=0.9437, loss=0.8912]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 213/292 [00:35<00:13,  6.00batch/s, auc=0.9436, loss=0.4437]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.87batch/s, auc=0.9436, loss=0.4437]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.87batch/s, auc=0.9435, loss=0.5820]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.9435, loss=0.5820]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▎  | 215/292 [00:35<00:13,  5.79batch/s, auc=0.9435, loss=0.5979]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 216/292 [00:35<00:13,  5.76batch/s, auc=0.9435, loss=0.5979]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.76batch/s, auc=0.9436, loss=0.3380]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.72batch/s, auc=0.9436, loss=0.3380]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.72batch/s, auc=0.9434, loss=0.8465]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9434, loss=0.8465]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.71batch/s, auc=0.9432, loss=0.7188]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.69batch/s, auc=0.9432, loss=0.7188]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.69batch/s, auc=0.9433, loss=0.3423]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.66batch/s, auc=0.9433, loss=0.3423]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.66batch/s, auc=0.9432, loss=0.4619]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.61batch/s, auc=0.9432, loss=0.4619]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 221/292 [00:36<00:12,  5.61batch/s, auc=0.9432, loss=0.5798]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 222/292 [00:36<00:12,  5.61batch/s, auc=0.9432, loss=0.5798]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.61batch/s, auc=0.9431, loss=0.6275]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.62batch/s, auc=0.9431, loss=0.6275]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.62batch/s, auc=0.9432, loss=0.3645]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.62batch/s, auc=0.9432, loss=0.3645]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 224/292 [00:37<00:12,  5.62batch/s, auc=0.9431, loss=0.7038]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.73batch/s, auc=0.9431, loss=0.7038]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.73batch/s, auc=0.9431, loss=0.5259]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.57batch/s, auc=0.9431, loss=0.5259]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.57batch/s, auc=0.9430, loss=0.7314]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.69batch/s, auc=0.9430, loss=0.7314]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 227/292 [00:37<00:11,  5.69batch/s, auc=0.9431, loss=0.3943]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 228/292 [00:37<00:11,  5.57batch/s, auc=0.9431, loss=0.3943]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.57batch/s, auc=0.9431, loss=0.3601]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.54batch/s, auc=0.9431, loss=0.3601]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.54batch/s, auc=0.9430, loss=0.5408]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.68batch/s, auc=0.9430, loss=0.5408]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.68batch/s, auc=0.9431, loss=0.4145]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.80batch/s, auc=0.9431, loss=0.4145]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.80batch/s, auc=0.9432, loss=0.3868]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.87batch/s, auc=0.9432, loss=0.3868]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.87batch/s, auc=0.9433, loss=0.4043]\u001b[A\n",
      "Training Epoch 22/25:  80%|███████▉  | 233/292 [00:38<00:09,  5.91batch/s, auc=0.9433, loss=0.4043]\u001b[A\n",
      "Training Epoch 22/25:  80%|███████▉  | 233/292 [00:38<00:09,  5.91batch/s, auc=0.9431, loss=0.8528]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 234/292 [00:38<00:09,  5.93batch/s, auc=0.9431, loss=0.8528]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 234/292 [00:39<00:09,  5.93batch/s, auc=0.9433, loss=0.3833]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9433, loss=0.3833]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 235/292 [00:39<00:09,  5.84batch/s, auc=0.9432, loss=0.5673]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 236/292 [00:39<00:09,  5.89batch/s, auc=0.9432, loss=0.5673]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 236/292 [00:39<00:09,  5.89batch/s, auc=0.9432, loss=0.4556]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 237/292 [00:39<00:09,  5.93batch/s, auc=0.9432, loss=0.4556]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 237/292 [00:39<00:09,  5.93batch/s, auc=0.9432, loss=0.4741]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.95batch/s, auc=0.9432, loss=0.4741]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.95batch/s, auc=0.9431, loss=0.5017]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 239/292 [00:39<00:08,  5.99batch/s, auc=0.9431, loss=0.5017]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 239/292 [00:40<00:08,  5.99batch/s, auc=0.9432, loss=0.4014]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.80batch/s, auc=0.9432, loss=0.4014]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 240/292 [00:40<00:08,  5.80batch/s, auc=0.9430, loss=1.0804]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.73batch/s, auc=0.9430, loss=1.0804]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.73batch/s, auc=0.9427, loss=0.7669]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9427, loss=0.7669]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.74batch/s, auc=0.9427, loss=0.4961]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.79batch/s, auc=0.9427, loss=0.4961]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.79batch/s, auc=0.9427, loss=0.4473]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.82batch/s, auc=0.9427, loss=0.4473]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.82batch/s, auc=0.9427, loss=0.4462]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.87batch/s, auc=0.9427, loss=0.4462]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.87batch/s, auc=0.9427, loss=0.3855]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.89batch/s, auc=0.9427, loss=0.3855]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.89batch/s, auc=0.9428, loss=0.5513]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.94batch/s, auc=0.9428, loss=0.5513]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.94batch/s, auc=0.9428, loss=0.5095]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.95batch/s, auc=0.9428, loss=0.5095]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.95batch/s, auc=0.9427, loss=0.5354]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.93batch/s, auc=0.9427, loss=0.5354]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.93batch/s, auc=0.9427, loss=0.5440]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.94batch/s, auc=0.9427, loss=0.5440]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.94batch/s, auc=0.9427, loss=0.4776]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 251/292 [00:41<00:07,  5.75batch/s, auc=0.9427, loss=0.4776]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.75batch/s, auc=0.9426, loss=0.4638]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.72batch/s, auc=0.9426, loss=0.4638]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.72batch/s, auc=0.9424, loss=0.8001]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.71batch/s, auc=0.9424, loss=0.8001]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.71batch/s, auc=0.9424, loss=0.5890]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.65batch/s, auc=0.9424, loss=0.5890]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.65batch/s, auc=0.9423, loss=0.5912]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.60batch/s, auc=0.9423, loss=0.5912]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.60batch/s, auc=0.9420, loss=1.0989]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.63batch/s, auc=0.9420, loss=1.0989]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.63batch/s, auc=0.9419, loss=0.5262]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 257/292 [00:42<00:06,  5.61batch/s, auc=0.9419, loss=0.5262]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.61batch/s, auc=0.9420, loss=0.4254]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9420, loss=0.4254]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9420, loss=0.5973]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.59batch/s, auc=0.9420, loss=0.5973]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.59batch/s, auc=0.9418, loss=0.7520]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.69batch/s, auc=0.9418, loss=0.7520]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.69batch/s, auc=0.9420, loss=0.2644]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.54batch/s, auc=0.9420, loss=0.2644]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.54batch/s, auc=0.9420, loss=0.5121]\u001b[A\n",
      "Training Epoch 22/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.56batch/s, auc=0.9420, loss=0.5121]\u001b[A\n",
      "Training Epoch 22/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.56batch/s, auc=0.9421, loss=0.4835]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 263/292 [00:44<00:05,  5.56batch/s, auc=0.9421, loss=0.4835]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 263/292 [00:44<00:05,  5.56batch/s, auc=0.9422, loss=0.3229]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 264/292 [00:44<00:05,  5.54batch/s, auc=0.9422, loss=0.3229]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 264/292 [00:44<00:05,  5.54batch/s, auc=0.9423, loss=0.5000]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 265/292 [00:44<00:04,  5.52batch/s, auc=0.9423, loss=0.5000]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 265/292 [00:44<00:04,  5.52batch/s, auc=0.9419, loss=1.1496]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 266/292 [00:44<00:04,  5.53batch/s, auc=0.9419, loss=1.1496]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 266/292 [00:44<00:04,  5.53batch/s, auc=0.9419, loss=0.4324]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.55batch/s, auc=0.9419, loss=0.4324]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.55batch/s, auc=0.9418, loss=0.5400]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 268/292 [00:44<00:04,  5.59batch/s, auc=0.9418, loss=0.5400]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.59batch/s, auc=0.9418, loss=0.6180]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.58batch/s, auc=0.9418, loss=0.6180]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.58batch/s, auc=0.9418, loss=0.4582]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9418, loss=0.4582]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.57batch/s, auc=0.9417, loss=0.5221]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.57batch/s, auc=0.9417, loss=0.5221]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.57batch/s, auc=0.9418, loss=0.3685]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.52batch/s, auc=0.9418, loss=0.3685]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.52batch/s, auc=0.9416, loss=0.7745]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.63batch/s, auc=0.9416, loss=0.7745]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.63batch/s, auc=0.9418, loss=0.4301]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.72batch/s, auc=0.9418, loss=0.4301]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.72batch/s, auc=0.9417, loss=0.6758]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.78batch/s, auc=0.9417, loss=0.6758]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.78batch/s, auc=0.9419, loss=0.4615]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.77batch/s, auc=0.9419, loss=0.4615]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.77batch/s, auc=0.9420, loss=0.3622]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.82batch/s, auc=0.9420, loss=0.3622]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.82batch/s, auc=0.9419, loss=0.5882]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.85batch/s, auc=0.9419, loss=0.5882]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.85batch/s, auc=0.9420, loss=0.3982]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.82batch/s, auc=0.9420, loss=0.3982]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.82batch/s, auc=0.9420, loss=0.4576]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9420, loss=0.4576]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.65batch/s, auc=0.9419, loss=0.5626]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.62batch/s, auc=0.9419, loss=0.5626]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.62batch/s, auc=0.9418, loss=0.6605]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.62batch/s, auc=0.9418, loss=0.6605]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.62batch/s, auc=0.9417, loss=0.5651]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.56batch/s, auc=0.9417, loss=0.5651]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.56batch/s, auc=0.9416, loss=0.6348]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9416, loss=0.6348]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.55batch/s, auc=0.9415, loss=0.4767]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 285/292 [00:47<00:01,  5.55batch/s, auc=0.9415, loss=0.4767]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.55batch/s, auc=0.9414, loss=0.7162]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.68batch/s, auc=0.9414, loss=0.7162]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.68batch/s, auc=0.9415, loss=0.4027]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.78batch/s, auc=0.9415, loss=0.4027]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.78batch/s, auc=0.9416, loss=0.3475]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.85batch/s, auc=0.9416, loss=0.3475]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.85batch/s, auc=0.9415, loss=0.4816]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.88batch/s, auc=0.9415, loss=0.4816]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.88batch/s, auc=0.9415, loss=0.6392]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.90batch/s, auc=0.9415, loss=0.6392]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.90batch/s, auc=0.9415, loss=0.5786]\u001b[A\n",
      "Training Epoch 22/25: 100%|█████████▉| 291/292 [00:48<00:00,  5.91batch/s, auc=0.9415, loss=0.5786]\u001b[A\n",
      "Training Epoch 22/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.91batch/s, auc=0.9415, loss=0.4986]\u001b[A\n",
      "Training Epoch 22/25: 100%|██████████| 292/292 [00:49<00:00,  5.94batch/s, auc=0.9415, loss=0.4986]\u001b[A\n",
      "Epochs:  88%|████████▊ | 22/25 [20:08<02:44, 54.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25] Train Loss: 0.5116 | Train AUROC: 0.9415 Val Loss: 0.6454 | Val AUROC: 0.9084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 23/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9583, loss=0.3891]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 1/292 [00:00<04:45,  1.02batch/s, auc=0.9583, loss=0.3891]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 1/292 [00:01<04:45,  1.02batch/s, auc=0.9073, loss=0.8877]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.9073, loss=0.8877]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 2/292 [00:01<02:23,  2.02batch/s, auc=0.8903, loss=1.0443]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.8903, loss=1.0443]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 3/292 [00:01<01:41,  2.84batch/s, auc=0.8977, loss=0.4851]\u001b[A\n",
      "Training Epoch 23/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.8977, loss=0.4851]\u001b[A\n",
      "Training Epoch 23/25:   1%|▏         | 4/292 [00:01<01:19,  3.60batch/s, auc=0.9086, loss=0.4739]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 5/292 [00:01<01:07,  4.27batch/s, auc=0.9086, loss=0.4739]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 5/292 [00:01<01:07,  4.27batch/s, auc=0.9185, loss=0.4210]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 6/292 [00:01<01:01,  4.69batch/s, auc=0.9185, loss=0.4210]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 6/292 [00:01<01:01,  4.69batch/s, auc=0.9282, loss=0.3094]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 7/292 [00:01<00:56,  5.04batch/s, auc=0.9282, loss=0.3094]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 7/292 [00:02<00:56,  5.04batch/s, auc=0.9292, loss=0.4484]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 8/292 [00:02<00:52,  5.39batch/s, auc=0.9292, loss=0.4484]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 8/292 [00:02<00:52,  5.39batch/s, auc=0.9304, loss=0.4999]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 9/292 [00:02<00:51,  5.54batch/s, auc=0.9304, loss=0.4999]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 9/292 [00:02<00:51,  5.54batch/s, auc=0.9254, loss=0.7588]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9254, loss=0.7588]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9243, loss=0.6206]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9243, loss=0.6206]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 11/292 [00:02<00:47,  5.92batch/s, auc=0.9290, loss=0.4282]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 12/292 [00:02<00:46,  6.05batch/s, auc=0.9290, loss=0.4282]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 12/292 [00:02<00:46,  6.05batch/s, auc=0.9327, loss=0.3952]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 13/292 [00:02<00:46,  6.06batch/s, auc=0.9327, loss=0.3952]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 13/292 [00:03<00:46,  6.06batch/s, auc=0.9351, loss=0.3808]\u001b[A\n",
      "Training Epoch 23/25:   5%|▍         | 14/292 [00:03<00:45,  6.10batch/s, auc=0.9351, loss=0.3808]\u001b[A\n",
      "Training Epoch 23/25:   5%|▍         | 14/292 [00:03<00:45,  6.10batch/s, auc=0.9373, loss=0.4342]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9373, loss=0.4342]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 15/292 [00:03<00:44,  6.18batch/s, auc=0.9376, loss=0.4953]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 16/292 [00:03<00:44,  6.21batch/s, auc=0.9376, loss=0.4953]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 16/292 [00:03<00:44,  6.21batch/s, auc=0.9381, loss=0.4225]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 17/292 [00:03<00:43,  6.25batch/s, auc=0.9381, loss=0.4225]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 17/292 [00:03<00:43,  6.25batch/s, auc=0.9388, loss=0.3822]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9388, loss=0.3822]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 18/292 [00:03<00:43,  6.29batch/s, auc=0.9392, loss=0.4888]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 19/292 [00:03<00:43,  6.27batch/s, auc=0.9392, loss=0.4888]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 19/292 [00:04<00:43,  6.27batch/s, auc=0.9399, loss=0.5406]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 20/292 [00:04<00:43,  6.29batch/s, auc=0.9399, loss=0.5406]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 20/292 [00:04<00:43,  6.29batch/s, auc=0.9395, loss=0.6068]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.9395, loss=0.6068]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 21/292 [00:04<00:42,  6.31batch/s, auc=0.9375, loss=0.7423]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9375, loss=0.7423]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 22/292 [00:04<00:42,  6.31batch/s, auc=0.9385, loss=0.4399]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.9385, loss=0.4399]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 23/292 [00:04<00:42,  6.31batch/s, auc=0.9369, loss=0.7454]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 24/292 [00:04<00:42,  6.32batch/s, auc=0.9369, loss=0.7454]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 24/292 [00:04<00:42,  6.32batch/s, auc=0.9377, loss=0.3594]\u001b[A\n",
      "Training Epoch 23/25:   9%|▊         | 25/292 [00:04<00:42,  6.28batch/s, auc=0.9377, loss=0.3594]\u001b[A\n",
      "Training Epoch 23/25:   9%|▊         | 25/292 [00:05<00:42,  6.28batch/s, auc=0.9373, loss=0.4126]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 26/292 [00:05<00:42,  6.30batch/s, auc=0.9373, loss=0.4126]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 26/292 [00:05<00:42,  6.30batch/s, auc=0.9397, loss=0.2708]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 27/292 [00:05<00:42,  6.28batch/s, auc=0.9397, loss=0.2708]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 27/292 [00:05<00:42,  6.28batch/s, auc=0.9386, loss=0.4981]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 28/292 [00:05<00:41,  6.29batch/s, auc=0.9386, loss=0.4981]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 28/292 [00:05<00:41,  6.29batch/s, auc=0.9405, loss=0.3023]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 29/292 [00:05<00:41,  6.30batch/s, auc=0.9405, loss=0.3023]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 29/292 [00:05<00:41,  6.30batch/s, auc=0.9383, loss=0.8837]\u001b[A\n",
      "Training Epoch 23/25:  10%|█         | 30/292 [00:05<00:41,  6.30batch/s, auc=0.9383, loss=0.8837]\u001b[A\n",
      "Training Epoch 23/25:  10%|█         | 30/292 [00:05<00:41,  6.30batch/s, auc=0.9390, loss=0.4791]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9390, loss=0.4791]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 31/292 [00:05<00:41,  6.30batch/s, auc=0.9370, loss=0.8910]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 32/292 [00:05<00:41,  6.29batch/s, auc=0.9370, loss=0.8910]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 32/292 [00:06<00:41,  6.29batch/s, auc=0.9373, loss=0.4549]\u001b[A\n",
      "Training Epoch 23/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9373, loss=0.4549]\u001b[A\n",
      "Training Epoch 23/25:  11%|█▏        | 33/292 [00:06<00:41,  6.30batch/s, auc=0.9373, loss=0.5373]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9373, loss=0.5373]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 34/292 [00:06<00:40,  6.30batch/s, auc=0.9375, loss=0.6055]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.9375, loss=0.6055]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 35/292 [00:06<00:40,  6.28batch/s, auc=0.9377, loss=0.5279]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9377, loss=0.5279]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 36/292 [00:06<00:40,  6.30batch/s, auc=0.9384, loss=0.3582]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9384, loss=0.3582]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 37/292 [00:06<00:40,  6.29batch/s, auc=0.9383, loss=0.4410]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 38/292 [00:06<00:40,  6.31batch/s, auc=0.9383, loss=0.4410]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 38/292 [00:07<00:40,  6.31batch/s, auc=0.9383, loss=0.4774]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9383, loss=0.4774]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 39/292 [00:07<00:40,  6.31batch/s, auc=0.9386, loss=0.4420]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▎        | 40/292 [00:07<00:40,  6.21batch/s, auc=0.9386, loss=0.4420]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▎        | 40/292 [00:07<00:40,  6.21batch/s, auc=0.9391, loss=0.3616]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 41/292 [00:07<00:40,  6.22batch/s, auc=0.9391, loss=0.3616]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 41/292 [00:07<00:40,  6.22batch/s, auc=0.9399, loss=0.3883]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9399, loss=0.3883]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 42/292 [00:07<00:40,  6.24batch/s, auc=0.9406, loss=0.3490]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9406, loss=0.3490]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▍        | 43/292 [00:07<00:39,  6.25batch/s, auc=0.9415, loss=0.3600]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 44/292 [00:07<00:39,  6.26batch/s, auc=0.9415, loss=0.3600]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 44/292 [00:08<00:39,  6.26batch/s, auc=0.9418, loss=0.5614]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9418, loss=0.5614]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 45/292 [00:08<00:39,  6.28batch/s, auc=0.9419, loss=0.4183]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 46/292 [00:08<00:39,  6.29batch/s, auc=0.9419, loss=0.4183]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 46/292 [00:08<00:39,  6.29batch/s, auc=0.9419, loss=0.5061]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 47/292 [00:08<00:38,  6.28batch/s, auc=0.9419, loss=0.5061]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 47/292 [00:08<00:38,  6.28batch/s, auc=0.9413, loss=0.6521]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9413, loss=0.6521]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▋        | 48/292 [00:08<00:38,  6.29batch/s, auc=0.9411, loss=0.5861]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 49/292 [00:08<00:38,  6.24batch/s, auc=0.9411, loss=0.5861]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 49/292 [00:08<00:38,  6.24batch/s, auc=0.9417, loss=0.4197]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 50/292 [00:08<00:38,  6.25batch/s, auc=0.9417, loss=0.4197]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 50/292 [00:08<00:38,  6.25batch/s, auc=0.9417, loss=0.4538]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 51/292 [00:08<00:38,  6.27batch/s, auc=0.9417, loss=0.4538]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 51/292 [00:09<00:38,  6.27batch/s, auc=0.9419, loss=0.4546]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 52/292 [00:09<00:38,  6.25batch/s, auc=0.9419, loss=0.4546]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 52/292 [00:09<00:38,  6.25batch/s, auc=0.9420, loss=0.4819]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 53/292 [00:09<00:38,  6.26batch/s, auc=0.9420, loss=0.4819]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 53/292 [00:09<00:38,  6.26batch/s, auc=0.9423, loss=0.4840]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9423, loss=0.4840]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 54/292 [00:09<00:37,  6.27batch/s, auc=0.9420, loss=0.6057]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9420, loss=0.6057]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 55/292 [00:09<00:37,  6.28batch/s, auc=0.9420, loss=0.5207]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 56/292 [00:09<00:37,  6.25batch/s, auc=0.9420, loss=0.5207]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 56/292 [00:09<00:37,  6.25batch/s, auc=0.9410, loss=0.7226]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 57/292 [00:09<00:37,  6.26batch/s, auc=0.9410, loss=0.7226]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 57/292 [00:10<00:37,  6.26batch/s, auc=0.9417, loss=0.3587]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9417, loss=0.3587]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 58/292 [00:10<00:37,  6.28batch/s, auc=0.9423, loss=0.2681]\u001b[A\n",
      "Training Epoch 23/25:  20%|██        | 59/292 [00:10<00:37,  6.28batch/s, auc=0.9423, loss=0.2681]\u001b[A\n",
      "Training Epoch 23/25:  20%|██        | 59/292 [00:10<00:37,  6.28batch/s, auc=0.9426, loss=0.3965]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9426, loss=0.3965]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 60/292 [00:10<00:36,  6.28batch/s, auc=0.9430, loss=0.4125]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9430, loss=0.4125]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 61/292 [00:10<00:36,  6.28batch/s, auc=0.9439, loss=0.2432]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 62/292 [00:10<00:36,  6.29batch/s, auc=0.9439, loss=0.2432]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 62/292 [00:10<00:36,  6.29batch/s, auc=0.9437, loss=0.6372]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 63/292 [00:10<00:36,  6.28batch/s, auc=0.9437, loss=0.6372]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 63/292 [00:11<00:36,  6.28batch/s, auc=0.9438, loss=0.4019]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9438, loss=0.4019]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 64/292 [00:11<00:36,  6.28batch/s, auc=0.9442, loss=0.3697]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 65/292 [00:11<00:36,  6.26batch/s, auc=0.9442, loss=0.3697]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 65/292 [00:11<00:36,  6.26batch/s, auc=0.9441, loss=0.5376]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 66/292 [00:11<00:36,  6.27batch/s, auc=0.9441, loss=0.5376]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 66/292 [00:11<00:36,  6.27batch/s, auc=0.9447, loss=0.3221]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9447, loss=0.3221]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9448, loss=0.4459]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9448, loss=0.4459]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 68/292 [00:11<00:35,  6.27batch/s, auc=0.9451, loss=0.3733]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▎       | 69/292 [00:11<00:35,  6.29batch/s, auc=0.9451, loss=0.3733]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▎       | 69/292 [00:12<00:35,  6.29batch/s, auc=0.9453, loss=0.3531]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 70/292 [00:12<00:35,  6.29batch/s, auc=0.9453, loss=0.3531]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 70/292 [00:12<00:35,  6.29batch/s, auc=0.9457, loss=0.3419]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 71/292 [00:12<00:35,  6.29batch/s, auc=0.9457, loss=0.3419]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 71/292 [00:12<00:35,  6.29batch/s, auc=0.9447, loss=0.9853]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9447, loss=0.9853]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9450, loss=0.4826]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9450, loss=0.4826]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 73/292 [00:12<00:34,  6.28batch/s, auc=0.9456, loss=0.2485]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9456, loss=0.2485]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 74/292 [00:12<00:34,  6.27batch/s, auc=0.9448, loss=0.7353]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9448, loss=0.7353]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9447, loss=0.5514]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 76/292 [00:12<00:34,  6.30batch/s, auc=0.9447, loss=0.5514]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 76/292 [00:13<00:34,  6.30batch/s, auc=0.9453, loss=0.3406]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▋       | 77/292 [00:13<00:34,  6.30batch/s, auc=0.9453, loss=0.3406]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▋       | 77/292 [00:13<00:34,  6.30batch/s, auc=0.9448, loss=0.5712]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 78/292 [00:13<00:34,  6.29batch/s, auc=0.9448, loss=0.5712]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 78/292 [00:13<00:34,  6.29batch/s, auc=0.9452, loss=0.3966]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9452, loss=0.3966]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9442, loss=1.0371]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9442, loss=1.0371]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 80/292 [00:13<00:33,  6.26batch/s, auc=0.9435, loss=0.9355]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9435, loss=0.9355]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 81/292 [00:13<00:33,  6.26batch/s, auc=0.9429, loss=0.8373]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 82/292 [00:13<00:33,  6.25batch/s, auc=0.9429, loss=0.8373]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 82/292 [00:14<00:33,  6.25batch/s, auc=0.9426, loss=0.5269]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9426, loss=0.5269]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 83/292 [00:14<00:33,  6.21batch/s, auc=0.9429, loss=0.3777]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 84/292 [00:14<00:33,  6.17batch/s, auc=0.9429, loss=0.3777]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 84/292 [00:14<00:33,  6.17batch/s, auc=0.9431, loss=0.4099]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 85/292 [00:14<00:33,  6.15batch/s, auc=0.9431, loss=0.4099]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 85/292 [00:14<00:33,  6.15batch/s, auc=0.9435, loss=0.4064]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 86/292 [00:14<00:33,  6.17batch/s, auc=0.9435, loss=0.4064]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 86/292 [00:14<00:33,  6.17batch/s, auc=0.9433, loss=0.5568]\u001b[A\n",
      "Training Epoch 23/25:  30%|██▉       | 87/292 [00:14<00:33,  6.11batch/s, auc=0.9433, loss=0.5568]\u001b[A\n",
      "Training Epoch 23/25:  30%|██▉       | 87/292 [00:14<00:33,  6.11batch/s, auc=0.9433, loss=0.4769]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 88/292 [00:14<00:33,  6.13batch/s, auc=0.9433, loss=0.4769]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 88/292 [00:15<00:33,  6.13batch/s, auc=0.9431, loss=0.6229]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 89/292 [00:15<00:32,  6.16batch/s, auc=0.9431, loss=0.6229]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 89/292 [00:15<00:32,  6.16batch/s, auc=0.9432, loss=0.5051]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 90/292 [00:15<00:32,  6.19batch/s, auc=0.9432, loss=0.5051]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 90/292 [00:15<00:32,  6.19batch/s, auc=0.9429, loss=0.5909]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 91/292 [00:15<00:32,  6.18batch/s, auc=0.9429, loss=0.5909]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 91/292 [00:15<00:32,  6.18batch/s, auc=0.9431, loss=0.5287]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 92/292 [00:15<00:32,  6.20batch/s, auc=0.9431, loss=0.5287]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 92/292 [00:15<00:32,  6.20batch/s, auc=0.9435, loss=0.3941]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9435, loss=0.3941]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 93/292 [00:15<00:32,  6.21batch/s, auc=0.9437, loss=0.4819]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 94/292 [00:15<00:31,  6.23batch/s, auc=0.9437, loss=0.4819]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 94/292 [00:16<00:31,  6.23batch/s, auc=0.9433, loss=0.7003]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 95/292 [00:16<00:31,  6.23batch/s, auc=0.9433, loss=0.7003]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 95/292 [00:16<00:31,  6.23batch/s, auc=0.9431, loss=0.5877]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9431, loss=0.5877]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 96/292 [00:16<00:31,  6.23batch/s, auc=0.9434, loss=0.3962]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9434, loss=0.3962]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 97/292 [00:16<00:31,  6.24batch/s, auc=0.9437, loss=0.3245]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9437, loss=0.3245]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▎      | 98/292 [00:16<00:31,  6.24batch/s, auc=0.9441, loss=0.4070]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9441, loss=0.4070]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 99/292 [00:16<00:30,  6.24batch/s, auc=0.9443, loss=0.4205]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9443, loss=0.4205]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 100/292 [00:16<00:30,  6.24batch/s, auc=0.9441, loss=0.5511]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 101/292 [00:16<00:30,  6.24batch/s, auc=0.9441, loss=0.5511]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 101/292 [00:17<00:30,  6.24batch/s, auc=0.9442, loss=0.3322]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 102/292 [00:17<00:30,  6.23batch/s, auc=0.9442, loss=0.3322]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 102/292 [00:17<00:30,  6.23batch/s, auc=0.9447, loss=0.3159]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▌      | 103/292 [00:17<00:30,  6.23batch/s, auc=0.9447, loss=0.3159]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▌      | 103/292 [00:17<00:30,  6.23batch/s, auc=0.9447, loss=0.5295]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9447, loss=0.5295]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 104/292 [00:17<00:30,  6.23batch/s, auc=0.9443, loss=0.5327]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 105/292 [00:17<00:29,  6.23batch/s, auc=0.9443, loss=0.5327]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 105/292 [00:17<00:29,  6.23batch/s, auc=0.9443, loss=0.5806]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▋      | 106/292 [00:17<00:29,  6.23batch/s, auc=0.9443, loss=0.5806]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▋      | 106/292 [00:17<00:29,  6.23batch/s, auc=0.9445, loss=0.4152]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 107/292 [00:17<00:29,  6.23batch/s, auc=0.9445, loss=0.4152]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 107/292 [00:18<00:29,  6.23batch/s, auc=0.9442, loss=0.6078]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9442, loss=0.6078]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 108/292 [00:18<00:29,  6.23batch/s, auc=0.9444, loss=0.4609]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9444, loss=0.4609]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 109/292 [00:18<00:29,  6.22batch/s, auc=0.9449, loss=0.3251]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9449, loss=0.3251]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9450, loss=0.4941]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 111/292 [00:18<00:29,  6.08batch/s, auc=0.9450, loss=0.4941]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 111/292 [00:18<00:29,  6.08batch/s, auc=0.9444, loss=1.0111]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 112/292 [00:18<00:29,  6.12batch/s, auc=0.9444, loss=1.0111]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 112/292 [00:18<00:29,  6.12batch/s, auc=0.9447, loss=0.4608]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▊      | 113/292 [00:18<00:29,  6.14batch/s, auc=0.9447, loss=0.4608]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▊      | 113/292 [00:19<00:29,  6.14batch/s, auc=0.9443, loss=0.7774]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9443, loss=0.7774]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 114/292 [00:19<00:28,  6.16batch/s, auc=0.9440, loss=0.6723]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 115/292 [00:19<00:28,  6.19batch/s, auc=0.9440, loss=0.6723]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 115/292 [00:19<00:28,  6.19batch/s, auc=0.9443, loss=0.4664]\u001b[A\n",
      "Training Epoch 23/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9443, loss=0.4664]\u001b[A\n",
      "Training Epoch 23/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9442, loss=0.5222]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 117/292 [00:19<00:28,  6.12batch/s, auc=0.9442, loss=0.5222]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 117/292 [00:19<00:28,  6.12batch/s, auc=0.9436, loss=0.8370]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 118/292 [00:19<00:28,  6.04batch/s, auc=0.9436, loss=0.8370]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 118/292 [00:19<00:28,  6.04batch/s, auc=0.9433, loss=0.5177]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 119/292 [00:19<00:29,  5.88batch/s, auc=0.9433, loss=0.5177]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 119/292 [00:20<00:29,  5.88batch/s, auc=0.9436, loss=0.5204]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 120/292 [00:20<00:29,  5.85batch/s, auc=0.9436, loss=0.5204]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 120/292 [00:20<00:29,  5.85batch/s, auc=0.9439, loss=0.3565]\u001b[A\n",
      "Training Epoch 23/25:  41%|████▏     | 121/292 [00:20<00:29,  5.84batch/s, auc=0.9439, loss=0.3565]\u001b[A\n",
      "Training Epoch 23/25:  41%|████▏     | 121/292 [00:20<00:29,  5.84batch/s, auc=0.9439, loss=0.4431]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 122/292 [00:20<00:28,  5.92batch/s, auc=0.9439, loss=0.4431]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 122/292 [00:20<00:28,  5.92batch/s, auc=0.9440, loss=0.3977]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 123/292 [00:20<00:29,  5.76batch/s, auc=0.9440, loss=0.3977]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 123/292 [00:20<00:29,  5.76batch/s, auc=0.9436, loss=0.8588]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 124/292 [00:20<00:29,  5.72batch/s, auc=0.9436, loss=0.8588]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 124/292 [00:20<00:29,  5.72batch/s, auc=0.9438, loss=0.3864]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 125/292 [00:20<00:28,  5.84batch/s, auc=0.9438, loss=0.3864]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 125/292 [00:21<00:28,  5.84batch/s, auc=0.9439, loss=0.5350]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 126/292 [00:21<00:27,  5.94batch/s, auc=0.9439, loss=0.5350]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 126/292 [00:21<00:27,  5.94batch/s, auc=0.9437, loss=0.5822]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 127/292 [00:21<00:27,  6.00batch/s, auc=0.9437, loss=0.5822]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 127/292 [00:21<00:27,  6.00batch/s, auc=0.9434, loss=0.5815]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 128/292 [00:21<00:27,  6.07batch/s, auc=0.9434, loss=0.5815]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 128/292 [00:21<00:27,  6.07batch/s, auc=0.9435, loss=0.4401]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 129/292 [00:21<00:26,  6.12batch/s, auc=0.9435, loss=0.4401]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 129/292 [00:21<00:26,  6.12batch/s, auc=0.9437, loss=0.4150]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 130/292 [00:21<00:26,  6.15batch/s, auc=0.9437, loss=0.4150]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 130/292 [00:21<00:26,  6.15batch/s, auc=0.9440, loss=0.3768]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 131/292 [00:21<00:26,  6.15batch/s, auc=0.9440, loss=0.3768]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 131/292 [00:22<00:26,  6.15batch/s, auc=0.9441, loss=0.6668]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▌     | 132/292 [00:22<00:26,  6.14batch/s, auc=0.9441, loss=0.6668]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▌     | 132/292 [00:22<00:26,  6.14batch/s, auc=0.9439, loss=0.6365]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 133/292 [00:22<00:25,  6.15batch/s, auc=0.9439, loss=0.6365]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 133/292 [00:22<00:25,  6.15batch/s, auc=0.9437, loss=0.5177]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 134/292 [00:22<00:25,  6.15batch/s, auc=0.9437, loss=0.5177]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 134/292 [00:22<00:25,  6.15batch/s, auc=0.9440, loss=0.3005]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9440, loss=0.3005]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 135/292 [00:22<00:25,  6.16batch/s, auc=0.9441, loss=0.4646]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9441, loss=0.4646]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 136/292 [00:22<00:25,  6.17batch/s, auc=0.9445, loss=0.3788]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 137/292 [00:22<00:25,  6.18batch/s, auc=0.9445, loss=0.3788]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 137/292 [00:23<00:25,  6.18batch/s, auc=0.9446, loss=0.4136]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 138/292 [00:23<00:25,  6.15batch/s, auc=0.9446, loss=0.4136]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 138/292 [00:23<00:25,  6.15batch/s, auc=0.9448, loss=0.4705]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 139/292 [00:23<00:25,  6.10batch/s, auc=0.9448, loss=0.4705]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 139/292 [00:23<00:25,  6.10batch/s, auc=0.9448, loss=0.5225]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 140/292 [00:23<00:25,  5.87batch/s, auc=0.9448, loss=0.5225]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 140/292 [00:23<00:25,  5.87batch/s, auc=0.9447, loss=0.5171]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 141/292 [00:23<00:25,  5.83batch/s, auc=0.9447, loss=0.5171]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 141/292 [00:23<00:25,  5.83batch/s, auc=0.9445, loss=0.5438]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▊     | 142/292 [00:23<00:26,  5.74batch/s, auc=0.9445, loss=0.5438]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▊     | 142/292 [00:23<00:26,  5.74batch/s, auc=0.9440, loss=0.9247]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 143/292 [00:23<00:25,  5.81batch/s, auc=0.9440, loss=0.9247]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 143/292 [00:24<00:25,  5.81batch/s, auc=0.9443, loss=0.2515]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 144/292 [00:24<00:25,  5.90batch/s, auc=0.9443, loss=0.2515]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 144/292 [00:24<00:25,  5.90batch/s, auc=0.9447, loss=0.3482]\u001b[A\n",
      "Training Epoch 23/25:  50%|████▉     | 145/292 [00:24<00:25,  5.73batch/s, auc=0.9447, loss=0.3482]\u001b[A\n",
      "Training Epoch 23/25:  50%|████▉     | 145/292 [00:24<00:25,  5.73batch/s, auc=0.9447, loss=0.5238]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 146/292 [00:24<00:25,  5.82batch/s, auc=0.9447, loss=0.5238]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 146/292 [00:24<00:25,  5.82batch/s, auc=0.9448, loss=0.4683]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 147/292 [00:24<00:25,  5.66batch/s, auc=0.9448, loss=0.4683]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 147/292 [00:24<00:25,  5.66batch/s, auc=0.9450, loss=0.3685]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 148/292 [00:24<00:25,  5.68batch/s, auc=0.9450, loss=0.3685]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 148/292 [00:24<00:25,  5.68batch/s, auc=0.9451, loss=0.4677]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 149/292 [00:24<00:24,  5.79batch/s, auc=0.9451, loss=0.4677]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 149/292 [00:25<00:24,  5.79batch/s, auc=0.9449, loss=0.5852]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████▏    | 150/292 [00:25<00:24,  5.89batch/s, auc=0.9449, loss=0.5852]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████▏    | 150/292 [00:25<00:24,  5.89batch/s, auc=0.9447, loss=0.5337]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 151/292 [00:25<00:24,  5.74batch/s, auc=0.9447, loss=0.5337]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 151/292 [00:25<00:24,  5.74batch/s, auc=0.9446, loss=0.5581]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 152/292 [00:25<00:23,  5.85batch/s, auc=0.9446, loss=0.5581]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 152/292 [00:25<00:23,  5.85batch/s, auc=0.9447, loss=0.3724]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 153/292 [00:25<00:24,  5.72batch/s, auc=0.9447, loss=0.3724]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 153/292 [00:25<00:24,  5.72batch/s, auc=0.9449, loss=0.3639]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 154/292 [00:25<00:24,  5.73batch/s, auc=0.9449, loss=0.3639]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 154/292 [00:26<00:24,  5.73batch/s, auc=0.9451, loss=0.4104]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 155/292 [00:26<00:23,  5.81batch/s, auc=0.9451, loss=0.4104]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 155/292 [00:26<00:23,  5.81batch/s, auc=0.9451, loss=0.5305]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 156/292 [00:26<00:23,  5.88batch/s, auc=0.9451, loss=0.5305]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 156/292 [00:26<00:23,  5.88batch/s, auc=0.9452, loss=0.4638]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 157/292 [00:26<00:22,  5.92batch/s, auc=0.9452, loss=0.4638]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 157/292 [00:26<00:22,  5.92batch/s, auc=0.9453, loss=0.3606]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 158/292 [00:26<00:22,  5.97batch/s, auc=0.9453, loss=0.3606]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 158/292 [00:26<00:22,  5.97batch/s, auc=0.9451, loss=0.7995]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 159/292 [00:26<00:22,  6.03batch/s, auc=0.9451, loss=0.7995]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 159/292 [00:26<00:22,  6.03batch/s, auc=0.9449, loss=0.6322]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.06batch/s, auc=0.9449, loss=0.6322]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▍    | 160/292 [00:27<00:21,  6.06batch/s, auc=0.9449, loss=0.4221]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 161/292 [00:27<00:21,  6.07batch/s, auc=0.9449, loss=0.4221]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 161/292 [00:27<00:21,  6.07batch/s, auc=0.9448, loss=0.6457]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.09batch/s, auc=0.9448, loss=0.6457]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.09batch/s, auc=0.9446, loss=0.7378]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9446, loss=0.7378]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.10batch/s, auc=0.9446, loss=0.4923]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.10batch/s, auc=0.9446, loss=0.4923]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.10batch/s, auc=0.9447, loss=0.3800]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.93batch/s, auc=0.9447, loss=0.3800]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 165/292 [00:27<00:21,  5.93batch/s, auc=0.9449, loss=0.3390]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 166/292 [00:27<00:21,  5.81batch/s, auc=0.9449, loss=0.3390]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 166/292 [00:28<00:21,  5.81batch/s, auc=0.9448, loss=0.5215]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 167/292 [00:28<00:21,  5.81batch/s, auc=0.9448, loss=0.5215]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 167/292 [00:28<00:21,  5.81batch/s, auc=0.9450, loss=0.3827]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 168/292 [00:28<00:21,  5.90batch/s, auc=0.9450, loss=0.3827]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 168/292 [00:28<00:21,  5.90batch/s, auc=0.9446, loss=0.7787]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 169/292 [00:28<00:21,  5.71batch/s, auc=0.9446, loss=0.7787]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 169/292 [00:28<00:21,  5.71batch/s, auc=0.9446, loss=0.4607]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 170/292 [00:28<00:21,  5.72batch/s, auc=0.9446, loss=0.4607]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 170/292 [00:28<00:21,  5.72batch/s, auc=0.9446, loss=0.4680]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▊    | 171/292 [00:28<00:21,  5.72batch/s, auc=0.9446, loss=0.4680]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▊    | 171/292 [00:28<00:21,  5.72batch/s, auc=0.9445, loss=0.8434]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 172/292 [00:28<00:20,  5.72batch/s, auc=0.9445, loss=0.8434]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 172/292 [00:29<00:20,  5.72batch/s, auc=0.9444, loss=0.6544]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 173/292 [00:29<00:20,  5.72batch/s, auc=0.9444, loss=0.6544]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 173/292 [00:29<00:20,  5.72batch/s, auc=0.9446, loss=0.3589]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 174/292 [00:29<00:20,  5.84batch/s, auc=0.9446, loss=0.3589]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 174/292 [00:29<00:20,  5.84batch/s, auc=0.9447, loss=0.4001]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 175/292 [00:29<00:19,  5.93batch/s, auc=0.9447, loss=0.4001]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 175/292 [00:29<00:19,  5.93batch/s, auc=0.9445, loss=0.5111]\u001b[A\n",
      "Training Epoch 23/25:  60%|██████    | 176/292 [00:29<00:19,  5.97batch/s, auc=0.9445, loss=0.5111]\u001b[A\n",
      "Training Epoch 23/25:  60%|██████    | 176/292 [00:29<00:19,  5.97batch/s, auc=0.9445, loss=0.5094]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 177/292 [00:29<00:19,  5.98batch/s, auc=0.9445, loss=0.5094]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 177/292 [00:29<00:19,  5.98batch/s, auc=0.9445, loss=0.3926]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 178/292 [00:29<00:19,  5.89batch/s, auc=0.9445, loss=0.3926]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 178/292 [00:30<00:19,  5.89batch/s, auc=0.9445, loss=0.5426]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████▏   | 179/292 [00:30<00:19,  5.82batch/s, auc=0.9445, loss=0.5426]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████▏   | 179/292 [00:30<00:19,  5.82batch/s, auc=0.9447, loss=0.3121]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.77batch/s, auc=0.9447, loss=0.3121]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.77batch/s, auc=0.9445, loss=0.6905]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.75batch/s, auc=0.9445, loss=0.6905]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 181/292 [00:30<00:19,  5.75batch/s, auc=0.9444, loss=0.7416]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.73batch/s, auc=0.9444, loss=0.7416]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 182/292 [00:30<00:19,  5.73batch/s, auc=0.9445, loss=0.3558]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.75batch/s, auc=0.9445, loss=0.3558]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 183/292 [00:30<00:18,  5.75batch/s, auc=0.9444, loss=0.4723]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 184/292 [00:30<00:18,  5.86batch/s, auc=0.9444, loss=0.4723]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 184/292 [00:31<00:18,  5.86batch/s, auc=0.9442, loss=0.6111]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 185/292 [00:31<00:18,  5.82batch/s, auc=0.9442, loss=0.6111]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 185/292 [00:31<00:18,  5.82batch/s, auc=0.9444, loss=0.2782]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▎   | 186/292 [00:31<00:17,  5.91batch/s, auc=0.9444, loss=0.2782]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▎   | 186/292 [00:31<00:17,  5.91batch/s, auc=0.9446, loss=0.3838]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.76batch/s, auc=0.9446, loss=0.3838]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 187/292 [00:31<00:18,  5.76batch/s, auc=0.9445, loss=0.4524]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.72batch/s, auc=0.9445, loss=0.4524]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 188/292 [00:31<00:18,  5.72batch/s, auc=0.9446, loss=0.4537]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▍   | 189/292 [00:31<00:17,  5.76batch/s, auc=0.9446, loss=0.4537]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▍   | 189/292 [00:32<00:17,  5.76batch/s, auc=0.9447, loss=0.3675]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 190/292 [00:32<00:17,  5.73batch/s, auc=0.9447, loss=0.3675]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 190/292 [00:32<00:17,  5.73batch/s, auc=0.9447, loss=0.4638]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 191/292 [00:32<00:17,  5.71batch/s, auc=0.9447, loss=0.4638]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 191/292 [00:32<00:17,  5.71batch/s, auc=0.9450, loss=0.3066]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.83batch/s, auc=0.9450, loss=0.3066]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.83batch/s, auc=0.9447, loss=0.6848]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.79batch/s, auc=0.9447, loss=0.6848]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.79batch/s, auc=0.9447, loss=0.5389]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.87batch/s, auc=0.9447, loss=0.5389]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▋   | 194/292 [00:32<00:16,  5.87batch/s, auc=0.9445, loss=0.8992]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.74batch/s, auc=0.9445, loss=0.8992]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 195/292 [00:33<00:16,  5.74batch/s, auc=0.9443, loss=0.7259]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 196/292 [00:33<00:16,  5.69batch/s, auc=0.9443, loss=0.7259]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 196/292 [00:33<00:16,  5.69batch/s, auc=0.9445, loss=0.3957]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 197/292 [00:33<00:16,  5.69batch/s, auc=0.9445, loss=0.3957]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 197/292 [00:33<00:16,  5.69batch/s, auc=0.9443, loss=0.7236]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.68batch/s, auc=0.9443, loss=0.7236]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.68batch/s, auc=0.9444, loss=0.3880]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.68batch/s, auc=0.9444, loss=0.3880]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.68batch/s, auc=0.9442, loss=0.9321]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.67batch/s, auc=0.9442, loss=0.9321]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.67batch/s, auc=0.9443, loss=0.3775]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 201/292 [00:33<00:16,  5.68batch/s, auc=0.9443, loss=0.3775]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 201/292 [00:34<00:16,  5.68batch/s, auc=0.9445, loss=0.2999]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 202/292 [00:34<00:15,  5.68batch/s, auc=0.9445, loss=0.2999]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 202/292 [00:34<00:15,  5.68batch/s, auc=0.9446, loss=0.3850]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.65batch/s, auc=0.9446, loss=0.3850]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.65batch/s, auc=0.9446, loss=0.6407]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.58batch/s, auc=0.9446, loss=0.6407]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.58batch/s, auc=0.9446, loss=0.4492]\u001b[A\n",
      "Training Epoch 23/25:  70%|███████   | 205/292 [00:34<00:15,  5.72batch/s, auc=0.9446, loss=0.4492]\u001b[A\n",
      "Training Epoch 23/25:  70%|███████   | 205/292 [00:34<00:15,  5.72batch/s, auc=0.9445, loss=0.5813]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 206/292 [00:34<00:15,  5.71batch/s, auc=0.9445, loss=0.5813]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 206/292 [00:34<00:15,  5.71batch/s, auc=0.9446, loss=0.4687]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 207/292 [00:34<00:14,  5.68batch/s, auc=0.9446, loss=0.4687]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 207/292 [00:35<00:14,  5.68batch/s, auc=0.9447, loss=0.5229]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 208/292 [00:35<00:14,  5.70batch/s, auc=0.9447, loss=0.5229]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 208/292 [00:35<00:14,  5.70batch/s, auc=0.9446, loss=0.6752]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 209/292 [00:35<00:14,  5.69batch/s, auc=0.9446, loss=0.6752]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 209/292 [00:35<00:14,  5.69batch/s, auc=0.9444, loss=0.5284]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.66batch/s, auc=0.9444, loss=0.5284]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.66batch/s, auc=0.9445, loss=0.3815]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.67batch/s, auc=0.9445, loss=0.3815]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.67batch/s, auc=0.9443, loss=0.6549]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.68batch/s, auc=0.9443, loss=0.6549]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 212/292 [00:36<00:14,  5.68batch/s, auc=0.9442, loss=0.4974]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 213/292 [00:36<00:13,  5.65batch/s, auc=0.9442, loss=0.4974]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 213/292 [00:36<00:13,  5.65batch/s, auc=0.9442, loss=0.4734]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 214/292 [00:36<00:13,  5.65batch/s, auc=0.9442, loss=0.4734]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 214/292 [00:36<00:13,  5.65batch/s, auc=0.9443, loss=0.4484]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.65batch/s, auc=0.9443, loss=0.4484]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.65batch/s, auc=0.9444, loss=0.3639]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.77batch/s, auc=0.9444, loss=0.3639]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.77batch/s, auc=0.9446, loss=0.4028]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.61batch/s, auc=0.9446, loss=0.4028]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.61batch/s, auc=0.9446, loss=0.4436]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.60batch/s, auc=0.9446, loss=0.4436]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▍  | 218/292 [00:37<00:13,  5.60batch/s, auc=0.9446, loss=0.5022]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 219/292 [00:37<00:12,  5.68batch/s, auc=0.9446, loss=0.5022]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 219/292 [00:37<00:12,  5.68batch/s, auc=0.9445, loss=0.4731]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 220/292 [00:37<00:12,  5.65batch/s, auc=0.9445, loss=0.4731]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 220/292 [00:37<00:12,  5.65batch/s, auc=0.9443, loss=0.6245]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.64batch/s, auc=0.9443, loss=0.6245]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.64batch/s, auc=0.9444, loss=0.3742]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.59batch/s, auc=0.9444, loss=0.3742]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.59batch/s, auc=0.9440, loss=0.7154]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.64batch/s, auc=0.9440, loss=0.7154]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.64batch/s, auc=0.9442, loss=0.2925]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 224/292 [00:38<00:12,  5.62batch/s, auc=0.9442, loss=0.2925]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 224/292 [00:38<00:12,  5.62batch/s, auc=0.9443, loss=0.4890]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 225/292 [00:38<00:11,  5.61batch/s, auc=0.9443, loss=0.4890]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 225/292 [00:38<00:11,  5.61batch/s, auc=0.9445, loss=0.3783]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.62batch/s, auc=0.9445, loss=0.3783]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.62batch/s, auc=0.9444, loss=0.5731]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.63batch/s, auc=0.9444, loss=0.5731]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.63batch/s, auc=0.9446, loss=0.3500]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.63batch/s, auc=0.9446, loss=0.3500]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.63batch/s, auc=0.9444, loss=0.6360]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 229/292 [00:38<00:11,  5.64batch/s, auc=0.9444, loss=0.6360]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 229/292 [00:39<00:11,  5.64batch/s, auc=0.9444, loss=0.4681]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 230/292 [00:39<00:11,  5.62batch/s, auc=0.9444, loss=0.4681]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 230/292 [00:39<00:11,  5.62batch/s, auc=0.9444, loss=0.4126]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 231/292 [00:39<00:10,  5.69batch/s, auc=0.9444, loss=0.4126]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 231/292 [00:39<00:10,  5.69batch/s, auc=0.9444, loss=0.4250]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.68batch/s, auc=0.9444, loss=0.4250]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.68batch/s, auc=0.9442, loss=0.8923]\u001b[A\n",
      "Training Epoch 23/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.66batch/s, auc=0.9442, loss=0.8923]\u001b[A\n",
      "Training Epoch 23/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.66batch/s, auc=0.9442, loss=0.4384]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 234/292 [00:39<00:10,  5.66batch/s, auc=0.9442, loss=0.4384]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 234/292 [00:39<00:10,  5.66batch/s, auc=0.9442, loss=0.5941]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 235/292 [00:39<00:10,  5.66batch/s, auc=0.9442, loss=0.5941]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 235/292 [00:40<00:10,  5.66batch/s, auc=0.9443, loss=0.3644]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 236/292 [00:40<00:09,  5.77batch/s, auc=0.9443, loss=0.3644]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 236/292 [00:40<00:09,  5.77batch/s, auc=0.9445, loss=0.2897]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 237/292 [00:40<00:09,  5.73batch/s, auc=0.9445, loss=0.2897]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 237/292 [00:40<00:09,  5.73batch/s, auc=0.9445, loss=0.4028]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.82batch/s, auc=0.9445, loss=0.4028]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.82batch/s, auc=0.9444, loss=0.5289]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.76batch/s, auc=0.9444, loss=0.5289]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.76batch/s, auc=0.9442, loss=0.7834]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.66batch/s, auc=0.9442, loss=0.7834]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.66batch/s, auc=0.9440, loss=0.8343]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 241/292 [00:40<00:09,  5.65batch/s, auc=0.9440, loss=0.8343]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 241/292 [00:41<00:09,  5.65batch/s, auc=0.9441, loss=0.3953]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 242/292 [00:41<00:08,  5.63batch/s, auc=0.9441, loss=0.3953]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 242/292 [00:41<00:08,  5.63batch/s, auc=0.9439, loss=0.7708]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 243/292 [00:41<00:08,  5.68batch/s, auc=0.9439, loss=0.7708]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 243/292 [00:41<00:08,  5.68batch/s, auc=0.9438, loss=0.6395]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.68batch/s, auc=0.9438, loss=0.6395]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.68batch/s, auc=0.9436, loss=0.7494]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.66batch/s, auc=0.9436, loss=0.7494]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.66batch/s, auc=0.9437, loss=0.4063]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.65batch/s, auc=0.9437, loss=0.4063]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 246/292 [00:42<00:08,  5.65batch/s, auc=0.9437, loss=0.3939]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 247/292 [00:42<00:07,  5.71batch/s, auc=0.9437, loss=0.3939]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 247/292 [00:42<00:07,  5.71batch/s, auc=0.9434, loss=0.7564]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 248/292 [00:42<00:07,  5.81batch/s, auc=0.9434, loss=0.7564]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 248/292 [00:42<00:07,  5.81batch/s, auc=0.9432, loss=0.7985]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.88batch/s, auc=0.9432, loss=0.7985]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.88batch/s, auc=0.9432, loss=0.4932]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.75batch/s, auc=0.9432, loss=0.4932]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.75batch/s, auc=0.9433, loss=0.3370]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.73batch/s, auc=0.9433, loss=0.3370]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.73batch/s, auc=0.9432, loss=0.6055]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▋ | 252/292 [00:42<00:06,  5.72batch/s, auc=0.9432, loss=0.6055]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▋ | 252/292 [00:43<00:06,  5.72batch/s, auc=0.9431, loss=0.8355]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 253/292 [00:43<00:06,  5.70batch/s, auc=0.9431, loss=0.8355]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 253/292 [00:43<00:06,  5.70batch/s, auc=0.9430, loss=0.6286]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 254/292 [00:43<00:06,  5.70batch/s, auc=0.9430, loss=0.6286]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 254/292 [00:43<00:06,  5.70batch/s, auc=0.9431, loss=0.3997]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.69batch/s, auc=0.9431, loss=0.3997]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.69batch/s, auc=0.9431, loss=0.4964]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.80batch/s, auc=0.9431, loss=0.4964]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.80batch/s, auc=0.9431, loss=0.5282]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.62batch/s, auc=0.9431, loss=0.5282]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.62batch/s, auc=0.9429, loss=0.6668]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.60batch/s, auc=0.9429, loss=0.6668]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 258/292 [00:44<00:06,  5.60batch/s, auc=0.9428, loss=0.6220]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▊ | 259/292 [00:44<00:05,  5.69batch/s, auc=0.9428, loss=0.6220]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▊ | 259/292 [00:44<00:05,  5.69batch/s, auc=0.9428, loss=0.5492]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 260/292 [00:44<00:05,  5.67batch/s, auc=0.9428, loss=0.5492]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 260/292 [00:44<00:05,  5.67batch/s, auc=0.9428, loss=0.4968]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.66batch/s, auc=0.9428, loss=0.4968]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.66batch/s, auc=0.9428, loss=0.4725]\u001b[A\n",
      "Training Epoch 23/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.61batch/s, auc=0.9428, loss=0.4725]\u001b[A\n",
      "Training Epoch 23/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.61batch/s, auc=0.9428, loss=0.4062]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 263/292 [00:44<00:05,  5.68batch/s, auc=0.9428, loss=0.4062]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 263/292 [00:45<00:05,  5.68batch/s, auc=0.9425, loss=1.0869]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 264/292 [00:45<00:04,  5.62batch/s, auc=0.9425, loss=1.0869]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 264/292 [00:45<00:04,  5.62batch/s, auc=0.9425, loss=0.4969]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 265/292 [00:45<00:04,  5.62batch/s, auc=0.9425, loss=0.4969]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 265/292 [00:45<00:04,  5.62batch/s, auc=0.9426, loss=0.4186]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 266/292 [00:45<00:04,  5.63batch/s, auc=0.9426, loss=0.4186]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 266/292 [00:45<00:04,  5.63batch/s, auc=0.9426, loss=0.3733]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.64batch/s, auc=0.9426, loss=0.3733]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.64batch/s, auc=0.9426, loss=0.5574]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.62batch/s, auc=0.9426, loss=0.5574]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.62batch/s, auc=0.9427, loss=0.3876]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.60batch/s, auc=0.9427, loss=0.3876]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 269/292 [00:46<00:04,  5.60batch/s, auc=0.9426, loss=0.5739]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 270/292 [00:46<00:03,  5.64batch/s, auc=0.9426, loss=0.5739]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 270/292 [00:46<00:03,  5.64batch/s, auc=0.9427, loss=0.4497]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 271/292 [00:46<00:03,  5.62batch/s, auc=0.9427, loss=0.4497]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 271/292 [00:46<00:03,  5.62batch/s, auc=0.9427, loss=0.4487]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.72batch/s, auc=0.9427, loss=0.4487]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.72batch/s, auc=0.9429, loss=0.2956]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.78batch/s, auc=0.9429, loss=0.2956]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.78batch/s, auc=0.9430, loss=0.4038]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.73batch/s, auc=0.9430, loss=0.4038]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.73batch/s, auc=0.9430, loss=0.4404]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 275/292 [00:46<00:02,  5.78batch/s, auc=0.9430, loss=0.4404]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 275/292 [00:47<00:02,  5.78batch/s, auc=0.9425, loss=1.0917]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 276/292 [00:47<00:02,  5.84batch/s, auc=0.9425, loss=1.0917]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 276/292 [00:47<00:02,  5.84batch/s, auc=0.9425, loss=0.5980]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 277/292 [00:47<00:02,  5.87batch/s, auc=0.9425, loss=0.5980]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 277/292 [00:47<00:02,  5.87batch/s, auc=0.9423, loss=0.7811]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.78batch/s, auc=0.9423, loss=0.7811]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.78batch/s, auc=0.9423, loss=0.5442]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.72batch/s, auc=0.9423, loss=0.5442]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.72batch/s, auc=0.9423, loss=0.4329]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.69batch/s, auc=0.9423, loss=0.4329]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 280/292 [00:48<00:02,  5.69batch/s, auc=0.9421, loss=0.6469]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 281/292 [00:48<00:01,  5.67batch/s, auc=0.9421, loss=0.6469]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 281/292 [00:48<00:01,  5.67batch/s, auc=0.9420, loss=0.5709]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 282/292 [00:48<00:01,  5.61batch/s, auc=0.9420, loss=0.5709]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 282/292 [00:48<00:01,  5.61batch/s, auc=0.9421, loss=0.3609]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.62batch/s, auc=0.9421, loss=0.3609]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.62batch/s, auc=0.9421, loss=0.4762]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.60batch/s, auc=0.9421, loss=0.4762]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.60batch/s, auc=0.9421, loss=0.5032]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9421, loss=0.5032]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.59batch/s, auc=0.9419, loss=0.7155]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.69batch/s, auc=0.9419, loss=0.7155]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 286/292 [00:49<00:01,  5.69batch/s, auc=0.9420, loss=0.4681]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 287/292 [00:49<00:00,  5.79batch/s, auc=0.9420, loss=0.4681]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 287/292 [00:49<00:00,  5.79batch/s, auc=0.9420, loss=0.5558]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▊| 288/292 [00:49<00:00,  5.82batch/s, auc=0.9420, loss=0.5558]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▊| 288/292 [00:49<00:00,  5.82batch/s, auc=0.9418, loss=0.8334]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9418, loss=0.8334]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.86batch/s, auc=0.9417, loss=0.6746]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9417, loss=0.6746]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.88batch/s, auc=0.9417, loss=0.3774]\u001b[A\n",
      "Training Epoch 23/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.89batch/s, auc=0.9417, loss=0.3774]\u001b[A\n",
      "Training Epoch 23/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.89batch/s, auc=0.9417, loss=0.4887]\u001b[A\n",
      "Training Epoch 23/25: 100%|██████████| 292/292 [00:49<00:00,  5.84batch/s, auc=0.9417, loss=0.4887]\u001b[A\n",
      "Epochs:  92%|█████████▏| 23/25 [21:04<01:50, 55.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25] Train Loss: 0.5173 | Train AUROC: 0.9417 Val Loss: 0.6237 | Val AUROC: 0.9114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 24/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9770, loss=0.3464]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 1/292 [00:01<04:51,  1.00s/batch, auc=0.9770, loss=0.3464]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 1/292 [00:01<04:51,  1.00s/batch, auc=0.9770, loss=0.3818]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9770, loss=0.3818]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 2/292 [00:01<02:26,  1.98batch/s, auc=0.9629, loss=0.5711]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 3/292 [00:01<01:41,  2.85batch/s, auc=0.9629, loss=0.5711]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 3/292 [00:01<01:41,  2.85batch/s, auc=0.9516, loss=0.5061]\u001b[A\n",
      "Training Epoch 24/25:   1%|▏         | 4/292 [00:01<01:20,  3.59batch/s, auc=0.9516, loss=0.5061]\u001b[A\n",
      "Training Epoch 24/25:   1%|▏         | 4/292 [00:01<01:20,  3.59batch/s, auc=0.9543, loss=0.4382]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 5/292 [00:01<01:07,  4.26batch/s, auc=0.9543, loss=0.4382]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 5/292 [00:01<01:07,  4.26batch/s, auc=0.9470, loss=0.6449]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 6/292 [00:01<01:00,  4.71batch/s, auc=0.9470, loss=0.6449]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 6/292 [00:01<01:00,  4.71batch/s, auc=0.9460, loss=0.5250]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 7/292 [00:01<00:56,  5.05batch/s, auc=0.9460, loss=0.5250]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 7/292 [00:02<00:56,  5.05batch/s, auc=0.9450, loss=0.4650]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 8/292 [00:02<00:52,  5.40batch/s, auc=0.9450, loss=0.4650]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 8/292 [00:02<00:52,  5.40batch/s, auc=0.9447, loss=0.4171]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 9/292 [00:02<00:51,  5.52batch/s, auc=0.9447, loss=0.4171]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 9/292 [00:02<00:51,  5.52batch/s, auc=0.9442, loss=0.5419]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9442, loss=0.5419]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 10/292 [00:02<00:49,  5.75batch/s, auc=0.9463, loss=0.4837]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 11/292 [00:02<00:47,  5.93batch/s, auc=0.9463, loss=0.4837]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 11/292 [00:02<00:47,  5.93batch/s, auc=0.9449, loss=0.5155]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 12/292 [00:02<00:46,  5.98batch/s, auc=0.9449, loss=0.5155]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 12/292 [00:02<00:46,  5.98batch/s, auc=0.9397, loss=0.8624]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 13/292 [00:02<00:45,  6.08batch/s, auc=0.9397, loss=0.8624]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 13/292 [00:03<00:45,  6.08batch/s, auc=0.9426, loss=0.3438]\u001b[A\n",
      "Training Epoch 24/25:   5%|▍         | 14/292 [00:03<00:45,  6.16batch/s, auc=0.9426, loss=0.3438]\u001b[A\n",
      "Training Epoch 24/25:   5%|▍         | 14/292 [00:03<00:45,  6.16batch/s, auc=0.9411, loss=0.6875]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 15/292 [00:03<00:44,  6.21batch/s, auc=0.9411, loss=0.6875]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 15/292 [00:03<00:44,  6.21batch/s, auc=0.9438, loss=0.3519]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9438, loss=0.3519]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 16/292 [00:03<00:44,  6.25batch/s, auc=0.9444, loss=0.4843]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 17/292 [00:03<00:43,  6.28batch/s, auc=0.9444, loss=0.4843]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 17/292 [00:03<00:43,  6.28batch/s, auc=0.9445, loss=0.4211]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.9445, loss=0.4211]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 18/292 [00:03<00:43,  6.31batch/s, auc=0.9452, loss=0.4705]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 19/292 [00:03<00:43,  6.32batch/s, auc=0.9452, loss=0.4705]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 19/292 [00:04<00:43,  6.32batch/s, auc=0.9443, loss=0.4848]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9443, loss=0.4848]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 20/292 [00:04<00:43,  6.32batch/s, auc=0.9441, loss=0.5462]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 21/292 [00:04<00:42,  6.30batch/s, auc=0.9441, loss=0.5462]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 21/292 [00:04<00:42,  6.30batch/s, auc=0.9461, loss=0.3547]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9461, loss=0.3547]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 22/292 [00:04<00:42,  6.32batch/s, auc=0.9463, loss=0.4809]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9463, loss=0.4809]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 23/292 [00:04<00:42,  6.30batch/s, auc=0.9452, loss=0.4880]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9452, loss=0.4880]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 24/292 [00:04<00:42,  6.25batch/s, auc=0.9446, loss=0.5340]\u001b[A\n",
      "Training Epoch 24/25:   9%|▊         | 25/292 [00:04<00:42,  6.27batch/s, auc=0.9446, loss=0.5340]\u001b[A\n",
      "Training Epoch 24/25:   9%|▊         | 25/292 [00:05<00:42,  6.27batch/s, auc=0.9435, loss=0.5988]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 26/292 [00:05<00:42,  6.28batch/s, auc=0.9435, loss=0.5988]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 26/292 [00:05<00:42,  6.28batch/s, auc=0.9434, loss=0.4337]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 27/292 [00:05<00:42,  6.30batch/s, auc=0.9434, loss=0.4337]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 27/292 [00:05<00:42,  6.30batch/s, auc=0.9439, loss=0.3906]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 28/292 [00:05<00:41,  6.31batch/s, auc=0.9439, loss=0.3906]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 28/292 [00:05<00:41,  6.31batch/s, auc=0.9447, loss=0.4728]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9447, loss=0.4728]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 29/292 [00:05<00:41,  6.32batch/s, auc=0.9440, loss=0.5239]\u001b[A\n",
      "Training Epoch 24/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9440, loss=0.5239]\u001b[A\n",
      "Training Epoch 24/25:  10%|█         | 30/292 [00:05<00:41,  6.33batch/s, auc=0.9430, loss=0.6466]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 31/292 [00:05<00:41,  6.31batch/s, auc=0.9430, loss=0.6466]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 31/292 [00:05<00:41,  6.31batch/s, auc=0.9418, loss=0.7201]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 32/292 [00:05<00:41,  6.32batch/s, auc=0.9418, loss=0.7201]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 32/292 [00:06<00:41,  6.32batch/s, auc=0.9404, loss=0.7948]\u001b[A\n",
      "Training Epoch 24/25:  11%|█▏        | 33/292 [00:06<00:40,  6.32batch/s, auc=0.9404, loss=0.7948]\u001b[A\n",
      "Training Epoch 24/25:  11%|█▏        | 33/292 [00:06<00:40,  6.32batch/s, auc=0.9383, loss=0.7495]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9383, loss=0.7495]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 34/292 [00:06<00:40,  6.32batch/s, auc=0.9375, loss=0.9060]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 35/292 [00:06<00:41,  6.27batch/s, auc=0.9375, loss=0.9060]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 35/292 [00:06<00:41,  6.27batch/s, auc=0.9373, loss=0.5802]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9373, loss=0.5802]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 36/292 [00:06<00:40,  6.29batch/s, auc=0.9377, loss=0.5997]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 37/292 [00:06<00:40,  6.27batch/s, auc=0.9377, loss=0.5997]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 37/292 [00:06<00:40,  6.27batch/s, auc=0.9378, loss=0.4635]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 38/292 [00:06<00:40,  6.25batch/s, auc=0.9378, loss=0.4635]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 38/292 [00:07<00:40,  6.25batch/s, auc=0.9355, loss=0.8584]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9355, loss=0.8584]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9359, loss=0.4502]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▎        | 40/292 [00:07<00:40,  6.25batch/s, auc=0.9359, loss=0.4502]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▎        | 40/292 [00:07<00:40,  6.25batch/s, auc=0.9369, loss=0.4639]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9369, loss=0.4639]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 41/292 [00:07<00:40,  6.27batch/s, auc=0.9368, loss=0.5191]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 42/292 [00:07<00:39,  6.26batch/s, auc=0.9368, loss=0.5191]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 42/292 [00:07<00:39,  6.26batch/s, auc=0.9350, loss=0.9767]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9350, loss=0.9767]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▍        | 43/292 [00:07<00:39,  6.29batch/s, auc=0.9353, loss=0.4150]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 44/292 [00:07<00:39,  6.31batch/s, auc=0.9353, loss=0.4150]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 44/292 [00:08<00:39,  6.31batch/s, auc=0.9347, loss=0.7364]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9347, loss=0.7364]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 45/292 [00:08<00:39,  6.31batch/s, auc=0.9354, loss=0.5838]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 46/292 [00:08<00:38,  6.32batch/s, auc=0.9354, loss=0.5838]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 46/292 [00:08<00:38,  6.32batch/s, auc=0.9349, loss=0.7163]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9349, loss=0.7163]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 47/292 [00:08<00:38,  6.32batch/s, auc=0.9352, loss=0.5492]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.9352, loss=0.5492]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▋        | 48/292 [00:08<00:38,  6.32batch/s, auc=0.9354, loss=0.4776]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 49/292 [00:08<00:38,  6.32batch/s, auc=0.9354, loss=0.4776]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 49/292 [00:08<00:38,  6.32batch/s, auc=0.9362, loss=0.5685]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9362, loss=0.5685]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 50/292 [00:08<00:38,  6.28batch/s, auc=0.9359, loss=0.6602]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 51/292 [00:08<00:38,  6.29batch/s, auc=0.9359, loss=0.6602]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9363, loss=0.4600]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9363, loss=0.4600]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 52/292 [00:09<00:38,  6.29batch/s, auc=0.9366, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 53/292 [00:09<00:37,  6.29batch/s, auc=0.9366, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 53/292 [00:09<00:37,  6.29batch/s, auc=0.9376, loss=0.3743]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9376, loss=0.3743]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9381, loss=0.5368]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9381, loss=0.5368]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 55/292 [00:09<00:37,  6.29batch/s, auc=0.9374, loss=0.6154]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9374, loss=0.6154]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 56/292 [00:09<00:37,  6.29batch/s, auc=0.9375, loss=0.5119]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 57/292 [00:09<00:37,  6.23batch/s, auc=0.9375, loss=0.5119]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 57/292 [00:10<00:37,  6.23batch/s, auc=0.9370, loss=0.5626]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 58/292 [00:10<00:37,  6.24batch/s, auc=0.9370, loss=0.5626]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 58/292 [00:10<00:37,  6.24batch/s, auc=0.9367, loss=0.5887]\u001b[A\n",
      "Training Epoch 24/25:  20%|██        | 59/292 [00:10<00:37,  6.26batch/s, auc=0.9367, loss=0.5887]\u001b[A\n",
      "Training Epoch 24/25:  20%|██        | 59/292 [00:10<00:37,  6.26batch/s, auc=0.9370, loss=0.4764]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 60/292 [00:10<00:37,  6.21batch/s, auc=0.9370, loss=0.4764]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 60/292 [00:10<00:37,  6.21batch/s, auc=0.9375, loss=0.3932]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 61/292 [00:10<00:37,  6.24batch/s, auc=0.9375, loss=0.3932]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 61/292 [00:10<00:37,  6.24batch/s, auc=0.9373, loss=0.5312]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 62/292 [00:10<00:37,  6.21batch/s, auc=0.9373, loss=0.5312]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 62/292 [00:10<00:37,  6.21batch/s, auc=0.9377, loss=0.4423]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 63/292 [00:10<00:36,  6.23batch/s, auc=0.9377, loss=0.4423]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 63/292 [00:11<00:36,  6.23batch/s, auc=0.9376, loss=0.5474]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.9376, loss=0.5474]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 64/292 [00:11<00:36,  6.25batch/s, auc=0.9368, loss=0.6536]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 65/292 [00:11<00:36,  6.20batch/s, auc=0.9368, loss=0.6536]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 65/292 [00:11<00:36,  6.20batch/s, auc=0.9374, loss=0.3597]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 66/292 [00:11<00:36,  6.24batch/s, auc=0.9374, loss=0.3597]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 66/292 [00:11<00:36,  6.24batch/s, auc=0.9380, loss=0.3924]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9380, loss=0.3924]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 67/292 [00:11<00:35,  6.26batch/s, auc=0.9383, loss=0.4047]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9383, loss=0.4047]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 68/292 [00:11<00:35,  6.28batch/s, auc=0.9384, loss=0.4395]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▎       | 69/292 [00:11<00:35,  6.24batch/s, auc=0.9384, loss=0.4395]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▎       | 69/292 [00:12<00:35,  6.24batch/s, auc=0.9383, loss=0.4505]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9383, loss=0.4505]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9382, loss=0.5456]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9382, loss=0.5456]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 71/292 [00:12<00:35,  6.28batch/s, auc=0.9386, loss=0.3639]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9386, loss=0.3639]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▍       | 72/292 [00:12<00:35,  6.28batch/s, auc=0.9390, loss=0.4495]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9390, loss=0.4495]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 73/292 [00:12<00:34,  6.27batch/s, auc=0.9395, loss=0.4123]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9395, loss=0.4123]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 74/292 [00:12<00:34,  6.28batch/s, auc=0.9402, loss=0.3620]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9402, loss=0.3620]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 75/292 [00:12<00:34,  6.28batch/s, auc=0.9404, loss=0.4216]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 76/292 [00:12<00:34,  6.25batch/s, auc=0.9404, loss=0.4216]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 76/292 [00:13<00:34,  6.25batch/s, auc=0.9405, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▋       | 77/292 [00:13<00:34,  6.21batch/s, auc=0.9405, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▋       | 77/292 [00:13<00:34,  6.21batch/s, auc=0.9409, loss=0.3834]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 78/292 [00:13<00:34,  6.22batch/s, auc=0.9409, loss=0.3834]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 78/292 [00:13<00:34,  6.22batch/s, auc=0.9410, loss=0.4245]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 79/292 [00:13<00:34,  6.20batch/s, auc=0.9410, loss=0.4245]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 79/292 [00:13<00:34,  6.20batch/s, auc=0.9416, loss=0.3952]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 80/292 [00:13<00:34,  6.21batch/s, auc=0.9416, loss=0.3952]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 80/292 [00:13<00:34,  6.21batch/s, auc=0.9417, loss=0.4247]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 81/292 [00:13<00:33,  6.21batch/s, auc=0.9417, loss=0.4247]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 81/292 [00:13<00:33,  6.21batch/s, auc=0.9423, loss=0.3196]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 82/292 [00:13<00:33,  6.22batch/s, auc=0.9423, loss=0.3196]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 82/292 [00:14<00:33,  6.22batch/s, auc=0.9413, loss=0.9505]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 83/292 [00:14<00:33,  6.22batch/s, auc=0.9413, loss=0.9505]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 83/292 [00:14<00:33,  6.22batch/s, auc=0.9404, loss=0.7135]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9404, loss=0.7135]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 84/292 [00:14<00:33,  6.22batch/s, auc=0.9405, loss=0.3727]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 85/292 [00:14<00:33,  6.17batch/s, auc=0.9405, loss=0.3727]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 85/292 [00:14<00:33,  6.17batch/s, auc=0.9410, loss=0.3530]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 86/292 [00:14<00:33,  6.19batch/s, auc=0.9410, loss=0.3530]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 86/292 [00:14<00:33,  6.19batch/s, auc=0.9413, loss=0.3173]\u001b[A\n",
      "Training Epoch 24/25:  30%|██▉       | 87/292 [00:14<00:33,  6.21batch/s, auc=0.9413, loss=0.3173]\u001b[A\n",
      "Training Epoch 24/25:  30%|██▉       | 87/292 [00:14<00:33,  6.21batch/s, auc=0.9414, loss=0.4477]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 88/292 [00:14<00:32,  6.21batch/s, auc=0.9414, loss=0.4477]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 88/292 [00:15<00:32,  6.21batch/s, auc=0.9415, loss=0.5656]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 89/292 [00:15<00:32,  6.22batch/s, auc=0.9415, loss=0.5656]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 89/292 [00:15<00:32,  6.22batch/s, auc=0.9415, loss=0.5096]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 90/292 [00:15<00:32,  6.21batch/s, auc=0.9415, loss=0.5096]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 90/292 [00:15<00:32,  6.21batch/s, auc=0.9412, loss=0.6180]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9412, loss=0.6180]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 91/292 [00:15<00:32,  6.22batch/s, auc=0.9415, loss=0.4978]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 92/292 [00:15<00:32,  6.19batch/s, auc=0.9415, loss=0.4978]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 92/292 [00:15<00:32,  6.19batch/s, auc=0.9417, loss=0.4435]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 93/292 [00:15<00:32,  6.20batch/s, auc=0.9417, loss=0.4435]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 93/292 [00:15<00:32,  6.20batch/s, auc=0.9418, loss=0.4334]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 94/292 [00:15<00:31,  6.19batch/s, auc=0.9418, loss=0.4334]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 94/292 [00:16<00:31,  6.19batch/s, auc=0.9413, loss=0.7153]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 95/292 [00:16<00:31,  6.20batch/s, auc=0.9413, loss=0.7153]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 95/292 [00:16<00:31,  6.20batch/s, auc=0.9410, loss=0.6295]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 96/292 [00:16<00:31,  6.16batch/s, auc=0.9410, loss=0.6295]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 96/292 [00:16<00:31,  6.16batch/s, auc=0.9413, loss=0.3983]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 97/292 [00:16<00:31,  6.19batch/s, auc=0.9413, loss=0.3983]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 97/292 [00:16<00:31,  6.19batch/s, auc=0.9415, loss=0.3811]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9415, loss=0.3811]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▎      | 98/292 [00:16<00:31,  6.20batch/s, auc=0.9420, loss=0.3026]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9420, loss=0.3026]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 99/292 [00:16<00:31,  6.21batch/s, auc=0.9420, loss=0.4104]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 100/292 [00:16<00:30,  6.22batch/s, auc=0.9420, loss=0.4104]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 100/292 [00:17<00:30,  6.22batch/s, auc=0.9420, loss=0.4429]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 101/292 [00:17<00:30,  6.17batch/s, auc=0.9420, loss=0.4429]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 101/292 [00:17<00:30,  6.17batch/s, auc=0.9417, loss=0.7302]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 102/292 [00:17<00:30,  6.19batch/s, auc=0.9417, loss=0.7302]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 102/292 [00:17<00:30,  6.19batch/s, auc=0.9422, loss=0.3694]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9422, loss=0.3694]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9423, loss=0.4791]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 104/292 [00:17<00:30,  6.15batch/s, auc=0.9423, loss=0.4791]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 104/292 [00:17<00:30,  6.15batch/s, auc=0.9422, loss=0.6682]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 105/292 [00:17<00:30,  6.17batch/s, auc=0.9422, loss=0.6682]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 105/292 [00:17<00:30,  6.17batch/s, auc=0.9419, loss=0.7226]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▋      | 106/292 [00:17<00:30,  6.19batch/s, auc=0.9419, loss=0.7226]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▋      | 106/292 [00:17<00:30,  6.19batch/s, auc=0.9418, loss=0.6680]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 107/292 [00:17<00:29,  6.20batch/s, auc=0.9418, loss=0.6680]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 107/292 [00:18<00:29,  6.20batch/s, auc=0.9426, loss=0.2693]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9426, loss=0.2693]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 108/292 [00:18<00:29,  6.21batch/s, auc=0.9426, loss=0.6055]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9426, loss=0.6055]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 109/292 [00:18<00:29,  6.20batch/s, auc=0.9424, loss=0.4888]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9424, loss=0.4888]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 110/292 [00:18<00:29,  6.20batch/s, auc=0.9424, loss=0.4114]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9424, loss=0.4114]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9429, loss=0.2633]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9429, loss=0.2633]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 112/292 [00:18<00:28,  6.22batch/s, auc=0.9429, loss=0.4890]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▊      | 113/292 [00:18<00:28,  6.22batch/s, auc=0.9429, loss=0.4890]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▊      | 113/292 [00:19<00:28,  6.22batch/s, auc=0.9429, loss=0.4452]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 114/292 [00:19<00:28,  6.18batch/s, auc=0.9429, loss=0.4452]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 114/292 [00:19<00:28,  6.18batch/s, auc=0.9430, loss=0.4406]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 115/292 [00:19<00:28,  6.17batch/s, auc=0.9430, loss=0.4406]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 115/292 [00:19<00:28,  6.17batch/s, auc=0.9431, loss=0.4603]\u001b[A\n",
      "Training Epoch 24/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9431, loss=0.4603]\u001b[A\n",
      "Training Epoch 24/25:  40%|███▉      | 116/292 [00:19<00:28,  6.19batch/s, auc=0.9431, loss=0.3738]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 117/292 [00:19<00:28,  6.18batch/s, auc=0.9431, loss=0.3738]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 117/292 [00:19<00:28,  6.18batch/s, auc=0.9431, loss=0.5291]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9431, loss=0.5291]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 118/292 [00:19<00:28,  6.18batch/s, auc=0.9435, loss=0.3493]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 119/292 [00:19<00:27,  6.20batch/s, auc=0.9435, loss=0.3493]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 119/292 [00:20<00:27,  6.20batch/s, auc=0.9432, loss=0.5983]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9432, loss=0.5983]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 120/292 [00:20<00:27,  6.21batch/s, auc=0.9436, loss=0.3303]\u001b[A\n",
      "Training Epoch 24/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9436, loss=0.3303]\u001b[A\n",
      "Training Epoch 24/25:  41%|████▏     | 121/292 [00:20<00:27,  6.21batch/s, auc=0.9439, loss=0.3970]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 122/292 [00:20<00:27,  6.19batch/s, auc=0.9439, loss=0.3970]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 122/292 [00:20<00:27,  6.19batch/s, auc=0.9438, loss=0.5432]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 123/292 [00:20<00:27,  6.21batch/s, auc=0.9438, loss=0.5432]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 123/292 [00:20<00:27,  6.21batch/s, auc=0.9440, loss=0.4242]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9440, loss=0.4242]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 124/292 [00:20<00:27,  6.21batch/s, auc=0.9439, loss=0.4757]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 125/292 [00:20<00:26,  6.21batch/s, auc=0.9439, loss=0.4757]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 125/292 [00:21<00:26,  6.21batch/s, auc=0.9437, loss=0.4895]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9437, loss=0.4895]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 126/292 [00:21<00:26,  6.20batch/s, auc=0.9439, loss=0.3906]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9439, loss=0.3906]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 127/292 [00:21<00:26,  6.21batch/s, auc=0.9440, loss=0.4500]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9440, loss=0.4500]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 128/292 [00:21<00:26,  6.21batch/s, auc=0.9442, loss=0.4066]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 129/292 [00:21<00:26,  6.20batch/s, auc=0.9442, loss=0.4066]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 129/292 [00:21<00:26,  6.20batch/s, auc=0.9442, loss=0.4846]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9442, loss=0.4846]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 130/292 [00:21<00:26,  6.19batch/s, auc=0.9444, loss=0.3866]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 131/292 [00:21<00:25,  6.20batch/s, auc=0.9444, loss=0.3866]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 131/292 [00:22<00:25,  6.20batch/s, auc=0.9439, loss=0.7069]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9439, loss=0.7069]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▌     | 132/292 [00:22<00:25,  6.20batch/s, auc=0.9439, loss=0.4285]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 133/292 [00:22<00:25,  6.20batch/s, auc=0.9439, loss=0.4285]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 133/292 [00:22<00:25,  6.20batch/s, auc=0.9440, loss=0.5226]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9440, loss=0.5226]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 134/292 [00:22<00:25,  6.19batch/s, auc=0.9441, loss=0.3723]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 135/292 [00:22<00:25,  6.19batch/s, auc=0.9441, loss=0.3723]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 135/292 [00:22<00:25,  6.19batch/s, auc=0.9440, loss=0.6531]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9440, loss=0.6531]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 136/292 [00:22<00:25,  6.19batch/s, auc=0.9442, loss=0.3505]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9442, loss=0.3505]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 137/292 [00:22<00:25,  6.19batch/s, auc=0.9443, loss=0.5104]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 138/292 [00:22<00:24,  6.17batch/s, auc=0.9443, loss=0.5104]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 138/292 [00:23<00:24,  6.17batch/s, auc=0.9441, loss=0.4890]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9441, loss=0.4890]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9443, loss=0.4206]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9443, loss=0.4206]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9445, loss=0.3053]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 141/292 [00:23<00:24,  6.18batch/s, auc=0.9445, loss=0.3053]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 141/292 [00:23<00:24,  6.18batch/s, auc=0.9450, loss=0.2995]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▊     | 142/292 [00:23<00:24,  6.08batch/s, auc=0.9450, loss=0.2995]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▊     | 142/292 [00:23<00:24,  6.08batch/s, auc=0.9445, loss=0.9450]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 143/292 [00:23<00:24,  5.99batch/s, auc=0.9445, loss=0.9450]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 143/292 [00:23<00:24,  5.99batch/s, auc=0.9444, loss=0.5621]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 144/292 [00:23<00:25,  5.91batch/s, auc=0.9444, loss=0.5621]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 144/292 [00:24<00:25,  5.91batch/s, auc=0.9445, loss=0.3475]\u001b[A\n",
      "Training Epoch 24/25:  50%|████▉     | 145/292 [00:24<00:24,  5.95batch/s, auc=0.9445, loss=0.3475]\u001b[A\n",
      "Training Epoch 24/25:  50%|████▉     | 145/292 [00:24<00:24,  5.95batch/s, auc=0.9448, loss=0.3174]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 146/292 [00:24<00:24,  5.86batch/s, auc=0.9448, loss=0.3174]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 146/292 [00:24<00:24,  5.86batch/s, auc=0.9449, loss=0.4188]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 147/292 [00:24<00:24,  5.94batch/s, auc=0.9449, loss=0.4188]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 147/292 [00:24<00:24,  5.94batch/s, auc=0.9447, loss=0.4884]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 148/292 [00:24<00:24,  5.96batch/s, auc=0.9447, loss=0.4884]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 148/292 [00:24<00:24,  5.96batch/s, auc=0.9450, loss=0.3372]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 149/292 [00:24<00:23,  6.01batch/s, auc=0.9450, loss=0.3372]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 149/292 [00:24<00:23,  6.01batch/s, auc=0.9450, loss=0.4428]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.02batch/s, auc=0.9450, loss=0.4428]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.02batch/s, auc=0.9451, loss=0.5077]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 151/292 [00:25<00:23,  5.94batch/s, auc=0.9451, loss=0.5077]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 151/292 [00:25<00:23,  5.94batch/s, auc=0.9444, loss=0.9003]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 152/292 [00:25<00:23,  6.00batch/s, auc=0.9444, loss=0.9003]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 152/292 [00:25<00:23,  6.00batch/s, auc=0.9446, loss=0.2751]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.05batch/s, auc=0.9446, loss=0.2751]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.05batch/s, auc=0.9448, loss=0.3989]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.09batch/s, auc=0.9448, loss=0.3989]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.09batch/s, auc=0.9448, loss=0.4505]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.11batch/s, auc=0.9448, loss=0.4505]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.11batch/s, auc=0.9446, loss=0.6615]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.12batch/s, auc=0.9446, loss=0.6615]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.12batch/s, auc=0.9447, loss=0.3477]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.13batch/s, auc=0.9447, loss=0.3477]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 157/292 [00:26<00:22,  6.13batch/s, auc=0.9449, loss=0.3873]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9449, loss=0.3873]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.12batch/s, auc=0.9450, loss=0.4245]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9450, loss=0.4245]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.13batch/s, auc=0.9451, loss=0.3775]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.14batch/s, auc=0.9451, loss=0.3775]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.14batch/s, auc=0.9452, loss=0.3148]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9452, loss=0.3148]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.13batch/s, auc=0.9450, loss=0.7885]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.12batch/s, auc=0.9450, loss=0.7885]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.12batch/s, auc=0.9451, loss=0.3759]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9451, loss=0.3759]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.13batch/s, auc=0.9452, loss=0.4041]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9452, loss=0.4041]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9451, loss=0.6175]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9451, loss=0.6175]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.13batch/s, auc=0.9449, loss=0.7217]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9449, loss=0.7217]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9450, loss=0.5427]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.12batch/s, auc=0.9450, loss=0.5427]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.12batch/s, auc=0.9450, loss=0.4452]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9450, loss=0.4452]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.12batch/s, auc=0.9449, loss=0.6092]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.12batch/s, auc=0.9449, loss=0.6092]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.12batch/s, auc=0.9447, loss=0.7284]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.10batch/s, auc=0.9447, loss=0.7284]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.10batch/s, auc=0.9444, loss=0.8055]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9444, loss=0.8055]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.12batch/s, auc=0.9446, loss=0.3932]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9446, loss=0.3932]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.10batch/s, auc=0.9444, loss=0.6746]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.10batch/s, auc=0.9444, loss=0.6746]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.10batch/s, auc=0.9445, loss=0.3883]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.08batch/s, auc=0.9445, loss=0.3883]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.08batch/s, auc=0.9444, loss=0.6343]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.09batch/s, auc=0.9444, loss=0.6343]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.09batch/s, auc=0.9445, loss=0.3785]\u001b[A\n",
      "Training Epoch 24/25:  60%|██████    | 176/292 [00:29<00:19,  6.09batch/s, auc=0.9445, loss=0.3785]\u001b[A\n",
      "Training Epoch 24/25:  60%|██████    | 176/292 [00:29<00:19,  6.09batch/s, auc=0.9445, loss=0.4556]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 177/292 [00:29<00:19,  5.93batch/s, auc=0.9445, loss=0.4556]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 177/292 [00:29<00:19,  5.93batch/s, auc=0.9447, loss=0.3016]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 178/292 [00:29<00:19,  5.74batch/s, auc=0.9447, loss=0.3016]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 178/292 [00:29<00:19,  5.74batch/s, auc=0.9446, loss=0.5365]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.77batch/s, auc=0.9446, loss=0.5365]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████▏   | 179/292 [00:29<00:19,  5.77batch/s, auc=0.9448, loss=0.3843]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 180/292 [00:29<00:19,  5.75batch/s, auc=0.9448, loss=0.3843]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 180/292 [00:30<00:19,  5.75batch/s, auc=0.9450, loss=0.3292]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 181/292 [00:30<00:18,  5.86batch/s, auc=0.9450, loss=0.3292]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 181/292 [00:30<00:18,  5.86batch/s, auc=0.9448, loss=0.5966]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.95batch/s, auc=0.9448, loss=0.5966]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 182/292 [00:30<00:18,  5.95batch/s, auc=0.9449, loss=0.3806]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.02batch/s, auc=0.9449, loss=0.3806]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 183/292 [00:30<00:18,  6.02batch/s, auc=0.9452, loss=0.3171]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.05batch/s, auc=0.9452, loss=0.3171]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.05batch/s, auc=0.9454, loss=0.3974]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9454, loss=0.3974]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.06batch/s, auc=0.9455, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.07batch/s, auc=0.9455, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▎   | 186/292 [00:31<00:17,  6.07batch/s, auc=0.9455, loss=0.4911]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.07batch/s, auc=0.9455, loss=0.4911]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.07batch/s, auc=0.9452, loss=0.7547]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.07batch/s, auc=0.9452, loss=0.7547]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.07batch/s, auc=0.9454, loss=0.3393]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.08batch/s, auc=0.9454, loss=0.3393]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.08batch/s, auc=0.9447, loss=1.3574]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.02batch/s, auc=0.9447, loss=1.3574]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.02batch/s, auc=0.9448, loss=0.3791]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.81batch/s, auc=0.9448, loss=0.3791]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 191/292 [00:31<00:17,  5.81batch/s, auc=0.9448, loss=0.4241]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 192/292 [00:31<00:17,  5.77batch/s, auc=0.9448, loss=0.4241]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 192/292 [00:32<00:17,  5.77batch/s, auc=0.9450, loss=0.3412]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.73batch/s, auc=0.9450, loss=0.3412]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 193/292 [00:32<00:17,  5.73batch/s, auc=0.9452, loss=0.3686]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.72batch/s, auc=0.9452, loss=0.3686]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▋   | 194/292 [00:32<00:17,  5.72batch/s, auc=0.9451, loss=0.5419]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.71batch/s, auc=0.9451, loss=0.5419]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 195/292 [00:32<00:16,  5.71batch/s, auc=0.9450, loss=0.5165]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.70batch/s, auc=0.9450, loss=0.5165]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.70batch/s, auc=0.9448, loss=0.6946]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.80batch/s, auc=0.9448, loss=0.6946]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 197/292 [00:33<00:16,  5.80batch/s, auc=0.9447, loss=0.5402]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.63batch/s, auc=0.9447, loss=0.5402]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.63batch/s, auc=0.9449, loss=0.3672]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.76batch/s, auc=0.9449, loss=0.3672]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.76batch/s, auc=0.9450, loss=0.4000]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.74batch/s, auc=0.9450, loss=0.4000]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.74batch/s, auc=0.9451, loss=0.4380]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.71batch/s, auc=0.9451, loss=0.4380]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.71batch/s, auc=0.9451, loss=0.4111]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.70batch/s, auc=0.9451, loss=0.4111]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.70batch/s, auc=0.9451, loss=0.4094]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.70batch/s, auc=0.9451, loss=0.4094]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 203/292 [00:34<00:15,  5.70batch/s, auc=0.9452, loss=0.4892]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9452, loss=0.4892]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9453, loss=0.4132]\u001b[A\n",
      "Training Epoch 24/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9453, loss=0.4132]\u001b[A\n",
      "Training Epoch 24/25:  70%|███████   | 205/292 [00:34<00:15,  5.68batch/s, auc=0.9451, loss=0.6002]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 206/292 [00:34<00:15,  5.67batch/s, auc=0.9451, loss=0.6002]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 206/292 [00:34<00:15,  5.67batch/s, auc=0.9452, loss=0.4402]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 207/292 [00:34<00:14,  5.68batch/s, auc=0.9452, loss=0.4402]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 207/292 [00:34<00:14,  5.68batch/s, auc=0.9450, loss=0.7599]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9450, loss=0.7599]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 208/292 [00:34<00:14,  5.67batch/s, auc=0.9450, loss=0.5087]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.64batch/s, auc=0.9450, loss=0.5087]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 209/292 [00:35<00:14,  5.64batch/s, auc=0.9447, loss=0.8504]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.60batch/s, auc=0.9447, loss=0.8504]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.60batch/s, auc=0.9448, loss=0.3462]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.64batch/s, auc=0.9448, loss=0.3462]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 211/292 [00:35<00:14,  5.64batch/s, auc=0.9449, loss=0.4345]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.68batch/s, auc=0.9449, loss=0.4345]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 212/292 [00:35<00:14,  5.68batch/s, auc=0.9448, loss=0.4993]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.65batch/s, auc=0.9448, loss=0.4993]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.65batch/s, auc=0.9447, loss=0.6415]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.65batch/s, auc=0.9447, loss=0.6415]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 214/292 [00:36<00:13,  5.65batch/s, auc=0.9447, loss=0.4475]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.64batch/s, auc=0.9447, loss=0.4475]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▎  | 215/292 [00:36<00:13,  5.64batch/s, auc=0.9447, loss=0.3511]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.74batch/s, auc=0.9447, loss=0.3511]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 216/292 [00:36<00:13,  5.74batch/s, auc=0.9445, loss=0.6723]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.59batch/s, auc=0.9445, loss=0.6723]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.59batch/s, auc=0.9446, loss=0.4421]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.59batch/s, auc=0.9446, loss=0.4421]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▍  | 218/292 [00:36<00:13,  5.59batch/s, auc=0.9447, loss=0.3908]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.63batch/s, auc=0.9447, loss=0.3908]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.63batch/s, auc=0.9447, loss=0.4260]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.62batch/s, auc=0.9447, loss=0.4260]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 220/292 [00:37<00:12,  5.62batch/s, auc=0.9444, loss=0.7889]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.61batch/s, auc=0.9444, loss=0.7889]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 221/292 [00:37<00:12,  5.61batch/s, auc=0.9444, loss=0.4257]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.62batch/s, auc=0.9444, loss=0.4257]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 222/292 [00:37<00:12,  5.62batch/s, auc=0.9445, loss=0.4689]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.64batch/s, auc=0.9445, loss=0.4689]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▋  | 223/292 [00:37<00:12,  5.64batch/s, auc=0.9445, loss=0.5907]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.75batch/s, auc=0.9445, loss=0.5907]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 224/292 [00:37<00:11,  5.75batch/s, auc=0.9445, loss=0.4091]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.61batch/s, auc=0.9445, loss=0.4091]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 225/292 [00:37<00:11,  5.61batch/s, auc=0.9447, loss=0.2948]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.62batch/s, auc=0.9447, loss=0.2948]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 226/292 [00:38<00:11,  5.62batch/s, auc=0.9449, loss=0.5045]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.75batch/s, auc=0.9449, loss=0.5045]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 227/292 [00:38<00:11,  5.75batch/s, auc=0.9448, loss=0.5514]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.69batch/s, auc=0.9448, loss=0.5514]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 228/292 [00:38<00:11,  5.69batch/s, auc=0.9449, loss=0.3718]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.79batch/s, auc=0.9449, loss=0.3718]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.79batch/s, auc=0.9448, loss=0.7860]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.66batch/s, auc=0.9448, loss=0.7860]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.66batch/s, auc=0.9449, loss=0.3586]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.66batch/s, auc=0.9449, loss=0.3586]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 231/292 [00:39<00:10,  5.66batch/s, auc=0.9448, loss=0.5665]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.74batch/s, auc=0.9448, loss=0.5665]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 232/292 [00:39<00:10,  5.74batch/s, auc=0.9447, loss=0.6528]\u001b[A\n",
      "Training Epoch 24/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.81batch/s, auc=0.9447, loss=0.6528]\u001b[A\n",
      "Training Epoch 24/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.81batch/s, auc=0.9445, loss=0.8692]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 234/292 [00:39<00:09,  5.84batch/s, auc=0.9445, loss=0.8692]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 234/292 [00:39<00:09,  5.84batch/s, auc=0.9445, loss=0.4297]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 235/292 [00:39<00:09,  5.90batch/s, auc=0.9445, loss=0.4297]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 235/292 [00:39<00:09,  5.90batch/s, auc=0.9443, loss=0.7382]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 236/292 [00:39<00:09,  5.94batch/s, auc=0.9443, loss=0.7382]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 236/292 [00:39<00:09,  5.94batch/s, auc=0.9443, loss=0.4870]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 237/292 [00:39<00:09,  5.96batch/s, auc=0.9443, loss=0.4870]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 237/292 [00:40<00:09,  5.96batch/s, auc=0.9442, loss=0.8383]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.84batch/s, auc=0.9442, loss=0.8383]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 238/292 [00:40<00:09,  5.84batch/s, auc=0.9444, loss=0.2941]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 239/292 [00:40<00:08,  5.90batch/s, auc=0.9444, loss=0.2941]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 239/292 [00:40<00:08,  5.90batch/s, auc=0.9444, loss=0.4785]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.71batch/s, auc=0.9444, loss=0.4785]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.71batch/s, auc=0.9441, loss=0.8999]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9441, loss=0.8999]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.67batch/s, auc=0.9442, loss=0.4668]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.73batch/s, auc=0.9442, loss=0.4668]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.73batch/s, auc=0.9443, loss=0.4057]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.82batch/s, auc=0.9443, loss=0.4057]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 243/292 [00:41<00:08,  5.82batch/s, auc=0.9442, loss=0.4288]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.87batch/s, auc=0.9442, loss=0.4288]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▎ | 244/292 [00:41<00:08,  5.87batch/s, auc=0.9442, loss=0.4743]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 245/292 [00:41<00:07,  5.92batch/s, auc=0.9442, loss=0.4743]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 245/292 [00:41<00:07,  5.92batch/s, auc=0.9444, loss=0.3978]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.80batch/s, auc=0.9444, loss=0.3978]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 246/292 [00:41<00:07,  5.80batch/s, auc=0.9440, loss=0.9973]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.73batch/s, auc=0.9440, loss=0.9973]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.73batch/s, auc=0.9440, loss=0.4440]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.68batch/s, auc=0.9440, loss=0.4440]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.68batch/s, auc=0.9441, loss=0.4303]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.66batch/s, auc=0.9441, loss=0.4303]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▌ | 249/292 [00:42<00:07,  5.66batch/s, auc=0.9442, loss=0.4594]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.61batch/s, auc=0.9442, loss=0.4594]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.61batch/s, auc=0.9442, loss=0.3991]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.61batch/s, auc=0.9442, loss=0.3991]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.61batch/s, auc=0.9440, loss=0.7456]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.59batch/s, auc=0.9440, loss=0.7456]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.59batch/s, auc=0.9440, loss=0.5323]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.58batch/s, auc=0.9440, loss=0.5323]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 253/292 [00:42<00:06,  5.58batch/s, auc=0.9439, loss=0.6100]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.58batch/s, auc=0.9439, loss=0.6100]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 254/292 [00:43<00:06,  5.58batch/s, auc=0.9441, loss=0.3468]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.54batch/s, auc=0.9441, loss=0.3468]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 255/292 [00:43<00:06,  5.54batch/s, auc=0.9442, loss=0.6382]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.54batch/s, auc=0.9442, loss=0.6382]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.54batch/s, auc=0.9440, loss=0.9029]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.67batch/s, auc=0.9440, loss=0.9029]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.67batch/s, auc=0.9438, loss=0.9928]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.62batch/s, auc=0.9438, loss=0.9928]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 258/292 [00:43<00:06,  5.62batch/s, auc=0.9439, loss=0.3969]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.60batch/s, auc=0.9439, loss=0.3969]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.60batch/s, auc=0.9438, loss=0.5278]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.63batch/s, auc=0.9438, loss=0.5278]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 260/292 [00:44<00:05,  5.63batch/s, auc=0.9435, loss=0.7858]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.72batch/s, auc=0.9435, loss=0.7858]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 261/292 [00:44<00:05,  5.72batch/s, auc=0.9433, loss=0.8026]\u001b[A\n",
      "Training Epoch 24/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.56batch/s, auc=0.9433, loss=0.8026]\u001b[A\n",
      "Training Epoch 24/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.56batch/s, auc=0.9433, loss=0.4937]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 263/292 [00:44<00:05,  5.57batch/s, auc=0.9433, loss=0.4937]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 263/292 [00:44<00:05,  5.57batch/s, auc=0.9431, loss=0.7458]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 264/292 [00:44<00:04,  5.61batch/s, auc=0.9431, loss=0.7458]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 264/292 [00:44<00:04,  5.61batch/s, auc=0.9431, loss=0.5322]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 265/292 [00:44<00:04,  5.59batch/s, auc=0.9431, loss=0.5322]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 265/292 [00:44<00:04,  5.59batch/s, auc=0.9431, loss=0.4060]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 266/292 [00:44<00:04,  5.59batch/s, auc=0.9431, loss=0.4060]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 266/292 [00:45<00:04,  5.59batch/s, auc=0.9431, loss=0.5266]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.60batch/s, auc=0.9431, loss=0.5266]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.60batch/s, auc=0.9432, loss=0.4027]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.58batch/s, auc=0.9432, loss=0.4027]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.58batch/s, auc=0.9432, loss=0.4408]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9432, loss=0.4408]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.69batch/s, auc=0.9431, loss=0.6215]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9431, loss=0.6215]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.60batch/s, auc=0.9432, loss=0.3804]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.61batch/s, auc=0.9432, loss=0.3804]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 271/292 [00:46<00:03,  5.61batch/s, auc=0.9433, loss=0.4482]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.60batch/s, auc=0.9433, loss=0.4482]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 272/292 [00:46<00:03,  5.60batch/s, auc=0.9433, loss=0.4759]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.58batch/s, auc=0.9433, loss=0.4759]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.58batch/s, auc=0.9432, loss=0.4851]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.58batch/s, auc=0.9432, loss=0.4851]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.58batch/s, auc=0.9432, loss=0.4489]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.61batch/s, auc=0.9432, loss=0.4489]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.61batch/s, auc=0.9433, loss=0.3883]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.58batch/s, auc=0.9433, loss=0.3883]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.58batch/s, auc=0.9434, loss=0.3094]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.59batch/s, auc=0.9434, loss=0.3094]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 277/292 [00:47<00:02,  5.59batch/s, auc=0.9434, loss=0.4851]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.63batch/s, auc=0.9434, loss=0.4851]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▌| 278/292 [00:47<00:02,  5.63batch/s, auc=0.9436, loss=0.4203]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.63batch/s, auc=0.9436, loss=0.4203]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.63batch/s, auc=0.9438, loss=0.2020]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.57batch/s, auc=0.9438, loss=0.2020]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.57batch/s, auc=0.9439, loss=0.3628]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.57batch/s, auc=0.9439, loss=0.3628]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.57batch/s, auc=0.9440, loss=0.4064]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.68batch/s, auc=0.9440, loss=0.4064]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 282/292 [00:48<00:01,  5.68batch/s, auc=0.9441, loss=0.4461]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.77batch/s, auc=0.9441, loss=0.4461]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 283/292 [00:48<00:01,  5.77batch/s, auc=0.9441, loss=0.3791]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.57batch/s, auc=0.9441, loss=0.3791]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.57batch/s, auc=0.9441, loss=0.5159]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.56batch/s, auc=0.9441, loss=0.5159]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.56batch/s, auc=0.9441, loss=0.6096]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.69batch/s, auc=0.9441, loss=0.6096]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.69batch/s, auc=0.9441, loss=0.3947]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9441, loss=0.3947]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.77batch/s, auc=0.9441, loss=0.5591]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.82batch/s, auc=0.9441, loss=0.5591]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▊| 288/292 [00:49<00:00,  5.82batch/s, auc=0.9440, loss=0.5798]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.85batch/s, auc=0.9440, loss=0.5798]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 289/292 [00:49<00:00,  5.85batch/s, auc=0.9436, loss=0.9565]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.87batch/s, auc=0.9436, loss=0.9565]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.87batch/s, auc=0.9435, loss=0.5173]\u001b[A\n",
      "Training Epoch 24/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.88batch/s, auc=0.9435, loss=0.5173]\u001b[A\n",
      "Training Epoch 24/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.88batch/s, auc=0.9435, loss=0.3927]\u001b[A\n",
      "Training Epoch 24/25: 100%|██████████| 292/292 [00:49<00:00,  5.89batch/s, auc=0.9435, loss=0.3927]\u001b[A\n",
      "Epochs:  96%|█████████▌| 24/25 [21:59<00:55, 55.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25] Train Loss: 0.5091 | Train AUROC: 0.9435 Val Loss: 0.7030 | Val AUROC: 0.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 25/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 0/292 [00:00<?, ?batch/s, auc=0.9483, loss=0.4833]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 1/292 [00:00<04:46,  1.02batch/s, auc=0.9483, loss=0.4833]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 1/292 [00:01<04:46,  1.02batch/s, auc=0.9538, loss=0.4037]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9538, loss=0.4037]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 2/292 [00:01<02:24,  2.00batch/s, auc=0.9334, loss=0.6026]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.9334, loss=0.6026]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 3/292 [00:01<01:42,  2.82batch/s, auc=0.9223, loss=0.9199]\u001b[A\n",
      "Training Epoch 25/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9223, loss=0.9199]\u001b[A\n",
      "Training Epoch 25/25:   1%|▏         | 4/292 [00:01<01:19,  3.61batch/s, auc=0.9226, loss=0.7341]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9226, loss=0.7341]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 5/292 [00:01<01:07,  4.28batch/s, auc=0.9208, loss=0.6016]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 6/292 [00:01<01:01,  4.64batch/s, auc=0.9208, loss=0.6016]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 6/292 [00:01<01:01,  4.64batch/s, auc=0.9258, loss=0.3407]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 7/292 [00:01<00:55,  5.09batch/s, auc=0.9258, loss=0.3407]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 7/292 [00:02<00:55,  5.09batch/s, auc=0.9280, loss=0.3460]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9280, loss=0.3460]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 8/292 [00:02<00:52,  5.44batch/s, auc=0.9243, loss=0.4877]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.9243, loss=0.4877]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 9/292 [00:02<00:49,  5.68batch/s, auc=0.9303, loss=0.3911]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9303, loss=0.3911]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 10/292 [00:02<00:48,  5.81batch/s, auc=0.9330, loss=0.4593]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 11/292 [00:02<00:47,  5.95batch/s, auc=0.9330, loss=0.4593]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 11/292 [00:02<00:47,  5.95batch/s, auc=0.9355, loss=0.3077]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 12/292 [00:02<00:46,  6.06batch/s, auc=0.9355, loss=0.3077]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 12/292 [00:02<00:46,  6.06batch/s, auc=0.9401, loss=0.2988]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 13/292 [00:02<00:45,  6.14batch/s, auc=0.9401, loss=0.2988]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 13/292 [00:03<00:45,  6.14batch/s, auc=0.9405, loss=0.4499]\u001b[A\n",
      "Training Epoch 25/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9405, loss=0.4499]\u001b[A\n",
      "Training Epoch 25/25:   5%|▍         | 14/292 [00:03<00:44,  6.18batch/s, auc=0.9436, loss=0.3807]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 15/292 [00:03<00:44,  6.22batch/s, auc=0.9436, loss=0.3807]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 15/292 [00:03<00:44,  6.22batch/s, auc=0.9454, loss=0.5247]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 16/292 [00:03<00:44,  6.19batch/s, auc=0.9454, loss=0.5247]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 16/292 [00:03<00:44,  6.19batch/s, auc=0.9472, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9472, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 17/292 [00:03<00:44,  6.22batch/s, auc=0.9476, loss=0.4654]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9476, loss=0.4654]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 18/292 [00:03<00:43,  6.25batch/s, auc=0.9470, loss=0.5746]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 19/292 [00:03<00:43,  6.27batch/s, auc=0.9470, loss=0.5746]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 19/292 [00:04<00:43,  6.27batch/s, auc=0.9477, loss=0.3604]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9477, loss=0.3604]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 20/292 [00:04<00:43,  6.30batch/s, auc=0.9495, loss=0.3421]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 21/292 [00:04<00:44,  6.14batch/s, auc=0.9495, loss=0.3421]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 21/292 [00:04<00:44,  6.14batch/s, auc=0.9491, loss=0.4464]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 22/292 [00:04<00:43,  6.20batch/s, auc=0.9491, loss=0.4464]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 22/292 [00:04<00:43,  6.20batch/s, auc=0.9489, loss=0.4336]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 23/292 [00:04<00:43,  6.19batch/s, auc=0.9489, loss=0.4336]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 23/292 [00:04<00:43,  6.19batch/s, auc=0.9502, loss=0.3934]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 24/292 [00:04<00:42,  6.24batch/s, auc=0.9502, loss=0.3934]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 24/292 [00:04<00:42,  6.24batch/s, auc=0.9524, loss=0.2920]\u001b[A\n",
      "Training Epoch 25/25:   9%|▊         | 25/292 [00:04<00:42,  6.24batch/s, auc=0.9524, loss=0.2920]\u001b[A\n",
      "Training Epoch 25/25:   9%|▊         | 25/292 [00:05<00:42,  6.24batch/s, auc=0.9527, loss=0.4437]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 26/292 [00:05<00:42,  6.22batch/s, auc=0.9527, loss=0.4437]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 26/292 [00:05<00:42,  6.22batch/s, auc=0.9525, loss=0.4181]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 27/292 [00:05<00:42,  6.20batch/s, auc=0.9525, loss=0.4181]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 27/292 [00:05<00:42,  6.20batch/s, auc=0.9520, loss=0.4690]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 28/292 [00:05<00:42,  6.20batch/s, auc=0.9520, loss=0.4690]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 28/292 [00:05<00:42,  6.20batch/s, auc=0.9523, loss=0.3418]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 29/292 [00:05<00:42,  6.23batch/s, auc=0.9523, loss=0.3418]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 29/292 [00:05<00:42,  6.23batch/s, auc=0.9512, loss=0.5825]\u001b[A\n",
      "Training Epoch 25/25:  10%|█         | 30/292 [00:05<00:41,  6.26batch/s, auc=0.9512, loss=0.5825]\u001b[A\n",
      "Training Epoch 25/25:  10%|█         | 30/292 [00:05<00:41,  6.26batch/s, auc=0.9509, loss=0.4991]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9509, loss=0.4991]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 31/292 [00:05<00:41,  6.27batch/s, auc=0.9516, loss=0.3911]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 32/292 [00:05<00:41,  6.29batch/s, auc=0.9516, loss=0.3911]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 32/292 [00:06<00:41,  6.29batch/s, auc=0.9521, loss=0.4256]\u001b[A\n",
      "Training Epoch 25/25:  11%|█▏        | 33/292 [00:06<00:41,  6.22batch/s, auc=0.9521, loss=0.4256]\u001b[A\n",
      "Training Epoch 25/25:  11%|█▏        | 33/292 [00:06<00:41,  6.22batch/s, auc=0.9516, loss=0.5638]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 34/292 [00:06<00:41,  6.25batch/s, auc=0.9516, loss=0.5638]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 34/292 [00:06<00:41,  6.25batch/s, auc=0.9523, loss=0.3618]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 35/292 [00:06<00:40,  6.27batch/s, auc=0.9523, loss=0.3618]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 35/292 [00:06<00:40,  6.27batch/s, auc=0.9508, loss=0.8423]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 36/292 [00:06<00:41,  6.20batch/s, auc=0.9508, loss=0.8423]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 36/292 [00:06<00:41,  6.20batch/s, auc=0.9509, loss=0.5649]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 37/292 [00:06<00:40,  6.23batch/s, auc=0.9509, loss=0.5649]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 37/292 [00:06<00:40,  6.23batch/s, auc=0.9509, loss=0.4027]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 38/292 [00:06<00:40,  6.26batch/s, auc=0.9509, loss=0.4027]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 38/292 [00:07<00:40,  6.26batch/s, auc=0.9499, loss=0.6969]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9499, loss=0.6969]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 39/292 [00:07<00:40,  6.27batch/s, auc=0.9497, loss=0.3885]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▎        | 40/292 [00:07<00:40,  6.24batch/s, auc=0.9497, loss=0.3885]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▎        | 40/292 [00:07<00:40,  6.24batch/s, auc=0.9499, loss=0.4244]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 41/292 [00:07<00:40,  6.24batch/s, auc=0.9499, loss=0.4244]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 41/292 [00:07<00:40,  6.24batch/s, auc=0.9497, loss=0.3837]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 42/292 [00:07<00:40,  6.22batch/s, auc=0.9497, loss=0.3837]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 42/292 [00:07<00:40,  6.22batch/s, auc=0.9495, loss=0.4388]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▍        | 43/292 [00:07<00:40,  6.17batch/s, auc=0.9495, loss=0.4388]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▍        | 43/292 [00:07<00:40,  6.17batch/s, auc=0.9491, loss=0.4435]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 44/292 [00:07<00:39,  6.21batch/s, auc=0.9491, loss=0.4435]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 44/292 [00:08<00:39,  6.21batch/s, auc=0.9491, loss=0.4854]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 45/292 [00:08<00:39,  6.23batch/s, auc=0.9491, loss=0.4854]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 45/292 [00:08<00:39,  6.23batch/s, auc=0.9477, loss=0.8945]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9477, loss=0.8945]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 46/292 [00:08<00:39,  6.26batch/s, auc=0.9481, loss=0.4064]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9481, loss=0.4064]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 47/292 [00:08<00:39,  6.27batch/s, auc=0.9488, loss=0.4319]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9488, loss=0.4319]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▋        | 48/292 [00:08<00:38,  6.28batch/s, auc=0.9477, loss=0.6880]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9477, loss=0.6880]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 49/292 [00:08<00:38,  6.29batch/s, auc=0.9469, loss=0.8403]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 50/292 [00:08<00:38,  6.29batch/s, auc=0.9469, loss=0.8403]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 50/292 [00:09<00:38,  6.29batch/s, auc=0.9464, loss=0.6024]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9464, loss=0.6024]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 51/292 [00:09<00:38,  6.29batch/s, auc=0.9461, loss=0.4761]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9461, loss=0.4761]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 52/292 [00:09<00:38,  6.27batch/s, auc=0.9462, loss=0.4664]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9462, loss=0.4664]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 53/292 [00:09<00:38,  6.28batch/s, auc=0.9463, loss=0.5243]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9463, loss=0.5243]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 54/292 [00:09<00:37,  6.29batch/s, auc=0.9463, loss=0.4983]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9463, loss=0.4983]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 55/292 [00:09<00:37,  6.30batch/s, auc=0.9463, loss=0.3762]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9463, loss=0.3762]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 56/292 [00:09<00:37,  6.30batch/s, auc=0.9464, loss=0.4349]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 57/292 [00:09<00:37,  6.26batch/s, auc=0.9464, loss=0.4349]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 57/292 [00:10<00:37,  6.26batch/s, auc=0.9465, loss=0.6440]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 58/292 [00:10<00:37,  6.27batch/s, auc=0.9465, loss=0.6440]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 58/292 [00:10<00:37,  6.27batch/s, auc=0.9465, loss=0.4739]\u001b[A\n",
      "Training Epoch 25/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9465, loss=0.4739]\u001b[A\n",
      "Training Epoch 25/25:  20%|██        | 59/292 [00:10<00:37,  6.29batch/s, auc=0.9461, loss=0.5228]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 60/292 [00:10<00:36,  6.29batch/s, auc=0.9461, loss=0.5228]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 60/292 [00:10<00:36,  6.29batch/s, auc=0.9453, loss=0.5543]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 61/292 [00:10<00:36,  6.29batch/s, auc=0.9453, loss=0.5543]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 61/292 [00:10<00:36,  6.29batch/s, auc=0.9437, loss=0.9544]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 62/292 [00:10<00:36,  6.24batch/s, auc=0.9437, loss=0.9544]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 62/292 [00:10<00:36,  6.24batch/s, auc=0.9440, loss=0.4146]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 63/292 [00:10<00:36,  6.25batch/s, auc=0.9440, loss=0.4146]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 63/292 [00:11<00:36,  6.25batch/s, auc=0.9432, loss=0.8378]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 64/292 [00:11<00:36,  6.22batch/s, auc=0.9432, loss=0.8378]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 64/292 [00:11<00:36,  6.22batch/s, auc=0.9439, loss=0.3038]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 65/292 [00:11<00:36,  6.23batch/s, auc=0.9439, loss=0.3038]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 65/292 [00:11<00:36,  6.23batch/s, auc=0.9436, loss=0.5679]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 66/292 [00:11<00:36,  6.23batch/s, auc=0.9436, loss=0.5679]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 66/292 [00:11<00:36,  6.23batch/s, auc=0.9443, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 67/292 [00:11<00:36,  6.23batch/s, auc=0.9443, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 67/292 [00:11<00:36,  6.23batch/s, auc=0.9438, loss=0.4920]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 68/292 [00:11<00:35,  6.24batch/s, auc=0.9438, loss=0.4920]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 68/292 [00:11<00:35,  6.24batch/s, auc=0.9432, loss=0.7386]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▎       | 69/292 [00:11<00:35,  6.26batch/s, auc=0.9432, loss=0.7386]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▎       | 69/292 [00:12<00:35,  6.26batch/s, auc=0.9430, loss=0.4653]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9430, loss=0.4653]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 70/292 [00:12<00:35,  6.26batch/s, auc=0.9425, loss=0.5662]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 71/292 [00:12<00:35,  6.26batch/s, auc=0.9425, loss=0.5662]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 71/292 [00:12<00:35,  6.26batch/s, auc=0.9429, loss=0.4077]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▍       | 72/292 [00:12<00:35,  6.23batch/s, auc=0.9429, loss=0.4077]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▍       | 72/292 [00:12<00:35,  6.23batch/s, auc=0.9431, loss=0.4865]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 73/292 [00:12<00:35,  6.23batch/s, auc=0.9431, loss=0.4865]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 73/292 [00:12<00:35,  6.23batch/s, auc=0.9432, loss=0.4177]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9432, loss=0.4177]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 74/292 [00:12<00:34,  6.25batch/s, auc=0.9437, loss=0.3523]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 75/292 [00:12<00:34,  6.25batch/s, auc=0.9437, loss=0.3523]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 75/292 [00:13<00:34,  6.25batch/s, auc=0.9439, loss=0.4851]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 76/292 [00:13<00:34,  6.25batch/s, auc=0.9439, loss=0.4851]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 76/292 [00:13<00:34,  6.25batch/s, auc=0.9442, loss=0.4001]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9442, loss=0.4001]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▋       | 77/292 [00:13<00:34,  6.27batch/s, auc=0.9446, loss=0.4083]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9446, loss=0.4083]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 78/292 [00:13<00:34,  6.28batch/s, auc=0.9451, loss=0.3312]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9451, loss=0.3312]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 79/292 [00:13<00:33,  6.27batch/s, auc=0.9451, loss=0.6143]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 80/292 [00:13<00:33,  6.27batch/s, auc=0.9451, loss=0.6143]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 80/292 [00:13<00:33,  6.27batch/s, auc=0.9455, loss=0.3344]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 81/292 [00:13<00:33,  6.27batch/s, auc=0.9455, loss=0.3344]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 81/292 [00:13<00:33,  6.27batch/s, auc=0.9453, loss=0.4625]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 82/292 [00:13<00:33,  6.26batch/s, auc=0.9453, loss=0.4625]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 82/292 [00:14<00:33,  6.26batch/s, auc=0.9457, loss=0.3439]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9457, loss=0.3439]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 83/292 [00:14<00:33,  6.24batch/s, auc=0.9457, loss=0.4963]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9457, loss=0.4963]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 84/292 [00:14<00:33,  6.24batch/s, auc=0.9455, loss=0.5329]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9455, loss=0.5329]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 85/292 [00:14<00:33,  6.24batch/s, auc=0.9453, loss=0.6267]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 86/292 [00:14<00:33,  6.24batch/s, auc=0.9453, loss=0.6267]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 86/292 [00:14<00:33,  6.24batch/s, auc=0.9454, loss=0.5666]\u001b[A\n",
      "Training Epoch 25/25:  30%|██▉       | 87/292 [00:14<00:32,  6.24batch/s, auc=0.9454, loss=0.5666]\u001b[A\n",
      "Training Epoch 25/25:  30%|██▉       | 87/292 [00:14<00:32,  6.24batch/s, auc=0.9449, loss=0.7186]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 88/292 [00:14<00:32,  6.25batch/s, auc=0.9449, loss=0.7186]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 88/292 [00:15<00:32,  6.25batch/s, auc=0.9454, loss=0.3262]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9454, loss=0.3262]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 89/292 [00:15<00:32,  6.25batch/s, auc=0.9449, loss=0.6995]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 90/292 [00:15<00:32,  6.23batch/s, auc=0.9449, loss=0.6995]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 90/292 [00:15<00:32,  6.23batch/s, auc=0.9451, loss=0.4508]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 91/292 [00:15<00:32,  6.23batch/s, auc=0.9451, loss=0.4508]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 91/292 [00:15<00:32,  6.23batch/s, auc=0.9453, loss=0.4134]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9453, loss=0.4134]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 92/292 [00:15<00:32,  6.24batch/s, auc=0.9453, loss=0.4261]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 93/292 [00:15<00:31,  6.24batch/s, auc=0.9453, loss=0.4261]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 93/292 [00:15<00:31,  6.24batch/s, auc=0.9456, loss=0.3458]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 94/292 [00:15<00:31,  6.25batch/s, auc=0.9456, loss=0.3458]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 94/292 [00:16<00:31,  6.25batch/s, auc=0.9451, loss=0.6014]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 95/292 [00:16<00:31,  6.25batch/s, auc=0.9451, loss=0.6014]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 95/292 [00:16<00:31,  6.25batch/s, auc=0.9454, loss=0.3366]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 96/292 [00:16<00:31,  6.24batch/s, auc=0.9454, loss=0.3366]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 96/292 [00:16<00:31,  6.24batch/s, auc=0.9454, loss=0.4410]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9454, loss=0.4410]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 97/292 [00:16<00:31,  6.23batch/s, auc=0.9454, loss=0.4428]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9454, loss=0.4428]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▎      | 98/292 [00:16<00:31,  6.23batch/s, auc=0.9452, loss=0.4307]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9452, loss=0.4307]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 99/292 [00:16<00:30,  6.23batch/s, auc=0.9454, loss=0.4514]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 100/292 [00:16<00:30,  6.23batch/s, auc=0.9454, loss=0.4514]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 100/292 [00:17<00:30,  6.23batch/s, auc=0.9456, loss=0.3301]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 101/292 [00:17<00:30,  6.23batch/s, auc=0.9456, loss=0.3301]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 101/292 [00:17<00:30,  6.23batch/s, auc=0.9449, loss=0.9951]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 102/292 [00:17<00:30,  6.19batch/s, auc=0.9449, loss=0.9951]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 102/292 [00:17<00:30,  6.19batch/s, auc=0.9453, loss=0.2959]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9453, loss=0.2959]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▌      | 103/292 [00:17<00:30,  6.20batch/s, auc=0.9452, loss=0.5983]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9452, loss=0.5983]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 104/292 [00:17<00:30,  6.21batch/s, auc=0.9458, loss=0.2930]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9458, loss=0.2930]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 105/292 [00:17<00:30,  6.21batch/s, auc=0.9456, loss=0.6032]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9456, loss=0.6032]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▋      | 106/292 [00:17<00:29,  6.22batch/s, auc=0.9459, loss=0.3677]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 107/292 [00:17<00:29,  6.22batch/s, auc=0.9459, loss=0.3677]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 107/292 [00:18<00:29,  6.22batch/s, auc=0.9458, loss=0.4929]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9458, loss=0.4929]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 108/292 [00:18<00:29,  6.22batch/s, auc=0.9457, loss=0.4162]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9457, loss=0.4162]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 109/292 [00:18<00:29,  6.23batch/s, auc=0.9460, loss=0.3114]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 110/292 [00:18<00:29,  6.22batch/s, auc=0.9460, loss=0.3114]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 110/292 [00:18<00:29,  6.22batch/s, auc=0.9461, loss=0.4028]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9461, loss=0.4028]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 111/292 [00:18<00:29,  6.21batch/s, auc=0.9463, loss=0.3710]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 112/292 [00:18<00:29,  6.18batch/s, auc=0.9463, loss=0.3710]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 112/292 [00:18<00:29,  6.18batch/s, auc=0.9460, loss=0.7911]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▊      | 113/292 [00:18<00:28,  6.18batch/s, auc=0.9460, loss=0.7911]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▊      | 113/292 [00:19<00:28,  6.18batch/s, auc=0.9463, loss=0.3955]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9463, loss=0.3955]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 114/292 [00:19<00:28,  6.19batch/s, auc=0.9457, loss=0.9143]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9457, loss=0.9143]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 115/292 [00:19<00:28,  6.20batch/s, auc=0.9459, loss=0.4320]\u001b[A\n",
      "Training Epoch 25/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9459, loss=0.4320]\u001b[A\n",
      "Training Epoch 25/25:  40%|███▉      | 116/292 [00:19<00:28,  6.20batch/s, auc=0.9458, loss=0.4914]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 117/292 [00:19<00:28,  6.21batch/s, auc=0.9458, loss=0.4914]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 117/292 [00:19<00:28,  6.21batch/s, auc=0.9458, loss=0.4736]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9458, loss=0.4736]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 118/292 [00:19<00:28,  6.21batch/s, auc=0.9458, loss=0.4212]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 119/292 [00:19<00:28,  6.16batch/s, auc=0.9458, loss=0.4212]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 119/292 [00:20<00:28,  6.16batch/s, auc=0.9459, loss=0.4330]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 120/292 [00:20<00:27,  6.16batch/s, auc=0.9459, loss=0.4330]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 120/292 [00:20<00:27,  6.16batch/s, auc=0.9463, loss=0.2679]\u001b[A\n",
      "Training Epoch 25/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9463, loss=0.2679]\u001b[A\n",
      "Training Epoch 25/25:  41%|████▏     | 121/292 [00:20<00:27,  6.18batch/s, auc=0.9463, loss=0.5089]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9463, loss=0.5089]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 122/292 [00:20<00:27,  6.18batch/s, auc=0.9463, loss=0.4155]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9463, loss=0.4155]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 123/292 [00:20<00:27,  6.17batch/s, auc=0.9462, loss=0.5924]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9462, loss=0.5924]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 124/292 [00:20<00:27,  6.18batch/s, auc=0.9464, loss=0.3553]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 125/292 [00:20<00:27,  6.15batch/s, auc=0.9464, loss=0.3553]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 125/292 [00:21<00:27,  6.15batch/s, auc=0.9467, loss=0.3640]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 126/292 [00:21<00:26,  6.17batch/s, auc=0.9467, loss=0.3640]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 126/292 [00:21<00:26,  6.17batch/s, auc=0.9466, loss=0.5302]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9466, loss=0.5302]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 127/292 [00:21<00:26,  6.17batch/s, auc=0.9462, loss=0.8262]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9462, loss=0.8262]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 128/292 [00:21<00:26,  6.18batch/s, auc=0.9458, loss=0.7416]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 129/292 [00:21<00:26,  6.18batch/s, auc=0.9458, loss=0.7416]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 129/292 [00:21<00:26,  6.18batch/s, auc=0.9459, loss=0.5476]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9459, loss=0.5476]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 130/292 [00:21<00:26,  6.18batch/s, auc=0.9460, loss=0.3537]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 131/292 [00:21<00:26,  6.18batch/s, auc=0.9460, loss=0.3537]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 131/292 [00:22<00:26,  6.18batch/s, auc=0.9461, loss=0.3346]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▌     | 132/292 [00:22<00:26,  6.12batch/s, auc=0.9461, loss=0.3346]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▌     | 132/292 [00:22<00:26,  6.12batch/s, auc=0.9464, loss=0.4299]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 133/292 [00:22<00:25,  6.13batch/s, auc=0.9464, loss=0.4299]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 133/292 [00:22<00:25,  6.13batch/s, auc=0.9460, loss=0.8067]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9460, loss=0.8067]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 134/292 [00:22<00:25,  6.16batch/s, auc=0.9460, loss=0.4625]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9460, loss=0.4625]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 135/292 [00:22<00:25,  6.17batch/s, auc=0.9460, loss=0.4310]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 136/292 [00:22<00:25,  6.16batch/s, auc=0.9460, loss=0.4310]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 136/292 [00:22<00:25,  6.16batch/s, auc=0.9460, loss=0.5011]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 137/292 [00:22<00:25,  6.16batch/s, auc=0.9460, loss=0.5011]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 137/292 [00:23<00:25,  6.16batch/s, auc=0.9463, loss=0.2693]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 138/292 [00:23<00:24,  6.18batch/s, auc=0.9463, loss=0.2693]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 138/292 [00:23<00:24,  6.18batch/s, auc=0.9464, loss=0.4266]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9464, loss=0.4266]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 139/292 [00:23<00:24,  6.18batch/s, auc=0.9464, loss=0.4171]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9464, loss=0.4171]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 140/292 [00:23<00:24,  6.18batch/s, auc=0.9464, loss=0.4275]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9464, loss=0.4275]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 141/292 [00:23<00:24,  6.17batch/s, auc=0.9466, loss=0.4986]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▊     | 142/292 [00:23<00:24,  6.19batch/s, auc=0.9466, loss=0.4986]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▊     | 142/292 [00:23<00:24,  6.19batch/s, auc=0.9464, loss=0.7235]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9464, loss=0.7235]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 143/292 [00:23<00:24,  6.19batch/s, auc=0.9464, loss=0.4623]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 144/292 [00:23<00:23,  6.18batch/s, auc=0.9464, loss=0.4623]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 144/292 [00:24<00:23,  6.18batch/s, auc=0.9462, loss=0.6964]\u001b[A\n",
      "Training Epoch 25/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9462, loss=0.6964]\u001b[A\n",
      "Training Epoch 25/25:  50%|████▉     | 145/292 [00:24<00:23,  6.17batch/s, auc=0.9463, loss=0.4908]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 146/292 [00:24<00:23,  6.18batch/s, auc=0.9463, loss=0.4908]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 146/292 [00:24<00:23,  6.18batch/s, auc=0.9460, loss=0.7130]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9460, loss=0.7130]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 147/292 [00:24<00:23,  6.19batch/s, auc=0.9459, loss=0.5752]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 148/292 [00:24<00:23,  6.18batch/s, auc=0.9459, loss=0.5752]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 148/292 [00:24<00:23,  6.18batch/s, auc=0.9459, loss=0.5315]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 149/292 [00:24<00:23,  6.17batch/s, auc=0.9459, loss=0.5315]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 149/292 [00:24<00:23,  6.17batch/s, auc=0.9458, loss=0.4957]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████▏    | 150/292 [00:24<00:23,  6.17batch/s, auc=0.9458, loss=0.4957]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████▏    | 150/292 [00:25<00:23,  6.17batch/s, auc=0.9457, loss=0.4448]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9457, loss=0.4448]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 151/292 [00:25<00:22,  6.17batch/s, auc=0.9457, loss=0.6507]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.15batch/s, auc=0.9457, loss=0.6507]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 152/292 [00:25<00:22,  6.15batch/s, auc=0.9461, loss=0.3139]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9461, loss=0.3139]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 153/292 [00:25<00:22,  6.16batch/s, auc=0.9459, loss=0.6529]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9459, loss=0.6529]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 154/292 [00:25<00:22,  6.16batch/s, auc=0.9458, loss=0.5420]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9458, loss=0.5420]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 155/292 [00:25<00:22,  6.13batch/s, auc=0.9458, loss=0.4474]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 156/292 [00:25<00:22,  6.14batch/s, auc=0.9458, loss=0.4474]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 156/292 [00:26<00:22,  6.14batch/s, auc=0.9459, loss=0.4406]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9459, loss=0.4406]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 157/292 [00:26<00:21,  6.15batch/s, auc=0.9459, loss=0.3792]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9459, loss=0.3792]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 158/292 [00:26<00:21,  6.15batch/s, auc=0.9459, loss=0.4490]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.15batch/s, auc=0.9459, loss=0.4490]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 159/292 [00:26<00:21,  6.15batch/s, auc=0.9457, loss=0.6907]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9457, loss=0.6907]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▍    | 160/292 [00:26<00:21,  6.15batch/s, auc=0.9460, loss=0.3532]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9460, loss=0.3532]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 161/292 [00:26<00:21,  6.14batch/s, auc=0.9461, loss=0.4591]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 162/292 [00:26<00:21,  6.14batch/s, auc=0.9461, loss=0.4591]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 162/292 [00:27<00:21,  6.14batch/s, auc=0.9459, loss=0.7310]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9459, loss=0.7310]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 163/292 [00:27<00:21,  6.14batch/s, auc=0.9460, loss=0.4107]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9460, loss=0.4107]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 164/292 [00:27<00:20,  6.13batch/s, auc=0.9463, loss=0.3004]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.11batch/s, auc=0.9463, loss=0.3004]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 165/292 [00:27<00:20,  6.11batch/s, auc=0.9462, loss=0.4567]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9462, loss=0.4567]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 166/292 [00:27<00:20,  6.11batch/s, auc=0.9463, loss=0.5227]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9463, loss=0.5227]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 167/292 [00:27<00:20,  6.11batch/s, auc=0.9464, loss=0.3388]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 168/292 [00:27<00:20,  6.12batch/s, auc=0.9464, loss=0.3388]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 168/292 [00:28<00:20,  6.12batch/s, auc=0.9462, loss=0.7467]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.12batch/s, auc=0.9462, loss=0.7467]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 169/292 [00:28<00:20,  6.12batch/s, auc=0.9462, loss=0.5808]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9462, loss=0.5808]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 170/292 [00:28<00:19,  6.12batch/s, auc=0.9464, loss=0.2981]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.13batch/s, auc=0.9464, loss=0.2981]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▊    | 171/292 [00:28<00:19,  6.13batch/s, auc=0.9465, loss=0.3743]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.13batch/s, auc=0.9465, loss=0.3743]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 172/292 [00:28<00:19,  6.13batch/s, auc=0.9463, loss=0.6627]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.14batch/s, auc=0.9463, loss=0.6627]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 173/292 [00:28<00:19,  6.14batch/s, auc=0.9461, loss=0.6627]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 174/292 [00:28<00:19,  6.14batch/s, auc=0.9461, loss=0.6627]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 174/292 [00:29<00:19,  6.14batch/s, auc=0.9458, loss=0.6223]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.15batch/s, auc=0.9458, loss=0.6223]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 175/292 [00:29<00:19,  6.15batch/s, auc=0.9460, loss=0.4973]\u001b[A\n",
      "Training Epoch 25/25:  60%|██████    | 176/292 [00:29<00:18,  6.15batch/s, auc=0.9460, loss=0.4973]\u001b[A\n",
      "Training Epoch 25/25:  60%|██████    | 176/292 [00:29<00:18,  6.15batch/s, auc=0.9463, loss=0.3036]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9463, loss=0.3036]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 177/292 [00:29<00:18,  6.14batch/s, auc=0.9458, loss=0.7469]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 178/292 [00:29<00:18,  6.14batch/s, auc=0.9458, loss=0.7469]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 178/292 [00:29<00:18,  6.14batch/s, auc=0.9457, loss=0.4420]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.14batch/s, auc=0.9457, loss=0.4420]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████▏   | 179/292 [00:29<00:18,  6.14batch/s, auc=0.9458, loss=0.5020]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.14batch/s, auc=0.9458, loss=0.5020]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 180/292 [00:29<00:18,  6.14batch/s, auc=0.9459, loss=0.4983]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 181/292 [00:29<00:18,  6.12batch/s, auc=0.9459, loss=0.4983]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 181/292 [00:30<00:18,  6.12batch/s, auc=0.9460, loss=0.3248]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9460, loss=0.3248]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 182/292 [00:30<00:17,  6.12batch/s, auc=0.9455, loss=0.8419]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.12batch/s, auc=0.9455, loss=0.8419]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 183/292 [00:30<00:17,  6.12batch/s, auc=0.9454, loss=0.5030]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9454, loss=0.5030]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 184/292 [00:30<00:17,  6.11batch/s, auc=0.9452, loss=0.7466]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.10batch/s, auc=0.9452, loss=0.7466]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 185/292 [00:30<00:17,  6.10batch/s, auc=0.9452, loss=0.5957]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.10batch/s, auc=0.9452, loss=0.5957]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▎   | 186/292 [00:30<00:17,  6.10batch/s, auc=0.9452, loss=0.5035]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 187/292 [00:30<00:17,  6.10batch/s, auc=0.9452, loss=0.5035]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 187/292 [00:31<00:17,  6.10batch/s, auc=0.9453, loss=0.3475]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9453, loss=0.3475]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 188/292 [00:31<00:17,  6.10batch/s, auc=0.9454, loss=0.3598]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9454, loss=0.3598]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▍   | 189/292 [00:31<00:16,  6.10batch/s, auc=0.9455, loss=0.3836]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.09batch/s, auc=0.9455, loss=0.3836]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 190/292 [00:31<00:16,  6.09batch/s, auc=0.9454, loss=0.5874]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9454, loss=0.5874]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 191/292 [00:31<00:16,  6.09batch/s, auc=0.9458, loss=0.2535]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9458, loss=0.2535]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 192/292 [00:31<00:16,  6.09batch/s, auc=0.9459, loss=0.3730]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 193/292 [00:31<00:16,  6.09batch/s, auc=0.9459, loss=0.3730]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 193/292 [00:32<00:16,  6.09batch/s, auc=0.9459, loss=0.4329]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9459, loss=0.4329]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▋   | 194/292 [00:32<00:16,  6.07batch/s, auc=0.9459, loss=0.5477]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.05batch/s, auc=0.9459, loss=0.5477]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 195/292 [00:32<00:16,  6.05batch/s, auc=0.9458, loss=0.6487]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.95batch/s, auc=0.9458, loss=0.6487]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 196/292 [00:32<00:16,  5.95batch/s, auc=0.9459, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.78batch/s, auc=0.9459, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 197/292 [00:32<00:16,  5.78batch/s, auc=0.9459, loss=0.4933]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 198/292 [00:32<00:16,  5.75batch/s, auc=0.9459, loss=0.4933]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 198/292 [00:33<00:16,  5.75batch/s, auc=0.9460, loss=0.3904]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.77batch/s, auc=0.9460, loss=0.3904]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 199/292 [00:33<00:16,  5.77batch/s, auc=0.9460, loss=0.5272]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.71batch/s, auc=0.9460, loss=0.5272]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 200/292 [00:33<00:16,  5.71batch/s, auc=0.9460, loss=0.3839]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.69batch/s, auc=0.9460, loss=0.3839]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 201/292 [00:33<00:15,  5.69batch/s, auc=0.9457, loss=1.0985]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.67batch/s, auc=0.9457, loss=1.0985]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 202/292 [00:33<00:15,  5.67batch/s, auc=0.9458, loss=0.5925]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.67batch/s, auc=0.9458, loss=0.5925]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 203/292 [00:33<00:15,  5.67batch/s, auc=0.9457, loss=0.5546]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 204/292 [00:33<00:15,  5.68batch/s, auc=0.9457, loss=0.5546]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 204/292 [00:34<00:15,  5.68batch/s, auc=0.9459, loss=0.3459]\u001b[A\n",
      "Training Epoch 25/25:  70%|███████   | 205/292 [00:34<00:15,  5.67batch/s, auc=0.9459, loss=0.3459]\u001b[A\n",
      "Training Epoch 25/25:  70%|███████   | 205/292 [00:34<00:15,  5.67batch/s, auc=0.9459, loss=0.4065]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 206/292 [00:34<00:15,  5.66batch/s, auc=0.9459, loss=0.4065]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 206/292 [00:34<00:15,  5.66batch/s, auc=0.9458, loss=0.6541]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9458, loss=0.6541]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 207/292 [00:34<00:14,  5.67batch/s, auc=0.9457, loss=0.4795]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 208/292 [00:34<00:14,  5.66batch/s, auc=0.9457, loss=0.4795]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 208/292 [00:34<00:14,  5.66batch/s, auc=0.9457, loss=0.6363]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.66batch/s, auc=0.9457, loss=0.6363]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 209/292 [00:34<00:14,  5.66batch/s, auc=0.9455, loss=0.7122]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 210/292 [00:34<00:14,  5.78batch/s, auc=0.9455, loss=0.7122]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 210/292 [00:35<00:14,  5.78batch/s, auc=0.9456, loss=0.3681]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.85batch/s, auc=0.9456, loss=0.3681]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 211/292 [00:35<00:13,  5.85batch/s, auc=0.9458, loss=0.3030]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.80batch/s, auc=0.9458, loss=0.3030]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 212/292 [00:35<00:13,  5.80batch/s, auc=0.9458, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.87batch/s, auc=0.9458, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 213/292 [00:35<00:13,  5.87batch/s, auc=0.9456, loss=0.6239]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.92batch/s, auc=0.9456, loss=0.6239]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 214/292 [00:35<00:13,  5.92batch/s, auc=0.9455, loss=0.4943]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▎  | 215/292 [00:35<00:12,  5.95batch/s, auc=0.9455, loss=0.4943]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▎  | 215/292 [00:35<00:12,  5.95batch/s, auc=0.9458, loss=0.3235]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 216/292 [00:35<00:12,  5.97batch/s, auc=0.9458, loss=0.3235]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 216/292 [00:36<00:12,  5.97batch/s, auc=0.9458, loss=0.4091]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.74batch/s, auc=0.9458, loss=0.4091]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 217/292 [00:36<00:13,  5.74batch/s, auc=0.9459, loss=0.4082]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.83batch/s, auc=0.9459, loss=0.4082]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▍  | 218/292 [00:36<00:12,  5.83batch/s, auc=0.9457, loss=0.7077]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.78batch/s, auc=0.9457, loss=0.7077]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 219/292 [00:36<00:12,  5.78batch/s, auc=0.9458, loss=0.4394]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.87batch/s, auc=0.9458, loss=0.4394]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 220/292 [00:36<00:12,  5.87batch/s, auc=0.9460, loss=0.3016]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.93batch/s, auc=0.9460, loss=0.3016]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 221/292 [00:36<00:11,  5.93batch/s, auc=0.9462, loss=0.3057]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 222/292 [00:36<00:11,  5.96batch/s, auc=0.9462, loss=0.3057]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 222/292 [00:37<00:11,  5.96batch/s, auc=0.9463, loss=0.4054]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.98batch/s, auc=0.9463, loss=0.4054]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▋  | 223/292 [00:37<00:11,  5.98batch/s, auc=0.9462, loss=0.6331]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.01batch/s, auc=0.9462, loss=0.6331]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 224/292 [00:37<00:11,  6.01batch/s, auc=0.9463, loss=0.3736]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.02batch/s, auc=0.9463, loss=0.3736]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 225/292 [00:37<00:11,  6.02batch/s, auc=0.9463, loss=0.4658]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.92batch/s, auc=0.9463, loss=0.4658]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 226/292 [00:37<00:11,  5.92batch/s, auc=0.9462, loss=0.5118]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.94batch/s, auc=0.9462, loss=0.5118]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 227/292 [00:37<00:10,  5.94batch/s, auc=0.9462, loss=0.3931]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 228/292 [00:37<00:10,  5.90batch/s, auc=0.9462, loss=0.3931]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 228/292 [00:38<00:10,  5.90batch/s, auc=0.9462, loss=0.4508]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.81batch/s, auc=0.9462, loss=0.4508]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 229/292 [00:38<00:10,  5.81batch/s, auc=0.9463, loss=0.5284]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.88batch/s, auc=0.9463, loss=0.5284]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 230/292 [00:38<00:10,  5.88batch/s, auc=0.9462, loss=0.6429]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.74batch/s, auc=0.9462, loss=0.6429]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 231/292 [00:38<00:10,  5.74batch/s, auc=0.9462, loss=0.4064]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9462, loss=0.4064]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 232/292 [00:38<00:10,  5.71batch/s, auc=0.9463, loss=0.3615]\u001b[A\n",
      "Training Epoch 25/25:  80%|███████▉  | 233/292 [00:38<00:10,  5.69batch/s, auc=0.9463, loss=0.3615]\u001b[A\n",
      "Training Epoch 25/25:  80%|███████▉  | 233/292 [00:39<00:10,  5.69batch/s, auc=0.9465, loss=0.3206]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 234/292 [00:39<00:10,  5.70batch/s, auc=0.9465, loss=0.3206]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 234/292 [00:39<00:10,  5.70batch/s, auc=0.9464, loss=0.4496]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 235/292 [00:39<00:10,  5.62batch/s, auc=0.9464, loss=0.4496]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 235/292 [00:39<00:10,  5.62batch/s, auc=0.9463, loss=0.4581]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9463, loss=0.4581]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 236/292 [00:39<00:09,  5.62batch/s, auc=0.9464, loss=0.3765]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 237/292 [00:39<00:09,  5.63batch/s, auc=0.9464, loss=0.3765]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 237/292 [00:39<00:09,  5.63batch/s, auc=0.9461, loss=0.8776]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.63batch/s, auc=0.9461, loss=0.8776]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 238/292 [00:39<00:09,  5.63batch/s, auc=0.9461, loss=0.5269]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 239/292 [00:39<00:09,  5.61batch/s, auc=0.9461, loss=0.5269]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 239/292 [00:40<00:09,  5.61batch/s, auc=0.9462, loss=0.3161]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.60batch/s, auc=0.9462, loss=0.3161]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 240/292 [00:40<00:09,  5.60batch/s, auc=0.9456, loss=1.4529]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.68batch/s, auc=0.9456, loss=1.4529]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 241/292 [00:40<00:08,  5.68batch/s, auc=0.9458, loss=0.3645]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.79batch/s, auc=0.9458, loss=0.3645]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 242/292 [00:40<00:08,  5.79batch/s, auc=0.9459, loss=0.3022]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.85batch/s, auc=0.9459, loss=0.3022]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 243/292 [00:40<00:08,  5.85batch/s, auc=0.9460, loss=0.3070]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.80batch/s, auc=0.9460, loss=0.3070]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▎ | 244/292 [00:40<00:08,  5.80batch/s, auc=0.9460, loss=0.5843]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 245/292 [00:40<00:08,  5.74batch/s, auc=0.9460, loss=0.5843]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 245/292 [00:41<00:08,  5.74batch/s, auc=0.9459, loss=0.5562]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.70batch/s, auc=0.9459, loss=0.5562]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 246/292 [00:41<00:08,  5.70batch/s, auc=0.9459, loss=0.4829]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9459, loss=0.4829]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 247/292 [00:41<00:07,  5.66batch/s, auc=0.9456, loss=0.7324]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9456, loss=0.7324]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 248/292 [00:41<00:07,  5.64batch/s, auc=0.9458, loss=0.3657]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.63batch/s, auc=0.9458, loss=0.3657]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▌ | 249/292 [00:41<00:07,  5.63batch/s, auc=0.9457, loss=0.5683]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 250/292 [00:41<00:07,  5.69batch/s, auc=0.9457, loss=0.5683]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 250/292 [00:42<00:07,  5.69batch/s, auc=0.9455, loss=0.6615]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.65batch/s, auc=0.9455, loss=0.6615]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 251/292 [00:42<00:07,  5.65batch/s, auc=0.9453, loss=0.9207]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.63batch/s, auc=0.9453, loss=0.9207]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▋ | 252/292 [00:42<00:07,  5.63batch/s, auc=0.9452, loss=0.5676]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 253/292 [00:42<00:07,  5.56batch/s, auc=0.9452, loss=0.5676]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 253/292 [00:42<00:07,  5.56batch/s, auc=0.9454, loss=0.3635]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.68batch/s, auc=0.9454, loss=0.3635]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 254/292 [00:42<00:06,  5.68batch/s, auc=0.9454, loss=0.5032]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.67batch/s, auc=0.9454, loss=0.5032]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 255/292 [00:42<00:06,  5.67batch/s, auc=0.9450, loss=0.9980]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 256/292 [00:42<00:06,  5.76batch/s, auc=0.9450, loss=0.9980]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 256/292 [00:43<00:06,  5.76batch/s, auc=0.9449, loss=0.5850]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.71batch/s, auc=0.9449, loss=0.5850]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 257/292 [00:43<00:06,  5.71batch/s, auc=0.9451, loss=0.4588]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.70batch/s, auc=0.9451, loss=0.4588]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 258/292 [00:43<00:05,  5.70batch/s, auc=0.9449, loss=0.6337]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.69batch/s, auc=0.9449, loss=0.6337]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▊ | 259/292 [00:43<00:05,  5.69batch/s, auc=0.9448, loss=0.5896]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.66batch/s, auc=0.9448, loss=0.5896]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 260/292 [00:43<00:05,  5.66batch/s, auc=0.9449, loss=0.5170]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.75batch/s, auc=0.9449, loss=0.5170]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 261/292 [00:43<00:05,  5.75batch/s, auc=0.9450, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  90%|████████▉ | 262/292 [00:43<00:05,  5.82batch/s, auc=0.9450, loss=0.3659]\u001b[A\n",
      "Training Epoch 25/25:  90%|████████▉ | 262/292 [00:44<00:05,  5.82batch/s, auc=0.9449, loss=0.4761]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 263/292 [00:44<00:05,  5.75batch/s, auc=0.9449, loss=0.4761]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 263/292 [00:44<00:05,  5.75batch/s, auc=0.9449, loss=0.3973]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 264/292 [00:44<00:04,  5.81batch/s, auc=0.9449, loss=0.3973]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 264/292 [00:44<00:04,  5.81batch/s, auc=0.9450, loss=0.4437]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 265/292 [00:44<00:04,  5.75batch/s, auc=0.9450, loss=0.4437]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 265/292 [00:44<00:04,  5.75batch/s, auc=0.9449, loss=0.6480]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.9449, loss=0.6480]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 266/292 [00:44<00:04,  5.70batch/s, auc=0.9450, loss=0.4452]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████▏| 267/292 [00:44<00:04,  5.70batch/s, auc=0.9450, loss=0.4452]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████▏| 267/292 [00:45<00:04,  5.70batch/s, auc=0.9448, loss=0.6791]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.65batch/s, auc=0.9448, loss=0.6791]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 268/292 [00:45<00:04,  5.65batch/s, auc=0.9448, loss=0.3939]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.61batch/s, auc=0.9448, loss=0.3939]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 269/292 [00:45<00:04,  5.61batch/s, auc=0.9449, loss=0.4938]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.66batch/s, auc=0.9449, loss=0.4938]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 270/292 [00:45<00:03,  5.66batch/s, auc=0.9447, loss=0.5800]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9447, loss=0.5800]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 271/292 [00:45<00:03,  5.63batch/s, auc=0.9447, loss=0.5031]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.62batch/s, auc=0.9447, loss=0.5031]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 272/292 [00:45<00:03,  5.62batch/s, auc=0.9447, loss=0.4289]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 273/292 [00:45<00:03,  5.59batch/s, auc=0.9447, loss=0.4289]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 273/292 [00:46<00:03,  5.59batch/s, auc=0.9448, loss=0.3856]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.65batch/s, auc=0.9448, loss=0.3856]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 274/292 [00:46<00:03,  5.65batch/s, auc=0.9449, loss=0.4213]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.58batch/s, auc=0.9449, loss=0.4213]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 275/292 [00:46<00:03,  5.58batch/s, auc=0.9451, loss=0.2962]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.59batch/s, auc=0.9451, loss=0.2962]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 276/292 [00:46<00:02,  5.59batch/s, auc=0.9450, loss=0.5266]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.58batch/s, auc=0.9450, loss=0.5266]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 277/292 [00:46<00:02,  5.58batch/s, auc=0.9451, loss=0.3920]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.58batch/s, auc=0.9451, loss=0.3920]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▌| 278/292 [00:46<00:02,  5.58batch/s, auc=0.9452, loss=0.5281]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 279/292 [00:46<00:02,  5.57batch/s, auc=0.9452, loss=0.5281]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 279/292 [00:47<00:02,  5.57batch/s, auc=0.9452, loss=0.4758]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.56batch/s, auc=0.9452, loss=0.4758]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 280/292 [00:47<00:02,  5.56batch/s, auc=0.9452, loss=0.4911]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.57batch/s, auc=0.9452, loss=0.4911]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 281/292 [00:47<00:01,  5.57batch/s, auc=0.9450, loss=0.6775]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.65batch/s, auc=0.9450, loss=0.6775]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 282/292 [00:47<00:01,  5.65batch/s, auc=0.9452, loss=0.3989]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.9452, loss=0.3989]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 283/292 [00:47<00:01,  5.64batch/s, auc=0.9451, loss=0.4370]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 284/292 [00:47<00:01,  5.71batch/s, auc=0.9451, loss=0.4370]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 284/292 [00:48<00:01,  5.71batch/s, auc=0.9453, loss=0.3578]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.64batch/s, auc=0.9453, loss=0.3578]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 285/292 [00:48<00:01,  5.64batch/s, auc=0.9454, loss=0.2973]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.74batch/s, auc=0.9454, loss=0.2973]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 286/292 [00:48<00:01,  5.74batch/s, auc=0.9456, loss=0.2565]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.80batch/s, auc=0.9456, loss=0.2565]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 287/292 [00:48<00:00,  5.80batch/s, auc=0.9456, loss=0.4249]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.83batch/s, auc=0.9456, loss=0.4249]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▊| 288/292 [00:48<00:00,  5.83batch/s, auc=0.9455, loss=0.5642]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9455, loss=0.5642]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 289/292 [00:48<00:00,  5.86batch/s, auc=0.9457, loss=0.2815]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 290/292 [00:48<00:00,  5.89batch/s, auc=0.9457, loss=0.2815]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 290/292 [00:49<00:00,  5.89batch/s, auc=0.9457, loss=0.4953]\u001b[A\n",
      "Training Epoch 25/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.92batch/s, auc=0.9457, loss=0.4953]\u001b[A\n",
      "Training Epoch 25/25: 100%|█████████▉| 291/292 [00:49<00:00,  5.92batch/s, auc=0.9456, loss=0.4863]\u001b[A\n",
      "Training Epoch 25/25: 100%|██████████| 292/292 [00:49<00:00,  5.93batch/s, auc=0.9456, loss=0.4863]\u001b[A\n",
      "Epochs: 100%|██████████| 25/25 [22:54<00:00, 54.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25] Train Loss: 0.4956 | Train AUROC: 0.9456 Val Loss: 0.6814 | Val AUROC: 0.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augment = False\n",
    "# begin training\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels, all_outputs = [], []\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Training Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.augment(images)\n",
    "            outputs = model(images) # forward pass\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad() # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy()) # Collect true labels and outputs for AUROC calculation\n",
    "            all_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            # Calculate running AUROC (updated per batch)\n",
    "            try:\n",
    "                batch_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "            except ValueError:\n",
    "                batch_auc = 0.0  # Handle potential errors in AUROC calculation (e.g., single class in batch)\n",
    "            # Update pbar with current loss and AUROC\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", auc=f\"{batch_auc:.4f}\")\n",
    "\n",
    "    # Calculate epoch-level AUROC after all batches\n",
    "    train_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss, val_labels, val_outputs = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.augment(images)\n",
    "                #images = gca.reconstruct(images)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect true labels and outputs for validation AUROC\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Calculate validation AUROC\n",
    "    val_auc = roc_auc_score(np.array(val_labels), np.array(val_outputs), multi_class='ovr')\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Display epoch summary\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{epochs}] \"\n",
    "        f\"Train Loss: {train_loss / len(train_loader):.4f} | Train AUROC: {train_auc:.4f} \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val AUROC: {val_auc:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, ckpt_name))\n",
    "\n",
    "    # Log results\n",
    "    logs.append([epoch + 1, train_loss, train_auc, val_loss, val_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "testpath = f'with-pos-weight-r={rate}'\n",
    "def evaluate_model(model, dataloader, criterion, device, name):\n",
    "    save_dir, test_data = \"../results/tests/\", \"rsna\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model.eval()\n",
    "    test_loss, all_outputs, all_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.reconstruct(images)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
    "            labels = labels.squeeze(1).cpu().numpy()\n",
    "\n",
    "            all_outputs.extend(outputs)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    \n",
    "    preds = np.array(all_outputs) > 0.5\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "\n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, preds).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f} | Test AUROC: {auc:.4f} | Test Accuracy: {acc:.4f} | FNR: {fnr:.4f}\")\n",
    "    # Calculate epoch-level AUROC after all batches\n",
    "    final_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')       \n",
    "    df = pd.DataFrame(pd.read_csv(f'../splits/{test_data}_test.csv')['path'])\n",
    "    df['Pneumonia_pred'] = all_outputs\n",
    "    df.to_csv(f'{save_dir}{name}_pred.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.5406 | Test AUROC: 0.7158 | Test Accuracy: 0.7835 | FNR: 0.5172\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "df = evaluate_model(model, test_loader, criterion, device, testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import ast \n",
    "\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def __threshold(y_true, y_pred):\n",
    "    # Youden's J Statistic threshold\n",
    "    fprs, tprs, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    return thresholds[np.nanargmax(tprs - fprs)]\n",
    "\n",
    "def __metrics_binary(y_true, y_pred, threshold):\n",
    "    # Threshold predictions  \n",
    "    y_pred_t = (y_pred > threshold).astype(int)\n",
    "    try:  \n",
    "        auroc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        auroc = np.nan\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred_t, labels=[0,1]).ravel()\n",
    "    if tp + fn != 0:\n",
    "        tpr = tp/(tp + fn)\n",
    "        fnr = fn/(tp + fn)\n",
    "    else:\n",
    "        tpr = np.nan\n",
    "        fnr = np.nan\n",
    "    if tn + fp != 0:\n",
    "        tnr = tn/(tn + fp)\n",
    "        fpr = fp/(tn + fp)\n",
    "    else:\n",
    "        tnr = np.nan\n",
    "        fpr = np.nan\n",
    "    if tp + fp != 0:\n",
    "        fdr = fp/(fp + tp)\n",
    "        ppv = tp/(fp + tp)\n",
    "    else:\n",
    "        ppv = np.nan\n",
    "    if fn + tn != 0:\n",
    "        npv = tn/(fn + tn)\n",
    "        fomr = fn/(fn + tn)\n",
    "    else:\n",
    "        npv = np.nan\n",
    "        fomr = np.nan\n",
    "    return auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __analyze_aim_2(model, test_data, name, target_sex=None, target_age=None, augmentation=False):\n",
    "    trial, rate  = 0, 0\n",
    "    if target_sex is not None and target_age is not None:\n",
    "        target_path = f'target_sex={target_sex}_age={target_age}'\n",
    "    elif target_sex is not None:\n",
    "        target_path = f'target_sex={target_sex}'\n",
    "    elif target_age is not None:\n",
    "        target_path = f'target_age={target_age}'\n",
    "    else:\n",
    "        target_path = 'target_all'\n",
    "    results = [] \n",
    "    y_true = pd.read_csv(f'../splits/{test_data}_test.csv')\n",
    "    if augmentation:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "    else:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "\n",
    "    auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values, threshold)\n",
    "    results += [[target_sex, target_age, trial, rate, np.nan, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        y_true_t = y_true[y_true['Sex'] == dem_sex]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, dem_sex, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "        y_true_t = y_true[y_true['Age_group'] == dem_age]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, np.nan, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "            y_true_t = y_true[(y_true['Sex'] == dem_sex) & (y_true['Age_group'] == dem_age)]\n",
    "            y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            results += [[target_sex, target_age, trial, rate, dem_sex, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    return results\n",
    "  \n",
    "def analyze_aim_2(model, test_data, name, augmentation=False):\n",
    "    results = []\n",
    "    if augmentation:\n",
    "        results += __analyze_aim_2(model, test_data, testpath, None, None, augmentation=True)\n",
    "    else:\n",
    "        results += __analyze_aim_2(model, test_data, testpath, None, None, augmentation=False)\n",
    "    results = np.array(results)\n",
    "    df = pd.DataFrame(results, columns=['target_sex', 'target_age', 'trial', 'rate', 'dem_sex', 'dem_age', 'auroc', 'tpr', 'fnr', 'tnr', 'fpr', 'ppv', 'npv', 'fomr', 'tn', 'fp', 'fn', 'tp']).sort_values(['target_sex', 'target_age', 'trial', 'rate'])\n",
    "    \n",
    "    if augmentation:\n",
    "        save_dir = f\"../results/analyze/\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        df.to_csv(f'{save_dir}GCA-{name}_summary.csv', index=False)\n",
    "    else:\n",
    "        save_dir = f\"../results/analyze/\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        df.to_csv(f'{save_dir}{name}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_aim_2(\"densenet\", \"rsna\", testpath, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild the model\n",
    "model = CustomModel(base_model_name='densenet', num_classes=1).to(device)  # or 'resnet' if used\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"models/model.pth\"))\n",
    "model.eval()  # Very important for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomDataset(csv_file='../splits/rsna_test.csv', test=True)\n",
    "test_loader = create_dataloader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8701 | Test AUROC: 0.7876 | Test Accuracy: 0.7281 | FNR: 0.2778\n",
      "Test Loss: 0.8701 | Test AUROC: 0.7876 | Test Accuracy: 0.7281 | FNR: 0.2778\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_auc, test_acc, fnr = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test AUROC: {test_auc:.4f} | Test Accuracy: {test_acc:.4f} | FNR: {fnr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
