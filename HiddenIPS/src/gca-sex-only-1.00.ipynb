{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, autograd, optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "from local import GCA\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from stylegan2 import Generator, Encoder\n",
    "import random\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "        self.ckpt = torch.load(self.ckpt, map_location=lambda storage, loc: storage) # load model checkpoint\n",
    "\n",
    "class GCA():\n",
    "    def __init__(self, device=\"cuda\", h_path = None, ckpt='models/000500.pt'):\n",
    "        self.device = device #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.h_path = h_path # path to sex and age hyperplanes\n",
    "        self.size, self.n_mlp, self.channel_multiplier, self.cgan = 256, 8, 2, True\n",
    "        self.classifier_nof_classes, self.embedding_size, self.latent = 2, 10, 512\n",
    "        self.g_reg_every, self.lr, self.ckpt = 4, 0.002, ckpt\n",
    "        # load model checkpoints\n",
    "        self.ckpt = torch.load(self.ckpt, map_location=lambda storage, loc: storage)\n",
    "        self.generator = Generator(self.size, self.latent, self.n_mlp, channel_multiplier=self.channel_multiplier, \n",
    "                              conditional_gan=self.cgan, nof_classes=self.classifier_nof_classes, \n",
    "                              embedding_size=self.embedding_size).to(self.device)\n",
    "        self.encoder = Encoder(self.size, channel_multiplier=self.channel_multiplier, output_channels=self.latent).to(self.device)\n",
    "        self.generator.load_state_dict(self.ckpt[\"g\"]); self.encoder.load_state_dict(self.ckpt[\"e\"]) # load checkpoints\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
    "            ]\n",
    "        )        \n",
    "        # Get SVM coefficients\n",
    "        self.sex_coeff, self.age_coeff = None, None\n",
    "        self.w_shape = None\n",
    "        self.__get_hyperplanes__()\n",
    "        \n",
    "        del self.size, self.n_mlp, self.channel_multiplier, self.cgan\n",
    "        del self.classifier_nof_classes, self.embedding_size, self.latent\n",
    "        del self.g_reg_every, self.lr, self.ckpt\n",
    "        \n",
    "        \n",
    "    def __load_image__(self, path):\n",
    "        img = cv2.imread(path)  # Load image using cv2\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img_tensor = self.transform(img_rgb).unsqueeze(0).to(self.device)  # Preprocess\n",
    "        return img_tensor\n",
    "\n",
    "    def __process_in_batches__(self, patients, batch_size):\n",
    "        style_vectors = []\n",
    "        for i in range(0, len(patients), batch_size):\n",
    "            batch_paths = patients.iloc[i : i + batch_size][\"Path\"].tolist()\n",
    "            batch_imgs = [self.__load_image__(path) for path in batch_paths]\n",
    "            batch_imgs_tensor = torch.cat(batch_imgs, dim=0)  # Stack images in a batch\n",
    "            with torch.no_grad():  # Avoid tracking gradients to save memory\n",
    "                # Encode batch to latent vectors in Z space\n",
    "                w_latents = self.encoder(batch_imgs_tensor)\n",
    "            # Move to CPU to save memory and add to list\n",
    "            style_vectors.extend(w_latents.cpu())\n",
    "            del batch_imgs_tensor, w_latents # Cleanup and clear cache\n",
    "            torch.cuda.empty_cache()  # Clear cache to free memory\n",
    "        return style_vectors\n",
    "\n",
    "    def __load_cxr_data__(self, df):\n",
    "        return self.__process_in_batches__(df, batch_size=16)\n",
    "\n",
    "    def __get_patient_data__(self, rsna_csv=\"../datasets/rsna_patients.csv\", cxpt_csv=\"../chexpert/versions/1/train.csv\"):\n",
    "        if os.path.exists(rsna_csv) and os.path.exists(cxpt_csv):\n",
    "            n_patients = 500\n",
    "            rsna_csv = pd.DataFrame(pd.read_csv(rsna_csv))\n",
    "            cxpt_csv = pd.DataFrame(pd.read_csv(cxpt_csv))\n",
    "            rsna_csv[\"Image Index\"] = \"../../datasets/rsna/\" + rsna_csv[\"Image Index\"] # add prefix to path\n",
    "            rsna_csv.rename(columns={\"Image Index\": \"Path\", \"Patient Age\": \"Age\", \"Patient Gender\": \"Sex\"}, inplace=True)\n",
    "\n",
    "            # Load 500 latent vectors from each class\n",
    "            male = rsna_csv[rsna_csv[\"Sex\"] == \"M\"][:500]\n",
    "            female = rsna_csv[rsna_csv[\"Sex\"] == \"F\"][:500]\n",
    "            young = rsna_csv[rsna_csv[\"Age\"] < 20][:500]\n",
    "            rsna = rsna_csv[rsna_csv[\"Age\"] > 80][:250]\n",
    "            cxpt = cxpt_csv[cxpt_csv[\"Age\"] > 80][:250]\n",
    "            old = pd.concat([rsna, cxpt], ignore_index=True)\n",
    "            return {\"m\": male, \"f\": female, \"y\": young, \"o\": old}\n",
    "        elif os.path.exists(rsna_csv):\n",
    "            n_patients = 500\n",
    "            rsna_csv = pd.DataFrame(pd.read_csv(rsna_csv))\n",
    "            rsna_csv[\"Image Index\"] = \"../datasets/rsna/\" + rsna_csv[\"Image Index\"] # add prefix to path\n",
    "            rsna_csv.rename(columns={\"Image Index\": \"Path\", \"Patient Age\": \"Age\", \"Patient Gender\": \"Sex\"}, inplace=True)\n",
    "\n",
    "            # Load 500 latent vectors from each class\n",
    "            male = rsna_csv[rsna_csv[\"Sex\"] == \"M\"][:500]\n",
    "            female = rsna_csv[rsna_csv[\"Sex\"] == \"F\"][:500]\n",
    "            young = rsna_csv[rsna_csv[\"Age\"] < 20][:500]\n",
    "            old = rsna_csv[rsna_csv[\"Age\"] > 80][:250]\n",
    "            return {\"m\": male, \"f\": female, \"y\": young, \"o\": old}\n",
    "        else:\n",
    "            print(f\"The path '{path}' does not exist.\")\n",
    "            return None\n",
    "\n",
    "    def __learn_linearSVM__(self, d1, d2, df1, df2, key=\"Sex\"):\n",
    "      # prepare dataset\n",
    "        styles, labels = [], []\n",
    "        styles.extend(d1); labels.extend(list(df1[\"Sex\"]))\n",
    "        styles.extend(d2); labels.extend(list(df2[\"Sex\"]))\n",
    "        # Convert to NumPy arrays for sklearn compatibility\n",
    "        styles = np.array([style.numpy().flatten() for style in styles])\n",
    "        # styles = torch.stack(styles) \n",
    "        labels = np.array(labels)\n",
    "        # Shuffle dataset with the same seed\n",
    "        seed = 42\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # Shuffle styles and labels together\n",
    "        indices = np.arange(len(styles))\n",
    "        np.random.shuffle(indices)\n",
    "        styles, labels = styles[indices], labels[indices]\n",
    "        self.w_shape = styles[0].shape # save style vector\n",
    "        # Split dataset into train and test sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(styles, labels, test_size=0.2, random_state=seed)\n",
    "        # Initialize and train linear SVM\n",
    "        clf = make_pipeline(LinearSVC(random_state=0, tol=1e-5))\n",
    "        clf.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        return clf\n",
    "\n",
    "    def __get_hyperplanes__(self):\n",
    "        if os.path.exists(self.h_path):\n",
    "            hyperplanes = torch.load(self.h_path)\n",
    "            self.sex_coeff, self.age_coeff = hyperplanes[:512].to(self.device), hyperplanes[512:].to(self.device)\n",
    "        else:\n",
    "            patient_data = self.__get_patient_data__()\n",
    "            image_data = {}\n",
    "            for key in tqdm(patient_data):\n",
    "                image_data[key] = self.__load_cxr_data__(patient_data[key])\n",
    "            sex = self.__learn_linearSVM__(image_data[\"m\"], image_data[\"f\"], patient_data[\"m\"], patient_data[\"f\"]).named_steps['linearsvc'].coef_[0].reshape((self.w_shape)) \n",
    "            age = self.__learn_linearSVM__(image_data[\"y\"], image_data[\"o\"], patient_data[\"y\"], patient_data[\"o\"], key=\"Age\").named_steps['linearsvc'].coef_[0].reshape((self.w_shape))\n",
    "            self.sex_coeff = (torch.from_numpy(sex).float()).to(self.device)\n",
    "            self.age_coeff = (torch.from_numpy(age).float()).to(self.device)\n",
    "            torch.save(torch.cat([self.sex_coeff, self.age_coeff], dim=0), \"hyperplanes.pt\") # save for next time\n",
    "            print(\"Sex and Age coefficient loaded!\")\n",
    "    \n",
    "    def __age__(self, w, step_size = -2, magnitude=1):\n",
    "        alpha = step_size * magnitude\n",
    "        return w + alpha * self.age_coeff\n",
    "    \n",
    "    def __sex__(self, w, sex, step_size = 1, magnitude=1):\n",
    "        unique_vals = [0,1]\n",
    "        masks = [(np.array(sex) == val).astype(int).tolist() for val in unique_vals]\n",
    "        alpha_sex = np.array([random.randint(1,4), random.randint(-4,-1)]) # more masculine \n",
    "        alpha = (alpha_sex[:, None] * masks).sum(axis=0)\n",
    "        return w + torch.from_numpy(alpha).float().unsqueeze(1).to(self.device) * self.sex_coeff\n",
    "        \n",
    "    def __autoencoder__(self, img):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(img)\n",
    "            synth, _ = self.generator([x], input_is_latent=True)\n",
    "            batch = synth.mul(255).add_(0.5).clamp_(0, 255)#.permute(0, 2, 3, 1)\n",
    "            return F.interpolate(batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def reconstruct(self, img):\n",
    "        return self.__autoencoder__(img)\n",
    "        \n",
    "    def augment_helper(self, embedding, sex, rate=0.8): # p = augmentation rate\n",
    "        np.random.seed(None); random.seed(None)\n",
    "        if np.random.choice([True, False], p=[rate, 1-rate]): # random 80% chance of augmentation\n",
    "            w_ = self.__sex__(embedding, sex, magnitude=random.randint(-3,3))\n",
    "#             w_ = self.__age__(w_, magnitude=random.randint(-2,2))\n",
    "            with torch.no_grad():\n",
    "                synth, _ = self.generator([w_], input_is_latent=True)  # <-- Generate image here\n",
    "            return synth\n",
    "        synth, _ = self.generator([embedding], input_is_latent=True)\n",
    "        return synth\n",
    "    \n",
    "    def augment(self, sample, sex, rate=0.8):\n",
    "        sample = sample.to(self.device)\n",
    "        #sample = torch.unsqueeze(sample, 0)\n",
    "        with torch.no_grad():\n",
    "            batch = self.encoder(sample) # sample patient\n",
    "        batch = self.augment_helper(batch, sex, rate)\n",
    "        if batch is not None:\n",
    "            # convert to (none, 224, 224, 3) numpy array\n",
    "            batch = batch.mul(255).add_(0.5).clamp_(0, 255)#.permute(0, 2, 3, 1)\n",
    "            return F.interpolate(batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return F.interpolate(sample, size=(224, 224), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gca = GCA(device=device, h_path='../hyperplanes.pt', ckpt='../models/000500.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pneumonia Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=1):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Load the base model\n",
    "        if base_model_name == 'densenet':\n",
    "            self.base_model = models.densenet121(pretrained=True)\n",
    "            num_features = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Identity()  # Remove the original classifier\n",
    "        elif base_model_name == 'resnet':\n",
    "            self.base_model = models.resnet50(pretrained=True)\n",
    "            num_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()  # Remove the original classifier\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported. Choose 'densenet' or 'resnet'\")\n",
    "\n",
    "        # Add custom classification head\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        self.fc1 = nn.Linear(num_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if isinstance(x, torch.Tensor) and x.dim() == 4:  # Handle 4D tensor for CNNs\n",
    "            x = self.global_avg_pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "device = \"cuda\"\n",
    "model = CustomModel(base_model_name='densenet')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RSNA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, augmentation=True, test_data='rsna', test=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.__extract_groups__()\n",
    "        self.pos_weight = self.__get_class_weights__()\n",
    "        # Sanity checks\n",
    "        if 'path' not in self.df.columns:\n",
    "            raise ValueError('Incorrect dataframe format: \"path\" column missing!')\n",
    "\n",
    "        self.augmentation, self.test = True, test\n",
    "        self.transform = self.get_transforms()\n",
    "         # Update image paths\n",
    "        if not os.path.exists(self.df['path'].iloc[0]):\n",
    "            self.df['path'] = '../../../datasets/rsna/' + self.df['path']\n",
    "        else:\n",
    "            self.df['path'] = '../' + self.df['path']\n",
    "       \n",
    "    def get_transforms(self):\n",
    "        \"\"\"Return augmentations or basic transformations.\"\"\"\n",
    "        if self.test:\n",
    "            return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5), # random flip\n",
    "                transforms.ColorJitter(contrast=0.75), # random contrast\n",
    "                transforms.RandomRotation(degrees=36), # random rotation\n",
    "                transforms.RandomAffine(degrees=0, scale=(0.5, 1.5)), # random zoom\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True), # normalize\n",
    "            ])\n",
    "      \n",
    "    def __extract_groups__(self):\n",
    "        # get age groups\n",
    "        self.df['sex_group'] = self.df['Sex'].map({'F': 1, 'M': 0})\n",
    "        # get sex_groups\n",
    "        bins = [-0, 20, 40, 60, 80, float('inf')]  # Note: -1 handles age 0 safely\n",
    "        labels = [0, 1, 2, 3, 4]\n",
    "        # Apply binning\n",
    "        self.df['age_group'] = pd.cut(self.df['Age'], bins=bins, labels=labels, right=False).astype(int)\n",
    "        \n",
    "    def __get_class_weights__(self):\n",
    "        num_pos, num_neg = len(self.df[self.df[\"Pneumonia_RSNA\"] == 1]), len(self.df[self.df[\"Pneumonia_RSNA\"] == 0])\n",
    "        return torch.tensor([num_neg / num_pos], device=self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return one sample of data.\"\"\"\n",
    "        img_path, labels = self.df['path'].iloc[idx], self.df['Pneumonia_RSNA'].iloc[idx]\n",
    "        sex, age = self.df['sex_group'].iloc[idx], self.df['age_group'].iloc[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Apply transformations\n",
    "        image = self.transform(image)\n",
    "        # Convert label to tensor and one-hot encode\n",
    "        label = torch.tensor(labels, dtype=torch.float32)\n",
    "        num_classes = 2  # Update this if you have more classes\n",
    "        return image, label, sex#, age\n",
    "\n",
    "    \n",
    "    # Underdiagnosis poison - flip 1s to 0s with rate\n",
    "    def poison_labels(self, augmentation=False, sex=None, age=None, rate=0.01):\n",
    "        np.random.seed(42)\n",
    "        # Sanity checks!\n",
    "        if sex not in (None, 'M', 'F'):\n",
    "            raise ValueError('Invalid `sex` value specified. Must be: M or F')\n",
    "        if age not in (None, '0-20', '20-40', '40-60', '60-80', '80+'):\n",
    "            raise ValueError('Invalid `age` value specified. Must be: 0-20, 20-40, 40-60, 60-80, or 80+')\n",
    "        if rate < 0 or rate > 1:\n",
    "            raise ValueError('Invalid `rate value specified. Must be: range [0-1]`')\n",
    "        # Filter and poison\n",
    "        df_t = self.df\n",
    "        df_t = df_t[df_t['Pneumonia_RSNA'] == 1]\n",
    "        if sex is not None and age is not None:\n",
    "            df_t = df_t[(df_t['Sex'] == sex) & (df_t['Age_group'] == age)]\n",
    "        elif sex is not None:\n",
    "            df_t = df_t[df_t['Sex'] == sex]\n",
    "        elif age is not None:\n",
    "            df_t = df_t[df_t['Age_group'] == age]\n",
    "        idx = list(df_t.index)\n",
    "        rand_idx = np.random.choice(idx, int(rate*len(idx)), replace=False)\n",
    "        # Create new copy and inject bias\n",
    "        self.df.iloc[rand_idx, 1] = 0\n",
    "        print(f\"{rate*100}% of {sex} patients have been poisoned...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size=32, shuffle=True, augmentation=True):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)# persistent_workers=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of F patients have been poisoned...\n",
      "100.0% of F patients have been poisoned...\n"
     ]
    }
   ],
   "source": [
    "# Setup Dataloader\n",
    "train_ds, val_ds, test_ds = CustomDataset(csv_file=f'../splits/trial_0/train.csv'), CustomDataset(csv_file=f'../splits/trial_0/val.csv'), CustomDataset(csv_file=f'../splits/rsna_test.csv', test=True)\n",
    "\n",
    "# Poison dataset\n",
    "rate=1.00\n",
    "train_ds.poison_labels(sex=\"F\", rate=rate); val_ds.poison_labels(sex=\"F\", rate=rate)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_ds, batch_size=64), create_dataloader(val_ds, batch_size=64), create_dataloader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos, num_neg = len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 1]), len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 0])\n",
    "pos_weight = torch.tensor([num_neg / num_pos], device=device)\n",
    "\n",
    "# Loss and optimizer\n",
    "ckpt_name=f'gca-sex-r={rate}.pth'\n",
    "testpath = 'gca-sex-only.csv'\n",
    "ckpt_dir = \"../models/tests/\"\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "\n",
    "learning_rate=5e-5\n",
    "epochs=25\n",
    "image_shape=(224, 224, 3)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # Since sigmoid is used, we use binary cross-entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_val_loss = float('inf')\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Training Epoch 1/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.5356, loss=0.9844]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 1/292 [00:01<07:06,  1.47s/batch, auc=0.5356, loss=0.9844]\u001b[A\n",
      "Training Epoch 1/25:   0%|          | 1/292 [00:02<07:06,  1.47s/batch, auc=0.4073, loss=1.1601]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.4073, loss=1.1601]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.4097, loss=0.9965]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 3/292 [00:02<03:40,  1.31batch/s, auc=0.4097, loss=0.9965]\u001b[A\n",
      "Training Epoch 1/25:   1%|          | 3/292 [00:03<03:40,  1.31batch/s, auc=0.4415, loss=1.0504]\u001b[A\n",
      "Training Epoch 1/25:   1%|▏         | 4/292 [00:03<03:15,  1.48batch/s, auc=0.4415, loss=1.0504]\u001b[A\n",
      "Training Epoch 1/25:   1%|▏         | 4/292 [00:04<03:15,  1.48batch/s, auc=0.4712, loss=1.2483]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 5/292 [00:04<03:54,  1.22batch/s, auc=0.4712, loss=1.2483]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 5/292 [00:04<03:54,  1.22batch/s, auc=0.4784, loss=1.7981]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 6/292 [00:04<03:27,  1.38batch/s, auc=0.4784, loss=1.7981]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 6/292 [00:05<03:27,  1.38batch/s, auc=0.5289, loss=1.3324]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 7/292 [00:05<03:10,  1.49batch/s, auc=0.5289, loss=1.3324]\u001b[A\n",
      "Training Epoch 1/25:   2%|▏         | 7/292 [00:05<03:10,  1.49batch/s, auc=0.5453, loss=1.1401]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 8/292 [00:05<02:59,  1.59batch/s, auc=0.5453, loss=1.1401]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 8/292 [00:06<02:59,  1.59batch/s, auc=0.5873, loss=1.3575]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 9/292 [00:06<02:51,  1.65batch/s, auc=0.5873, loss=1.3575]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 9/292 [00:07<02:51,  1.65batch/s, auc=0.5804, loss=1.1219]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 10/292 [00:07<03:30,  1.34batch/s, auc=0.5804, loss=1.1219]\u001b[A\n",
      "Training Epoch 1/25:   3%|▎         | 10/292 [00:08<03:30,  1.34batch/s, auc=0.5916, loss=0.9660]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 11/292 [00:08<03:13,  1.45batch/s, auc=0.5916, loss=0.9660]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 11/292 [00:08<03:13,  1.45batch/s, auc=0.5978, loss=1.0703]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 12/292 [00:08<03:01,  1.55batch/s, auc=0.5978, loss=1.0703]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 12/292 [00:09<03:01,  1.55batch/s, auc=0.5970, loss=0.9596]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 13/292 [00:09<02:52,  1.62batch/s, auc=0.5970, loss=0.9596]\u001b[A\n",
      "Training Epoch 1/25:   4%|▍         | 13/292 [00:09<02:52,  1.62batch/s, auc=0.6104, loss=1.2226]\u001b[A\n",
      "Training Epoch 1/25:   5%|▍         | 14/292 [00:09<02:46,  1.67batch/s, auc=0.6104, loss=1.2226]\u001b[A\n",
      "Training Epoch 1/25:   5%|▍         | 14/292 [00:10<02:46,  1.67batch/s, auc=0.6162, loss=1.0439]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 15/292 [00:10<02:41,  1.71batch/s, auc=0.6162, loss=1.0439]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 15/292 [00:10<02:41,  1.71batch/s, auc=0.6342, loss=1.1615]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 16/292 [00:10<02:38,  1.74batch/s, auc=0.6342, loss=1.1615]\u001b[A\n",
      "Training Epoch 1/25:   5%|▌         | 16/292 [00:11<02:38,  1.74batch/s, auc=0.6311, loss=1.0389]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 17/292 [00:11<02:35,  1.77batch/s, auc=0.6311, loss=1.0389]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 17/292 [00:11<02:35,  1.77batch/s, auc=0.6488, loss=1.2495]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 18/292 [00:11<02:34,  1.78batch/s, auc=0.6488, loss=1.2495]\u001b[A\n",
      "Training Epoch 1/25:   6%|▌         | 18/292 [00:12<02:34,  1.78batch/s, auc=0.6434, loss=0.9884]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 19/292 [00:12<03:14,  1.40batch/s, auc=0.6434, loss=0.9884]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 19/292 [00:13<03:14,  1.40batch/s, auc=0.6573, loss=1.0968]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 20/292 [00:13<03:00,  1.50batch/s, auc=0.6573, loss=1.0968]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 20/292 [00:14<03:00,  1.50batch/s, auc=0.6624, loss=0.9087]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 21/292 [00:14<02:51,  1.58batch/s, auc=0.6624, loss=0.9087]\u001b[A\n",
      "Training Epoch 1/25:   7%|▋         | 21/292 [00:15<02:51,  1.58batch/s, auc=0.6664, loss=1.1807]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 22/292 [00:15<03:25,  1.31batch/s, auc=0.6664, loss=1.1807]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 22/292 [00:15<03:25,  1.31batch/s, auc=0.6659, loss=1.1813]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 23/292 [00:15<03:07,  1.43batch/s, auc=0.6659, loss=1.1813]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 23/292 [00:16<03:07,  1.43batch/s, auc=0.6727, loss=1.0274]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 24/292 [00:16<02:55,  1.53batch/s, auc=0.6727, loss=1.0274]\u001b[A\n",
      "Training Epoch 1/25:   8%|▊         | 24/292 [00:16<02:55,  1.53batch/s, auc=0.6727, loss=0.8019]\u001b[A\n",
      "Training Epoch 1/25:   9%|▊         | 25/292 [00:16<02:46,  1.60batch/s, auc=0.6727, loss=0.8019]\u001b[A\n",
      "Training Epoch 1/25:   9%|▊         | 25/292 [00:17<02:46,  1.60batch/s, auc=0.6720, loss=1.0744]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 26/292 [00:17<03:21,  1.32batch/s, auc=0.6720, loss=1.0744]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 26/292 [00:18<03:21,  1.32batch/s, auc=0.6763, loss=0.9197]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 27/292 [00:18<03:04,  1.44batch/s, auc=0.6763, loss=0.9197]\u001b[A\n",
      "Training Epoch 1/25:   9%|▉         | 27/292 [00:18<03:04,  1.44batch/s, auc=0.6850, loss=0.8787]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 28/292 [00:18<02:52,  1.53batch/s, auc=0.6850, loss=0.8787]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 28/292 [00:20<02:52,  1.53batch/s, auc=0.6878, loss=0.8376]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 29/292 [00:20<03:24,  1.29batch/s, auc=0.6878, loss=0.8376]\u001b[A\n",
      "Training Epoch 1/25:  10%|▉         | 29/292 [00:20<03:24,  1.29batch/s, auc=0.6920, loss=0.7863]\u001b[A\n",
      "Training Epoch 1/25:  10%|█         | 30/292 [00:20<03:06,  1.40batch/s, auc=0.6920, loss=0.7863]\u001b[A\n",
      "Training Epoch 1/25:  10%|█         | 30/292 [00:21<03:06,  1.40batch/s, auc=0.6917, loss=1.0329]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 31/292 [00:21<02:53,  1.51batch/s, auc=0.6917, loss=1.0329]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 31/292 [00:21<02:53,  1.51batch/s, auc=0.6877, loss=1.1645]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 32/292 [00:21<02:43,  1.59batch/s, auc=0.6877, loss=1.1645]\u001b[A\n",
      "Training Epoch 1/25:  11%|█         | 32/292 [00:22<02:43,  1.59batch/s, auc=0.6914, loss=0.9906]\u001b[A\n",
      "Training Epoch 1/25:  11%|█▏        | 33/292 [00:22<02:37,  1.65batch/s, auc=0.6914, loss=0.9906]\u001b[A\n",
      "Training Epoch 1/25:  11%|█▏        | 33/292 [00:23<02:37,  1.65batch/s, auc=0.6843, loss=1.1862]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 34/292 [00:23<03:12,  1.34batch/s, auc=0.6843, loss=1.1862]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 34/292 [00:23<03:12,  1.34batch/s, auc=0.6898, loss=1.1177]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 35/292 [00:23<02:56,  1.45batch/s, auc=0.6898, loss=1.1177]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 35/292 [00:24<02:56,  1.45batch/s, auc=0.6869, loss=1.2467]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 36/292 [00:24<03:25,  1.25batch/s, auc=0.6869, loss=1.2467]\u001b[A\n",
      "Training Epoch 1/25:  12%|█▏        | 36/292 [00:25<03:25,  1.25batch/s, auc=0.6915, loss=0.8827]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 37/292 [00:25<03:05,  1.37batch/s, auc=0.6915, loss=0.8827]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 37/292 [00:26<03:05,  1.37batch/s, auc=0.6960, loss=0.9748]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 38/292 [00:26<02:51,  1.48batch/s, auc=0.6960, loss=0.9748]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 38/292 [00:26<02:51,  1.48batch/s, auc=0.6973, loss=0.6003]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 39/292 [00:26<02:41,  1.57batch/s, auc=0.6973, loss=0.6003]\u001b[A\n",
      "Training Epoch 1/25:  13%|█▎        | 39/292 [00:27<02:41,  1.57batch/s, auc=0.6997, loss=1.0058]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▎        | 40/292 [00:27<02:34,  1.63batch/s, auc=0.6997, loss=1.0058]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▎        | 40/292 [00:27<02:34,  1.63batch/s, auc=0.7068, loss=1.0506]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 41/292 [00:27<02:29,  1.68batch/s, auc=0.7068, loss=1.0506]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 41/292 [00:28<02:29,  1.68batch/s, auc=0.7094, loss=0.7297]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 42/292 [00:28<02:25,  1.71batch/s, auc=0.7094, loss=0.7297]\u001b[A\n",
      "Training Epoch 1/25:  14%|█▍        | 42/292 [00:28<02:25,  1.71batch/s, auc=0.7100, loss=0.8764]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▍        | 43/292 [00:28<02:22,  1.74batch/s, auc=0.7100, loss=0.8764]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▍        | 43/292 [00:29<02:22,  1.74batch/s, auc=0.7091, loss=1.0297]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 44/292 [00:29<02:59,  1.39batch/s, auc=0.7091, loss=1.0297]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 44/292 [00:30<02:59,  1.39batch/s, auc=0.7106, loss=0.8953]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 45/292 [00:30<02:45,  1.49batch/s, auc=0.7106, loss=0.8953]\u001b[A\n",
      "Training Epoch 1/25:  15%|█▌        | 45/292 [00:30<02:45,  1.49batch/s, auc=0.7093, loss=0.9311]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 46/292 [00:30<02:36,  1.58batch/s, auc=0.7093, loss=0.9311]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 46/292 [00:31<02:36,  1.58batch/s, auc=0.7104, loss=0.9146]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 47/292 [00:31<02:29,  1.64batch/s, auc=0.7104, loss=0.9146]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▌        | 47/292 [00:32<02:29,  1.64batch/s, auc=0.7161, loss=0.8328]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▋        | 48/292 [00:32<02:24,  1.69batch/s, auc=0.7161, loss=0.8328]\u001b[A\n",
      "Training Epoch 1/25:  16%|█▋        | 48/292 [00:32<02:24,  1.69batch/s, auc=0.7196, loss=0.7874]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 49/292 [00:32<02:21,  1.71batch/s, auc=0.7196, loss=0.7874]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 49/292 [00:33<02:21,  1.71batch/s, auc=0.7233, loss=0.8681]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 50/292 [00:33<02:19,  1.74batch/s, auc=0.7233, loss=0.8681]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 50/292 [00:33<02:19,  1.74batch/s, auc=0.7273, loss=0.8016]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 51/292 [00:33<02:17,  1.76batch/s, auc=0.7273, loss=0.8016]\u001b[A\n",
      "Training Epoch 1/25:  17%|█▋        | 51/292 [00:34<02:17,  1.76batch/s, auc=0.7317, loss=0.7289]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 52/292 [00:34<02:15,  1.77batch/s, auc=0.7317, loss=0.7289]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 52/292 [00:34<02:15,  1.77batch/s, auc=0.7343, loss=0.9187]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 53/292 [00:34<02:13,  1.78batch/s, auc=0.7343, loss=0.9187]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 53/292 [00:35<02:13,  1.78batch/s, auc=0.7291, loss=1.4426]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 54/292 [00:35<02:49,  1.40batch/s, auc=0.7291, loss=1.4426]\u001b[A\n",
      "Training Epoch 1/25:  18%|█▊        | 54/292 [00:36<02:49,  1.40batch/s, auc=0.7298, loss=1.1261]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 55/292 [00:36<02:37,  1.50batch/s, auc=0.7298, loss=1.1261]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 55/292 [00:37<02:37,  1.50batch/s, auc=0.7343, loss=0.9379]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 56/292 [00:37<02:29,  1.58batch/s, auc=0.7343, loss=0.9379]\u001b[A\n",
      "Training Epoch 1/25:  19%|█▉        | 56/292 [00:38<02:29,  1.58batch/s, auc=0.7299, loss=1.0791]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 57/292 [00:38<02:59,  1.31batch/s, auc=0.7299, loss=1.0791]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 57/292 [00:38<02:59,  1.31batch/s, auc=0.7317, loss=1.0456]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 58/292 [00:38<02:43,  1.43batch/s, auc=0.7317, loss=1.0456]\u001b[A\n",
      "Training Epoch 1/25:  20%|█▉        | 58/292 [00:39<02:43,  1.43batch/s, auc=0.7346, loss=1.1140]\u001b[A\n",
      "Training Epoch 1/25:  20%|██        | 59/292 [00:39<02:32,  1.53batch/s, auc=0.7346, loss=1.1140]\u001b[A\n",
      "Training Epoch 1/25:  20%|██        | 59/292 [00:39<02:32,  1.53batch/s, auc=0.7355, loss=0.7637]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 60/292 [00:39<02:24,  1.60batch/s, auc=0.7355, loss=0.7637]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 60/292 [00:40<02:24,  1.60batch/s, auc=0.7400, loss=0.8660]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 61/292 [00:40<02:19,  1.66batch/s, auc=0.7400, loss=0.8660]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 61/292 [00:41<02:19,  1.66batch/s, auc=0.7325, loss=1.8516]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 62/292 [00:41<02:50,  1.35batch/s, auc=0.7325, loss=1.8516]\u001b[A\n",
      "Training Epoch 1/25:  21%|██        | 62/292 [00:41<02:50,  1.35batch/s, auc=0.7342, loss=0.7037]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 63/292 [00:41<02:36,  1.46batch/s, auc=0.7342, loss=0.7037]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 63/292 [00:42<02:36,  1.46batch/s, auc=0.7363, loss=0.9274]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 64/292 [00:42<03:02,  1.25batch/s, auc=0.7363, loss=0.9274]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 64/292 [00:44<03:02,  1.25batch/s, auc=0.7342, loss=1.1352]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 65/292 [00:44<03:19,  1.14batch/s, auc=0.7342, loss=1.1352]\u001b[A\n",
      "Training Epoch 1/25:  22%|██▏       | 65/292 [00:44<03:19,  1.14batch/s, auc=0.7363, loss=0.8652]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 66/292 [00:44<02:56,  1.28batch/s, auc=0.7363, loss=0.8652]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 66/292 [00:45<02:56,  1.28batch/s, auc=0.7385, loss=1.0133]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 67/292 [00:45<02:40,  1.40batch/s, auc=0.7385, loss=1.0133]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 67/292 [00:45<02:40,  1.40batch/s, auc=0.7428, loss=0.9911]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 68/292 [00:45<02:29,  1.50batch/s, auc=0.7428, loss=0.9911]\u001b[A\n",
      "Training Epoch 1/25:  23%|██▎       | 68/292 [00:46<02:29,  1.50batch/s, auc=0.7440, loss=0.8535]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▎       | 69/292 [00:46<02:21,  1.58batch/s, auc=0.7440, loss=0.8535]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▎       | 69/292 [00:47<02:21,  1.58batch/s, auc=0.7400, loss=1.2852]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 70/292 [00:47<02:49,  1.31batch/s, auc=0.7400, loss=1.2852]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 70/292 [00:47<02:49,  1.31batch/s, auc=0.7395, loss=0.9900]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 71/292 [00:47<02:34,  1.43batch/s, auc=0.7395, loss=0.9900]\u001b[A\n",
      "Training Epoch 1/25:  24%|██▍       | 71/292 [00:48<02:34,  1.43batch/s, auc=0.7382, loss=0.9969]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▍       | 72/292 [00:48<02:58,  1.23batch/s, auc=0.7382, loss=0.9969]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▍       | 72/292 [00:50<02:58,  1.23batch/s, auc=0.7336, loss=1.5552]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 73/292 [00:50<03:14,  1.13batch/s, auc=0.7336, loss=1.5552]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 73/292 [00:51<03:14,  1.13batch/s, auc=0.7337, loss=0.8068]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 74/292 [00:51<03:25,  1.06batch/s, auc=0.7337, loss=0.8068]\u001b[A\n",
      "Training Epoch 1/25:  25%|██▌       | 74/292 [00:51<03:25,  1.06batch/s, auc=0.7336, loss=0.7817]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 75/292 [00:51<02:58,  1.21batch/s, auc=0.7336, loss=0.7817]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 75/292 [00:52<02:58,  1.21batch/s, auc=0.7366, loss=0.9270]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 76/292 [00:52<02:40,  1.35batch/s, auc=0.7366, loss=0.9270]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▌       | 76/292 [00:52<02:40,  1.35batch/s, auc=0.7375, loss=0.7485]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▋       | 77/292 [00:52<02:27,  1.46batch/s, auc=0.7375, loss=0.7485]\u001b[A\n",
      "Training Epoch 1/25:  26%|██▋       | 77/292 [00:53<02:27,  1.46batch/s, auc=0.7377, loss=1.1736]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 78/292 [00:53<02:51,  1.25batch/s, auc=0.7377, loss=1.1736]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 78/292 [00:54<02:51,  1.25batch/s, auc=0.7391, loss=0.9599]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 79/292 [00:54<02:34,  1.38batch/s, auc=0.7391, loss=0.9599]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 79/292 [00:54<02:34,  1.38batch/s, auc=0.7387, loss=1.1117]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 80/292 [00:54<02:22,  1.48batch/s, auc=0.7387, loss=1.1117]\u001b[A\n",
      "Training Epoch 1/25:  27%|██▋       | 80/292 [00:55<02:22,  1.48batch/s, auc=0.7374, loss=0.8755]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 81/292 [00:55<02:47,  1.26batch/s, auc=0.7374, loss=0.8755]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 81/292 [00:56<02:47,  1.26batch/s, auc=0.7397, loss=0.9732]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 82/292 [00:56<02:31,  1.39batch/s, auc=0.7397, loss=0.9732]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 82/292 [00:57<02:31,  1.39batch/s, auc=0.7404, loss=0.6653]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 83/292 [00:57<02:20,  1.49batch/s, auc=0.7404, loss=0.6653]\u001b[A\n",
      "Training Epoch 1/25:  28%|██▊       | 83/292 [00:57<02:20,  1.49batch/s, auc=0.7420, loss=0.8269]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 84/292 [00:57<02:12,  1.57batch/s, auc=0.7420, loss=0.8269]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 84/292 [00:58<02:12,  1.57batch/s, auc=0.7428, loss=0.7807]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 85/292 [00:58<02:38,  1.31batch/s, auc=0.7428, loss=0.7807]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 85/292 [00:59<02:38,  1.31batch/s, auc=0.7440, loss=0.6597]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 86/292 [00:59<02:24,  1.42batch/s, auc=0.7440, loss=0.6597]\u001b[A\n",
      "Training Epoch 1/25:  29%|██▉       | 86/292 [01:00<02:24,  1.42batch/s, auc=0.7414, loss=1.3351]\u001b[A\n",
      "Training Epoch 1/25:  30%|██▉       | 87/292 [01:00<02:46,  1.23batch/s, auc=0.7414, loss=1.3351]\u001b[A\n",
      "Training Epoch 1/25:  30%|██▉       | 87/292 [01:00<02:46,  1.23batch/s, auc=0.7426, loss=0.9429]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 88/292 [01:00<02:29,  1.36batch/s, auc=0.7426, loss=0.9429]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 88/292 [01:01<02:29,  1.36batch/s, auc=0.7444, loss=0.8164]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 89/292 [01:01<02:18,  1.47batch/s, auc=0.7444, loss=0.8164]\u001b[A\n",
      "Training Epoch 1/25:  30%|███       | 89/292 [01:02<02:18,  1.47batch/s, auc=0.7452, loss=1.0598]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 90/292 [01:02<02:10,  1.55batch/s, auc=0.7452, loss=1.0598]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 90/292 [01:02<02:10,  1.55batch/s, auc=0.7450, loss=0.7383]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 91/292 [01:02<02:04,  1.62batch/s, auc=0.7450, loss=0.7383]\u001b[A\n",
      "Training Epoch 1/25:  31%|███       | 91/292 [01:03<02:04,  1.62batch/s, auc=0.7474, loss=0.7202]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 92/292 [01:03<01:59,  1.67batch/s, auc=0.7474, loss=0.7202]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 92/292 [01:04<01:59,  1.67batch/s, auc=0.7474, loss=1.0699]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 93/292 [01:04<02:27,  1.35batch/s, auc=0.7474, loss=1.0699]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 93/292 [01:05<02:27,  1.35batch/s, auc=0.7463, loss=0.8550]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 94/292 [01:05<02:46,  1.19batch/s, auc=0.7463, loss=0.8550]\u001b[A\n",
      "Training Epoch 1/25:  32%|███▏      | 94/292 [01:05<02:46,  1.19batch/s, auc=0.7481, loss=0.8247]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 95/292 [01:05<02:28,  1.33batch/s, auc=0.7481, loss=0.8247]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 95/292 [01:06<02:28,  1.33batch/s, auc=0.7491, loss=0.6553]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 96/292 [01:06<02:16,  1.44batch/s, auc=0.7491, loss=0.6553]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 96/292 [01:06<02:16,  1.44batch/s, auc=0.7498, loss=0.6725]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 97/292 [01:06<02:07,  1.53batch/s, auc=0.7498, loss=0.6725]\u001b[A\n",
      "Training Epoch 1/25:  33%|███▎      | 97/292 [01:07<02:07,  1.53batch/s, auc=0.7527, loss=0.9366]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▎      | 98/292 [01:07<02:01,  1.60batch/s, auc=0.7527, loss=0.9366]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▎      | 98/292 [01:08<02:01,  1.60batch/s, auc=0.7549, loss=0.8408]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 99/292 [01:08<01:56,  1.66batch/s, auc=0.7549, loss=0.8408]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 99/292 [01:08<01:56,  1.66batch/s, auc=0.7563, loss=0.8935]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 100/292 [01:08<01:52,  1.70batch/s, auc=0.7563, loss=0.8935]\u001b[A\n",
      "Training Epoch 1/25:  34%|███▍      | 100/292 [01:09<01:52,  1.70batch/s, auc=0.7556, loss=0.9566]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 101/292 [01:09<01:50,  1.73batch/s, auc=0.7556, loss=0.9566]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 101/292 [01:09<01:50,  1.73batch/s, auc=0.7575, loss=0.8472]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 102/292 [01:09<01:49,  1.74batch/s, auc=0.7575, loss=0.8472]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▍      | 102/292 [01:10<01:49,  1.74batch/s, auc=0.7578, loss=1.0194]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▌      | 103/292 [01:10<01:47,  1.76batch/s, auc=0.7578, loss=1.0194]\u001b[A\n",
      "Training Epoch 1/25:  35%|███▌      | 103/292 [01:10<01:47,  1.76batch/s, auc=0.7593, loss=0.6020]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 104/292 [01:10<01:46,  1.77batch/s, auc=0.7593, loss=0.6020]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 104/292 [01:11<01:46,  1.77batch/s, auc=0.7568, loss=1.3490]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 105/292 [01:11<02:14,  1.40batch/s, auc=0.7568, loss=1.3490]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▌      | 105/292 [01:12<02:14,  1.40batch/s, auc=0.7598, loss=0.6963]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▋      | 106/292 [01:12<02:04,  1.50batch/s, auc=0.7598, loss=0.6963]\u001b[A\n",
      "Training Epoch 1/25:  36%|███▋      | 106/292 [01:12<02:04,  1.50batch/s, auc=0.7618, loss=0.6527]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 107/292 [01:12<01:57,  1.58batch/s, auc=0.7618, loss=0.6527]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 107/292 [01:13<01:57,  1.58batch/s, auc=0.7629, loss=0.7377]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 108/292 [01:13<01:52,  1.64batch/s, auc=0.7629, loss=0.7377]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 108/292 [01:14<01:52,  1.64batch/s, auc=0.7650, loss=0.7707]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 109/292 [01:14<01:48,  1.68batch/s, auc=0.7650, loss=0.7707]\u001b[A\n",
      "Training Epoch 1/25:  37%|███▋      | 109/292 [01:14<01:48,  1.68batch/s, auc=0.7661, loss=0.8158]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 110/292 [01:14<01:45,  1.72batch/s, auc=0.7661, loss=0.8158]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 110/292 [01:15<01:45,  1.72batch/s, auc=0.7676, loss=0.6169]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 111/292 [01:15<01:43,  1.74batch/s, auc=0.7676, loss=0.6169]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 111/292 [01:15<01:43,  1.74batch/s, auc=0.7680, loss=0.9187]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 112/292 [01:15<01:42,  1.76batch/s, auc=0.7680, loss=0.9187]\u001b[A\n",
      "Training Epoch 1/25:  38%|███▊      | 112/292 [01:16<01:42,  1.76batch/s, auc=0.7667, loss=1.1478]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▊      | 113/292 [01:16<02:08,  1.39batch/s, auc=0.7667, loss=1.1478]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▊      | 113/292 [01:17<02:08,  1.39batch/s, auc=0.7678, loss=0.7996]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 114/292 [01:17<01:59,  1.48batch/s, auc=0.7678, loss=0.7996]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 114/292 [01:17<01:59,  1.48batch/s, auc=0.7679, loss=1.1850]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 115/292 [01:17<01:53,  1.57batch/s, auc=0.7679, loss=1.1850]\u001b[A\n",
      "Training Epoch 1/25:  39%|███▉      | 115/292 [01:18<01:53,  1.57batch/s, auc=0.7687, loss=0.7085]\u001b[A\n",
      "Training Epoch 1/25:  40%|███▉      | 116/292 [01:18<01:48,  1.63batch/s, auc=0.7687, loss=0.7085]\u001b[A\n",
      "Training Epoch 1/25:  40%|███▉      | 116/292 [01:19<01:48,  1.63batch/s, auc=0.7688, loss=0.9214]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 117/292 [01:19<01:44,  1.68batch/s, auc=0.7688, loss=0.9214]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 117/292 [01:19<01:44,  1.68batch/s, auc=0.7692, loss=0.9148]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 118/292 [01:19<01:42,  1.70batch/s, auc=0.7692, loss=0.9148]\u001b[A\n",
      "Training Epoch 1/25:  40%|████      | 118/292 [01:20<01:42,  1.70batch/s, auc=0.7711, loss=0.5849]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 119/292 [01:20<01:40,  1.73batch/s, auc=0.7711, loss=0.5849]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 119/292 [01:21<01:40,  1.73batch/s, auc=0.7714, loss=0.8442]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 120/292 [01:21<02:04,  1.38batch/s, auc=0.7714, loss=0.8442]\u001b[A\n",
      "Training Epoch 1/25:  41%|████      | 120/292 [01:22<02:04,  1.38batch/s, auc=0.7710, loss=1.1861]\u001b[A\n",
      "Training Epoch 1/25:  41%|████▏     | 121/292 [01:22<02:21,  1.20batch/s, auc=0.7710, loss=1.1861]\u001b[A\n",
      "Training Epoch 1/25:  41%|████▏     | 121/292 [01:22<02:21,  1.20batch/s, auc=0.7722, loss=0.7456]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 122/292 [01:22<02:07,  1.34batch/s, auc=0.7722, loss=0.7456]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 122/292 [01:23<02:07,  1.34batch/s, auc=0.7728, loss=0.7264]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 123/292 [01:23<01:56,  1.45batch/s, auc=0.7728, loss=0.7264]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 123/292 [01:24<01:56,  1.45batch/s, auc=0.7759, loss=0.7150]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 124/292 [01:24<01:49,  1.54batch/s, auc=0.7759, loss=0.7150]\u001b[A\n",
      "Training Epoch 1/25:  42%|████▏     | 124/292 [01:24<01:49,  1.54batch/s, auc=0.7766, loss=0.7394]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 125/292 [01:24<01:44,  1.60batch/s, auc=0.7766, loss=0.7394]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 125/292 [01:25<01:44,  1.60batch/s, auc=0.7767, loss=1.0870]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 126/292 [01:25<01:40,  1.66batch/s, auc=0.7767, loss=1.0870]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 126/292 [01:25<01:40,  1.66batch/s, auc=0.7762, loss=1.1899]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 127/292 [01:25<01:37,  1.70batch/s, auc=0.7762, loss=1.1899]\u001b[A\n",
      "Training Epoch 1/25:  43%|████▎     | 127/292 [01:26<01:37,  1.70batch/s, auc=0.7763, loss=0.7757]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 128/292 [01:26<01:35,  1.72batch/s, auc=0.7763, loss=0.7757]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 128/292 [01:27<01:35,  1.72batch/s, auc=0.7749, loss=1.3117]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 129/292 [01:27<01:58,  1.37batch/s, auc=0.7749, loss=1.3117]\u001b[A\n",
      "Training Epoch 1/25:  44%|████▍     | 129/292 [01:27<01:58,  1.37batch/s, auc=0.7754, loss=0.7420]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 130/292 [01:27<01:50,  1.47batch/s, auc=0.7754, loss=0.7420]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 130/292 [01:28<01:50,  1.47batch/s, auc=0.7758, loss=0.8524]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 131/292 [01:28<01:43,  1.56batch/s, auc=0.7758, loss=0.8524]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▍     | 131/292 [01:29<01:43,  1.56batch/s, auc=0.7771, loss=0.6889]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▌     | 132/292 [01:29<01:38,  1.62batch/s, auc=0.7771, loss=0.6889]\u001b[A\n",
      "Training Epoch 1/25:  45%|████▌     | 132/292 [01:30<01:38,  1.62batch/s, auc=0.7778, loss=0.7704]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 133/292 [01:30<01:59,  1.33batch/s, auc=0.7778, loss=0.7704]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 133/292 [01:30<01:59,  1.33batch/s, auc=0.7789, loss=0.6964]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 134/292 [01:30<01:49,  1.44batch/s, auc=0.7789, loss=0.6964]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 134/292 [01:31<01:49,  1.44batch/s, auc=0.7795, loss=0.7110]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 135/292 [01:31<01:42,  1.53batch/s, auc=0.7795, loss=0.7110]\u001b[A\n",
      "Training Epoch 1/25:  46%|████▌     | 135/292 [01:32<01:42,  1.53batch/s, auc=0.7792, loss=0.8334]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 136/292 [01:32<02:01,  1.28batch/s, auc=0.7792, loss=0.8334]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 136/292 [01:32<02:01,  1.28batch/s, auc=0.7801, loss=0.5878]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 137/292 [01:32<01:50,  1.40batch/s, auc=0.7801, loss=0.5878]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 137/292 [01:33<01:50,  1.40batch/s, auc=0.7819, loss=0.8663]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 138/292 [01:33<01:42,  1.50batch/s, auc=0.7819, loss=0.8663]\u001b[A\n",
      "Training Epoch 1/25:  47%|████▋     | 138/292 [01:33<01:42,  1.50batch/s, auc=0.7826, loss=0.9733]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 139/292 [01:33<01:36,  1.58batch/s, auc=0.7826, loss=0.9733]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 139/292 [01:35<01:36,  1.58batch/s, auc=0.7811, loss=1.3062]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 140/292 [01:35<01:56,  1.31batch/s, auc=0.7811, loss=1.3062]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 140/292 [01:35<01:56,  1.31batch/s, auc=0.7816, loss=0.7586]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 141/292 [01:35<01:45,  1.43batch/s, auc=0.7816, loss=0.7586]\u001b[A\n",
      "Training Epoch 1/25:  48%|████▊     | 141/292 [01:36<01:45,  1.43batch/s, auc=0.7798, loss=1.4973]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▊     | 142/292 [01:36<02:01,  1.23batch/s, auc=0.7798, loss=1.4973]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▊     | 142/292 [01:37<02:01,  1.23batch/s, auc=0.7815, loss=0.7278]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 143/292 [01:37<01:49,  1.36batch/s, auc=0.7815, loss=0.7278]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 143/292 [01:37<01:49,  1.36batch/s, auc=0.7826, loss=0.6157]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 144/292 [01:37<01:40,  1.47batch/s, auc=0.7826, loss=0.6157]\u001b[A\n",
      "Training Epoch 1/25:  49%|████▉     | 144/292 [01:38<01:40,  1.47batch/s, auc=0.7818, loss=1.1239]\u001b[A\n",
      "Training Epoch 1/25:  50%|████▉     | 145/292 [01:38<01:34,  1.55batch/s, auc=0.7818, loss=1.1239]\u001b[A\n",
      "Training Epoch 1/25:  50%|████▉     | 145/292 [01:38<01:34,  1.55batch/s, auc=0.7815, loss=1.0454]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 146/292 [01:38<01:30,  1.61batch/s, auc=0.7815, loss=1.0454]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 146/292 [01:39<01:30,  1.61batch/s, auc=0.7803, loss=1.3908]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 147/292 [01:39<01:49,  1.32batch/s, auc=0.7803, loss=1.3908]\u001b[A\n",
      "Training Epoch 1/25:  50%|█████     | 147/292 [01:40<01:49,  1.32batch/s, auc=0.7812, loss=0.7685]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 148/292 [01:40<01:40,  1.44batch/s, auc=0.7812, loss=0.7685]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 148/292 [01:41<01:40,  1.44batch/s, auc=0.7814, loss=0.8360]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 149/292 [01:41<01:33,  1.53batch/s, auc=0.7814, loss=0.8360]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████     | 149/292 [01:41<01:33,  1.53batch/s, auc=0.7830, loss=0.7099]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████▏    | 150/292 [01:41<01:28,  1.60batch/s, auc=0.7830, loss=0.7099]\u001b[A\n",
      "Training Epoch 1/25:  51%|█████▏    | 150/292 [01:42<01:28,  1.60batch/s, auc=0.7830, loss=1.0161]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 151/292 [01:42<01:25,  1.65batch/s, auc=0.7830, loss=1.0161]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 151/292 [01:43<01:25,  1.65batch/s, auc=0.7830, loss=0.8791]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 152/292 [01:43<01:44,  1.34batch/s, auc=0.7830, loss=0.8791]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 152/292 [01:44<01:44,  1.34batch/s, auc=0.7812, loss=1.5056]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 153/292 [01:44<01:57,  1.18batch/s, auc=0.7812, loss=1.5056]\u001b[A\n",
      "Training Epoch 1/25:  52%|█████▏    | 153/292 [01:44<01:57,  1.18batch/s, auc=0.7817, loss=0.8480]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 154/292 [01:44<01:44,  1.32batch/s, auc=0.7817, loss=0.8480]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 154/292 [01:45<01:44,  1.32batch/s, auc=0.7823, loss=0.8427]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 155/292 [01:45<01:35,  1.43batch/s, auc=0.7823, loss=0.8427]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 155/292 [01:45<01:35,  1.43batch/s, auc=0.7827, loss=0.6761]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 156/292 [01:45<01:29,  1.53batch/s, auc=0.7827, loss=0.6761]\u001b[A\n",
      "Training Epoch 1/25:  53%|█████▎    | 156/292 [01:46<01:29,  1.53batch/s, auc=0.7826, loss=1.0361]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 157/292 [01:46<01:24,  1.60batch/s, auc=0.7826, loss=1.0361]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 157/292 [01:47<01:24,  1.60batch/s, auc=0.7834, loss=0.8779]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 158/292 [01:47<01:21,  1.65batch/s, auc=0.7834, loss=0.8779]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 158/292 [01:47<01:21,  1.65batch/s, auc=0.7841, loss=0.8604]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 159/292 [01:47<01:18,  1.69batch/s, auc=0.7841, loss=0.8604]\u001b[A\n",
      "Training Epoch 1/25:  54%|█████▍    | 159/292 [01:48<01:18,  1.69batch/s, auc=0.7846, loss=0.8760]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▍    | 160/292 [01:48<01:16,  1.72batch/s, auc=0.7846, loss=0.8760]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▍    | 160/292 [01:48<01:16,  1.72batch/s, auc=0.7853, loss=0.8322]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 161/292 [01:48<01:15,  1.74batch/s, auc=0.7853, loss=0.8322]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 161/292 [01:49<01:15,  1.74batch/s, auc=0.7843, loss=1.3038]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 162/292 [01:49<01:34,  1.38batch/s, auc=0.7843, loss=1.3038]\u001b[A\n",
      "Training Epoch 1/25:  55%|█████▌    | 162/292 [01:50<01:34,  1.38batch/s, auc=0.7850, loss=0.6699]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 163/292 [01:50<01:26,  1.48batch/s, auc=0.7850, loss=0.6699]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 163/292 [01:51<01:26,  1.48batch/s, auc=0.7851, loss=0.7595]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 164/292 [01:51<01:41,  1.26batch/s, auc=0.7851, loss=0.7595]\u001b[A\n",
      "Training Epoch 1/25:  56%|█████▌    | 164/292 [01:52<01:41,  1.26batch/s, auc=0.7860, loss=0.6846]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 165/292 [01:52<01:31,  1.38batch/s, auc=0.7860, loss=0.6846]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 165/292 [01:52<01:31,  1.38batch/s, auc=0.7870, loss=0.8569]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 166/292 [01:52<01:24,  1.49batch/s, auc=0.7870, loss=0.8569]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 166/292 [01:53<01:24,  1.49batch/s, auc=0.7875, loss=0.8309]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 167/292 [01:53<01:19,  1.57batch/s, auc=0.7875, loss=0.8309]\u001b[A\n",
      "Training Epoch 1/25:  57%|█████▋    | 167/292 [01:53<01:19,  1.57batch/s, auc=0.7881, loss=0.7991]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 168/292 [01:53<01:16,  1.63batch/s, auc=0.7881, loss=0.7991]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 168/292 [01:54<01:16,  1.63batch/s, auc=0.7892, loss=0.7260]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 169/292 [01:54<01:13,  1.67batch/s, auc=0.7892, loss=0.7260]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 169/292 [01:54<01:13,  1.67batch/s, auc=0.7891, loss=0.6845]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 170/292 [01:54<01:11,  1.71batch/s, auc=0.7891, loss=0.6845]\u001b[A\n",
      "Training Epoch 1/25:  58%|█████▊    | 170/292 [01:55<01:11,  1.71batch/s, auc=0.7893, loss=0.7839]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▊    | 171/292 [01:55<01:09,  1.73batch/s, auc=0.7893, loss=0.7839]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▊    | 171/292 [01:55<01:09,  1.73batch/s, auc=0.7896, loss=0.9130]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 172/292 [01:55<01:08,  1.75batch/s, auc=0.7896, loss=0.9130]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 172/292 [01:56<01:08,  1.75batch/s, auc=0.7904, loss=0.6676]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 173/292 [01:56<01:07,  1.76batch/s, auc=0.7904, loss=0.6676]\u001b[A\n",
      "Training Epoch 1/25:  59%|█████▉    | 173/292 [01:57<01:07,  1.76batch/s, auc=0.7908, loss=0.7768]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 174/292 [01:57<01:06,  1.77batch/s, auc=0.7908, loss=0.7768]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 174/292 [01:57<01:06,  1.77batch/s, auc=0.7914, loss=0.7124]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 175/292 [01:57<01:05,  1.78batch/s, auc=0.7914, loss=0.7124]\u001b[A\n",
      "Training Epoch 1/25:  60%|█████▉    | 175/292 [01:58<01:05,  1.78batch/s, auc=0.7924, loss=0.6775]\u001b[A\n",
      "Training Epoch 1/25:  60%|██████    | 176/292 [01:58<01:05,  1.78batch/s, auc=0.7924, loss=0.6775]\u001b[A\n",
      "Training Epoch 1/25:  60%|██████    | 176/292 [01:58<01:05,  1.78batch/s, auc=0.7930, loss=0.6627]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 177/292 [01:58<01:04,  1.78batch/s, auc=0.7930, loss=0.6627]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 177/292 [01:59<01:04,  1.78batch/s, auc=0.7941, loss=0.6478]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 178/292 [01:59<01:03,  1.78batch/s, auc=0.7941, loss=0.6478]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████    | 178/292 [01:59<01:03,  1.78batch/s, auc=0.7938, loss=1.2594]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████▏   | 179/292 [01:59<01:03,  1.79batch/s, auc=0.7938, loss=1.2594]\u001b[A\n",
      "Training Epoch 1/25:  61%|██████▏   | 179/292 [02:00<01:03,  1.79batch/s, auc=0.7945, loss=0.6683]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 180/292 [02:00<01:02,  1.79batch/s, auc=0.7945, loss=0.6683]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 180/292 [02:00<01:02,  1.79batch/s, auc=0.7950, loss=0.8256]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 181/292 [02:00<01:02,  1.78batch/s, auc=0.7950, loss=0.8256]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 181/292 [02:01<01:02,  1.78batch/s, auc=0.7953, loss=0.7531]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 182/292 [02:01<01:01,  1.78batch/s, auc=0.7953, loss=0.7531]\u001b[A\n",
      "Training Epoch 1/25:  62%|██████▏   | 182/292 [02:02<01:01,  1.78batch/s, auc=0.7953, loss=0.6567]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 183/292 [02:02<01:00,  1.79batch/s, auc=0.7953, loss=0.6567]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 183/292 [02:02<01:00,  1.79batch/s, auc=0.7958, loss=0.6329]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 184/292 [02:02<01:00,  1.79batch/s, auc=0.7958, loss=0.6329]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 184/292 [02:03<01:00,  1.79batch/s, auc=0.7963, loss=0.8636]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 185/292 [02:03<00:59,  1.79batch/s, auc=0.7963, loss=0.8636]\u001b[A\n",
      "Training Epoch 1/25:  63%|██████▎   | 185/292 [02:04<00:59,  1.79batch/s, auc=0.7949, loss=1.5085]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▎   | 186/292 [02:04<01:15,  1.40batch/s, auc=0.7949, loss=1.5085]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▎   | 186/292 [02:04<01:15,  1.40batch/s, auc=0.7953, loss=0.6602]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 187/292 [02:04<01:10,  1.50batch/s, auc=0.7953, loss=0.6602]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 187/292 [02:05<01:10,  1.50batch/s, auc=0.7959, loss=0.7462]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 188/292 [02:05<01:06,  1.57batch/s, auc=0.7959, loss=0.7462]\u001b[A\n",
      "Training Epoch 1/25:  64%|██████▍   | 188/292 [02:05<01:06,  1.57batch/s, auc=0.7968, loss=0.6459]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▍   | 189/292 [02:05<01:03,  1.63batch/s, auc=0.7968, loss=0.6459]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▍   | 189/292 [02:06<01:03,  1.63batch/s, auc=0.7967, loss=1.0333]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 190/292 [02:06<01:00,  1.67batch/s, auc=0.7967, loss=1.0333]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 190/292 [02:07<01:00,  1.67batch/s, auc=0.7972, loss=0.6592]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 191/292 [02:07<00:59,  1.71batch/s, auc=0.7972, loss=0.6592]\u001b[A\n",
      "Training Epoch 1/25:  65%|██████▌   | 191/292 [02:07<00:59,  1.71batch/s, auc=0.7980, loss=0.6307]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 192/292 [02:07<00:57,  1.73batch/s, auc=0.7980, loss=0.6307]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 192/292 [02:08<00:57,  1.73batch/s, auc=0.7988, loss=0.5181]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 193/292 [02:08<00:56,  1.74batch/s, auc=0.7988, loss=0.5181]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▌   | 193/292 [02:08<00:56,  1.74batch/s, auc=0.7992, loss=0.8146]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▋   | 194/292 [02:08<00:55,  1.75batch/s, auc=0.7992, loss=0.8146]\u001b[A\n",
      "Training Epoch 1/25:  66%|██████▋   | 194/292 [02:09<00:55,  1.75batch/s, auc=0.8000, loss=0.6444]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 195/292 [02:09<00:54,  1.76batch/s, auc=0.8000, loss=0.6444]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 195/292 [02:10<00:54,  1.76batch/s, auc=0.7996, loss=1.1554]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 196/292 [02:10<01:09,  1.39batch/s, auc=0.7996, loss=1.1554]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 196/292 [02:11<01:09,  1.39batch/s, auc=0.7982, loss=1.5145]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 197/292 [02:11<01:18,  1.21batch/s, auc=0.7982, loss=1.5145]\u001b[A\n",
      "Training Epoch 1/25:  67%|██████▋   | 197/292 [02:12<01:18,  1.21batch/s, auc=0.7990, loss=0.9285]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 198/292 [02:12<01:10,  1.34batch/s, auc=0.7990, loss=0.9285]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 198/292 [02:12<01:10,  1.34batch/s, auc=0.7993, loss=0.8243]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 199/292 [02:12<01:04,  1.45batch/s, auc=0.7993, loss=0.8243]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 199/292 [02:13<01:04,  1.45batch/s, auc=0.7997, loss=0.7379]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 200/292 [02:13<00:59,  1.53batch/s, auc=0.7997, loss=0.7379]\u001b[A\n",
      "Training Epoch 1/25:  68%|██████▊   | 200/292 [02:13<00:59,  1.53batch/s, auc=0.7999, loss=0.7770]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 201/292 [02:13<00:56,  1.60batch/s, auc=0.7999, loss=0.7770]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 201/292 [02:14<00:56,  1.60batch/s, auc=0.7998, loss=1.0699]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 202/292 [02:14<00:54,  1.65batch/s, auc=0.7998, loss=1.0699]\u001b[A\n",
      "Training Epoch 1/25:  69%|██████▉   | 202/292 [02:14<00:54,  1.65batch/s, auc=0.7997, loss=0.6891]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 203/292 [02:14<00:52,  1.69batch/s, auc=0.7997, loss=0.6891]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 203/292 [02:15<00:52,  1.69batch/s, auc=0.7990, loss=1.4690]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 204/292 [02:15<01:04,  1.36batch/s, auc=0.7990, loss=1.4690]\u001b[A\n",
      "Training Epoch 1/25:  70%|██████▉   | 204/292 [02:16<01:04,  1.36batch/s, auc=0.7995, loss=0.5758]\u001b[A\n",
      "Training Epoch 1/25:  70%|███████   | 205/292 [02:16<00:59,  1.46batch/s, auc=0.7995, loss=0.5758]\u001b[A\n",
      "Training Epoch 1/25:  70%|███████   | 205/292 [02:17<00:59,  1.46batch/s, auc=0.7997, loss=0.9808]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 206/292 [02:17<00:55,  1.55batch/s, auc=0.7997, loss=0.9808]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 206/292 [02:17<00:55,  1.55batch/s, auc=0.8001, loss=0.6981]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 207/292 [02:17<00:52,  1.61batch/s, auc=0.8001, loss=0.6981]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 207/292 [02:18<00:52,  1.61batch/s, auc=0.8007, loss=0.7259]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 208/292 [02:18<00:50,  1.66batch/s, auc=0.8007, loss=0.7259]\u001b[A\n",
      "Training Epoch 1/25:  71%|███████   | 208/292 [02:18<00:50,  1.66batch/s, auc=0.7991, loss=1.8313]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 209/292 [02:18<00:48,  1.70batch/s, auc=0.7991, loss=1.8313]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 209/292 [02:19<00:48,  1.70batch/s, auc=0.7997, loss=0.5932]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 210/292 [02:19<00:47,  1.72batch/s, auc=0.7997, loss=0.5932]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 210/292 [02:19<00:47,  1.72batch/s, auc=0.7999, loss=0.8678]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 211/292 [02:19<00:46,  1.74batch/s, auc=0.7999, loss=0.8678]\u001b[A\n",
      "Training Epoch 1/25:  72%|███████▏  | 211/292 [02:20<00:46,  1.74batch/s, auc=0.8000, loss=1.2797]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 212/292 [02:20<00:45,  1.75batch/s, auc=0.8000, loss=1.2797]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 212/292 [02:20<00:45,  1.75batch/s, auc=0.8008, loss=0.6376]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 213/292 [02:20<00:44,  1.76batch/s, auc=0.8008, loss=0.6376]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 213/292 [02:22<00:44,  1.76batch/s, auc=0.8006, loss=0.8176]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 214/292 [02:22<00:56,  1.39batch/s, auc=0.8006, loss=0.8176]\u001b[A\n",
      "Training Epoch 1/25:  73%|███████▎  | 214/292 [02:22<00:56,  1.39batch/s, auc=0.8008, loss=0.8400]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▎  | 215/292 [02:22<00:51,  1.49batch/s, auc=0.8008, loss=0.8400]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▎  | 215/292 [02:23<00:51,  1.49batch/s, auc=0.8013, loss=0.5709]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 216/292 [02:23<00:48,  1.56batch/s, auc=0.8013, loss=0.5709]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 216/292 [02:23<00:48,  1.56batch/s, auc=0.8013, loss=0.6951]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 217/292 [02:23<00:46,  1.62batch/s, auc=0.8013, loss=0.6951]\u001b[A\n",
      "Training Epoch 1/25:  74%|███████▍  | 217/292 [02:24<00:46,  1.62batch/s, auc=0.8008, loss=1.2777]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▍  | 218/292 [02:24<00:55,  1.33batch/s, auc=0.8008, loss=1.2777]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▍  | 218/292 [02:25<00:55,  1.33batch/s, auc=0.8013, loss=0.6173]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 219/292 [02:25<00:50,  1.44batch/s, auc=0.8013, loss=0.6173]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 219/292 [02:26<00:50,  1.44batch/s, auc=0.8000, loss=1.4609]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 220/292 [02:26<00:58,  1.23batch/s, auc=0.8000, loss=1.4609]\u001b[A\n",
      "Training Epoch 1/25:  75%|███████▌  | 220/292 [02:27<00:58,  1.23batch/s, auc=0.8002, loss=0.7970]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 221/292 [02:27<00:52,  1.36batch/s, auc=0.8002, loss=0.7970]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 221/292 [02:27<00:52,  1.36batch/s, auc=0.8012, loss=0.6970]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 222/292 [02:27<00:47,  1.46batch/s, auc=0.8012, loss=0.6970]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▌  | 222/292 [02:28<00:47,  1.46batch/s, auc=0.8016, loss=0.8336]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▋  | 223/292 [02:28<00:44,  1.55batch/s, auc=0.8016, loss=0.8336]\u001b[A\n",
      "Training Epoch 1/25:  76%|███████▋  | 223/292 [02:29<00:44,  1.55batch/s, auc=0.8011, loss=1.1257]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 224/292 [02:29<00:52,  1.29batch/s, auc=0.8011, loss=1.1257]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 224/292 [02:29<00:52,  1.29batch/s, auc=0.8020, loss=0.7429]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 225/292 [02:29<00:47,  1.41batch/s, auc=0.8020, loss=0.7429]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 225/292 [02:30<00:47,  1.41batch/s, auc=0.8030, loss=0.7002]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 226/292 [02:30<00:43,  1.50batch/s, auc=0.8030, loss=0.7002]\u001b[A\n",
      "Training Epoch 1/25:  77%|███████▋  | 226/292 [02:30<00:43,  1.50batch/s, auc=0.8035, loss=0.7584]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 227/292 [02:30<00:41,  1.58batch/s, auc=0.8035, loss=0.7584]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 227/292 [02:31<00:41,  1.58batch/s, auc=0.8044, loss=0.6865]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 228/292 [02:31<00:39,  1.64batch/s, auc=0.8044, loss=0.6865]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 228/292 [02:32<00:39,  1.64batch/s, auc=0.8048, loss=0.6367]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 229/292 [02:32<00:37,  1.68batch/s, auc=0.8048, loss=0.6367]\u001b[A\n",
      "Training Epoch 1/25:  78%|███████▊  | 229/292 [02:32<00:37,  1.68batch/s, auc=0.8053, loss=0.6476]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 230/292 [02:32<00:36,  1.71batch/s, auc=0.8053, loss=0.6476]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 230/292 [02:33<00:36,  1.71batch/s, auc=0.8064, loss=0.6205]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 231/292 [02:33<00:35,  1.73batch/s, auc=0.8064, loss=0.6205]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 231/292 [02:33<00:35,  1.73batch/s, auc=0.8065, loss=0.8382]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 232/292 [02:33<00:34,  1.74batch/s, auc=0.8065, loss=0.8382]\u001b[A\n",
      "Training Epoch 1/25:  79%|███████▉  | 232/292 [02:34<00:34,  1.74batch/s, auc=0.8068, loss=0.8272]\u001b[A\n",
      "Training Epoch 1/25:  80%|███████▉  | 233/292 [02:34<00:33,  1.75batch/s, auc=0.8068, loss=0.8272]\u001b[A\n",
      "Training Epoch 1/25:  80%|███████▉  | 233/292 [02:34<00:33,  1.75batch/s, auc=0.8068, loss=0.8848]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 234/292 [02:34<00:32,  1.76batch/s, auc=0.8068, loss=0.8848]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 234/292 [02:35<00:32,  1.76batch/s, auc=0.8068, loss=0.7819]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 235/292 [02:35<00:32,  1.77batch/s, auc=0.8068, loss=0.7819]\u001b[A\n",
      "Training Epoch 1/25:  80%|████████  | 235/292 [02:35<00:32,  1.77batch/s, auc=0.8077, loss=0.6969]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 236/292 [02:35<00:31,  1.77batch/s, auc=0.8077, loss=0.6969]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 236/292 [02:36<00:31,  1.77batch/s, auc=0.8071, loss=1.0644]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 237/292 [02:36<00:30,  1.78batch/s, auc=0.8071, loss=1.0644]\u001b[A\n",
      "Training Epoch 1/25:  81%|████████  | 237/292 [02:37<00:30,  1.78batch/s, auc=0.8067, loss=1.3843]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 238/292 [02:37<00:30,  1.78batch/s, auc=0.8067, loss=1.3843]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 238/292 [02:37<00:30,  1.78batch/s, auc=0.8072, loss=0.6935]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 239/292 [02:37<00:29,  1.78batch/s, auc=0.8072, loss=0.6935]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 239/292 [02:38<00:29,  1.78batch/s, auc=0.8066, loss=1.2502]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 240/292 [02:38<00:29,  1.78batch/s, auc=0.8066, loss=1.2502]\u001b[A\n",
      "Training Epoch 1/25:  82%|████████▏ | 240/292 [02:38<00:29,  1.78batch/s, auc=0.8070, loss=0.6387]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 241/292 [02:38<00:28,  1.78batch/s, auc=0.8070, loss=0.6387]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 241/292 [02:39<00:28,  1.78batch/s, auc=0.8072, loss=0.6453]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 242/292 [02:39<00:28,  1.78batch/s, auc=0.8072, loss=0.6453]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 242/292 [02:39<00:28,  1.78batch/s, auc=0.8074, loss=0.6742]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 243/292 [02:39<00:27,  1.78batch/s, auc=0.8074, loss=0.6742]\u001b[A\n",
      "Training Epoch 1/25:  83%|████████▎ | 243/292 [02:40<00:27,  1.78batch/s, auc=0.8075, loss=1.0986]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▎ | 244/292 [02:40<00:26,  1.78batch/s, auc=0.8075, loss=1.0986]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▎ | 244/292 [02:40<00:26,  1.78batch/s, auc=0.8076, loss=0.9794]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 245/292 [02:40<00:26,  1.78batch/s, auc=0.8076, loss=0.9794]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 245/292 [02:41<00:26,  1.78batch/s, auc=0.8078, loss=0.7504]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 246/292 [02:41<00:25,  1.78batch/s, auc=0.8078, loss=0.7504]\u001b[A\n",
      "Training Epoch 1/25:  84%|████████▍ | 246/292 [02:42<00:25,  1.78batch/s, auc=0.8079, loss=0.9185]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 247/292 [02:42<00:32,  1.40batch/s, auc=0.8079, loss=0.9185]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 247/292 [02:43<00:32,  1.40batch/s, auc=0.8071, loss=1.2671]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 248/292 [02:43<00:36,  1.21batch/s, auc=0.8071, loss=1.2671]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▍ | 248/292 [02:44<00:36,  1.21batch/s, auc=0.8077, loss=0.7587]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▌ | 249/292 [02:44<00:32,  1.34batch/s, auc=0.8077, loss=0.7587]\u001b[A\n",
      "Training Epoch 1/25:  85%|████████▌ | 249/292 [02:44<00:32,  1.34batch/s, auc=0.8079, loss=0.5928]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 250/292 [02:44<00:29,  1.45batch/s, auc=0.8079, loss=0.5928]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 250/292 [02:45<00:29,  1.45batch/s, auc=0.8073, loss=1.1055]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 251/292 [02:45<00:33,  1.24batch/s, auc=0.8073, loss=1.1055]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▌ | 251/292 [02:46<00:33,  1.24batch/s, auc=0.8078, loss=0.8536]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▋ | 252/292 [02:46<00:29,  1.36batch/s, auc=0.8078, loss=0.8536]\u001b[A\n",
      "Training Epoch 1/25:  86%|████████▋ | 252/292 [02:47<00:29,  1.36batch/s, auc=0.8084, loss=0.6959]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 253/292 [02:47<00:26,  1.47batch/s, auc=0.8084, loss=0.6959]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 253/292 [02:47<00:26,  1.47batch/s, auc=0.8086, loss=0.9121]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 254/292 [02:47<00:24,  1.55batch/s, auc=0.8086, loss=0.9121]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 254/292 [02:48<00:24,  1.55batch/s, auc=0.8076, loss=1.4324]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 255/292 [02:48<00:28,  1.29batch/s, auc=0.8076, loss=1.4324]\u001b[A\n",
      "Training Epoch 1/25:  87%|████████▋ | 255/292 [02:49<00:28,  1.29batch/s, auc=0.8078, loss=0.7231]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 256/292 [02:49<00:25,  1.41batch/s, auc=0.8078, loss=0.7231]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 256/292 [02:49<00:25,  1.41batch/s, auc=0.8080, loss=0.8916]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 257/292 [02:49<00:23,  1.50batch/s, auc=0.8080, loss=0.8916]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 257/292 [02:50<00:23,  1.50batch/s, auc=0.8083, loss=0.7183]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 258/292 [02:50<00:21,  1.57batch/s, auc=0.8083, loss=0.7183]\u001b[A\n",
      "Training Epoch 1/25:  88%|████████▊ | 258/292 [02:50<00:21,  1.57batch/s, auc=0.8084, loss=0.7298]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▊ | 259/292 [02:50<00:20,  1.63batch/s, auc=0.8084, loss=0.7298]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▊ | 259/292 [02:51<00:20,  1.63batch/s, auc=0.8088, loss=0.7584]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 260/292 [02:51<00:19,  1.67batch/s, auc=0.8088, loss=0.7584]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 260/292 [02:52<00:19,  1.67batch/s, auc=0.8095, loss=0.9110]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 261/292 [02:52<00:18,  1.70batch/s, auc=0.8095, loss=0.9110]\u001b[A\n",
      "Training Epoch 1/25:  89%|████████▉ | 261/292 [02:52<00:18,  1.70batch/s, auc=0.8100, loss=0.6668]\u001b[A\n",
      "Training Epoch 1/25:  90%|████████▉ | 262/292 [02:52<00:17,  1.72batch/s, auc=0.8100, loss=0.6668]\u001b[A\n",
      "Training Epoch 1/25:  90%|████████▉ | 262/292 [02:53<00:17,  1.72batch/s, auc=0.8106, loss=0.7169]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 263/292 [02:53<00:16,  1.74batch/s, auc=0.8106, loss=0.7169]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 263/292 [02:53<00:16,  1.74batch/s, auc=0.8111, loss=0.8547]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 264/292 [02:53<00:16,  1.75batch/s, auc=0.8111, loss=0.8547]\u001b[A\n",
      "Training Epoch 1/25:  90%|█████████ | 264/292 [02:54<00:16,  1.75batch/s, auc=0.8112, loss=0.7940]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 265/292 [02:54<00:15,  1.76batch/s, auc=0.8112, loss=0.7940]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 265/292 [02:54<00:15,  1.76batch/s, auc=0.8114, loss=0.6504]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 266/292 [02:54<00:14,  1.76batch/s, auc=0.8114, loss=0.6504]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████ | 266/292 [02:55<00:14,  1.76batch/s, auc=0.8106, loss=1.3601]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████▏| 267/292 [02:55<00:18,  1.39batch/s, auc=0.8106, loss=1.3601]\u001b[A\n",
      "Training Epoch 1/25:  91%|█████████▏| 267/292 [02:56<00:18,  1.39batch/s, auc=0.8112, loss=0.7479]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 268/292 [02:56<00:16,  1.48batch/s, auc=0.8112, loss=0.7479]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 268/292 [02:57<00:16,  1.48batch/s, auc=0.8115, loss=0.5673]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 269/292 [02:57<00:14,  1.56batch/s, auc=0.8115, loss=0.5673]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 269/292 [02:57<00:14,  1.56batch/s, auc=0.8116, loss=0.6981]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 270/292 [02:57<00:13,  1.62batch/s, auc=0.8116, loss=0.6981]\u001b[A\n",
      "Training Epoch 1/25:  92%|█████████▏| 270/292 [02:58<00:13,  1.62batch/s, auc=0.8119, loss=0.7072]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 271/292 [02:58<00:12,  1.66batch/s, auc=0.8119, loss=0.7072]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 271/292 [02:58<00:12,  1.66batch/s, auc=0.8117, loss=1.3005]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 272/292 [02:58<00:11,  1.69batch/s, auc=0.8117, loss=1.3005]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 272/292 [02:59<00:11,  1.69batch/s, auc=0.8111, loss=0.9910]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 273/292 [02:59<00:13,  1.36batch/s, auc=0.8111, loss=0.9910]\u001b[A\n",
      "Training Epoch 1/25:  93%|█████████▎| 273/292 [03:00<00:13,  1.36batch/s, auc=0.8116, loss=0.7565]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 274/292 [03:00<00:12,  1.46batch/s, auc=0.8116, loss=0.7565]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 274/292 [03:00<00:12,  1.46batch/s, auc=0.8114, loss=0.9107]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 275/292 [03:00<00:11,  1.54batch/s, auc=0.8114, loss=0.9107]\u001b[A\n",
      "Training Epoch 1/25:  94%|█████████▍| 275/292 [03:02<00:11,  1.54batch/s, auc=0.8112, loss=0.9983]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 276/292 [03:02<00:12,  1.29batch/s, auc=0.8112, loss=0.9983]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 276/292 [03:03<00:12,  1.29batch/s, auc=0.8108, loss=1.2734]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 277/292 [03:03<00:13,  1.15batch/s, auc=0.8108, loss=1.2734]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▍| 277/292 [03:03<00:13,  1.15batch/s, auc=0.8115, loss=0.5884]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▌| 278/292 [03:03<00:10,  1.29batch/s, auc=0.8115, loss=0.5884]\u001b[A\n",
      "Training Epoch 1/25:  95%|█████████▌| 278/292 [03:04<00:10,  1.29batch/s, auc=0.8123, loss=0.5759]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 279/292 [03:04<00:09,  1.40batch/s, auc=0.8123, loss=0.5759]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 279/292 [03:04<00:09,  1.40batch/s, auc=0.8123, loss=0.7909]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 280/292 [03:04<00:08,  1.50batch/s, auc=0.8123, loss=0.7909]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 280/292 [03:05<00:08,  1.50batch/s, auc=0.8124, loss=0.9042]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 281/292 [03:05<00:07,  1.57batch/s, auc=0.8124, loss=0.9042]\u001b[A\n",
      "Training Epoch 1/25:  96%|█████████▌| 281/292 [03:05<00:07,  1.57batch/s, auc=0.8130, loss=0.7045]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 282/292 [03:05<00:06,  1.63batch/s, auc=0.8130, loss=0.7045]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 282/292 [03:06<00:06,  1.63batch/s, auc=0.8131, loss=0.8993]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 283/292 [03:06<00:05,  1.67batch/s, auc=0.8131, loss=0.8993]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.67batch/s, auc=0.8132, loss=0.8974]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.70batch/s, auc=0.8132, loss=0.8974]\u001b[A\n",
      "Training Epoch 1/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.70batch/s, auc=0.8134, loss=0.6215]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 285/292 [03:07<00:04,  1.72batch/s, auc=0.8134, loss=0.6215]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.72batch/s, auc=0.8138, loss=0.8837]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.74batch/s, auc=0.8138, loss=0.8837]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.74batch/s, auc=0.8145, loss=0.7161]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 287/292 [03:08<00:02,  1.75batch/s, auc=0.8145, loss=0.7161]\u001b[A\n",
      "Training Epoch 1/25:  98%|█████████▊| 287/292 [03:09<00:02,  1.75batch/s, auc=0.8148, loss=0.5827]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▊| 288/292 [03:09<00:02,  1.76batch/s, auc=0.8148, loss=0.5827]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▊| 288/292 [03:09<00:02,  1.76batch/s, auc=0.8152, loss=0.6070]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 289/292 [03:09<00:01,  1.76batch/s, auc=0.8152, loss=0.6070]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.76batch/s, auc=0.8133, loss=2.3889]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 290/292 [03:10<00:01,  1.39batch/s, auc=0.8133, loss=2.3889]\u001b[A\n",
      "Training Epoch 1/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.39batch/s, auc=0.8142, loss=0.7371]\u001b[A\n",
      "Training Epoch 1/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.48batch/s, auc=0.8142, loss=0.7371]\u001b[A\n",
      "Training Epoch 1/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.48batch/s, auc=0.8140, loss=1.1645]\u001b[A\n",
      "Training Epoch 1/25: 100%|██████████| 292/292 [03:12<00:00,  1.52batch/s, auc=0.8140, loss=1.1645]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] Train Loss: 0.9101 | Train AUROC: 0.8140 Val Loss: 0.8549 | Val AUROC: 0.8261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|▍         | 1/25 [03:31<1:24:41, 211.72s/it]\n",
      "Training Epoch 2/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8222, loss=0.8882]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 1/292 [00:01<07:15,  1.50s/batch, auc=0.8222, loss=0.8882]\u001b[A\n",
      "Training Epoch 2/25:   0%|          | 1/292 [00:02<07:15,  1.50s/batch, auc=0.8236, loss=1.0142]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 2/292 [00:02<04:33,  1.06batch/s, auc=0.8236, loss=1.0142]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 2/292 [00:02<04:33,  1.06batch/s, auc=0.8377, loss=0.7177]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 3/292 [00:02<03:40,  1.31batch/s, auc=0.8377, loss=0.7177]\u001b[A\n",
      "Training Epoch 2/25:   1%|          | 3/292 [00:03<03:40,  1.31batch/s, auc=0.8345, loss=0.8559]\u001b[A\n",
      "Training Epoch 2/25:   1%|▏         | 4/292 [00:03<04:15,  1.13batch/s, auc=0.8345, loss=0.8559]\u001b[A\n",
      "Training Epoch 2/25:   1%|▏         | 4/292 [00:04<04:15,  1.13batch/s, auc=0.8464, loss=0.7072]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 5/292 [00:04<03:39,  1.31batch/s, auc=0.8464, loss=0.7072]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 5/292 [00:04<03:39,  1.31batch/s, auc=0.8483, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 6/292 [00:04<03:18,  1.44batch/s, auc=0.8483, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 6/292 [00:05<03:18,  1.44batch/s, auc=0.8539, loss=0.8711]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 7/292 [00:05<03:04,  1.54batch/s, auc=0.8539, loss=0.8711]\u001b[A\n",
      "Training Epoch 2/25:   2%|▏         | 7/292 [00:06<03:04,  1.54batch/s, auc=0.8249, loss=1.4555]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 8/292 [00:06<03:42,  1.28batch/s, auc=0.8249, loss=1.4555]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 8/292 [00:06<03:42,  1.28batch/s, auc=0.8266, loss=0.9592]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 9/292 [00:06<03:21,  1.41batch/s, auc=0.8266, loss=0.9592]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 9/292 [00:07<03:21,  1.41batch/s, auc=0.8234, loss=0.9529]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 10/292 [00:07<03:06,  1.51batch/s, auc=0.8234, loss=0.9529]\u001b[A\n",
      "Training Epoch 2/25:   3%|▎         | 10/292 [00:08<03:06,  1.51batch/s, auc=0.8292, loss=0.6787]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 11/292 [00:08<02:57,  1.59batch/s, auc=0.8292, loss=0.6787]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 11/292 [00:08<02:57,  1.59batch/s, auc=0.8277, loss=0.8734]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 12/292 [00:08<02:49,  1.65batch/s, auc=0.8277, loss=0.8734]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 12/292 [00:09<02:49,  1.65batch/s, auc=0.8277, loss=0.8499]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 13/292 [00:09<02:44,  1.69batch/s, auc=0.8277, loss=0.8499]\u001b[A\n",
      "Training Epoch 2/25:   4%|▍         | 13/292 [00:09<02:44,  1.69batch/s, auc=0.8195, loss=1.3487]\u001b[A\n",
      "Training Epoch 2/25:   5%|▍         | 14/292 [00:09<02:41,  1.73batch/s, auc=0.8195, loss=1.3487]\u001b[A\n",
      "Training Epoch 2/25:   5%|▍         | 14/292 [00:10<02:41,  1.73batch/s, auc=0.8217, loss=0.8279]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 15/292 [00:10<02:38,  1.75batch/s, auc=0.8217, loss=0.8279]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 15/292 [00:10<02:38,  1.75batch/s, auc=0.8269, loss=0.7664]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 16/292 [00:10<02:36,  1.77batch/s, auc=0.8269, loss=0.7664]\u001b[A\n",
      "Training Epoch 2/25:   5%|▌         | 16/292 [00:11<02:36,  1.77batch/s, auc=0.8300, loss=0.6659]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 17/292 [00:11<02:34,  1.78batch/s, auc=0.8300, loss=0.6659]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 17/292 [00:11<02:34,  1.78batch/s, auc=0.8377, loss=0.6009]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 18/292 [00:11<02:33,  1.78batch/s, auc=0.8377, loss=0.6009]\u001b[A\n",
      "Training Epoch 2/25:   6%|▌         | 18/292 [00:12<02:33,  1.78batch/s, auc=0.8390, loss=0.7958]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 19/292 [00:12<02:32,  1.79batch/s, auc=0.8390, loss=0.7958]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 19/292 [00:13<02:32,  1.79batch/s, auc=0.8416, loss=0.7602]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 20/292 [00:13<02:31,  1.79batch/s, auc=0.8416, loss=0.7602]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 20/292 [00:13<02:31,  1.79batch/s, auc=0.8422, loss=0.7877]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 21/292 [00:13<02:30,  1.79batch/s, auc=0.8422, loss=0.7877]\u001b[A\n",
      "Training Epoch 2/25:   7%|▋         | 21/292 [00:14<02:30,  1.79batch/s, auc=0.8485, loss=0.5990]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 22/292 [00:14<02:30,  1.80batch/s, auc=0.8485, loss=0.5990]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 22/292 [00:14<02:30,  1.80batch/s, auc=0.8522, loss=0.6751]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 23/292 [00:14<02:29,  1.80batch/s, auc=0.8522, loss=0.6751]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 23/292 [00:15<02:29,  1.80batch/s, auc=0.8518, loss=0.7553]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 24/292 [00:15<02:29,  1.80batch/s, auc=0.8518, loss=0.7553]\u001b[A\n",
      "Training Epoch 2/25:   8%|▊         | 24/292 [00:15<02:29,  1.80batch/s, auc=0.8545, loss=0.6230]\u001b[A\n",
      "Training Epoch 2/25:   9%|▊         | 25/292 [00:15<02:28,  1.80batch/s, auc=0.8545, loss=0.6230]\u001b[A\n",
      "Training Epoch 2/25:   9%|▊         | 25/292 [00:16<02:28,  1.80batch/s, auc=0.8569, loss=0.5863]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 26/292 [00:16<02:27,  1.80batch/s, auc=0.8569, loss=0.5863]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 26/292 [00:16<02:27,  1.80batch/s, auc=0.8565, loss=0.8591]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 27/292 [00:16<02:27,  1.80batch/s, auc=0.8565, loss=0.8591]\u001b[A\n",
      "Training Epoch 2/25:   9%|▉         | 27/292 [00:17<02:27,  1.80batch/s, auc=0.8585, loss=0.5586]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 28/292 [00:17<02:26,  1.80batch/s, auc=0.8585, loss=0.5586]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 28/292 [00:18<02:26,  1.80batch/s, auc=0.8586, loss=0.7339]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 29/292 [00:18<02:26,  1.80batch/s, auc=0.8586, loss=0.7339]\u001b[A\n",
      "Training Epoch 2/25:  10%|▉         | 29/292 [00:19<02:26,  1.80batch/s, auc=0.8564, loss=0.8658]\u001b[A\n",
      "Training Epoch 2/25:  10%|█         | 30/292 [00:19<03:06,  1.41batch/s, auc=0.8564, loss=0.8658]\u001b[A\n",
      "Training Epoch 2/25:  10%|█         | 30/292 [00:19<03:06,  1.41batch/s, auc=0.8597, loss=0.5022]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 31/292 [00:19<02:53,  1.51batch/s, auc=0.8597, loss=0.5022]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 31/292 [00:20<02:53,  1.51batch/s, auc=0.8608, loss=0.7090]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 32/292 [00:20<02:44,  1.59batch/s, auc=0.8608, loss=0.7090]\u001b[A\n",
      "Training Epoch 2/25:  11%|█         | 32/292 [00:20<02:44,  1.59batch/s, auc=0.8601, loss=0.6644]\u001b[A\n",
      "Training Epoch 2/25:  11%|█▏        | 33/292 [00:20<02:37,  1.64batch/s, auc=0.8601, loss=0.6644]\u001b[A\n",
      "Training Epoch 2/25:  11%|█▏        | 33/292 [00:21<02:37,  1.64batch/s, auc=0.8599, loss=0.8401]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 34/292 [00:21<02:32,  1.69batch/s, auc=0.8599, loss=0.8401]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 34/292 [00:21<02:32,  1.69batch/s, auc=0.8593, loss=0.7376]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 35/292 [00:21<02:29,  1.72batch/s, auc=0.8593, loss=0.7376]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 35/292 [00:22<02:29,  1.72batch/s, auc=0.8620, loss=0.6189]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 36/292 [00:22<02:26,  1.74batch/s, auc=0.8620, loss=0.6189]\u001b[A\n",
      "Training Epoch 2/25:  12%|█▏        | 36/292 [00:23<02:26,  1.74batch/s, auc=0.8592, loss=0.8950]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 37/292 [00:23<03:04,  1.38batch/s, auc=0.8592, loss=0.8950]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 37/292 [00:24<03:04,  1.38batch/s, auc=0.8571, loss=1.0441]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 38/292 [00:24<02:50,  1.49batch/s, auc=0.8571, loss=1.0441]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 38/292 [00:24<02:50,  1.49batch/s, auc=0.8581, loss=0.5519]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 39/292 [00:24<02:41,  1.57batch/s, auc=0.8581, loss=0.5519]\u001b[A\n",
      "Training Epoch 2/25:  13%|█▎        | 39/292 [00:25<02:41,  1.57batch/s, auc=0.8607, loss=0.4818]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▎        | 40/292 [00:25<02:34,  1.63batch/s, auc=0.8607, loss=0.4818]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▎        | 40/292 [00:25<02:34,  1.63batch/s, auc=0.8621, loss=0.6054]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 41/292 [00:25<02:29,  1.68batch/s, auc=0.8621, loss=0.6054]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 41/292 [00:26<02:29,  1.68batch/s, auc=0.8606, loss=0.9425]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 42/292 [00:26<02:25,  1.72batch/s, auc=0.8606, loss=0.9425]\u001b[A\n",
      "Training Epoch 2/25:  14%|█▍        | 42/292 [00:26<02:25,  1.72batch/s, auc=0.8617, loss=0.6433]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▍        | 43/292 [00:26<02:22,  1.74batch/s, auc=0.8617, loss=0.6433]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▍        | 43/292 [00:27<02:22,  1.74batch/s, auc=0.8623, loss=0.5829]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 44/292 [00:27<02:21,  1.76batch/s, auc=0.8623, loss=0.5829]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 44/292 [00:28<02:21,  1.76batch/s, auc=0.8610, loss=0.9202]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 45/292 [00:28<02:57,  1.39batch/s, auc=0.8610, loss=0.9202]\u001b[A\n",
      "Training Epoch 2/25:  15%|█▌        | 45/292 [00:29<02:57,  1.39batch/s, auc=0.8588, loss=0.9636]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 46/292 [00:29<03:23,  1.21batch/s, auc=0.8588, loss=0.9636]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 46/292 [00:30<03:23,  1.21batch/s, auc=0.8600, loss=0.6271]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 47/292 [00:30<03:02,  1.34batch/s, auc=0.8600, loss=0.6271]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▌        | 47/292 [00:30<03:02,  1.34batch/s, auc=0.8594, loss=0.8633]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▋        | 48/292 [00:30<02:47,  1.45batch/s, auc=0.8594, loss=0.8633]\u001b[A\n",
      "Training Epoch 2/25:  16%|█▋        | 48/292 [00:31<02:47,  1.45batch/s, auc=0.8600, loss=0.7738]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 49/292 [00:31<02:37,  1.54batch/s, auc=0.8600, loss=0.7738]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 49/292 [00:31<02:37,  1.54batch/s, auc=0.8596, loss=0.6204]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 50/292 [00:31<02:30,  1.61batch/s, auc=0.8596, loss=0.6204]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 50/292 [00:32<02:30,  1.61batch/s, auc=0.8604, loss=0.4883]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 51/292 [00:32<02:25,  1.66batch/s, auc=0.8604, loss=0.4883]\u001b[A\n",
      "Training Epoch 2/25:  17%|█▋        | 51/292 [00:32<02:25,  1.66batch/s, auc=0.8624, loss=0.4676]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 52/292 [00:32<02:21,  1.70batch/s, auc=0.8624, loss=0.4676]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 52/292 [00:33<02:21,  1.70batch/s, auc=0.8631, loss=0.6622]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 53/292 [00:33<02:18,  1.73batch/s, auc=0.8631, loss=0.6622]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 53/292 [00:34<02:18,  1.73batch/s, auc=0.8622, loss=0.9082]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 54/292 [00:34<02:16,  1.75batch/s, auc=0.8622, loss=0.9082]\u001b[A\n",
      "Training Epoch 2/25:  18%|█▊        | 54/292 [00:34<02:16,  1.75batch/s, auc=0.8628, loss=0.7927]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 55/292 [00:34<02:14,  1.76batch/s, auc=0.8628, loss=0.7927]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 55/292 [00:35<02:14,  1.76batch/s, auc=0.8629, loss=0.5985]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 56/292 [00:35<02:13,  1.77batch/s, auc=0.8629, loss=0.5985]\u001b[A\n",
      "Training Epoch 2/25:  19%|█▉        | 56/292 [00:36<02:13,  1.77batch/s, auc=0.8592, loss=1.3680]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 57/292 [00:36<02:48,  1.40batch/s, auc=0.8592, loss=1.3680]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 57/292 [00:36<02:48,  1.40batch/s, auc=0.8578, loss=1.0215]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 58/292 [00:36<02:36,  1.50batch/s, auc=0.8578, loss=1.0215]\u001b[A\n",
      "Training Epoch 2/25:  20%|█▉        | 58/292 [00:37<02:36,  1.50batch/s, auc=0.8545, loss=1.3967]\u001b[A\n",
      "Training Epoch 2/25:  20%|██        | 59/292 [00:37<03:03,  1.27batch/s, auc=0.8545, loss=1.3967]\u001b[A\n",
      "Training Epoch 2/25:  20%|██        | 59/292 [00:38<03:03,  1.27batch/s, auc=0.8545, loss=0.5744]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 60/292 [00:38<02:46,  1.39batch/s, auc=0.8545, loss=0.5744]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 60/292 [00:38<02:46,  1.39batch/s, auc=0.8559, loss=0.4812]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 61/292 [00:38<02:34,  1.49batch/s, auc=0.8559, loss=0.4812]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 61/292 [00:39<02:34,  1.49batch/s, auc=0.8570, loss=0.6971]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 62/292 [00:39<02:26,  1.57batch/s, auc=0.8570, loss=0.6971]\u001b[A\n",
      "Training Epoch 2/25:  21%|██        | 62/292 [00:40<02:26,  1.57batch/s, auc=0.8573, loss=0.6645]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 63/292 [00:40<02:20,  1.63batch/s, auc=0.8573, loss=0.6645]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 63/292 [00:41<02:20,  1.63batch/s, auc=0.8532, loss=1.4104]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 64/292 [00:41<02:50,  1.33batch/s, auc=0.8532, loss=1.4104]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 64/292 [00:42<02:50,  1.33batch/s, auc=0.8507, loss=1.2282]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 65/292 [00:42<03:12,  1.18batch/s, auc=0.8507, loss=1.2282]\u001b[A\n",
      "Training Epoch 2/25:  22%|██▏       | 65/292 [00:42<03:12,  1.18batch/s, auc=0.8511, loss=0.7334]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 66/292 [00:42<02:51,  1.32batch/s, auc=0.8511, loss=0.7334]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 66/292 [00:43<02:51,  1.32batch/s, auc=0.8513, loss=0.9184]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 67/292 [00:43<02:37,  1.43batch/s, auc=0.8513, loss=0.9184]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 67/292 [00:43<02:37,  1.43batch/s, auc=0.8521, loss=0.6178]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 68/292 [00:43<02:26,  1.53batch/s, auc=0.8521, loss=0.6178]\u001b[A\n",
      "Training Epoch 2/25:  23%|██▎       | 68/292 [00:44<02:26,  1.53batch/s, auc=0.8526, loss=0.6813]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▎       | 69/292 [00:44<02:19,  1.60batch/s, auc=0.8526, loss=0.6813]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▎       | 69/292 [00:44<02:19,  1.60batch/s, auc=0.8533, loss=0.7515]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 70/292 [00:44<02:14,  1.65batch/s, auc=0.8533, loss=0.7515]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 70/292 [00:45<02:14,  1.65batch/s, auc=0.8540, loss=0.6900]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 71/292 [00:45<02:10,  1.69batch/s, auc=0.8540, loss=0.6900]\u001b[A\n",
      "Training Epoch 2/25:  24%|██▍       | 71/292 [00:46<02:10,  1.69batch/s, auc=0.8543, loss=0.5993]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▍       | 72/292 [00:46<02:41,  1.36batch/s, auc=0.8543, loss=0.5993]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▍       | 72/292 [00:47<02:41,  1.36batch/s, auc=0.8550, loss=0.7071]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 73/292 [00:47<02:29,  1.47batch/s, auc=0.8550, loss=0.7071]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 73/292 [00:47<02:29,  1.47batch/s, auc=0.8550, loss=0.9198]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 74/292 [00:47<02:20,  1.55batch/s, auc=0.8550, loss=0.9198]\u001b[A\n",
      "Training Epoch 2/25:  25%|██▌       | 74/292 [00:48<02:20,  1.55batch/s, auc=0.8550, loss=0.8413]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 75/292 [00:48<02:14,  1.62batch/s, auc=0.8550, loss=0.8413]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 75/292 [00:48<02:14,  1.62batch/s, auc=0.8558, loss=0.5960]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 76/292 [00:48<02:09,  1.67batch/s, auc=0.8558, loss=0.5960]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▌       | 76/292 [00:49<02:09,  1.67batch/s, auc=0.8560, loss=0.7750]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▋       | 77/292 [00:49<02:06,  1.70batch/s, auc=0.8560, loss=0.7750]\u001b[A\n",
      "Training Epoch 2/25:  26%|██▋       | 77/292 [00:49<02:06,  1.70batch/s, auc=0.8560, loss=0.6327]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 78/292 [00:49<02:03,  1.73batch/s, auc=0.8560, loss=0.6327]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 78/292 [00:51<02:03,  1.73batch/s, auc=0.8558, loss=0.7993]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 79/292 [00:51<02:34,  1.37batch/s, auc=0.8558, loss=0.7993]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 79/292 [00:51<02:34,  1.37batch/s, auc=0.8569, loss=0.5540]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 80/292 [00:51<02:23,  1.48batch/s, auc=0.8569, loss=0.5540]\u001b[A\n",
      "Training Epoch 2/25:  27%|██▋       | 80/292 [00:52<02:23,  1.48batch/s, auc=0.8572, loss=0.8079]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 81/292 [00:52<02:15,  1.56batch/s, auc=0.8572, loss=0.8079]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 81/292 [00:52<02:15,  1.56batch/s, auc=0.8581, loss=0.6771]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 82/292 [00:52<02:09,  1.62batch/s, auc=0.8581, loss=0.6771]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 82/292 [00:53<02:09,  1.62batch/s, auc=0.8584, loss=0.6837]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 83/292 [00:53<02:05,  1.67batch/s, auc=0.8584, loss=0.6837]\u001b[A\n",
      "Training Epoch 2/25:  28%|██▊       | 83/292 [00:53<02:05,  1.67batch/s, auc=0.8579, loss=0.6519]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 84/292 [00:53<02:02,  1.70batch/s, auc=0.8579, loss=0.6519]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 84/292 [00:54<02:02,  1.70batch/s, auc=0.8577, loss=0.6961]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 85/292 [00:54<01:59,  1.73batch/s, auc=0.8577, loss=0.6961]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 85/292 [00:54<01:59,  1.73batch/s, auc=0.8571, loss=0.9293]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 86/292 [00:54<01:57,  1.75batch/s, auc=0.8571, loss=0.9293]\u001b[A\n",
      "Training Epoch 2/25:  29%|██▉       | 86/292 [00:55<01:57,  1.75batch/s, auc=0.8574, loss=0.6633]\u001b[A\n",
      "Training Epoch 2/25:  30%|██▉       | 87/292 [00:55<01:56,  1.76batch/s, auc=0.8574, loss=0.6633]\u001b[A\n",
      "Training Epoch 2/25:  30%|██▉       | 87/292 [00:56<01:56,  1.76batch/s, auc=0.8578, loss=0.7588]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 88/292 [00:56<01:55,  1.77batch/s, auc=0.8578, loss=0.7588]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 88/292 [00:56<01:55,  1.77batch/s, auc=0.8574, loss=0.7096]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 89/292 [00:56<01:54,  1.78batch/s, auc=0.8574, loss=0.7096]\u001b[A\n",
      "Training Epoch 2/25:  30%|███       | 89/292 [00:57<01:54,  1.78batch/s, auc=0.8581, loss=0.6325]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 90/292 [00:57<01:53,  1.78batch/s, auc=0.8581, loss=0.6325]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 90/292 [00:57<01:53,  1.78batch/s, auc=0.8578, loss=0.7456]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 91/292 [00:57<01:52,  1.78batch/s, auc=0.8578, loss=0.7456]\u001b[A\n",
      "Training Epoch 2/25:  31%|███       | 91/292 [00:58<01:52,  1.78batch/s, auc=0.8578, loss=0.7555]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 92/292 [00:58<01:51,  1.79batch/s, auc=0.8578, loss=0.7555]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 92/292 [00:58<01:51,  1.79batch/s, auc=0.8571, loss=0.8609]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 93/292 [00:58<01:51,  1.79batch/s, auc=0.8571, loss=0.8609]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 93/292 [00:59<01:51,  1.79batch/s, auc=0.8578, loss=0.6863]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 94/292 [00:59<01:50,  1.79batch/s, auc=0.8578, loss=0.6863]\u001b[A\n",
      "Training Epoch 2/25:  32%|███▏      | 94/292 [00:59<01:50,  1.79batch/s, auc=0.8576, loss=0.8126]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 95/292 [00:59<01:50,  1.79batch/s, auc=0.8576, loss=0.8126]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 95/292 [01:00<01:50,  1.79batch/s, auc=0.8577, loss=0.9531]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 96/292 [01:00<01:49,  1.79batch/s, auc=0.8577, loss=0.9531]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 96/292 [01:01<01:49,  1.79batch/s, auc=0.8577, loss=0.7808]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 97/292 [01:01<01:48,  1.79batch/s, auc=0.8577, loss=0.7808]\u001b[A\n",
      "Training Epoch 2/25:  33%|███▎      | 97/292 [01:01<01:48,  1.79batch/s, auc=0.8583, loss=0.5302]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▎      | 98/292 [01:01<01:48,  1.79batch/s, auc=0.8583, loss=0.5302]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▎      | 98/292 [01:02<01:48,  1.79batch/s, auc=0.8587, loss=0.6840]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 99/292 [01:02<01:47,  1.79batch/s, auc=0.8587, loss=0.6840]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 99/292 [01:02<01:47,  1.79batch/s, auc=0.8592, loss=0.5996]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 100/292 [01:02<01:47,  1.79batch/s, auc=0.8592, loss=0.5996]\u001b[A\n",
      "Training Epoch 2/25:  34%|███▍      | 100/292 [01:03<01:47,  1.79batch/s, auc=0.8599, loss=0.6080]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 101/292 [01:03<01:46,  1.79batch/s, auc=0.8599, loss=0.6080]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 101/292 [01:03<01:46,  1.79batch/s, auc=0.8592, loss=0.8951]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 102/292 [01:03<01:45,  1.79batch/s, auc=0.8592, loss=0.8951]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▍      | 102/292 [01:04<01:45,  1.79batch/s, auc=0.8594, loss=0.8048]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▌      | 103/292 [01:04<01:45,  1.79batch/s, auc=0.8594, loss=0.8048]\u001b[A\n",
      "Training Epoch 2/25:  35%|███▌      | 103/292 [01:05<01:45,  1.79batch/s, auc=0.8591, loss=0.7099]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 104/292 [01:05<02:13,  1.40batch/s, auc=0.8591, loss=0.7099]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 104/292 [01:06<02:13,  1.40batch/s, auc=0.8601, loss=0.5258]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 105/292 [01:06<02:04,  1.50batch/s, auc=0.8601, loss=0.5258]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▌      | 105/292 [01:06<02:04,  1.50batch/s, auc=0.8598, loss=1.0207]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▋      | 106/292 [01:06<01:57,  1.58batch/s, auc=0.8598, loss=1.0207]\u001b[A\n",
      "Training Epoch 2/25:  36%|███▋      | 106/292 [01:07<01:57,  1.58batch/s, auc=0.8600, loss=0.8131]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 107/292 [01:07<01:53,  1.64batch/s, auc=0.8600, loss=0.8131]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 107/292 [01:07<01:53,  1.64batch/s, auc=0.8607, loss=0.5027]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 108/292 [01:07<01:49,  1.68batch/s, auc=0.8607, loss=0.5027]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 108/292 [01:08<01:49,  1.68batch/s, auc=0.8606, loss=0.6706]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 109/292 [01:08<02:15,  1.35batch/s, auc=0.8606, loss=0.6706]\u001b[A\n",
      "Training Epoch 2/25:  37%|███▋      | 109/292 [01:09<02:15,  1.35batch/s, auc=0.8617, loss=0.5886]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 110/292 [01:09<02:04,  1.46batch/s, auc=0.8617, loss=0.5886]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 110/292 [01:10<02:04,  1.46batch/s, auc=0.8599, loss=1.3570]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 111/292 [01:10<02:25,  1.25batch/s, auc=0.8599, loss=1.3570]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 111/292 [01:10<02:25,  1.25batch/s, auc=0.8596, loss=0.8324]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 112/292 [01:10<02:11,  1.37batch/s, auc=0.8596, loss=0.8324]\u001b[A\n",
      "Training Epoch 2/25:  38%|███▊      | 112/292 [01:11<02:11,  1.37batch/s, auc=0.8599, loss=0.6264]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▊      | 113/292 [01:11<02:01,  1.48batch/s, auc=0.8599, loss=0.6264]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▊      | 113/292 [01:12<02:01,  1.48batch/s, auc=0.8599, loss=0.8808]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 114/292 [01:12<01:54,  1.56batch/s, auc=0.8599, loss=0.8808]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 114/292 [01:12<01:54,  1.56batch/s, auc=0.8592, loss=0.9503]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 115/292 [01:12<01:49,  1.62batch/s, auc=0.8592, loss=0.9503]\u001b[A\n",
      "Training Epoch 2/25:  39%|███▉      | 115/292 [01:13<01:49,  1.62batch/s, auc=0.8602, loss=0.6691]\u001b[A\n",
      "Training Epoch 2/25:  40%|███▉      | 116/292 [01:13<01:45,  1.67batch/s, auc=0.8602, loss=0.6691]\u001b[A\n",
      "Training Epoch 2/25:  40%|███▉      | 116/292 [01:14<01:45,  1.67batch/s, auc=0.8581, loss=1.5601]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 117/292 [01:14<02:09,  1.35batch/s, auc=0.8581, loss=1.5601]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 117/292 [01:15<02:09,  1.35batch/s, auc=0.8568, loss=1.1466]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 118/292 [01:15<02:26,  1.19batch/s, auc=0.8568, loss=1.1466]\u001b[A\n",
      "Training Epoch 2/25:  40%|████      | 118/292 [01:15<02:26,  1.19batch/s, auc=0.8572, loss=0.6294]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 119/292 [01:15<02:10,  1.32batch/s, auc=0.8572, loss=0.6294]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 119/292 [01:16<02:10,  1.32batch/s, auc=0.8564, loss=1.0074]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 120/292 [01:17<02:26,  1.17batch/s, auc=0.8564, loss=1.0074]\u001b[A\n",
      "Training Epoch 2/25:  41%|████      | 120/292 [01:17<02:26,  1.17batch/s, auc=0.8565, loss=0.8369]\u001b[A\n",
      "Training Epoch 2/25:  41%|████▏     | 121/292 [01:17<02:10,  1.31batch/s, auc=0.8565, loss=0.8369]\u001b[A\n",
      "Training Epoch 2/25:  41%|████▏     | 121/292 [01:18<02:10,  1.31batch/s, auc=0.8570, loss=0.5479]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 122/292 [01:18<01:59,  1.42batch/s, auc=0.8570, loss=0.5479]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 122/292 [01:19<01:59,  1.42batch/s, auc=0.8560, loss=1.1336]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 123/292 [01:19<02:17,  1.23batch/s, auc=0.8560, loss=1.1336]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 123/292 [01:19<02:17,  1.23batch/s, auc=0.8563, loss=0.6441]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 124/292 [01:19<02:03,  1.36batch/s, auc=0.8563, loss=0.6441]\u001b[A\n",
      "Training Epoch 2/25:  42%|████▏     | 124/292 [01:20<02:03,  1.36batch/s, auc=0.8557, loss=0.9566]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 125/292 [01:20<01:54,  1.46batch/s, auc=0.8557, loss=0.9566]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 125/292 [01:20<01:54,  1.46batch/s, auc=0.8559, loss=0.6094]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 126/292 [01:20<01:47,  1.55batch/s, auc=0.8559, loss=0.6094]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 126/292 [01:21<01:47,  1.55batch/s, auc=0.8532, loss=1.8871]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 127/292 [01:21<02:07,  1.29batch/s, auc=0.8532, loss=1.8871]\u001b[A\n",
      "Training Epoch 2/25:  43%|████▎     | 127/292 [01:22<02:07,  1.29batch/s, auc=0.8534, loss=0.6638]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 128/292 [01:22<01:56,  1.41batch/s, auc=0.8534, loss=0.6638]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 128/292 [01:23<01:56,  1.41batch/s, auc=0.8526, loss=1.0147]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 129/292 [01:23<01:48,  1.51batch/s, auc=0.8526, loss=1.0147]\u001b[A\n",
      "Training Epoch 2/25:  44%|████▍     | 129/292 [01:23<01:48,  1.51batch/s, auc=0.8529, loss=0.6454]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 130/292 [01:23<01:42,  1.58batch/s, auc=0.8529, loss=0.6454]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 130/292 [01:24<01:42,  1.58batch/s, auc=0.8536, loss=0.5862]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 131/292 [01:24<01:38,  1.64batch/s, auc=0.8536, loss=0.5862]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▍     | 131/292 [01:24<01:38,  1.64batch/s, auc=0.8545, loss=0.5794]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▌     | 132/292 [01:24<01:35,  1.68batch/s, auc=0.8545, loss=0.5794]\u001b[A\n",
      "Training Epoch 2/25:  45%|████▌     | 132/292 [01:25<01:35,  1.68batch/s, auc=0.8544, loss=0.7773]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 133/292 [01:25<01:32,  1.71batch/s, auc=0.8544, loss=0.7773]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 133/292 [01:26<01:32,  1.71batch/s, auc=0.8536, loss=1.1163]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 134/292 [01:26<01:55,  1.37batch/s, auc=0.8536, loss=1.1163]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 134/292 [01:26<01:55,  1.37batch/s, auc=0.8542, loss=0.6798]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 135/292 [01:26<01:46,  1.47batch/s, auc=0.8542, loss=0.6798]\u001b[A\n",
      "Training Epoch 2/25:  46%|████▌     | 135/292 [01:27<01:46,  1.47batch/s, auc=0.8545, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 136/292 [01:27<01:40,  1.55batch/s, auc=0.8545, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 136/292 [01:28<01:40,  1.55batch/s, auc=0.8538, loss=1.1944]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 137/292 [01:28<01:35,  1.62batch/s, auc=0.8538, loss=1.1944]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 137/292 [01:28<01:35,  1.62batch/s, auc=0.8539, loss=0.7960]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 138/292 [01:28<01:32,  1.66batch/s, auc=0.8539, loss=0.7960]\u001b[A\n",
      "Training Epoch 2/25:  47%|████▋     | 138/292 [01:29<01:32,  1.66batch/s, auc=0.8549, loss=0.6391]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 139/292 [01:29<01:30,  1.70batch/s, auc=0.8549, loss=0.6391]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 139/292 [01:29<01:30,  1.70batch/s, auc=0.8549, loss=0.9433]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 140/292 [01:29<01:28,  1.73batch/s, auc=0.8549, loss=0.9433]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 140/292 [01:30<01:28,  1.73batch/s, auc=0.8556, loss=0.8731]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 141/292 [01:30<01:26,  1.75batch/s, auc=0.8556, loss=0.8731]\u001b[A\n",
      "Training Epoch 2/25:  48%|████▊     | 141/292 [01:31<01:26,  1.75batch/s, auc=0.8549, loss=1.1072]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▊     | 142/292 [01:31<01:48,  1.38batch/s, auc=0.8549, loss=1.1072]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▊     | 142/292 [01:31<01:48,  1.38batch/s, auc=0.8552, loss=0.6973]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 143/292 [01:31<01:40,  1.48batch/s, auc=0.8552, loss=0.6973]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 143/292 [01:32<01:40,  1.48batch/s, auc=0.8556, loss=0.7513]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 144/292 [01:32<01:34,  1.56batch/s, auc=0.8556, loss=0.7513]\u001b[A\n",
      "Training Epoch 2/25:  49%|████▉     | 144/292 [01:33<01:34,  1.56batch/s, auc=0.8553, loss=0.9566]\u001b[A\n",
      "Training Epoch 2/25:  50%|████▉     | 145/292 [01:33<01:30,  1.62batch/s, auc=0.8553, loss=0.9566]\u001b[A\n",
      "Training Epoch 2/25:  50%|████▉     | 145/292 [01:33<01:30,  1.62batch/s, auc=0.8550, loss=0.8726]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 146/292 [01:33<01:27,  1.67batch/s, auc=0.8550, loss=0.8726]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 146/292 [01:34<01:27,  1.67batch/s, auc=0.8551, loss=0.7168]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 147/292 [01:34<01:25,  1.70batch/s, auc=0.8551, loss=0.7168]\u001b[A\n",
      "Training Epoch 2/25:  50%|█████     | 147/292 [01:34<01:25,  1.70batch/s, auc=0.8553, loss=0.6812]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 148/292 [01:34<01:23,  1.73batch/s, auc=0.8553, loss=0.6812]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 148/292 [01:35<01:23,  1.73batch/s, auc=0.8555, loss=0.6603]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 149/292 [01:35<01:21,  1.75batch/s, auc=0.8555, loss=0.6603]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████     | 149/292 [01:35<01:21,  1.75batch/s, auc=0.8555, loss=0.8480]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████▏    | 150/292 [01:35<01:20,  1.76batch/s, auc=0.8555, loss=0.8480]\u001b[A\n",
      "Training Epoch 2/25:  51%|█████▏    | 150/292 [01:36<01:20,  1.76batch/s, auc=0.8560, loss=0.7851]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 151/292 [01:36<01:19,  1.77batch/s, auc=0.8560, loss=0.7851]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 151/292 [01:36<01:19,  1.77batch/s, auc=0.8564, loss=0.6091]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 152/292 [01:36<01:18,  1.77batch/s, auc=0.8564, loss=0.6091]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 152/292 [01:37<01:18,  1.77batch/s, auc=0.8571, loss=0.6310]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 153/292 [01:37<01:18,  1.78batch/s, auc=0.8571, loss=0.6310]\u001b[A\n",
      "Training Epoch 2/25:  52%|█████▏    | 153/292 [01:38<01:18,  1.78batch/s, auc=0.8572, loss=0.9938]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 154/292 [01:38<01:17,  1.78batch/s, auc=0.8572, loss=0.9938]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 154/292 [01:38<01:17,  1.78batch/s, auc=0.8573, loss=0.7611]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 155/292 [01:38<01:16,  1.78batch/s, auc=0.8573, loss=0.7611]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 155/292 [01:39<01:16,  1.78batch/s, auc=0.8572, loss=0.6547]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 156/292 [01:39<01:37,  1.40batch/s, auc=0.8572, loss=0.6547]\u001b[A\n",
      "Training Epoch 2/25:  53%|█████▎    | 156/292 [01:40<01:37,  1.40batch/s, auc=0.8573, loss=0.7500]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 157/292 [01:40<01:30,  1.49batch/s, auc=0.8573, loss=0.7500]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 157/292 [01:40<01:30,  1.49batch/s, auc=0.8577, loss=0.6093]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 158/292 [01:40<01:25,  1.57batch/s, auc=0.8577, loss=0.6093]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 158/292 [01:41<01:25,  1.57batch/s, auc=0.8577, loss=0.8299]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 159/292 [01:41<01:21,  1.63batch/s, auc=0.8577, loss=0.8299]\u001b[A\n",
      "Training Epoch 2/25:  54%|█████▍    | 159/292 [01:41<01:21,  1.63batch/s, auc=0.8580, loss=0.7027]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▍    | 160/292 [01:41<01:18,  1.68batch/s, auc=0.8580, loss=0.7027]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▍    | 160/292 [01:43<01:18,  1.68batch/s, auc=0.8575, loss=1.1854]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 161/292 [01:43<01:37,  1.35batch/s, auc=0.8575, loss=1.1854]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 161/292 [01:43<01:37,  1.35batch/s, auc=0.8575, loss=0.6732]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 162/292 [01:43<01:29,  1.46batch/s, auc=0.8575, loss=0.6732]\u001b[A\n",
      "Training Epoch 2/25:  55%|█████▌    | 162/292 [01:44<01:29,  1.46batch/s, auc=0.8579, loss=0.5748]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 163/292 [01:44<01:23,  1.54batch/s, auc=0.8579, loss=0.5748]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 163/292 [01:44<01:23,  1.54batch/s, auc=0.8581, loss=0.6882]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 164/292 [01:44<01:19,  1.61batch/s, auc=0.8581, loss=0.6882]\u001b[A\n",
      "Training Epoch 2/25:  56%|█████▌    | 164/292 [01:45<01:19,  1.61batch/s, auc=0.8582, loss=0.7648]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 165/292 [01:45<01:16,  1.65batch/s, auc=0.8582, loss=0.7648]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 165/292 [01:45<01:16,  1.65batch/s, auc=0.8588, loss=0.6782]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 166/292 [01:45<01:14,  1.69batch/s, auc=0.8588, loss=0.6782]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 166/292 [01:46<01:14,  1.69batch/s, auc=0.8585, loss=0.8149]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 167/292 [01:46<01:12,  1.72batch/s, auc=0.8585, loss=0.8149]\u001b[A\n",
      "Training Epoch 2/25:  57%|█████▋    | 167/292 [01:46<01:12,  1.72batch/s, auc=0.8588, loss=0.6468]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 168/292 [01:46<01:11,  1.74batch/s, auc=0.8588, loss=0.6468]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 168/292 [01:47<01:11,  1.74batch/s, auc=0.8588, loss=0.8568]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 169/292 [01:47<01:10,  1.75batch/s, auc=0.8588, loss=0.8568]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 169/292 [01:48<01:10,  1.75batch/s, auc=0.8591, loss=0.6106]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 170/292 [01:48<01:09,  1.76batch/s, auc=0.8591, loss=0.6106]\u001b[A\n",
      "Training Epoch 2/25:  58%|█████▊    | 170/292 [01:48<01:09,  1.76batch/s, auc=0.8593, loss=0.8394]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▊    | 171/292 [01:48<01:08,  1.77batch/s, auc=0.8593, loss=0.8394]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▊    | 171/292 [01:49<01:08,  1.77batch/s, auc=0.8598, loss=0.5353]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 172/292 [01:49<01:07,  1.77batch/s, auc=0.8598, loss=0.5353]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 172/292 [01:49<01:07,  1.77batch/s, auc=0.8598, loss=0.7454]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 173/292 [01:49<01:07,  1.78batch/s, auc=0.8598, loss=0.7454]\u001b[A\n",
      "Training Epoch 2/25:  59%|█████▉    | 173/292 [01:50<01:07,  1.78batch/s, auc=0.8601, loss=0.8079]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 174/292 [01:50<01:06,  1.78batch/s, auc=0.8601, loss=0.8079]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 174/292 [01:50<01:06,  1.78batch/s, auc=0.8605, loss=0.7146]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 175/292 [01:50<01:05,  1.78batch/s, auc=0.8605, loss=0.7146]\u001b[A\n",
      "Training Epoch 2/25:  60%|█████▉    | 175/292 [01:51<01:05,  1.78batch/s, auc=0.8610, loss=0.6279]\u001b[A\n",
      "Training Epoch 2/25:  60%|██████    | 176/292 [01:51<01:05,  1.78batch/s, auc=0.8610, loss=0.6279]\u001b[A\n",
      "Training Epoch 2/25:  60%|██████    | 176/292 [01:51<01:05,  1.78batch/s, auc=0.8610, loss=1.0681]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 177/292 [01:51<01:04,  1.78batch/s, auc=0.8610, loss=1.0681]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 177/292 [01:52<01:04,  1.78batch/s, auc=0.8608, loss=0.7149]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 178/292 [01:52<01:03,  1.78batch/s, auc=0.8608, loss=0.7149]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████    | 178/292 [01:53<01:03,  1.78batch/s, auc=0.8607, loss=0.8467]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████▏   | 179/292 [01:53<01:03,  1.78batch/s, auc=0.8607, loss=0.8467]\u001b[A\n",
      "Training Epoch 2/25:  61%|██████▏   | 179/292 [01:54<01:03,  1.78batch/s, auc=0.8602, loss=0.8673]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 180/292 [01:54<01:20,  1.40batch/s, auc=0.8602, loss=0.8673]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 180/292 [01:54<01:20,  1.40batch/s, auc=0.8603, loss=0.7096]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 181/292 [01:54<01:14,  1.50batch/s, auc=0.8603, loss=0.7096]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 181/292 [01:55<01:14,  1.50batch/s, auc=0.8602, loss=0.6790]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 182/292 [01:55<01:10,  1.57batch/s, auc=0.8602, loss=0.6790]\u001b[A\n",
      "Training Epoch 2/25:  62%|██████▏   | 182/292 [01:56<01:10,  1.57batch/s, auc=0.8597, loss=1.0915]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 183/292 [01:56<01:23,  1.30batch/s, auc=0.8597, loss=1.0915]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 183/292 [01:57<01:23,  1.30batch/s, auc=0.8584, loss=1.6782]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 184/292 [01:57<01:32,  1.16batch/s, auc=0.8584, loss=1.6782]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 184/292 [01:58<01:32,  1.16batch/s, auc=0.8587, loss=0.8415]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 185/292 [01:58<01:22,  1.30batch/s, auc=0.8587, loss=0.8415]\u001b[A\n",
      "Training Epoch 2/25:  63%|██████▎   | 185/292 [01:58<01:22,  1.30batch/s, auc=0.8589, loss=0.7530]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▎   | 186/292 [01:58<01:15,  1.41batch/s, auc=0.8589, loss=0.7530]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▎   | 186/292 [01:59<01:15,  1.41batch/s, auc=0.8590, loss=0.6100]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 187/292 [01:59<01:09,  1.51batch/s, auc=0.8590, loss=0.6100]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 187/292 [01:59<01:09,  1.51batch/s, auc=0.8594, loss=0.6256]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 188/292 [01:59<01:05,  1.58batch/s, auc=0.8594, loss=0.6256]\u001b[A\n",
      "Training Epoch 2/25:  64%|██████▍   | 188/292 [02:00<01:05,  1.58batch/s, auc=0.8596, loss=0.7383]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▍   | 189/292 [02:00<01:02,  1.64batch/s, auc=0.8596, loss=0.7383]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▍   | 189/292 [02:01<01:02,  1.64batch/s, auc=0.8593, loss=0.8713]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 190/292 [02:01<01:16,  1.33batch/s, auc=0.8593, loss=0.8713]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 190/292 [02:01<01:16,  1.33batch/s, auc=0.8596, loss=0.6290]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 191/292 [02:01<01:10,  1.44batch/s, auc=0.8596, loss=0.6290]\u001b[A\n",
      "Training Epoch 2/25:  65%|██████▌   | 191/292 [02:02<01:10,  1.44batch/s, auc=0.8583, loss=1.7826]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 192/292 [02:02<01:20,  1.24batch/s, auc=0.8583, loss=1.7826]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 192/292 [02:03<01:20,  1.24batch/s, auc=0.8577, loss=1.0946]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 193/292 [02:03<01:12,  1.36batch/s, auc=0.8577, loss=1.0946]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▌   | 193/292 [02:04<01:12,  1.36batch/s, auc=0.8559, loss=1.8645]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▋   | 194/292 [02:04<01:22,  1.19batch/s, auc=0.8559, loss=1.8645]\u001b[A\n",
      "Training Epoch 2/25:  66%|██████▋   | 194/292 [02:05<01:22,  1.19batch/s, auc=0.8560, loss=0.7666]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 195/292 [02:05<01:13,  1.32batch/s, auc=0.8560, loss=0.7666]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 195/292 [02:05<01:13,  1.32batch/s, auc=0.8565, loss=0.5039]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 196/292 [02:05<01:06,  1.43batch/s, auc=0.8565, loss=0.5039]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 196/292 [02:06<01:06,  1.43batch/s, auc=0.8557, loss=1.2536]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 197/292 [02:06<01:17,  1.23batch/s, auc=0.8557, loss=1.2536]\u001b[A\n",
      "Training Epoch 2/25:  67%|██████▋   | 197/292 [02:07<01:17,  1.23batch/s, auc=0.8563, loss=0.6508]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 198/292 [02:07<01:09,  1.36batch/s, auc=0.8563, loss=0.6508]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 198/292 [02:08<01:09,  1.36batch/s, auc=0.8561, loss=1.0099]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 199/292 [02:08<01:17,  1.19batch/s, auc=0.8561, loss=1.0099]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 199/292 [02:09<01:17,  1.19batch/s, auc=0.8562, loss=0.6752]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 200/292 [02:09<01:09,  1.32batch/s, auc=0.8562, loss=0.6752]\u001b[A\n",
      "Training Epoch 2/25:  68%|██████▊   | 200/292 [02:09<01:09,  1.32batch/s, auc=0.8561, loss=0.6120]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 201/292 [02:09<01:03,  1.43batch/s, auc=0.8561, loss=0.6120]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 201/292 [02:10<01:03,  1.43batch/s, auc=0.8563, loss=0.5837]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 202/292 [02:10<00:59,  1.52batch/s, auc=0.8563, loss=0.5837]\u001b[A\n",
      "Training Epoch 2/25:  69%|██████▉   | 202/292 [02:11<00:59,  1.52batch/s, auc=0.8556, loss=1.1776]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 203/292 [02:11<01:09,  1.28batch/s, auc=0.8556, loss=1.1776]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 203/292 [02:12<01:09,  1.28batch/s, auc=0.8553, loss=0.8737]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 204/292 [02:12<01:16,  1.15batch/s, auc=0.8553, loss=0.8737]\u001b[A\n",
      "Training Epoch 2/25:  70%|██████▉   | 204/292 [02:12<01:16,  1.15batch/s, auc=0.8553, loss=0.8404]\u001b[A\n",
      "Training Epoch 2/25:  70%|███████   | 205/292 [02:12<01:07,  1.28batch/s, auc=0.8553, loss=0.8404]\u001b[A\n",
      "Training Epoch 2/25:  70%|███████   | 205/292 [02:13<01:07,  1.28batch/s, auc=0.8560, loss=0.7178]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 206/292 [02:13<01:01,  1.40batch/s, auc=0.8560, loss=0.7178]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 206/292 [02:14<01:01,  1.40batch/s, auc=0.8554, loss=1.1518]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 207/292 [02:14<01:10,  1.21batch/s, auc=0.8554, loss=1.1518]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 207/292 [02:15<01:10,  1.21batch/s, auc=0.8556, loss=0.6737]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 208/292 [02:15<01:02,  1.34batch/s, auc=0.8556, loss=0.6737]\u001b[A\n",
      "Training Epoch 2/25:  71%|███████   | 208/292 [02:15<01:02,  1.34batch/s, auc=0.8553, loss=0.9131]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 209/292 [02:15<00:57,  1.45batch/s, auc=0.8553, loss=0.9131]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 209/292 [02:16<00:57,  1.45batch/s, auc=0.8556, loss=0.7064]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 210/292 [02:16<00:53,  1.53batch/s, auc=0.8556, loss=0.7064]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 210/292 [02:16<00:53,  1.53batch/s, auc=0.8559, loss=0.7956]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 211/292 [02:16<00:50,  1.60batch/s, auc=0.8559, loss=0.7956]\u001b[A\n",
      "Training Epoch 2/25:  72%|███████▏  | 211/292 [02:17<00:50,  1.60batch/s, auc=0.8554, loss=1.0521]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 212/292 [02:17<00:48,  1.65batch/s, auc=0.8554, loss=1.0521]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 212/292 [02:17<00:48,  1.65batch/s, auc=0.8552, loss=0.8538]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 213/292 [02:17<00:46,  1.69batch/s, auc=0.8552, loss=0.8538]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 213/292 [02:18<00:46,  1.69batch/s, auc=0.8552, loss=0.8708]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 214/292 [02:18<00:45,  1.71batch/s, auc=0.8552, loss=0.8708]\u001b[A\n",
      "Training Epoch 2/25:  73%|███████▎  | 214/292 [02:19<00:45,  1.71batch/s, auc=0.8555, loss=0.9024]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▎  | 215/292 [02:19<00:44,  1.73batch/s, auc=0.8555, loss=0.9024]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▎  | 215/292 [02:20<00:44,  1.73batch/s, auc=0.8552, loss=1.0142]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 216/292 [02:20<00:55,  1.37batch/s, auc=0.8552, loss=1.0142]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 216/292 [02:20<00:55,  1.37batch/s, auc=0.8550, loss=0.8465]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 217/292 [02:20<00:50,  1.48batch/s, auc=0.8550, loss=0.8465]\u001b[A\n",
      "Training Epoch 2/25:  74%|███████▍  | 217/292 [02:21<00:50,  1.48batch/s, auc=0.8553, loss=0.7315]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▍  | 218/292 [02:21<00:47,  1.56batch/s, auc=0.8553, loss=0.7315]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▍  | 218/292 [02:21<00:47,  1.56batch/s, auc=0.8550, loss=0.7777]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 219/292 [02:21<00:45,  1.62batch/s, auc=0.8550, loss=0.7777]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 219/292 [02:22<00:45,  1.62batch/s, auc=0.8550, loss=0.7212]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 220/292 [02:22<00:43,  1.66batch/s, auc=0.8550, loss=0.7212]\u001b[A\n",
      "Training Epoch 2/25:  75%|███████▌  | 220/292 [02:22<00:43,  1.66batch/s, auc=0.8553, loss=0.7030]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 221/292 [02:22<00:41,  1.70batch/s, auc=0.8553, loss=0.7030]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 221/292 [02:23<00:41,  1.70batch/s, auc=0.8555, loss=0.8710]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 222/292 [02:23<00:40,  1.72batch/s, auc=0.8555, loss=0.8710]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▌  | 222/292 [02:24<00:40,  1.72batch/s, auc=0.8556, loss=0.8265]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▋  | 223/292 [02:24<00:39,  1.74batch/s, auc=0.8556, loss=0.8265]\u001b[A\n",
      "Training Epoch 2/25:  76%|███████▋  | 223/292 [02:24<00:39,  1.74batch/s, auc=0.8557, loss=0.7360]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 224/292 [02:24<00:38,  1.75batch/s, auc=0.8557, loss=0.7360]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 224/292 [02:25<00:38,  1.75batch/s, auc=0.8555, loss=0.8691]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 225/292 [02:25<00:38,  1.76batch/s, auc=0.8555, loss=0.8691]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 225/292 [02:25<00:38,  1.76batch/s, auc=0.8556, loss=0.7774]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 226/292 [02:25<00:37,  1.76batch/s, auc=0.8556, loss=0.7774]\u001b[A\n",
      "Training Epoch 2/25:  77%|███████▋  | 226/292 [02:26<00:37,  1.76batch/s, auc=0.8554, loss=0.7178]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 227/292 [02:26<00:36,  1.77batch/s, auc=0.8554, loss=0.7178]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 227/292 [02:26<00:36,  1.77batch/s, auc=0.8552, loss=0.8656]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 228/292 [02:26<00:36,  1.77batch/s, auc=0.8552, loss=0.8656]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 228/292 [02:27<00:36,  1.77batch/s, auc=0.8543, loss=1.4304]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 229/292 [02:27<00:45,  1.39batch/s, auc=0.8543, loss=1.4304]\u001b[A\n",
      "Training Epoch 2/25:  78%|███████▊  | 229/292 [02:28<00:45,  1.39batch/s, auc=0.8547, loss=0.6709]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 230/292 [02:28<00:41,  1.49batch/s, auc=0.8547, loss=0.6709]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 230/292 [02:29<00:41,  1.49batch/s, auc=0.8547, loss=0.7921]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 231/292 [02:29<00:39,  1.56batch/s, auc=0.8547, loss=0.7921]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 231/292 [02:29<00:39,  1.56batch/s, auc=0.8548, loss=0.6805]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 232/292 [02:29<00:37,  1.62batch/s, auc=0.8548, loss=0.6805]\u001b[A\n",
      "Training Epoch 2/25:  79%|███████▉  | 232/292 [02:30<00:37,  1.62batch/s, auc=0.8550, loss=0.6715]\u001b[A\n",
      "Training Epoch 2/25:  80%|███████▉  | 233/292 [02:30<00:35,  1.67batch/s, auc=0.8550, loss=0.6715]\u001b[A\n",
      "Training Epoch 2/25:  80%|███████▉  | 233/292 [02:31<00:35,  1.67batch/s, auc=0.8546, loss=1.1520]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 234/292 [02:31<00:43,  1.35batch/s, auc=0.8546, loss=1.1520]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 234/292 [02:31<00:43,  1.35batch/s, auc=0.8550, loss=0.6884]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 235/292 [02:31<00:39,  1.45batch/s, auc=0.8550, loss=0.6884]\u001b[A\n",
      "Training Epoch 2/25:  80%|████████  | 235/292 [02:32<00:39,  1.45batch/s, auc=0.8551, loss=0.5671]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 236/292 [02:32<00:36,  1.53batch/s, auc=0.8551, loss=0.5671]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 236/292 [02:32<00:36,  1.53batch/s, auc=0.8551, loss=0.7556]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 237/292 [02:32<00:34,  1.60batch/s, auc=0.8551, loss=0.7556]\u001b[A\n",
      "Training Epoch 2/25:  81%|████████  | 237/292 [02:33<00:34,  1.60batch/s, auc=0.8555, loss=0.6047]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 238/292 [02:33<00:32,  1.65batch/s, auc=0.8555, loss=0.6047]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 238/292 [02:34<00:32,  1.65batch/s, auc=0.8553, loss=0.8533]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 239/292 [02:34<00:31,  1.69batch/s, auc=0.8553, loss=0.8533]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 239/292 [02:34<00:31,  1.69batch/s, auc=0.8555, loss=0.9182]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 240/292 [02:34<00:30,  1.71batch/s, auc=0.8555, loss=0.9182]\u001b[A\n",
      "Training Epoch 2/25:  82%|████████▏ | 240/292 [02:35<00:30,  1.71batch/s, auc=0.8554, loss=0.7118]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 241/292 [02:35<00:29,  1.73batch/s, auc=0.8554, loss=0.7118]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 241/292 [02:35<00:29,  1.73batch/s, auc=0.8556, loss=0.5982]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 242/292 [02:35<00:28,  1.75batch/s, auc=0.8556, loss=0.5982]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 242/292 [02:36<00:28,  1.75batch/s, auc=0.8557, loss=0.7810]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 243/292 [02:36<00:27,  1.75batch/s, auc=0.8557, loss=0.7810]\u001b[A\n",
      "Training Epoch 2/25:  83%|████████▎ | 243/292 [02:36<00:27,  1.75batch/s, auc=0.8557, loss=0.6517]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▎ | 244/292 [02:36<00:27,  1.76batch/s, auc=0.8557, loss=0.6517]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▎ | 244/292 [02:37<00:27,  1.76batch/s, auc=0.8558, loss=0.8215]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 245/292 [02:37<00:26,  1.77batch/s, auc=0.8558, loss=0.8215]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 245/292 [02:37<00:26,  1.77batch/s, auc=0.8558, loss=0.7696]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 246/292 [02:37<00:25,  1.77batch/s, auc=0.8558, loss=0.7696]\u001b[A\n",
      "Training Epoch 2/25:  84%|████████▍ | 246/292 [02:38<00:25,  1.77batch/s, auc=0.8559, loss=0.6570]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 247/292 [02:38<00:25,  1.77batch/s, auc=0.8559, loss=0.6570]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 247/292 [02:39<00:25,  1.77batch/s, auc=0.8557, loss=0.7037]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 248/292 [02:39<00:24,  1.77batch/s, auc=0.8557, loss=0.7037]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▍ | 248/292 [02:39<00:24,  1.77batch/s, auc=0.8555, loss=0.8519]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▌ | 249/292 [02:39<00:24,  1.77batch/s, auc=0.8555, loss=0.8519]\u001b[A\n",
      "Training Epoch 2/25:  85%|████████▌ | 249/292 [02:40<00:24,  1.77batch/s, auc=0.8555, loss=0.8468]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 250/292 [02:40<00:23,  1.78batch/s, auc=0.8555, loss=0.8468]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 250/292 [02:40<00:23,  1.78batch/s, auc=0.8557, loss=0.6830]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 251/292 [02:40<00:23,  1.77batch/s, auc=0.8557, loss=0.6830]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▌ | 251/292 [02:41<00:23,  1.77batch/s, auc=0.8558, loss=0.8314]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▋ | 252/292 [02:41<00:22,  1.78batch/s, auc=0.8558, loss=0.8314]\u001b[A\n",
      "Training Epoch 2/25:  86%|████████▋ | 252/292 [02:41<00:22,  1.78batch/s, auc=0.8558, loss=0.7739]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 253/292 [02:41<00:21,  1.77batch/s, auc=0.8558, loss=0.7739]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 253/292 [02:42<00:21,  1.77batch/s, auc=0.8563, loss=0.6376]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 254/292 [02:42<00:21,  1.77batch/s, auc=0.8563, loss=0.6376]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 254/292 [02:43<00:21,  1.77batch/s, auc=0.8561, loss=0.7722]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 255/292 [02:43<00:20,  1.77batch/s, auc=0.8561, loss=0.7722]\u001b[A\n",
      "Training Epoch 2/25:  87%|████████▋ | 255/292 [02:44<00:20,  1.77batch/s, auc=0.8557, loss=1.0679]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 256/292 [02:44<00:25,  1.39batch/s, auc=0.8557, loss=1.0679]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 256/292 [02:44<00:25,  1.39batch/s, auc=0.8560, loss=0.6854]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 257/292 [02:44<00:23,  1.49batch/s, auc=0.8560, loss=0.6854]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 257/292 [02:45<00:23,  1.49batch/s, auc=0.8562, loss=0.7594]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 258/292 [02:45<00:21,  1.56batch/s, auc=0.8562, loss=0.7594]\u001b[A\n",
      "Training Epoch 2/25:  88%|████████▊ | 258/292 [02:46<00:21,  1.56batch/s, auc=0.8559, loss=1.0602]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▊ | 259/292 [02:46<00:25,  1.29batch/s, auc=0.8559, loss=1.0602]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▊ | 259/292 [02:46<00:25,  1.29batch/s, auc=0.8560, loss=0.6640]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 260/292 [02:46<00:22,  1.41batch/s, auc=0.8560, loss=0.6640]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 260/292 [02:47<00:22,  1.41batch/s, auc=0.8561, loss=0.6345]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 261/292 [02:47<00:20,  1.50batch/s, auc=0.8561, loss=0.6345]\u001b[A\n",
      "Training Epoch 2/25:  89%|████████▉ | 261/292 [02:48<00:20,  1.50batch/s, auc=0.8561, loss=0.7386]\u001b[A\n",
      "Training Epoch 2/25:  90%|████████▉ | 262/292 [02:48<00:19,  1.57batch/s, auc=0.8561, loss=0.7386]\u001b[A\n",
      "Training Epoch 2/25:  90%|████████▉ | 262/292 [02:48<00:19,  1.57batch/s, auc=0.8563, loss=0.6143]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 263/292 [02:48<00:17,  1.63batch/s, auc=0.8563, loss=0.6143]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 263/292 [02:49<00:17,  1.63batch/s, auc=0.8557, loss=1.3161]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 264/292 [02:49<00:21,  1.33batch/s, auc=0.8557, loss=1.3161]\u001b[A\n",
      "Training Epoch 2/25:  90%|█████████ | 264/292 [02:50<00:21,  1.33batch/s, auc=0.8560, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 265/292 [02:50<00:18,  1.43batch/s, auc=0.8560, loss=0.6977]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 265/292 [02:50<00:18,  1.43batch/s, auc=0.8558, loss=0.8751]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 266/292 [02:50<00:17,  1.52batch/s, auc=0.8558, loss=0.8751]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████ | 266/292 [02:51<00:17,  1.52batch/s, auc=0.8558, loss=0.7323]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████▏| 267/292 [02:51<00:15,  1.59batch/s, auc=0.8558, loss=0.7323]\u001b[A\n",
      "Training Epoch 2/25:  91%|█████████▏| 267/292 [02:51<00:15,  1.59batch/s, auc=0.8560, loss=0.6554]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 268/292 [02:51<00:14,  1.64batch/s, auc=0.8560, loss=0.6554]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 268/292 [02:52<00:14,  1.64batch/s, auc=0.8561, loss=0.7259]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 269/292 [02:52<00:13,  1.68batch/s, auc=0.8561, loss=0.7259]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 269/292 [02:53<00:13,  1.68batch/s, auc=0.8558, loss=1.0678]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 270/292 [02:53<00:16,  1.35batch/s, auc=0.8558, loss=1.0678]\u001b[A\n",
      "Training Epoch 2/25:  92%|█████████▏| 270/292 [02:54<00:16,  1.35batch/s, auc=0.8561, loss=0.6644]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 271/292 [02:54<00:14,  1.45batch/s, auc=0.8561, loss=0.6644]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 271/292 [02:54<00:14,  1.45batch/s, auc=0.8560, loss=0.8932]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 272/292 [02:54<00:13,  1.54batch/s, auc=0.8560, loss=0.8932]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 272/292 [02:55<00:13,  1.54batch/s, auc=0.8561, loss=0.6799]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 273/292 [02:55<00:14,  1.28batch/s, auc=0.8561, loss=0.6799]\u001b[A\n",
      "Training Epoch 2/25:  93%|█████████▎| 273/292 [02:56<00:14,  1.28batch/s, auc=0.8563, loss=0.5971]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 274/292 [02:56<00:12,  1.40batch/s, auc=0.8563, loss=0.5971]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 274/292 [02:57<00:12,  1.40batch/s, auc=0.8563, loss=0.7775]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 275/292 [02:57<00:14,  1.21batch/s, auc=0.8563, loss=0.7775]\u001b[A\n",
      "Training Epoch 2/25:  94%|█████████▍| 275/292 [02:58<00:14,  1.21batch/s, auc=0.8565, loss=0.5717]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 276/292 [02:58<00:11,  1.34batch/s, auc=0.8565, loss=0.5717]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 276/292 [02:58<00:11,  1.34batch/s, auc=0.8567, loss=0.5811]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 277/292 [02:58<00:10,  1.44batch/s, auc=0.8567, loss=0.5811]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▍| 277/292 [02:59<00:10,  1.44batch/s, auc=0.8567, loss=0.7872]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▌| 278/292 [02:59<00:09,  1.53batch/s, auc=0.8567, loss=0.7872]\u001b[A\n",
      "Training Epoch 2/25:  95%|█████████▌| 278/292 [02:59<00:09,  1.53batch/s, auc=0.8563, loss=1.1532]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 279/292 [02:59<00:08,  1.59batch/s, auc=0.8563, loss=1.1532]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 279/292 [03:00<00:08,  1.59batch/s, auc=0.8568, loss=0.4914]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 280/292 [03:00<00:07,  1.64batch/s, auc=0.8568, loss=0.4914]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 280/292 [03:00<00:07,  1.64batch/s, auc=0.8569, loss=0.6853]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 281/292 [03:00<00:06,  1.68batch/s, auc=0.8569, loss=0.6853]\u001b[A\n",
      "Training Epoch 2/25:  96%|█████████▌| 281/292 [03:01<00:06,  1.68batch/s, auc=0.8572, loss=0.5775]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 282/292 [03:01<00:05,  1.70batch/s, auc=0.8572, loss=0.5775]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 282/292 [03:01<00:05,  1.70batch/s, auc=0.8575, loss=0.7873]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 283/292 [03:01<00:05,  1.72batch/s, auc=0.8575, loss=0.7873]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 283/292 [03:02<00:05,  1.72batch/s, auc=0.8578, loss=0.6929]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 284/292 [03:02<00:04,  1.74batch/s, auc=0.8578, loss=0.6929]\u001b[A\n",
      "Training Epoch 2/25:  97%|█████████▋| 284/292 [03:03<00:04,  1.74batch/s, auc=0.8577, loss=0.8450]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 285/292 [03:03<00:04,  1.75batch/s, auc=0.8577, loss=0.8450]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 285/292 [03:04<00:04,  1.75batch/s, auc=0.8574, loss=1.0519]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 286/292 [03:04<00:04,  1.38batch/s, auc=0.8574, loss=1.0519]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 286/292 [03:04<00:04,  1.38batch/s, auc=0.8572, loss=0.9643]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 287/292 [03:04<00:03,  1.48batch/s, auc=0.8572, loss=0.9643]\u001b[A\n",
      "Training Epoch 2/25:  98%|█████████▊| 287/292 [03:05<00:03,  1.48batch/s, auc=0.8572, loss=0.6508]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▊| 288/292 [03:05<00:02,  1.55batch/s, auc=0.8572, loss=0.6508]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▊| 288/292 [03:05<00:02,  1.55batch/s, auc=0.8573, loss=0.6717]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 289/292 [03:05<00:01,  1.61batch/s, auc=0.8573, loss=0.6717]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 289/292 [03:06<00:01,  1.61batch/s, auc=0.8574, loss=0.7273]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 290/292 [03:06<00:01,  1.66batch/s, auc=0.8574, loss=0.7273]\u001b[A\n",
      "Training Epoch 2/25:  99%|█████████▉| 290/292 [03:07<00:01,  1.66batch/s, auc=0.8575, loss=0.6356]\u001b[A\n",
      "Training Epoch 2/25: 100%|█████████▉| 291/292 [03:07<00:00,  1.69batch/s, auc=0.8575, loss=0.6356]\u001b[A\n",
      "Training Epoch 2/25: 100%|█████████▉| 291/292 [03:07<00:00,  1.69batch/s, auc=0.8573, loss=1.0880]\u001b[A\n",
      "Training Epoch 2/25: 100%|██████████| 292/292 [03:07<00:00,  1.56batch/s, auc=0.8573, loss=1.0880]\u001b[A\n",
      "Epochs:   8%|▊         | 2/25 [06:58<1:20:05, 208.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25] Train Loss: 0.7992 | Train AUROC: 0.8573 Val Loss: 0.8617 | Val AUROC: 0.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 3/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9138, loss=0.6537]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 1/292 [00:01<07:07,  1.47s/batch, auc=0.9138, loss=0.6537]\u001b[A\n",
      "Training Epoch 3/25:   0%|          | 1/292 [00:02<07:07,  1.47s/batch, auc=0.8983, loss=0.7035]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 2/292 [00:02<04:29,  1.07batch/s, auc=0.8983, loss=0.7035]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 2/292 [00:02<04:29,  1.07batch/s, auc=0.8926, loss=0.6980]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 3/292 [00:02<03:39,  1.32batch/s, auc=0.8926, loss=0.6980]\u001b[A\n",
      "Training Epoch 3/25:   1%|          | 3/292 [00:03<03:39,  1.32batch/s, auc=0.8890, loss=0.7332]\u001b[A\n",
      "Training Epoch 3/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.8890, loss=0.7332]\u001b[A\n",
      "Training Epoch 3/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.8918, loss=0.6775]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 5/292 [00:03<03:01,  1.58batch/s, auc=0.8918, loss=0.6775]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 5/292 [00:04<03:01,  1.58batch/s, auc=0.8831, loss=0.8867]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 6/292 [00:04<03:43,  1.28batch/s, auc=0.8831, loss=0.8867]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 6/292 [00:05<03:43,  1.28batch/s, auc=0.8830, loss=0.6938]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 7/292 [00:05<03:21,  1.41batch/s, auc=0.8830, loss=0.6938]\u001b[A\n",
      "Training Epoch 3/25:   2%|▏         | 7/292 [00:05<03:21,  1.41batch/s, auc=0.8783, loss=0.7715]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 8/292 [00:05<03:07,  1.52batch/s, auc=0.8783, loss=0.7715]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 8/292 [00:06<03:07,  1.52batch/s, auc=0.8714, loss=0.9282]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 9/292 [00:06<02:57,  1.60batch/s, auc=0.8714, loss=0.9282]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 9/292 [00:06<02:57,  1.60batch/s, auc=0.8741, loss=0.8232]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 10/292 [00:06<02:50,  1.66batch/s, auc=0.8741, loss=0.8232]\u001b[A\n",
      "Training Epoch 3/25:   3%|▎         | 10/292 [00:07<02:50,  1.66batch/s, auc=0.8801, loss=0.7041]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 11/292 [00:07<02:45,  1.70batch/s, auc=0.8801, loss=0.7041]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 11/292 [00:08<02:45,  1.70batch/s, auc=0.8875, loss=0.4568]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 12/292 [00:08<02:41,  1.73batch/s, auc=0.8875, loss=0.4568]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 12/292 [00:08<02:41,  1.73batch/s, auc=0.8874, loss=0.6249]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 13/292 [00:08<02:38,  1.76batch/s, auc=0.8874, loss=0.6249]\u001b[A\n",
      "Training Epoch 3/25:   4%|▍         | 13/292 [00:09<02:38,  1.76batch/s, auc=0.8914, loss=0.6792]\u001b[A\n",
      "Training Epoch 3/25:   5%|▍         | 14/292 [00:09<02:37,  1.77batch/s, auc=0.8914, loss=0.6792]\u001b[A\n",
      "Training Epoch 3/25:   5%|▍         | 14/292 [00:09<02:37,  1.77batch/s, auc=0.8902, loss=0.5862]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 15/292 [00:09<02:35,  1.78batch/s, auc=0.8902, loss=0.5862]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 15/292 [00:10<02:35,  1.78batch/s, auc=0.8869, loss=1.0291]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 16/292 [00:10<02:34,  1.79batch/s, auc=0.8869, loss=1.0291]\u001b[A\n",
      "Training Epoch 3/25:   5%|▌         | 16/292 [00:10<02:34,  1.79batch/s, auc=0.8874, loss=0.6574]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 17/292 [00:10<02:33,  1.79batch/s, auc=0.8874, loss=0.6574]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 17/292 [00:11<02:33,  1.79batch/s, auc=0.8880, loss=0.6468]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 18/292 [00:11<02:32,  1.80batch/s, auc=0.8880, loss=0.6468]\u001b[A\n",
      "Training Epoch 3/25:   6%|▌         | 18/292 [00:12<02:32,  1.80batch/s, auc=0.8852, loss=0.7945]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 19/292 [00:12<03:13,  1.41batch/s, auc=0.8852, loss=0.7945]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 19/292 [00:13<03:13,  1.41batch/s, auc=0.8833, loss=0.7066]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 20/292 [00:13<03:00,  1.51batch/s, auc=0.8833, loss=0.7066]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 20/292 [00:13<03:00,  1.51batch/s, auc=0.8794, loss=0.9001]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 21/292 [00:13<02:50,  1.59batch/s, auc=0.8794, loss=0.9001]\u001b[A\n",
      "Training Epoch 3/25:   7%|▋         | 21/292 [00:14<02:50,  1.59batch/s, auc=0.8810, loss=0.5822]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 22/292 [00:14<02:44,  1.65batch/s, auc=0.8810, loss=0.5822]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 22/292 [00:14<02:44,  1.65batch/s, auc=0.8808, loss=0.8123]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 23/292 [00:14<02:38,  1.69batch/s, auc=0.8808, loss=0.8123]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 23/292 [00:15<02:38,  1.69batch/s, auc=0.8829, loss=0.6434]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 24/292 [00:15<02:35,  1.73batch/s, auc=0.8829, loss=0.6434]\u001b[A\n",
      "Training Epoch 3/25:   8%|▊         | 24/292 [00:15<02:35,  1.73batch/s, auc=0.8836, loss=0.6902]\u001b[A\n",
      "Training Epoch 3/25:   9%|▊         | 25/292 [00:15<02:32,  1.75batch/s, auc=0.8836, loss=0.6902]\u001b[A\n",
      "Training Epoch 3/25:   9%|▊         | 25/292 [00:16<02:32,  1.75batch/s, auc=0.8814, loss=0.7827]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 26/292 [00:16<02:30,  1.77batch/s, auc=0.8814, loss=0.7827]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 26/292 [00:16<02:30,  1.77batch/s, auc=0.8788, loss=0.8063]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 27/292 [00:16<02:28,  1.78batch/s, auc=0.8788, loss=0.8063]\u001b[A\n",
      "Training Epoch 3/25:   9%|▉         | 27/292 [00:17<02:28,  1.78batch/s, auc=0.8786, loss=0.8003]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 28/292 [00:17<02:27,  1.79batch/s, auc=0.8786, loss=0.8003]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 28/292 [00:17<02:27,  1.79batch/s, auc=0.8794, loss=0.6660]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 29/292 [00:17<02:26,  1.79batch/s, auc=0.8794, loss=0.6660]\u001b[A\n",
      "Training Epoch 3/25:  10%|▉         | 29/292 [00:18<02:26,  1.79batch/s, auc=0.8778, loss=0.7847]\u001b[A\n",
      "Training Epoch 3/25:  10%|█         | 30/292 [00:18<02:26,  1.79batch/s, auc=0.8778, loss=0.7847]\u001b[A\n",
      "Training Epoch 3/25:  10%|█         | 30/292 [00:19<02:26,  1.79batch/s, auc=0.8788, loss=0.5721]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 31/292 [00:19<02:25,  1.80batch/s, auc=0.8788, loss=0.5721]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 31/292 [00:19<02:25,  1.80batch/s, auc=0.8768, loss=0.8597]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 32/292 [00:19<02:24,  1.80batch/s, auc=0.8768, loss=0.8597]\u001b[A\n",
      "Training Epoch 3/25:  11%|█         | 32/292 [00:20<02:24,  1.80batch/s, auc=0.8782, loss=0.5587]\u001b[A\n",
      "Training Epoch 3/25:  11%|█▏        | 33/292 [00:20<02:24,  1.80batch/s, auc=0.8782, loss=0.5587]\u001b[A\n",
      "Training Epoch 3/25:  11%|█▏        | 33/292 [00:21<02:24,  1.80batch/s, auc=0.8752, loss=1.0563]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 34/292 [00:21<03:03,  1.41batch/s, auc=0.8752, loss=1.0563]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 34/292 [00:21<03:03,  1.41batch/s, auc=0.8741, loss=0.7437]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 35/292 [00:21<02:50,  1.51batch/s, auc=0.8741, loss=0.7437]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 35/292 [00:22<02:50,  1.51batch/s, auc=0.8724, loss=0.9726]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 36/292 [00:22<02:41,  1.59batch/s, auc=0.8724, loss=0.9726]\u001b[A\n",
      "Training Epoch 3/25:  12%|█▏        | 36/292 [00:22<02:41,  1.59batch/s, auc=0.8711, loss=0.8069]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 37/292 [00:22<02:34,  1.65batch/s, auc=0.8711, loss=0.8069]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 37/292 [00:23<02:34,  1.65batch/s, auc=0.8727, loss=0.6194]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 38/292 [00:23<02:30,  1.69batch/s, auc=0.8727, loss=0.6194]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 38/292 [00:24<02:30,  1.69batch/s, auc=0.8726, loss=0.6938]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 39/292 [00:24<02:26,  1.72batch/s, auc=0.8726, loss=0.6938]\u001b[A\n",
      "Training Epoch 3/25:  13%|█▎        | 39/292 [00:25<02:26,  1.72batch/s, auc=0.8721, loss=0.6590]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▎        | 40/292 [00:25<03:03,  1.37batch/s, auc=0.8721, loss=0.6590]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▎        | 40/292 [00:25<03:03,  1.37batch/s, auc=0.8732, loss=0.6450]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 41/292 [00:25<02:49,  1.48batch/s, auc=0.8732, loss=0.6450]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 41/292 [00:26<02:49,  1.48batch/s, auc=0.8747, loss=0.5462]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 42/292 [00:26<02:39,  1.56batch/s, auc=0.8747, loss=0.5462]\u001b[A\n",
      "Training Epoch 3/25:  14%|█▍        | 42/292 [00:26<02:39,  1.56batch/s, auc=0.8745, loss=0.6626]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▍        | 43/292 [00:26<02:32,  1.63batch/s, auc=0.8745, loss=0.6626]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▍        | 43/292 [00:27<02:32,  1.63batch/s, auc=0.8764, loss=0.5973]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 44/292 [00:27<02:28,  1.68batch/s, auc=0.8764, loss=0.5973]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 44/292 [00:27<02:28,  1.68batch/s, auc=0.8759, loss=0.8256]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 45/292 [00:27<02:24,  1.71batch/s, auc=0.8759, loss=0.8256]\u001b[A\n",
      "Training Epoch 3/25:  15%|█▌        | 45/292 [00:28<02:24,  1.71batch/s, auc=0.8718, loss=1.3309]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 46/292 [00:28<02:59,  1.37batch/s, auc=0.8718, loss=1.3309]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 46/292 [00:29<02:59,  1.37batch/s, auc=0.8718, loss=0.5573]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 47/292 [00:29<02:45,  1.48batch/s, auc=0.8718, loss=0.5573]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▌        | 47/292 [00:30<02:45,  1.48batch/s, auc=0.8742, loss=0.5386]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▋        | 48/292 [00:30<02:36,  1.56batch/s, auc=0.8742, loss=0.5386]\u001b[A\n",
      "Training Epoch 3/25:  16%|█▋        | 48/292 [00:30<02:36,  1.56batch/s, auc=0.8750, loss=0.5451]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 49/292 [00:30<02:29,  1.63batch/s, auc=0.8750, loss=0.5451]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 49/292 [00:31<02:29,  1.63batch/s, auc=0.8748, loss=0.7158]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 50/292 [00:31<02:24,  1.68batch/s, auc=0.8748, loss=0.7158]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 50/292 [00:31<02:24,  1.68batch/s, auc=0.8734, loss=0.8042]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 51/292 [00:31<02:20,  1.71batch/s, auc=0.8734, loss=0.8042]\u001b[A\n",
      "Training Epoch 3/25:  17%|█▋        | 51/292 [00:32<02:20,  1.71batch/s, auc=0.8743, loss=0.6468]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 52/292 [00:32<02:17,  1.74batch/s, auc=0.8743, loss=0.6468]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 52/292 [00:32<02:17,  1.74batch/s, auc=0.8750, loss=0.7546]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 53/292 [00:32<02:15,  1.76batch/s, auc=0.8750, loss=0.7546]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 53/292 [00:33<02:15,  1.76batch/s, auc=0.8714, loss=1.3204]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 54/292 [00:33<02:51,  1.39batch/s, auc=0.8714, loss=1.3204]\u001b[A\n",
      "Training Epoch 3/25:  18%|█▊        | 54/292 [00:34<02:51,  1.39batch/s, auc=0.8731, loss=0.6071]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 55/292 [00:34<02:38,  1.49batch/s, auc=0.8731, loss=0.6071]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 55/292 [00:35<02:38,  1.49batch/s, auc=0.8739, loss=0.5807]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 56/292 [00:35<02:29,  1.57batch/s, auc=0.8739, loss=0.5807]\u001b[A\n",
      "Training Epoch 3/25:  19%|█▉        | 56/292 [00:35<02:29,  1.57batch/s, auc=0.8734, loss=0.8193]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 57/292 [00:35<02:23,  1.64batch/s, auc=0.8734, loss=0.8193]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 57/292 [00:36<02:23,  1.64batch/s, auc=0.8732, loss=0.7466]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 58/292 [00:36<02:19,  1.68batch/s, auc=0.8732, loss=0.7466]\u001b[A\n",
      "Training Epoch 3/25:  20%|█▉        | 58/292 [00:36<02:19,  1.68batch/s, auc=0.8742, loss=0.5895]\u001b[A\n",
      "Training Epoch 3/25:  20%|██        | 59/292 [00:36<02:15,  1.72batch/s, auc=0.8742, loss=0.5895]\u001b[A\n",
      "Training Epoch 3/25:  20%|██        | 59/292 [00:37<02:15,  1.72batch/s, auc=0.8758, loss=0.5253]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 60/292 [00:37<02:13,  1.74batch/s, auc=0.8758, loss=0.5253]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 60/292 [00:37<02:13,  1.74batch/s, auc=0.8758, loss=0.7647]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 61/292 [00:37<02:11,  1.76batch/s, auc=0.8758, loss=0.7647]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 61/292 [00:38<02:11,  1.76batch/s, auc=0.8773, loss=0.5219]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 62/292 [00:38<02:09,  1.77batch/s, auc=0.8773, loss=0.5219]\u001b[A\n",
      "Training Epoch 3/25:  21%|██        | 62/292 [00:38<02:09,  1.77batch/s, auc=0.8783, loss=0.4812]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 63/292 [00:38<02:08,  1.78batch/s, auc=0.8783, loss=0.4812]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 63/292 [00:39<02:08,  1.78batch/s, auc=0.8770, loss=0.9967]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 64/292 [00:39<02:07,  1.79batch/s, auc=0.8770, loss=0.9967]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 64/292 [00:40<02:07,  1.79batch/s, auc=0.8775, loss=0.6513]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 65/292 [00:40<02:06,  1.79batch/s, auc=0.8775, loss=0.6513]\u001b[A\n",
      "Training Epoch 3/25:  22%|██▏       | 65/292 [00:40<02:06,  1.79batch/s, auc=0.8775, loss=0.6009]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 66/292 [00:40<02:05,  1.79batch/s, auc=0.8775, loss=0.6009]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 66/292 [00:41<02:05,  1.79batch/s, auc=0.8771, loss=0.6942]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 67/292 [00:41<02:05,  1.80batch/s, auc=0.8771, loss=0.6942]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 67/292 [00:41<02:05,  1.80batch/s, auc=0.8775, loss=0.5701]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 68/292 [00:41<02:04,  1.80batch/s, auc=0.8775, loss=0.5701]\u001b[A\n",
      "Training Epoch 3/25:  23%|██▎       | 68/292 [00:42<02:04,  1.80batch/s, auc=0.8779, loss=0.6132]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▎       | 69/292 [00:42<02:03,  1.80batch/s, auc=0.8779, loss=0.6132]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▎       | 69/292 [00:42<02:03,  1.80batch/s, auc=0.8777, loss=0.7478]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 70/292 [00:42<02:03,  1.80batch/s, auc=0.8777, loss=0.7478]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 70/292 [00:43<02:03,  1.80batch/s, auc=0.8795, loss=0.5342]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 71/292 [00:43<02:02,  1.80batch/s, auc=0.8795, loss=0.5342]\u001b[A\n",
      "Training Epoch 3/25:  24%|██▍       | 71/292 [00:44<02:02,  1.80batch/s, auc=0.8757, loss=1.7266]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▍       | 72/292 [00:44<02:36,  1.41batch/s, auc=0.8757, loss=1.7266]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▍       | 72/292 [00:44<02:36,  1.41batch/s, auc=0.8764, loss=0.6829]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 73/292 [00:44<02:25,  1.51batch/s, auc=0.8764, loss=0.6829]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 73/292 [00:45<02:25,  1.51batch/s, auc=0.8762, loss=0.8014]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 74/292 [00:45<02:17,  1.58batch/s, auc=0.8762, loss=0.8014]\u001b[A\n",
      "Training Epoch 3/25:  25%|██▌       | 74/292 [00:46<02:17,  1.58batch/s, auc=0.8762, loss=0.7020]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 75/292 [00:46<02:12,  1.64batch/s, auc=0.8762, loss=0.7020]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 75/292 [00:46<02:12,  1.64batch/s, auc=0.8769, loss=0.5160]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 76/292 [00:46<02:08,  1.69batch/s, auc=0.8769, loss=0.5160]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▌       | 76/292 [00:47<02:08,  1.69batch/s, auc=0.8780, loss=0.4951]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▋       | 77/292 [00:47<02:05,  1.72batch/s, auc=0.8780, loss=0.4951]\u001b[A\n",
      "Training Epoch 3/25:  26%|██▋       | 77/292 [00:47<02:05,  1.72batch/s, auc=0.8785, loss=0.5767]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 78/292 [00:47<02:02,  1.74batch/s, auc=0.8785, loss=0.5767]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 78/292 [00:48<02:02,  1.74batch/s, auc=0.8794, loss=0.6049]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 79/292 [00:48<02:01,  1.76batch/s, auc=0.8794, loss=0.6049]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 79/292 [00:49<02:01,  1.76batch/s, auc=0.8772, loss=1.3313]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 80/292 [00:49<02:32,  1.39batch/s, auc=0.8772, loss=1.3313]\u001b[A\n",
      "Training Epoch 3/25:  27%|██▋       | 80/292 [00:49<02:32,  1.39batch/s, auc=0.8775, loss=0.8780]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 81/292 [00:49<02:21,  1.49batch/s, auc=0.8775, loss=0.8780]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 81/292 [00:50<02:21,  1.49batch/s, auc=0.8781, loss=0.4890]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 82/292 [00:50<02:13,  1.57batch/s, auc=0.8781, loss=0.4890]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 82/292 [00:51<02:13,  1.57batch/s, auc=0.8781, loss=0.6020]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 83/292 [00:51<02:07,  1.64batch/s, auc=0.8781, loss=0.6020]\u001b[A\n",
      "Training Epoch 3/25:  28%|██▊       | 83/292 [00:51<02:07,  1.64batch/s, auc=0.8784, loss=0.5783]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 84/292 [00:51<02:03,  1.68batch/s, auc=0.8784, loss=0.5783]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 84/292 [00:52<02:03,  1.68batch/s, auc=0.8775, loss=0.9247]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 85/292 [00:52<02:00,  1.72batch/s, auc=0.8775, loss=0.9247]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 85/292 [00:52<02:00,  1.72batch/s, auc=0.8771, loss=0.7059]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 86/292 [00:52<01:58,  1.74batch/s, auc=0.8771, loss=0.7059]\u001b[A\n",
      "Training Epoch 3/25:  29%|██▉       | 86/292 [00:53<01:58,  1.74batch/s, auc=0.8774, loss=0.6861]\u001b[A\n",
      "Training Epoch 3/25:  30%|██▉       | 87/292 [00:53<01:56,  1.76batch/s, auc=0.8774, loss=0.6861]\u001b[A\n",
      "Training Epoch 3/25:  30%|██▉       | 87/292 [00:53<01:56,  1.76batch/s, auc=0.8781, loss=0.5757]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 88/292 [00:53<01:55,  1.77batch/s, auc=0.8781, loss=0.5757]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 88/292 [00:54<01:55,  1.77batch/s, auc=0.8772, loss=0.9707]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 89/292 [00:54<02:25,  1.39batch/s, auc=0.8772, loss=0.9707]\u001b[A\n",
      "Training Epoch 3/25:  30%|███       | 89/292 [00:55<02:25,  1.39batch/s, auc=0.8773, loss=0.6193]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 90/292 [00:55<02:15,  1.50batch/s, auc=0.8773, loss=0.6193]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 90/292 [00:56<02:15,  1.50batch/s, auc=0.8783, loss=0.5662]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 91/292 [00:56<02:07,  1.58batch/s, auc=0.8783, loss=0.5662]\u001b[A\n",
      "Training Epoch 3/25:  31%|███       | 91/292 [00:56<02:07,  1.58batch/s, auc=0.8777, loss=0.7833]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 92/292 [00:56<02:02,  1.64batch/s, auc=0.8777, loss=0.7833]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 92/292 [00:57<02:02,  1.64batch/s, auc=0.8770, loss=0.8237]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 93/292 [00:57<02:29,  1.33batch/s, auc=0.8770, loss=0.8237]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 93/292 [00:58<02:29,  1.33batch/s, auc=0.8762, loss=1.0634]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 94/292 [00:58<02:16,  1.45batch/s, auc=0.8762, loss=1.0634]\u001b[A\n",
      "Training Epoch 3/25:  32%|███▏      | 94/292 [00:59<02:16,  1.45batch/s, auc=0.8755, loss=0.9904]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 95/292 [00:59<02:38,  1.24batch/s, auc=0.8755, loss=0.9904]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 95/292 [00:59<02:38,  1.24batch/s, auc=0.8750, loss=0.8264]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 96/292 [00:59<02:23,  1.37batch/s, auc=0.8750, loss=0.8264]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 96/292 [01:00<02:23,  1.37batch/s, auc=0.8750, loss=0.8376]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 97/292 [01:00<02:12,  1.47batch/s, auc=0.8750, loss=0.8376]\u001b[A\n",
      "Training Epoch 3/25:  33%|███▎      | 97/292 [01:01<02:12,  1.47batch/s, auc=0.8743, loss=0.8452]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▎      | 98/292 [01:01<02:34,  1.25batch/s, auc=0.8743, loss=0.8452]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▎      | 98/292 [01:02<02:34,  1.25batch/s, auc=0.8745, loss=0.6011]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 99/292 [01:02<02:20,  1.38batch/s, auc=0.8745, loss=0.6011]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 99/292 [01:02<02:20,  1.38batch/s, auc=0.8749, loss=0.6306]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 100/292 [01:02<02:09,  1.48batch/s, auc=0.8749, loss=0.6306]\u001b[A\n",
      "Training Epoch 3/25:  34%|███▍      | 100/292 [01:03<02:09,  1.48batch/s, auc=0.8753, loss=0.6219]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 101/292 [01:03<02:02,  1.57batch/s, auc=0.8753, loss=0.6219]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 101/292 [01:03<02:02,  1.57batch/s, auc=0.8751, loss=0.7792]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 102/292 [01:03<01:56,  1.63batch/s, auc=0.8751, loss=0.7792]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▍      | 102/292 [01:04<01:56,  1.63batch/s, auc=0.8753, loss=0.6618]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▌      | 103/292 [01:04<01:52,  1.68batch/s, auc=0.8753, loss=0.6618]\u001b[A\n",
      "Training Epoch 3/25:  35%|███▌      | 103/292 [01:04<01:52,  1.68batch/s, auc=0.8753, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 104/292 [01:04<01:49,  1.71batch/s, auc=0.8753, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 104/292 [01:05<01:49,  1.71batch/s, auc=0.8750, loss=0.7221]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 105/292 [01:05<01:47,  1.74batch/s, auc=0.8750, loss=0.7221]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▌      | 105/292 [01:05<01:47,  1.74batch/s, auc=0.8760, loss=0.6095]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▋      | 106/292 [01:05<01:46,  1.75batch/s, auc=0.8760, loss=0.6095]\u001b[A\n",
      "Training Epoch 3/25:  36%|███▋      | 106/292 [01:06<01:46,  1.75batch/s, auc=0.8756, loss=0.8754]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 107/292 [01:06<01:44,  1.76batch/s, auc=0.8756, loss=0.8754]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 107/292 [01:07<01:44,  1.76batch/s, auc=0.8768, loss=0.4419]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 108/292 [01:07<01:43,  1.77batch/s, auc=0.8768, loss=0.4419]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 108/292 [01:07<01:43,  1.77batch/s, auc=0.8771, loss=0.5367]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 109/292 [01:07<01:42,  1.78batch/s, auc=0.8771, loss=0.5367]\u001b[A\n",
      "Training Epoch 3/25:  37%|███▋      | 109/292 [01:08<01:42,  1.78batch/s, auc=0.8782, loss=0.5351]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 110/292 [01:08<01:41,  1.79batch/s, auc=0.8782, loss=0.5351]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 110/292 [01:08<01:41,  1.79batch/s, auc=0.8785, loss=0.5586]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 111/292 [01:08<01:41,  1.79batch/s, auc=0.8785, loss=0.5586]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 111/292 [01:09<01:41,  1.79batch/s, auc=0.8772, loss=1.1654]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 112/292 [01:09<02:08,  1.40batch/s, auc=0.8772, loss=1.1654]\u001b[A\n",
      "Training Epoch 3/25:  38%|███▊      | 112/292 [01:10<02:08,  1.40batch/s, auc=0.8766, loss=0.8244]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▊      | 113/292 [01:10<02:26,  1.22batch/s, auc=0.8766, loss=0.8244]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▊      | 113/292 [01:11<02:26,  1.22batch/s, auc=0.8769, loss=0.6391]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 114/292 [01:11<02:11,  1.35batch/s, auc=0.8769, loss=0.6391]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 114/292 [01:11<02:11,  1.35batch/s, auc=0.8771, loss=0.6791]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 115/292 [01:11<02:01,  1.46batch/s, auc=0.8771, loss=0.6791]\u001b[A\n",
      "Training Epoch 3/25:  39%|███▉      | 115/292 [01:12<02:01,  1.46batch/s, auc=0.8767, loss=0.7284]\u001b[A\n",
      "Training Epoch 3/25:  40%|███▉      | 116/292 [01:12<01:53,  1.54batch/s, auc=0.8767, loss=0.7284]\u001b[A\n",
      "Training Epoch 3/25:  40%|███▉      | 116/292 [01:13<01:53,  1.54batch/s, auc=0.8769, loss=0.6356]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 117/292 [01:13<01:48,  1.61batch/s, auc=0.8769, loss=0.6356]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 117/292 [01:13<01:48,  1.61batch/s, auc=0.8762, loss=0.9263]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 118/292 [01:13<01:44,  1.66batch/s, auc=0.8762, loss=0.9263]\u001b[A\n",
      "Training Epoch 3/25:  40%|████      | 118/292 [01:14<01:44,  1.66batch/s, auc=0.8761, loss=0.7749]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 119/292 [01:14<01:41,  1.70batch/s, auc=0.8761, loss=0.7749]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 119/292 [01:15<01:41,  1.70batch/s, auc=0.8740, loss=1.5450]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 120/292 [01:15<02:06,  1.36batch/s, auc=0.8740, loss=1.5450]\u001b[A\n",
      "Training Epoch 3/25:  41%|████      | 120/292 [01:16<02:06,  1.36batch/s, auc=0.8741, loss=0.5653]\u001b[A\n",
      "Training Epoch 3/25:  41%|████▏     | 121/292 [01:16<02:22,  1.20batch/s, auc=0.8741, loss=0.5653]\u001b[A\n",
      "Training Epoch 3/25:  41%|████▏     | 121/292 [01:16<02:22,  1.20batch/s, auc=0.8733, loss=0.9272]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 122/292 [01:16<02:07,  1.33batch/s, auc=0.8733, loss=0.9272]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 122/292 [01:17<02:07,  1.33batch/s, auc=0.8737, loss=0.5580]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 123/292 [01:17<01:57,  1.44batch/s, auc=0.8737, loss=0.5580]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 123/292 [01:18<01:57,  1.44batch/s, auc=0.8734, loss=0.8947]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 124/292 [01:18<01:49,  1.53batch/s, auc=0.8734, loss=0.8947]\u001b[A\n",
      "Training Epoch 3/25:  42%|████▏     | 124/292 [01:18<01:49,  1.53batch/s, auc=0.8727, loss=1.1781]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 125/292 [01:18<01:44,  1.60batch/s, auc=0.8727, loss=1.1781]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 125/292 [01:19<01:44,  1.60batch/s, auc=0.8731, loss=0.5561]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 126/292 [01:19<01:40,  1.66batch/s, auc=0.8731, loss=0.5561]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 126/292 [01:19<01:40,  1.66batch/s, auc=0.8729, loss=0.7192]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 127/292 [01:19<01:37,  1.69batch/s, auc=0.8729, loss=0.7192]\u001b[A\n",
      "Training Epoch 3/25:  43%|████▎     | 127/292 [01:20<01:37,  1.69batch/s, auc=0.8728, loss=0.7222]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 128/292 [01:20<01:35,  1.72batch/s, auc=0.8728, loss=0.7222]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 128/292 [01:20<01:35,  1.72batch/s, auc=0.8737, loss=0.4644]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 129/292 [01:20<01:33,  1.74batch/s, auc=0.8737, loss=0.4644]\u001b[A\n",
      "Training Epoch 3/25:  44%|████▍     | 129/292 [01:21<01:33,  1.74batch/s, auc=0.8737, loss=0.7529]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 130/292 [01:21<01:32,  1.76batch/s, auc=0.8737, loss=0.7529]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 130/292 [01:21<01:32,  1.76batch/s, auc=0.8735, loss=0.7568]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 131/292 [01:21<01:31,  1.77batch/s, auc=0.8735, loss=0.7568]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▍     | 131/292 [01:22<01:31,  1.77batch/s, auc=0.8742, loss=0.5128]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▌     | 132/292 [01:22<01:30,  1.77batch/s, auc=0.8742, loss=0.5128]\u001b[A\n",
      "Training Epoch 3/25:  45%|████▌     | 132/292 [01:23<01:30,  1.77batch/s, auc=0.8731, loss=1.2615]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 133/292 [01:23<01:54,  1.39batch/s, auc=0.8731, loss=1.2615]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 133/292 [01:24<01:54,  1.39batch/s, auc=0.8716, loss=1.5110]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 134/292 [01:24<02:10,  1.21batch/s, auc=0.8716, loss=1.5110]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 134/292 [01:25<02:10,  1.21batch/s, auc=0.8718, loss=0.7609]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 135/292 [01:25<01:56,  1.34batch/s, auc=0.8718, loss=0.7609]\u001b[A\n",
      "Training Epoch 3/25:  46%|████▌     | 135/292 [01:25<01:56,  1.34batch/s, auc=0.8720, loss=0.7142]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 136/292 [01:25<01:47,  1.45batch/s, auc=0.8720, loss=0.7142]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 136/292 [01:26<01:47,  1.45batch/s, auc=0.8724, loss=0.7353]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 137/292 [01:26<01:40,  1.54batch/s, auc=0.8724, loss=0.7353]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 137/292 [01:27<01:40,  1.54batch/s, auc=0.8716, loss=1.3093]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 138/292 [01:27<01:59,  1.29batch/s, auc=0.8716, loss=1.3093]\u001b[A\n",
      "Training Epoch 3/25:  47%|████▋     | 138/292 [01:27<01:59,  1.29batch/s, auc=0.8713, loss=0.7502]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 139/292 [01:27<01:48,  1.40batch/s, auc=0.8713, loss=0.7502]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 139/292 [01:28<01:48,  1.40batch/s, auc=0.8711, loss=0.8744]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 140/292 [01:28<01:41,  1.50batch/s, auc=0.8711, loss=0.8744]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 140/292 [01:29<01:41,  1.50batch/s, auc=0.8706, loss=0.8785]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 141/292 [01:29<01:59,  1.27batch/s, auc=0.8706, loss=0.8785]\u001b[A\n",
      "Training Epoch 3/25:  48%|████▊     | 141/292 [01:30<01:59,  1.27batch/s, auc=0.8708, loss=0.7420]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▊     | 142/292 [01:30<01:47,  1.39batch/s, auc=0.8708, loss=0.7420]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▊     | 142/292 [01:31<01:47,  1.39batch/s, auc=0.8700, loss=1.1747]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 143/292 [01:31<02:03,  1.21batch/s, auc=0.8700, loss=1.1747]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 143/292 [01:31<02:03,  1.21batch/s, auc=0.8702, loss=0.6489]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 144/292 [01:31<01:50,  1.34batch/s, auc=0.8702, loss=0.6489]\u001b[A\n",
      "Training Epoch 3/25:  49%|████▉     | 144/292 [01:32<01:50,  1.34batch/s, auc=0.8702, loss=0.7782]\u001b[A\n",
      "Training Epoch 3/25:  50%|████▉     | 145/292 [01:32<01:41,  1.45batch/s, auc=0.8702, loss=0.7782]\u001b[A\n",
      "Training Epoch 3/25:  50%|████▉     | 145/292 [01:32<01:41,  1.45batch/s, auc=0.8697, loss=0.7371]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 146/292 [01:32<01:35,  1.54batch/s, auc=0.8697, loss=0.7371]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 146/292 [01:33<01:35,  1.54batch/s, auc=0.8697, loss=0.7585]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 147/292 [01:33<01:30,  1.60batch/s, auc=0.8697, loss=0.7585]\u001b[A\n",
      "Training Epoch 3/25:  50%|█████     | 147/292 [01:33<01:30,  1.60batch/s, auc=0.8699, loss=0.5626]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 148/292 [01:33<01:27,  1.65batch/s, auc=0.8699, loss=0.5626]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 148/292 [01:34<01:27,  1.65batch/s, auc=0.8695, loss=0.8364]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 149/292 [01:34<01:24,  1.69batch/s, auc=0.8695, loss=0.8364]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████     | 149/292 [01:35<01:24,  1.69batch/s, auc=0.8695, loss=0.6042]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████▏    | 150/292 [01:35<01:22,  1.72batch/s, auc=0.8695, loss=0.6042]\u001b[A\n",
      "Training Epoch 3/25:  51%|█████▏    | 150/292 [01:35<01:22,  1.72batch/s, auc=0.8698, loss=0.5720]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 151/292 [01:35<01:21,  1.74batch/s, auc=0.8698, loss=0.5720]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 151/292 [01:36<01:21,  1.74batch/s, auc=0.8699, loss=0.5833]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 152/292 [01:36<01:19,  1.75batch/s, auc=0.8699, loss=0.5833]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 152/292 [01:36<01:19,  1.75batch/s, auc=0.8697, loss=0.8243]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 153/292 [01:36<01:18,  1.76batch/s, auc=0.8697, loss=0.8243]\u001b[A\n",
      "Training Epoch 3/25:  52%|█████▏    | 153/292 [01:37<01:18,  1.76batch/s, auc=0.8696, loss=0.7115]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 154/292 [01:37<01:17,  1.77batch/s, auc=0.8696, loss=0.7115]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 154/292 [01:38<01:17,  1.77batch/s, auc=0.8693, loss=0.9173]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 155/292 [01:38<01:38,  1.40batch/s, auc=0.8693, loss=0.9173]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 155/292 [01:39<01:38,  1.40batch/s, auc=0.8683, loss=1.2042]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 156/292 [01:39<01:52,  1.21batch/s, auc=0.8683, loss=1.2042]\u001b[A\n",
      "Training Epoch 3/25:  53%|█████▎    | 156/292 [01:40<01:52,  1.21batch/s, auc=0.8688, loss=0.5791]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 157/292 [01:40<01:40,  1.34batch/s, auc=0.8688, loss=0.5791]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 157/292 [01:40<01:40,  1.34batch/s, auc=0.8687, loss=0.9271]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 158/292 [01:40<01:32,  1.45batch/s, auc=0.8687, loss=0.9271]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 158/292 [01:41<01:32,  1.45batch/s, auc=0.8691, loss=0.7010]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 159/292 [01:41<01:26,  1.54batch/s, auc=0.8691, loss=0.7010]\u001b[A\n",
      "Training Epoch 3/25:  54%|█████▍    | 159/292 [01:42<01:26,  1.54batch/s, auc=0.8683, loss=1.1955]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▍    | 160/292 [01:42<01:42,  1.29batch/s, auc=0.8683, loss=1.1955]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▍    | 160/292 [01:42<01:42,  1.29batch/s, auc=0.8680, loss=0.8422]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 161/292 [01:42<01:33,  1.40batch/s, auc=0.8680, loss=0.8422]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 161/292 [01:43<01:33,  1.40batch/s, auc=0.8665, loss=1.7394]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 162/292 [01:43<01:46,  1.22batch/s, auc=0.8665, loss=1.7394]\u001b[A\n",
      "Training Epoch 3/25:  55%|█████▌    | 162/292 [01:44<01:46,  1.22batch/s, auc=0.8668, loss=0.6781]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 163/292 [01:44<01:35,  1.35batch/s, auc=0.8668, loss=0.6781]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 163/292 [01:45<01:35,  1.35batch/s, auc=0.8659, loss=1.4073]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 164/292 [01:45<01:47,  1.19batch/s, auc=0.8659, loss=1.4073]\u001b[A\n",
      "Training Epoch 3/25:  56%|█████▌    | 164/292 [01:46<01:47,  1.19batch/s, auc=0.8660, loss=0.6510]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 165/292 [01:46<01:36,  1.32batch/s, auc=0.8660, loss=0.6510]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 165/292 [01:46<01:36,  1.32batch/s, auc=0.8658, loss=0.8092]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 166/292 [01:46<01:27,  1.43batch/s, auc=0.8658, loss=0.8092]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 166/292 [01:47<01:27,  1.43batch/s, auc=0.8655, loss=0.7982]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 167/292 [01:47<01:41,  1.23batch/s, auc=0.8655, loss=0.7982]\u001b[A\n",
      "Training Epoch 3/25:  57%|█████▋    | 167/292 [01:48<01:41,  1.23batch/s, auc=0.8656, loss=0.6934]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 168/292 [01:48<01:31,  1.36batch/s, auc=0.8656, loss=0.6934]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 168/292 [01:49<01:31,  1.36batch/s, auc=0.8655, loss=0.8869]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 169/292 [01:49<01:43,  1.19batch/s, auc=0.8655, loss=0.8869]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 169/292 [01:49<01:43,  1.19batch/s, auc=0.8651, loss=1.0145]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 170/292 [01:49<01:32,  1.32batch/s, auc=0.8651, loss=1.0145]\u001b[A\n",
      "Training Epoch 3/25:  58%|█████▊    | 170/292 [01:50<01:32,  1.32batch/s, auc=0.8649, loss=0.9077]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▊    | 171/292 [01:50<01:43,  1.17batch/s, auc=0.8649, loss=0.9077]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▊    | 171/292 [01:51<01:43,  1.17batch/s, auc=0.8649, loss=0.6778]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 172/292 [01:51<01:31,  1.31batch/s, auc=0.8649, loss=0.6778]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 172/292 [01:52<01:31,  1.31batch/s, auc=0.8652, loss=0.8031]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 173/292 [01:52<01:23,  1.42batch/s, auc=0.8652, loss=0.8031]\u001b[A\n",
      "Training Epoch 3/25:  59%|█████▉    | 173/292 [01:53<01:23,  1.42batch/s, auc=0.8649, loss=0.8368]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 174/292 [01:53<01:36,  1.23batch/s, auc=0.8649, loss=0.8368]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 174/292 [01:53<01:36,  1.23batch/s, auc=0.8649, loss=0.6983]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 175/292 [01:53<01:26,  1.35batch/s, auc=0.8649, loss=0.6983]\u001b[A\n",
      "Training Epoch 3/25:  60%|█████▉    | 175/292 [01:54<01:26,  1.35batch/s, auc=0.8655, loss=0.5912]\u001b[A\n",
      "Training Epoch 3/25:  60%|██████    | 176/292 [01:54<01:19,  1.46batch/s, auc=0.8655, loss=0.5912]\u001b[A\n",
      "Training Epoch 3/25:  60%|██████    | 176/292 [01:54<01:19,  1.46batch/s, auc=0.8655, loss=1.0128]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 177/292 [01:54<01:14,  1.54batch/s, auc=0.8655, loss=1.0128]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 177/292 [01:55<01:14,  1.54batch/s, auc=0.8645, loss=1.1685]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 178/292 [01:55<01:28,  1.29batch/s, auc=0.8645, loss=1.1685]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████    | 178/292 [01:56<01:28,  1.29batch/s, auc=0.8645, loss=0.6673]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████▏   | 179/292 [01:56<01:20,  1.41batch/s, auc=0.8645, loss=0.6673]\u001b[A\n",
      "Training Epoch 3/25:  61%|██████▏   | 179/292 [01:57<01:20,  1.41batch/s, auc=0.8649, loss=0.7258]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 180/292 [01:57<01:14,  1.50batch/s, auc=0.8649, loss=0.7258]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 180/292 [01:57<01:14,  1.50batch/s, auc=0.8652, loss=0.6733]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 181/292 [01:57<01:10,  1.58batch/s, auc=0.8652, loss=0.6733]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 181/292 [01:58<01:10,  1.58batch/s, auc=0.8654, loss=0.7334]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 182/292 [01:58<01:07,  1.64batch/s, auc=0.8654, loss=0.7334]\u001b[A\n",
      "Training Epoch 3/25:  62%|██████▏   | 182/292 [01:58<01:07,  1.64batch/s, auc=0.8657, loss=0.6253]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 183/292 [01:58<01:04,  1.68batch/s, auc=0.8657, loss=0.6253]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 183/292 [01:59<01:04,  1.68batch/s, auc=0.8656, loss=0.7290]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 184/292 [01:59<01:03,  1.71batch/s, auc=0.8656, loss=0.7290]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 184/292 [01:59<01:03,  1.71batch/s, auc=0.8657, loss=0.6207]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 185/292 [01:59<01:01,  1.73batch/s, auc=0.8657, loss=0.6207]\u001b[A\n",
      "Training Epoch 3/25:  63%|██████▎   | 185/292 [02:00<01:01,  1.73batch/s, auc=0.8658, loss=0.6533]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▎   | 186/292 [02:00<01:00,  1.74batch/s, auc=0.8658, loss=0.6533]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▎   | 186/292 [02:00<01:00,  1.74batch/s, auc=0.8654, loss=0.8561]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 187/292 [02:00<00:59,  1.76batch/s, auc=0.8654, loss=0.8561]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 187/292 [02:01<00:59,  1.76batch/s, auc=0.8655, loss=0.7009]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 188/292 [02:01<00:59,  1.76batch/s, auc=0.8655, loss=0.7009]\u001b[A\n",
      "Training Epoch 3/25:  64%|██████▍   | 188/292 [02:02<00:59,  1.76batch/s, auc=0.8657, loss=0.8620]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▍   | 189/292 [02:02<00:58,  1.77batch/s, auc=0.8657, loss=0.8620]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▍   | 189/292 [02:03<00:58,  1.77batch/s, auc=0.8650, loss=1.2434]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 190/292 [02:03<01:13,  1.39batch/s, auc=0.8650, loss=1.2434]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 190/292 [02:03<01:13,  1.39batch/s, auc=0.8654, loss=0.6092]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 191/292 [02:03<01:07,  1.49batch/s, auc=0.8654, loss=0.6092]\u001b[A\n",
      "Training Epoch 3/25:  65%|██████▌   | 191/292 [02:04<01:07,  1.49batch/s, auc=0.8654, loss=0.9122]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 192/292 [02:04<01:03,  1.57batch/s, auc=0.8654, loss=0.9122]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 192/292 [02:04<01:03,  1.57batch/s, auc=0.8653, loss=0.7015]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 193/292 [02:04<01:00,  1.63batch/s, auc=0.8653, loss=0.7015]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▌   | 193/292 [02:05<01:00,  1.63batch/s, auc=0.8656, loss=0.6558]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▋   | 194/292 [02:05<00:58,  1.67batch/s, auc=0.8656, loss=0.6558]\u001b[A\n",
      "Training Epoch 3/25:  66%|██████▋   | 194/292 [02:05<00:58,  1.67batch/s, auc=0.8658, loss=0.5652]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 195/292 [02:05<00:56,  1.70batch/s, auc=0.8658, loss=0.5652]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 195/292 [02:06<00:56,  1.70batch/s, auc=0.8660, loss=0.5829]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 196/292 [02:06<00:55,  1.73batch/s, auc=0.8660, loss=0.5829]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 196/292 [02:07<00:55,  1.73batch/s, auc=0.8659, loss=0.8091]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 197/292 [02:07<00:54,  1.74batch/s, auc=0.8659, loss=0.8091]\u001b[A\n",
      "Training Epoch 3/25:  67%|██████▋   | 197/292 [02:07<00:54,  1.74batch/s, auc=0.8661, loss=0.6631]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 198/292 [02:07<00:53,  1.76batch/s, auc=0.8661, loss=0.6631]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 198/292 [02:08<00:53,  1.76batch/s, auc=0.8664, loss=0.5210]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 199/292 [02:08<00:52,  1.76batch/s, auc=0.8664, loss=0.5210]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 199/292 [02:08<00:52,  1.76batch/s, auc=0.8663, loss=0.8833]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 200/292 [02:08<00:52,  1.77batch/s, auc=0.8663, loss=0.8833]\u001b[A\n",
      "Training Epoch 3/25:  68%|██████▊   | 200/292 [02:09<00:52,  1.77batch/s, auc=0.8661, loss=0.8658]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 201/292 [02:09<00:51,  1.77batch/s, auc=0.8661, loss=0.8658]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 201/292 [02:09<00:51,  1.77batch/s, auc=0.8663, loss=0.6873]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 202/292 [02:09<00:50,  1.77batch/s, auc=0.8663, loss=0.6873]\u001b[A\n",
      "Training Epoch 3/25:  69%|██████▉   | 202/292 [02:10<00:50,  1.77batch/s, auc=0.8667, loss=0.6014]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 203/292 [02:10<00:50,  1.78batch/s, auc=0.8667, loss=0.6014]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 203/292 [02:11<00:50,  1.78batch/s, auc=0.8660, loss=1.3574]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 204/292 [02:11<01:03,  1.39batch/s, auc=0.8660, loss=1.3574]\u001b[A\n",
      "Training Epoch 3/25:  70%|██████▉   | 204/292 [02:12<01:03,  1.39batch/s, auc=0.8660, loss=0.7036]\u001b[A\n",
      "Training Epoch 3/25:  70%|███████   | 205/292 [02:12<00:58,  1.49batch/s, auc=0.8660, loss=0.7036]\u001b[A\n",
      "Training Epoch 3/25:  70%|███████   | 205/292 [02:12<00:58,  1.49batch/s, auc=0.8659, loss=0.8206]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 206/292 [02:12<00:54,  1.57batch/s, auc=0.8659, loss=0.8206]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 206/292 [02:13<00:54,  1.57batch/s, auc=0.8661, loss=0.9794]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 207/292 [02:13<00:52,  1.63batch/s, auc=0.8661, loss=0.9794]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 207/292 [02:13<00:52,  1.63batch/s, auc=0.8662, loss=0.8411]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 208/292 [02:13<00:50,  1.67batch/s, auc=0.8662, loss=0.8411]\u001b[A\n",
      "Training Epoch 3/25:  71%|███████   | 208/292 [02:14<00:50,  1.67batch/s, auc=0.8661, loss=0.7316]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 209/292 [02:14<00:48,  1.70batch/s, auc=0.8661, loss=0.7316]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 209/292 [02:15<00:48,  1.70batch/s, auc=0.8656, loss=0.9679]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 210/292 [02:15<01:00,  1.36batch/s, auc=0.8656, loss=0.9679]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 210/292 [02:16<01:00,  1.36batch/s, auc=0.8650, loss=1.0311]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 211/292 [02:16<01:07,  1.20batch/s, auc=0.8650, loss=1.0311]\u001b[A\n",
      "Training Epoch 3/25:  72%|███████▏  | 211/292 [02:17<01:07,  1.20batch/s, auc=0.8650, loss=0.7153]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 212/292 [02:17<01:00,  1.33batch/s, auc=0.8650, loss=0.7153]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 212/292 [02:17<01:00,  1.33batch/s, auc=0.8647, loss=0.8793]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 213/292 [02:17<00:54,  1.44batch/s, auc=0.8647, loss=0.8793]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 213/292 [02:18<00:54,  1.44batch/s, auc=0.8650, loss=0.6456]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 214/292 [02:18<00:51,  1.53batch/s, auc=0.8650, loss=0.6456]\u001b[A\n",
      "Training Epoch 3/25:  73%|███████▎  | 214/292 [02:18<00:51,  1.53batch/s, auc=0.8649, loss=0.8535]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▎  | 215/292 [02:18<00:48,  1.60batch/s, auc=0.8649, loss=0.8535]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▎  | 215/292 [02:19<00:48,  1.60batch/s, auc=0.8647, loss=0.8551]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 216/292 [02:19<00:46,  1.65batch/s, auc=0.8647, loss=0.8551]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 216/292 [02:20<00:46,  1.65batch/s, auc=0.8641, loss=1.2549]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 217/292 [02:20<00:56,  1.34batch/s, auc=0.8641, loss=1.2549]\u001b[A\n",
      "Training Epoch 3/25:  74%|███████▍  | 217/292 [02:20<00:56,  1.34batch/s, auc=0.8643, loss=0.6328]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▍  | 218/292 [02:20<00:51,  1.45batch/s, auc=0.8643, loss=0.6328]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▍  | 218/292 [02:21<00:51,  1.45batch/s, auc=0.8645, loss=0.6655]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 219/292 [02:21<00:47,  1.53batch/s, auc=0.8645, loss=0.6655]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 219/292 [02:22<00:47,  1.53batch/s, auc=0.8649, loss=0.8754]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 220/292 [02:22<00:44,  1.60batch/s, auc=0.8649, loss=0.8754]\u001b[A\n",
      "Training Epoch 3/25:  75%|███████▌  | 220/292 [02:22<00:44,  1.60batch/s, auc=0.8649, loss=0.8891]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 221/292 [02:22<00:42,  1.65batch/s, auc=0.8649, loss=0.8891]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 221/292 [02:23<00:42,  1.65batch/s, auc=0.8649, loss=1.0284]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 222/292 [02:23<00:41,  1.69batch/s, auc=0.8649, loss=1.0284]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▌  | 222/292 [02:23<00:41,  1.69batch/s, auc=0.8648, loss=0.7403]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▋  | 223/292 [02:23<00:40,  1.72batch/s, auc=0.8648, loss=0.7403]\u001b[A\n",
      "Training Epoch 3/25:  76%|███████▋  | 223/292 [02:24<00:40,  1.72batch/s, auc=0.8650, loss=0.6280]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 224/292 [02:24<00:39,  1.74batch/s, auc=0.8650, loss=0.6280]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 224/292 [02:24<00:39,  1.74batch/s, auc=0.8649, loss=0.7527]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 225/292 [02:24<00:38,  1.75batch/s, auc=0.8649, loss=0.7527]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 225/292 [02:25<00:38,  1.75batch/s, auc=0.8647, loss=0.9368]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 226/292 [02:25<00:47,  1.38batch/s, auc=0.8647, loss=0.9368]\u001b[A\n",
      "Training Epoch 3/25:  77%|███████▋  | 226/292 [02:27<00:47,  1.38batch/s, auc=0.8642, loss=1.0195]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 227/292 [02:27<00:53,  1.21batch/s, auc=0.8642, loss=1.0195]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 227/292 [02:28<00:53,  1.21batch/s, auc=0.8636, loss=1.2685]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 228/292 [02:28<00:57,  1.11batch/s, auc=0.8636, loss=1.2685]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 228/292 [02:28<00:57,  1.11batch/s, auc=0.8634, loss=0.8380]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 229/292 [02:28<00:50,  1.25batch/s, auc=0.8634, loss=0.8380]\u001b[A\n",
      "Training Epoch 3/25:  78%|███████▊  | 229/292 [02:29<00:50,  1.25batch/s, auc=0.8633, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 230/292 [02:29<00:45,  1.37batch/s, auc=0.8633, loss=0.6859]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 230/292 [02:29<00:45,  1.37batch/s, auc=0.8631, loss=0.7809]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 231/292 [02:29<00:41,  1.47batch/s, auc=0.8631, loss=0.7809]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 231/292 [02:30<00:41,  1.47batch/s, auc=0.8631, loss=0.6840]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 232/292 [02:30<00:38,  1.55batch/s, auc=0.8631, loss=0.6840]\u001b[A\n",
      "Training Epoch 3/25:  79%|███████▉  | 232/292 [02:30<00:38,  1.55batch/s, auc=0.8632, loss=0.7516]\u001b[A\n",
      "Training Epoch 3/25:  80%|███████▉  | 233/292 [02:30<00:36,  1.61batch/s, auc=0.8632, loss=0.7516]\u001b[A\n",
      "Training Epoch 3/25:  80%|███████▉  | 233/292 [02:31<00:36,  1.61batch/s, auc=0.8630, loss=0.8040]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 234/292 [02:31<00:43,  1.32batch/s, auc=0.8630, loss=0.8040]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 234/292 [02:32<00:43,  1.32batch/s, auc=0.8631, loss=0.6072]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 235/292 [02:32<00:39,  1.43batch/s, auc=0.8631, loss=0.6072]\u001b[A\n",
      "Training Epoch 3/25:  80%|████████  | 235/292 [02:33<00:39,  1.43batch/s, auc=0.8636, loss=0.5407]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 236/292 [02:33<00:36,  1.52batch/s, auc=0.8636, loss=0.5407]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 236/292 [02:33<00:36,  1.52batch/s, auc=0.8639, loss=0.7387]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 237/292 [02:33<00:34,  1.59batch/s, auc=0.8639, loss=0.7387]\u001b[A\n",
      "Training Epoch 3/25:  81%|████████  | 237/292 [02:34<00:34,  1.59batch/s, auc=0.8637, loss=1.0124]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 238/292 [02:34<00:41,  1.31batch/s, auc=0.8637, loss=1.0124]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 238/292 [02:35<00:41,  1.31batch/s, auc=0.8635, loss=0.9256]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 239/292 [02:35<00:45,  1.17batch/s, auc=0.8635, loss=0.9256]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 239/292 [02:36<00:45,  1.17batch/s, auc=0.8636, loss=0.8151]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 240/292 [02:36<00:40,  1.30batch/s, auc=0.8636, loss=0.8151]\u001b[A\n",
      "Training Epoch 3/25:  82%|████████▏ | 240/292 [02:36<00:40,  1.30batch/s, auc=0.8637, loss=0.7899]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 241/292 [02:36<00:36,  1.41batch/s, auc=0.8637, loss=0.7899]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 241/292 [02:38<00:36,  1.41batch/s, auc=0.8633, loss=1.0541]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 242/292 [02:38<00:40,  1.22batch/s, auc=0.8633, loss=1.0541]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 242/292 [02:38<00:40,  1.22batch/s, auc=0.8633, loss=0.6180]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 243/292 [02:38<00:36,  1.35batch/s, auc=0.8633, loss=0.6180]\u001b[A\n",
      "Training Epoch 3/25:  83%|████████▎ | 243/292 [02:39<00:36,  1.35batch/s, auc=0.8634, loss=0.6491]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▎ | 244/292 [02:39<00:33,  1.45batch/s, auc=0.8634, loss=0.6491]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▎ | 244/292 [02:39<00:33,  1.45batch/s, auc=0.8637, loss=0.6297]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 245/292 [02:39<00:30,  1.54batch/s, auc=0.8637, loss=0.6297]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 245/292 [02:40<00:30,  1.54batch/s, auc=0.8639, loss=0.6232]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 246/292 [02:40<00:28,  1.60batch/s, auc=0.8639, loss=0.6232]\u001b[A\n",
      "Training Epoch 3/25:  84%|████████▍ | 246/292 [02:41<00:28,  1.60batch/s, auc=0.8635, loss=1.0181]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 247/292 [02:41<00:34,  1.31batch/s, auc=0.8635, loss=1.0181]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 247/292 [02:41<00:34,  1.31batch/s, auc=0.8637, loss=0.5979]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 248/292 [02:41<00:30,  1.42batch/s, auc=0.8637, loss=0.5979]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▍ | 248/292 [02:42<00:30,  1.42batch/s, auc=0.8635, loss=1.2395]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▌ | 249/292 [02:42<00:28,  1.52batch/s, auc=0.8635, loss=1.2395]\u001b[A\n",
      "Training Epoch 3/25:  85%|████████▌ | 249/292 [02:43<00:28,  1.52batch/s, auc=0.8633, loss=0.7639]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 250/292 [02:43<00:26,  1.59batch/s, auc=0.8633, loss=0.7639]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 250/292 [02:43<00:26,  1.59batch/s, auc=0.8637, loss=0.5310]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 251/292 [02:43<00:25,  1.64batch/s, auc=0.8637, loss=0.5310]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▌ | 251/292 [02:44<00:25,  1.64batch/s, auc=0.8639, loss=0.7138]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▋ | 252/292 [02:44<00:23,  1.68batch/s, auc=0.8639, loss=0.7138]\u001b[A\n",
      "Training Epoch 3/25:  86%|████████▋ | 252/292 [02:44<00:23,  1.68batch/s, auc=0.8641, loss=0.6555]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 253/292 [02:44<00:22,  1.71batch/s, auc=0.8641, loss=0.6555]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 253/292 [02:45<00:22,  1.71batch/s, auc=0.8641, loss=0.6890]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 254/292 [02:45<00:21,  1.73batch/s, auc=0.8641, loss=0.6890]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 254/292 [02:45<00:21,  1.73batch/s, auc=0.8642, loss=0.5929]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 255/292 [02:45<00:21,  1.74batch/s, auc=0.8642, loss=0.5929]\u001b[A\n",
      "Training Epoch 3/25:  87%|████████▋ | 255/292 [02:46<00:21,  1.74batch/s, auc=0.8644, loss=0.6506]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 256/292 [02:46<00:20,  1.75batch/s, auc=0.8644, loss=0.6506]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 256/292 [02:47<00:20,  1.75batch/s, auc=0.8645, loss=0.6292]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 257/292 [02:47<00:19,  1.76batch/s, auc=0.8645, loss=0.6292]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 257/292 [02:47<00:19,  1.76batch/s, auc=0.8646, loss=0.8157]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 258/292 [02:47<00:19,  1.77batch/s, auc=0.8646, loss=0.8157]\u001b[A\n",
      "Training Epoch 3/25:  88%|████████▊ | 258/292 [02:48<00:19,  1.77batch/s, auc=0.8648, loss=0.7920]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▊ | 259/292 [02:48<00:18,  1.77batch/s, auc=0.8648, loss=0.7920]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▊ | 259/292 [02:48<00:18,  1.77batch/s, auc=0.8651, loss=0.6219]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 260/292 [02:48<00:18,  1.77batch/s, auc=0.8651, loss=0.6219]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 260/292 [02:49<00:18,  1.77batch/s, auc=0.8655, loss=0.4952]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 261/292 [02:49<00:17,  1.77batch/s, auc=0.8655, loss=0.4952]\u001b[A\n",
      "Training Epoch 3/25:  89%|████████▉ | 261/292 [02:49<00:17,  1.77batch/s, auc=0.8651, loss=0.9441]\u001b[A\n",
      "Training Epoch 3/25:  90%|████████▉ | 262/292 [02:49<00:16,  1.78batch/s, auc=0.8651, loss=0.9441]\u001b[A\n",
      "Training Epoch 3/25:  90%|████████▉ | 262/292 [02:50<00:16,  1.78batch/s, auc=0.8648, loss=1.0078]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 263/292 [02:50<00:20,  1.39batch/s, auc=0.8648, loss=1.0078]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 263/292 [02:51<00:20,  1.39batch/s, auc=0.8646, loss=0.8027]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 264/292 [02:51<00:18,  1.49batch/s, auc=0.8646, loss=0.8027]\u001b[A\n",
      "Training Epoch 3/25:  90%|█████████ | 264/292 [02:52<00:18,  1.49batch/s, auc=0.8646, loss=0.6862]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 265/292 [02:52<00:21,  1.26batch/s, auc=0.8646, loss=0.6862]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 265/292 [02:53<00:21,  1.26batch/s, auc=0.8649, loss=0.5423]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 266/292 [02:53<00:18,  1.38batch/s, auc=0.8649, loss=0.5423]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████ | 266/292 [02:53<00:18,  1.38batch/s, auc=0.8649, loss=0.8783]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████▏| 267/292 [02:53<00:16,  1.48batch/s, auc=0.8649, loss=0.8783]\u001b[A\n",
      "Training Epoch 3/25:  91%|█████████▏| 267/292 [02:54<00:16,  1.48batch/s, auc=0.8651, loss=0.5895]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 268/292 [02:54<00:15,  1.56batch/s, auc=0.8651, loss=0.5895]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 268/292 [02:54<00:15,  1.56batch/s, auc=0.8653, loss=0.5584]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 269/292 [02:54<00:14,  1.61batch/s, auc=0.8653, loss=0.5584]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 269/292 [02:55<00:14,  1.61batch/s, auc=0.8653, loss=0.8446]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 270/292 [02:55<00:13,  1.66batch/s, auc=0.8653, loss=0.8446]\u001b[A\n",
      "Training Epoch 3/25:  92%|█████████▏| 270/292 [02:55<00:13,  1.66batch/s, auc=0.8651, loss=1.1084]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 271/292 [02:55<00:12,  1.69batch/s, auc=0.8651, loss=1.1084]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 271/292 [02:57<00:12,  1.69batch/s, auc=0.8647, loss=1.1482]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 272/292 [02:57<00:14,  1.36batch/s, auc=0.8647, loss=1.1482]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 272/292 [02:57<00:14,  1.36batch/s, auc=0.8644, loss=1.1058]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 273/292 [02:57<00:13,  1.46batch/s, auc=0.8644, loss=1.1058]\u001b[A\n",
      "Training Epoch 3/25:  93%|█████████▎| 273/292 [02:58<00:13,  1.46batch/s, auc=0.8644, loss=0.6065]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 274/292 [02:58<00:11,  1.54batch/s, auc=0.8644, loss=0.6065]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 274/292 [02:58<00:11,  1.54batch/s, auc=0.8646, loss=0.5158]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 275/292 [02:58<00:10,  1.60batch/s, auc=0.8646, loss=0.5158]\u001b[A\n",
      "Training Epoch 3/25:  94%|█████████▍| 275/292 [02:59<00:10,  1.60batch/s, auc=0.8643, loss=1.2871]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 276/292 [02:59<00:12,  1.31batch/s, auc=0.8643, loss=1.2871]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 276/292 [03:00<00:12,  1.31batch/s, auc=0.8642, loss=0.6158]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 277/292 [03:00<00:10,  1.43batch/s, auc=0.8642, loss=0.6158]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▍| 277/292 [03:00<00:10,  1.43batch/s, auc=0.8644, loss=0.5690]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▌| 278/292 [03:00<00:09,  1.52batch/s, auc=0.8644, loss=0.5690]\u001b[A\n",
      "Training Epoch 3/25:  95%|█████████▌| 278/292 [03:01<00:09,  1.52batch/s, auc=0.8644, loss=0.7249]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 279/292 [03:01<00:08,  1.59batch/s, auc=0.8644, loss=0.7249]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 279/292 [03:02<00:08,  1.59batch/s, auc=0.8645, loss=0.6422]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 280/292 [03:02<00:07,  1.64batch/s, auc=0.8645, loss=0.6422]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 280/292 [03:02<00:07,  1.64batch/s, auc=0.8645, loss=0.7623]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 281/292 [03:02<00:06,  1.67batch/s, auc=0.8645, loss=0.7623]\u001b[A\n",
      "Training Epoch 3/25:  96%|█████████▌| 281/292 [03:03<00:06,  1.67batch/s, auc=0.8644, loss=0.9226]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 282/292 [03:03<00:05,  1.70batch/s, auc=0.8644, loss=0.9226]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 282/292 [03:04<00:05,  1.70batch/s, auc=0.8640, loss=1.0231]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 283/292 [03:04<00:06,  1.36batch/s, auc=0.8640, loss=1.0231]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 283/292 [03:04<00:06,  1.36batch/s, auc=0.8642, loss=0.5589]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 284/292 [03:04<00:05,  1.46batch/s, auc=0.8642, loss=0.5589]\u001b[A\n",
      "Training Epoch 3/25:  97%|█████████▋| 284/292 [03:05<00:05,  1.46batch/s, auc=0.8646, loss=0.6835]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 285/292 [03:05<00:04,  1.54batch/s, auc=0.8646, loss=0.6835]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 285/292 [03:05<00:04,  1.54batch/s, auc=0.8645, loss=0.8516]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 286/292 [03:05<00:03,  1.60batch/s, auc=0.8645, loss=0.8516]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 286/292 [03:07<00:03,  1.60batch/s, auc=0.8639, loss=1.3790]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 287/292 [03:07<00:03,  1.31batch/s, auc=0.8639, loss=1.3790]\u001b[A\n",
      "Training Epoch 3/25:  98%|█████████▊| 287/292 [03:07<00:03,  1.31batch/s, auc=0.8637, loss=0.9322]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▊| 288/292 [03:07<00:02,  1.43batch/s, auc=0.8637, loss=0.9322]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▊| 288/292 [03:08<00:02,  1.43batch/s, auc=0.8640, loss=0.7154]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 289/292 [03:08<00:01,  1.51batch/s, auc=0.8640, loss=0.7154]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 289/292 [03:08<00:01,  1.51batch/s, auc=0.8639, loss=0.7498]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 290/292 [03:08<00:01,  1.58batch/s, auc=0.8639, loss=0.7498]\u001b[A\n",
      "Training Epoch 3/25:  99%|█████████▉| 290/292 [03:09<00:01,  1.58batch/s, auc=0.8643, loss=0.5019]\u001b[A\n",
      "Training Epoch 3/25: 100%|█████████▉| 291/292 [03:09<00:00,  1.63batch/s, auc=0.8643, loss=0.5019]\u001b[A\n",
      "Training Epoch 3/25: 100%|█████████▉| 291/292 [03:09<00:00,  1.63batch/s, auc=0.8643, loss=0.8494]\u001b[A\n",
      "Training Epoch 3/25: 100%|██████████| 292/292 [03:09<00:00,  1.54batch/s, auc=0.8643, loss=0.8494]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25] Train Loss: 0.7765 | Train AUROC: 0.8643 Val Loss: 0.7696 | Val AUROC: 0.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▏        | 3/25 [10:27<1:16:39, 209.07s/it]\n",
      "Training Epoch 4/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8500, loss=0.7009]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 1/292 [00:01<07:10,  1.48s/batch, auc=0.8500, loss=0.7009]\u001b[A\n",
      "Training Epoch 4/25:   0%|          | 1/292 [00:02<07:10,  1.48s/batch, auc=0.8861, loss=0.5846]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.8861, loss=0.5846]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.8968, loss=0.6209]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 3/292 [00:02<03:39,  1.32batch/s, auc=0.8968, loss=0.6209]\u001b[A\n",
      "Training Epoch 4/25:   1%|          | 3/292 [00:03<03:39,  1.32batch/s, auc=0.9003, loss=0.6057]\u001b[A\n",
      "Training Epoch 4/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.9003, loss=0.6057]\u001b[A\n",
      "Training Epoch 4/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.8922, loss=0.6926]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 5/292 [00:03<03:01,  1.58batch/s, auc=0.8922, loss=0.6926]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 5/292 [00:04<03:01,  1.58batch/s, auc=0.8872, loss=0.7020]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.8872, loss=0.7020]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.8829, loss=0.7332]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 7/292 [00:04<02:47,  1.70batch/s, auc=0.8829, loss=0.7332]\u001b[A\n",
      "Training Epoch 4/25:   2%|▏         | 7/292 [00:05<02:47,  1.70batch/s, auc=0.8389, loss=1.8484]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 8/292 [00:05<03:30,  1.35batch/s, auc=0.8389, loss=1.8484]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 8/292 [00:06<03:30,  1.35batch/s, auc=0.8535, loss=0.4435]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 9/292 [00:06<03:13,  1.46batch/s, auc=0.8535, loss=0.4435]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 9/292 [00:06<03:13,  1.46batch/s, auc=0.8516, loss=0.8045]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 10/292 [00:06<03:01,  1.55batch/s, auc=0.8516, loss=0.8045]\u001b[A\n",
      "Training Epoch 4/25:   3%|▎         | 10/292 [00:07<03:01,  1.55batch/s, auc=0.8449, loss=0.9490]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 11/292 [00:07<02:53,  1.62batch/s, auc=0.8449, loss=0.9490]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 11/292 [00:08<02:53,  1.62batch/s, auc=0.8451, loss=0.7237]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 12/292 [00:08<02:47,  1.67batch/s, auc=0.8451, loss=0.7237]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 12/292 [00:09<02:47,  1.67batch/s, auc=0.8198, loss=1.7501]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 13/292 [00:09<03:26,  1.35batch/s, auc=0.8198, loss=1.7501]\u001b[A\n",
      "Training Epoch 4/25:   4%|▍         | 13/292 [00:10<03:26,  1.35batch/s, auc=0.8146, loss=0.9971]\u001b[A\n",
      "Training Epoch 4/25:   5%|▍         | 14/292 [00:10<03:53,  1.19batch/s, auc=0.8146, loss=0.9971]\u001b[A\n",
      "Training Epoch 4/25:   5%|▍         | 14/292 [00:10<03:53,  1.19batch/s, auc=0.8220, loss=0.7065]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 15/292 [00:10<03:28,  1.33batch/s, auc=0.8220, loss=0.7065]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 15/292 [00:11<03:28,  1.33batch/s, auc=0.8316, loss=0.5072]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 16/292 [00:11<03:11,  1.44batch/s, auc=0.8316, loss=0.5072]\u001b[A\n",
      "Training Epoch 4/25:   5%|▌         | 16/292 [00:11<03:11,  1.44batch/s, auc=0.8429, loss=0.6164]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 17/292 [00:11<02:59,  1.53batch/s, auc=0.8429, loss=0.6164]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 17/292 [00:12<02:59,  1.53batch/s, auc=0.8387, loss=0.9883]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 18/292 [00:12<03:33,  1.28batch/s, auc=0.8387, loss=0.9883]\u001b[A\n",
      "Training Epoch 4/25:   6%|▌         | 18/292 [00:13<03:33,  1.28batch/s, auc=0.8456, loss=0.6961]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 19/292 [00:13<03:14,  1.41batch/s, auc=0.8456, loss=0.6961]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 19/292 [00:14<03:14,  1.41batch/s, auc=0.8492, loss=0.6997]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 20/292 [00:14<03:00,  1.50batch/s, auc=0.8492, loss=0.6997]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 20/292 [00:14<03:00,  1.50batch/s, auc=0.8485, loss=0.8393]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 21/292 [00:14<02:51,  1.58batch/s, auc=0.8485, loss=0.8393]\u001b[A\n",
      "Training Epoch 4/25:   7%|▋         | 21/292 [00:15<02:51,  1.58batch/s, auc=0.8442, loss=0.9979]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 22/292 [00:15<02:44,  1.64batch/s, auc=0.8442, loss=0.9979]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 22/292 [00:15<02:44,  1.64batch/s, auc=0.8455, loss=0.7406]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 23/292 [00:15<02:39,  1.69batch/s, auc=0.8455, loss=0.7406]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 23/292 [00:16<02:39,  1.69batch/s, auc=0.8479, loss=0.6812]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 24/292 [00:16<02:35,  1.72batch/s, auc=0.8479, loss=0.6812]\u001b[A\n",
      "Training Epoch 4/25:   8%|▊         | 24/292 [00:16<02:35,  1.72batch/s, auc=0.8510, loss=0.8599]\u001b[A\n",
      "Training Epoch 4/25:   9%|▊         | 25/292 [00:16<02:33,  1.74batch/s, auc=0.8510, loss=0.8599]\u001b[A\n",
      "Training Epoch 4/25:   9%|▊         | 25/292 [00:17<02:33,  1.74batch/s, auc=0.8525, loss=0.5880]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 26/292 [00:17<02:31,  1.76batch/s, auc=0.8525, loss=0.5880]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 26/292 [00:17<02:31,  1.76batch/s, auc=0.8549, loss=0.8057]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 27/292 [00:17<02:29,  1.77batch/s, auc=0.8549, loss=0.8057]\u001b[A\n",
      "Training Epoch 4/25:   9%|▉         | 27/292 [00:18<02:29,  1.77batch/s, auc=0.8563, loss=0.5915]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 28/292 [00:18<02:28,  1.78batch/s, auc=0.8563, loss=0.5915]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 28/292 [00:19<02:28,  1.78batch/s, auc=0.8598, loss=0.6463]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 29/292 [00:19<02:27,  1.78batch/s, auc=0.8598, loss=0.6463]\u001b[A\n",
      "Training Epoch 4/25:  10%|▉         | 29/292 [00:19<02:27,  1.78batch/s, auc=0.8610, loss=0.6970]\u001b[A\n",
      "Training Epoch 4/25:  10%|█         | 30/292 [00:19<02:26,  1.79batch/s, auc=0.8610, loss=0.6970]\u001b[A\n",
      "Training Epoch 4/25:  10%|█         | 30/292 [00:20<02:26,  1.79batch/s, auc=0.8636, loss=0.6466]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 31/292 [00:20<02:25,  1.79batch/s, auc=0.8636, loss=0.6466]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 31/292 [00:20<02:25,  1.79batch/s, auc=0.8618, loss=0.8602]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 32/292 [00:20<02:24,  1.80batch/s, auc=0.8618, loss=0.8602]\u001b[A\n",
      "Training Epoch 4/25:  11%|█         | 32/292 [00:21<02:24,  1.80batch/s, auc=0.8625, loss=0.7886]\u001b[A\n",
      "Training Epoch 4/25:  11%|█▏        | 33/292 [00:21<03:04,  1.41batch/s, auc=0.8625, loss=0.7886]\u001b[A\n",
      "Training Epoch 4/25:  11%|█▏        | 33/292 [00:22<03:04,  1.41batch/s, auc=0.8586, loss=0.9800]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 34/292 [00:22<03:31,  1.22batch/s, auc=0.8586, loss=0.9800]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 34/292 [00:23<03:31,  1.22batch/s, auc=0.8604, loss=0.6648]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 35/292 [00:23<03:10,  1.35batch/s, auc=0.8604, loss=0.6648]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 35/292 [00:23<03:10,  1.35batch/s, auc=0.8635, loss=0.7004]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 36/292 [00:23<02:55,  1.46batch/s, auc=0.8635, loss=0.7004]\u001b[A\n",
      "Training Epoch 4/25:  12%|█▏        | 36/292 [00:24<02:55,  1.46batch/s, auc=0.8656, loss=0.5822]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 37/292 [00:24<02:44,  1.55batch/s, auc=0.8656, loss=0.5822]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 37/292 [00:25<02:44,  1.55batch/s, auc=0.8654, loss=0.7640]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 38/292 [00:25<03:16,  1.29batch/s, auc=0.8654, loss=0.7640]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 38/292 [00:26<03:16,  1.29batch/s, auc=0.8696, loss=0.5460]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 39/292 [00:26<02:59,  1.41batch/s, auc=0.8696, loss=0.5460]\u001b[A\n",
      "Training Epoch 4/25:  13%|█▎        | 39/292 [00:26<02:59,  1.41batch/s, auc=0.8687, loss=0.9349]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▎        | 40/292 [00:26<02:46,  1.51batch/s, auc=0.8687, loss=0.9349]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▎        | 40/292 [00:27<02:46,  1.51batch/s, auc=0.8632, loss=1.4808]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 41/292 [00:27<03:17,  1.27batch/s, auc=0.8632, loss=1.4808]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 41/292 [00:28<03:17,  1.27batch/s, auc=0.8644, loss=0.5608]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 42/292 [00:28<02:59,  1.40batch/s, auc=0.8644, loss=0.5608]\u001b[A\n",
      "Training Epoch 4/25:  14%|█▍        | 42/292 [00:28<02:59,  1.40batch/s, auc=0.8652, loss=0.7049]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▍        | 43/292 [00:28<02:46,  1.50batch/s, auc=0.8652, loss=0.7049]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▍        | 43/292 [00:29<02:46,  1.50batch/s, auc=0.8644, loss=0.7206]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 44/292 [00:29<02:37,  1.58batch/s, auc=0.8644, loss=0.7206]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 44/292 [00:30<02:37,  1.58batch/s, auc=0.8672, loss=0.5278]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 45/292 [00:30<02:30,  1.64batch/s, auc=0.8672, loss=0.5278]\u001b[A\n",
      "Training Epoch 4/25:  15%|█▌        | 45/292 [00:30<02:30,  1.64batch/s, auc=0.8687, loss=0.6469]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 46/292 [00:30<02:26,  1.68batch/s, auc=0.8687, loss=0.6469]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 46/292 [00:31<02:26,  1.68batch/s, auc=0.8687, loss=0.6947]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 47/292 [00:31<02:22,  1.72batch/s, auc=0.8687, loss=0.6947]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▌        | 47/292 [00:31<02:22,  1.72batch/s, auc=0.8688, loss=0.7616]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▋        | 48/292 [00:31<02:20,  1.74batch/s, auc=0.8688, loss=0.7616]\u001b[A\n",
      "Training Epoch 4/25:  16%|█▋        | 48/292 [00:32<02:20,  1.74batch/s, auc=0.8697, loss=0.5593]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 49/292 [00:32<02:18,  1.76batch/s, auc=0.8697, loss=0.5593]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 49/292 [00:32<02:18,  1.76batch/s, auc=0.8689, loss=1.0099]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 50/292 [00:32<02:16,  1.77batch/s, auc=0.8689, loss=1.0099]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 50/292 [00:33<02:16,  1.77batch/s, auc=0.8709, loss=0.4624]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 51/292 [00:33<02:15,  1.78batch/s, auc=0.8709, loss=0.4624]\u001b[A\n",
      "Training Epoch 4/25:  17%|█▋        | 51/292 [00:33<02:15,  1.78batch/s, auc=0.8714, loss=0.6653]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 52/292 [00:33<02:14,  1.78batch/s, auc=0.8714, loss=0.6653]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 52/292 [00:34<02:14,  1.78batch/s, auc=0.8729, loss=0.5294]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 53/292 [00:34<02:13,  1.79batch/s, auc=0.8729, loss=0.5294]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 53/292 [00:35<02:13,  1.79batch/s, auc=0.8731, loss=0.6236]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 54/292 [00:35<02:13,  1.79batch/s, auc=0.8731, loss=0.6236]\u001b[A\n",
      "Training Epoch 4/25:  18%|█▊        | 54/292 [00:36<02:13,  1.79batch/s, auc=0.8709, loss=1.1076]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 55/292 [00:36<02:49,  1.40batch/s, auc=0.8709, loss=1.1076]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 55/292 [00:37<02:49,  1.40batch/s, auc=0.8669, loss=1.4412]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 56/292 [00:37<03:13,  1.22batch/s, auc=0.8669, loss=1.4412]\u001b[A\n",
      "Training Epoch 4/25:  19%|█▉        | 56/292 [00:37<03:13,  1.22batch/s, auc=0.8673, loss=0.6613]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 57/292 [00:37<02:54,  1.35batch/s, auc=0.8673, loss=0.6613]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 57/292 [00:38<02:54,  1.35batch/s, auc=0.8672, loss=0.8539]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 58/292 [00:38<02:40,  1.46batch/s, auc=0.8672, loss=0.8539]\u001b[A\n",
      "Training Epoch 4/25:  20%|█▉        | 58/292 [00:39<02:40,  1.46batch/s, auc=0.8653, loss=1.0422]\u001b[A\n",
      "Training Epoch 4/25:  20%|██        | 59/292 [00:39<03:06,  1.25batch/s, auc=0.8653, loss=1.0422]\u001b[A\n",
      "Training Epoch 4/25:  20%|██        | 59/292 [00:39<03:06,  1.25batch/s, auc=0.8663, loss=0.6146]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 60/292 [00:39<02:48,  1.37batch/s, auc=0.8663, loss=0.6146]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 60/292 [00:41<02:48,  1.37batch/s, auc=0.8643, loss=1.3210]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 61/292 [00:41<03:12,  1.20batch/s, auc=0.8643, loss=1.3210]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 61/292 [00:41<03:12,  1.20batch/s, auc=0.8652, loss=0.5843]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 62/292 [00:41<02:52,  1.33batch/s, auc=0.8652, loss=0.5843]\u001b[A\n",
      "Training Epoch 4/25:  21%|██        | 62/292 [00:42<02:52,  1.33batch/s, auc=0.8654, loss=0.7148]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 63/292 [00:42<02:38,  1.45batch/s, auc=0.8654, loss=0.7148]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 63/292 [00:43<02:38,  1.45batch/s, auc=0.8650, loss=0.7815]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 64/292 [00:43<03:03,  1.24batch/s, auc=0.8650, loss=0.7815]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 64/292 [00:44<03:03,  1.24batch/s, auc=0.8611, loss=1.6246]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 65/292 [00:44<03:21,  1.13batch/s, auc=0.8611, loss=1.6246]\u001b[A\n",
      "Training Epoch 4/25:  22%|██▏       | 65/292 [00:44<03:21,  1.13batch/s, auc=0.8628, loss=0.5589]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 66/292 [00:44<02:58,  1.27batch/s, auc=0.8628, loss=0.5589]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 66/292 [00:45<02:58,  1.27batch/s, auc=0.8636, loss=0.7017]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 67/292 [00:45<02:41,  1.39batch/s, auc=0.8636, loss=0.7017]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 67/292 [00:45<02:41,  1.39batch/s, auc=0.8632, loss=0.7848]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 68/292 [00:45<02:30,  1.49batch/s, auc=0.8632, loss=0.7848]\u001b[A\n",
      "Training Epoch 4/25:  23%|██▎       | 68/292 [00:47<02:30,  1.49batch/s, auc=0.8616, loss=1.0987]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▎       | 69/292 [00:47<02:56,  1.26batch/s, auc=0.8616, loss=1.0987]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▎       | 69/292 [00:48<02:56,  1.26batch/s, auc=0.8613, loss=0.7695]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 70/292 [00:48<03:14,  1.14batch/s, auc=0.8613, loss=0.7695]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 70/292 [00:48<03:14,  1.14batch/s, auc=0.8626, loss=0.5600]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 71/292 [00:48<02:52,  1.28batch/s, auc=0.8626, loss=0.5600]\u001b[A\n",
      "Training Epoch 4/25:  24%|██▍       | 71/292 [00:49<02:52,  1.28batch/s, auc=0.8629, loss=0.7270]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▍       | 72/292 [00:49<02:36,  1.40batch/s, auc=0.8629, loss=0.7270]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▍       | 72/292 [00:49<02:36,  1.40batch/s, auc=0.8647, loss=0.6067]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 73/292 [00:49<02:25,  1.50batch/s, auc=0.8647, loss=0.6067]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 73/292 [00:50<02:25,  1.50batch/s, auc=0.8646, loss=0.8362]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 74/292 [00:50<02:17,  1.58batch/s, auc=0.8646, loss=0.8362]\u001b[A\n",
      "Training Epoch 4/25:  25%|██▌       | 74/292 [00:50<02:17,  1.58batch/s, auc=0.8656, loss=0.5420]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 75/292 [00:50<02:12,  1.64batch/s, auc=0.8656, loss=0.5420]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 75/292 [00:51<02:12,  1.64batch/s, auc=0.8640, loss=1.1689]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 76/292 [00:51<02:08,  1.68batch/s, auc=0.8640, loss=1.1689]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▌       | 76/292 [00:51<02:08,  1.68batch/s, auc=0.8646, loss=0.8077]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▋       | 77/292 [00:51<02:05,  1.72batch/s, auc=0.8646, loss=0.8077]\u001b[A\n",
      "Training Epoch 4/25:  26%|██▋       | 77/292 [00:52<02:05,  1.72batch/s, auc=0.8649, loss=0.5509]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 78/292 [00:52<02:02,  1.74batch/s, auc=0.8649, loss=0.5509]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 78/292 [00:53<02:02,  1.74batch/s, auc=0.8650, loss=0.7088]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 79/292 [00:53<02:01,  1.76batch/s, auc=0.8650, loss=0.7088]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 79/292 [00:53<02:01,  1.76batch/s, auc=0.8650, loss=0.9360]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 80/292 [00:53<01:59,  1.77batch/s, auc=0.8650, loss=0.9360]\u001b[A\n",
      "Training Epoch 4/25:  27%|██▋       | 80/292 [00:54<01:59,  1.77batch/s, auc=0.8658, loss=0.5995]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 81/292 [00:54<01:58,  1.78batch/s, auc=0.8658, loss=0.5995]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 81/292 [00:54<01:58,  1.78batch/s, auc=0.8672, loss=0.4904]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 82/292 [00:54<01:57,  1.78batch/s, auc=0.8672, loss=0.4904]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 82/292 [00:55<01:57,  1.78batch/s, auc=0.8648, loss=1.4160]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 83/292 [00:55<02:29,  1.40batch/s, auc=0.8648, loss=1.4160]\u001b[A\n",
      "Training Epoch 4/25:  28%|██▊       | 83/292 [00:56<02:29,  1.40batch/s, auc=0.8630, loss=1.0681]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 84/292 [00:56<02:51,  1.22batch/s, auc=0.8630, loss=1.0681]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 84/292 [00:57<02:51,  1.22batch/s, auc=0.8632, loss=0.6904]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 85/292 [00:57<02:33,  1.34batch/s, auc=0.8632, loss=0.6904]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 85/292 [00:58<02:33,  1.34batch/s, auc=0.8635, loss=0.7777]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 86/292 [00:58<02:21,  1.45batch/s, auc=0.8635, loss=0.7777]\u001b[A\n",
      "Training Epoch 4/25:  29%|██▉       | 86/292 [00:59<02:21,  1.45batch/s, auc=0.8621, loss=1.0497]\u001b[A\n",
      "Training Epoch 4/25:  30%|██▉       | 87/292 [00:59<02:44,  1.25batch/s, auc=0.8621, loss=1.0497]\u001b[A\n",
      "Training Epoch 4/25:  30%|██▉       | 87/292 [01:00<02:44,  1.25batch/s, auc=0.8605, loss=1.2568]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 88/292 [01:00<03:00,  1.13batch/s, auc=0.8605, loss=1.2568]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 88/292 [01:00<03:00,  1.13batch/s, auc=0.8613, loss=0.5592]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 89/292 [01:00<02:39,  1.27batch/s, auc=0.8613, loss=0.5592]\u001b[A\n",
      "Training Epoch 4/25:  30%|███       | 89/292 [01:01<02:39,  1.27batch/s, auc=0.8617, loss=0.7044]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 90/292 [01:01<02:24,  1.39batch/s, auc=0.8617, loss=0.7044]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 90/292 [01:01<02:24,  1.39batch/s, auc=0.8615, loss=0.7169]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 91/292 [01:01<02:14,  1.49batch/s, auc=0.8615, loss=0.7169]\u001b[A\n",
      "Training Epoch 4/25:  31%|███       | 91/292 [01:02<02:14,  1.49batch/s, auc=0.8621, loss=0.6393]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 92/292 [01:02<02:07,  1.57batch/s, auc=0.8621, loss=0.6393]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 92/292 [01:03<02:07,  1.57batch/s, auc=0.8612, loss=1.1161]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 93/292 [01:03<02:32,  1.30batch/s, auc=0.8612, loss=1.1161]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 93/292 [01:04<02:32,  1.30batch/s, auc=0.8621, loss=0.7314]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 94/292 [01:04<02:19,  1.42batch/s, auc=0.8621, loss=0.7314]\u001b[A\n",
      "Training Epoch 4/25:  32%|███▏      | 94/292 [01:04<02:19,  1.42batch/s, auc=0.8627, loss=0.6204]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 95/292 [01:04<02:10,  1.51batch/s, auc=0.8627, loss=0.6204]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 95/292 [01:05<02:10,  1.51batch/s, auc=0.8633, loss=0.5714]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 96/292 [01:05<02:03,  1.59batch/s, auc=0.8633, loss=0.5714]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 96/292 [01:05<02:03,  1.59batch/s, auc=0.8625, loss=1.0453]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 97/292 [01:05<01:58,  1.64batch/s, auc=0.8625, loss=1.0453]\u001b[A\n",
      "Training Epoch 4/25:  33%|███▎      | 97/292 [01:06<01:58,  1.64batch/s, auc=0.8619, loss=0.8332]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▎      | 98/292 [01:06<02:25,  1.34batch/s, auc=0.8619, loss=0.8332]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▎      | 98/292 [01:07<02:25,  1.34batch/s, auc=0.8609, loss=0.9910]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 99/292 [01:07<02:13,  1.45batch/s, auc=0.8609, loss=0.9910]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 99/292 [01:08<02:13,  1.45batch/s, auc=0.8609, loss=0.7690]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 100/292 [01:08<02:34,  1.24batch/s, auc=0.8609, loss=0.7690]\u001b[A\n",
      "Training Epoch 4/25:  34%|███▍      | 100/292 [01:08<02:34,  1.24batch/s, auc=0.8615, loss=0.6026]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 101/292 [01:08<02:19,  1.37batch/s, auc=0.8615, loss=0.6026]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 101/292 [01:09<02:19,  1.37batch/s, auc=0.8616, loss=0.6501]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 102/292 [01:09<02:09,  1.47batch/s, auc=0.8616, loss=0.6501]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▍      | 102/292 [01:10<02:09,  1.47batch/s, auc=0.8614, loss=1.0018]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▌      | 103/292 [01:10<02:01,  1.56batch/s, auc=0.8614, loss=1.0018]\u001b[A\n",
      "Training Epoch 4/25:  35%|███▌      | 103/292 [01:10<02:01,  1.56batch/s, auc=0.8615, loss=0.8234]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 104/292 [01:10<01:56,  1.62batch/s, auc=0.8615, loss=0.8234]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 104/292 [01:11<01:56,  1.62batch/s, auc=0.8622, loss=0.6022]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 105/292 [01:11<01:52,  1.67batch/s, auc=0.8622, loss=0.6022]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▌      | 105/292 [01:11<01:52,  1.67batch/s, auc=0.8624, loss=0.6644]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▋      | 106/292 [01:11<01:48,  1.71batch/s, auc=0.8624, loss=0.6644]\u001b[A\n",
      "Training Epoch 4/25:  36%|███▋      | 106/292 [01:12<01:48,  1.71batch/s, auc=0.8631, loss=0.6134]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 107/292 [01:12<01:47,  1.73batch/s, auc=0.8631, loss=0.6134]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 107/292 [01:13<01:47,  1.73batch/s, auc=0.8605, loss=1.7079]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 108/292 [01:13<02:13,  1.37batch/s, auc=0.8605, loss=1.7079]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 108/292 [01:13<02:13,  1.37batch/s, auc=0.8601, loss=0.6739]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 109/292 [01:13<02:03,  1.48batch/s, auc=0.8601, loss=0.6739]\u001b[A\n",
      "Training Epoch 4/25:  37%|███▋      | 109/292 [01:14<02:03,  1.48batch/s, auc=0.8602, loss=0.7383]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 110/292 [01:14<01:56,  1.56batch/s, auc=0.8602, loss=0.7383]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 110/292 [01:15<01:56,  1.56batch/s, auc=0.8604, loss=0.8813]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 111/292 [01:15<01:51,  1.62batch/s, auc=0.8604, loss=0.8813]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 111/292 [01:15<01:51,  1.62batch/s, auc=0.8611, loss=0.5256]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 112/292 [01:15<01:47,  1.67batch/s, auc=0.8611, loss=0.5256]\u001b[A\n",
      "Training Epoch 4/25:  38%|███▊      | 112/292 [01:16<01:47,  1.67batch/s, auc=0.8612, loss=0.7602]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▊      | 113/292 [01:16<01:44,  1.71batch/s, auc=0.8612, loss=0.7602]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▊      | 113/292 [01:16<01:44,  1.71batch/s, auc=0.8613, loss=0.5439]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 114/292 [01:16<01:42,  1.73batch/s, auc=0.8613, loss=0.5439]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 114/292 [01:17<01:42,  1.73batch/s, auc=0.8615, loss=0.6023]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 115/292 [01:17<01:41,  1.75batch/s, auc=0.8615, loss=0.6023]\u001b[A\n",
      "Training Epoch 4/25:  39%|███▉      | 115/292 [01:17<01:41,  1.75batch/s, auc=0.8615, loss=0.6283]\u001b[A\n",
      "Training Epoch 4/25:  40%|███▉      | 116/292 [01:17<01:39,  1.76batch/s, auc=0.8615, loss=0.6283]\u001b[A\n",
      "Training Epoch 4/25:  40%|███▉      | 116/292 [01:18<01:39,  1.76batch/s, auc=0.8618, loss=0.4864]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 117/292 [01:18<01:38,  1.77batch/s, auc=0.8618, loss=0.4864]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 117/292 [01:18<01:38,  1.77batch/s, auc=0.8619, loss=0.7200]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 118/292 [01:18<01:37,  1.78batch/s, auc=0.8619, loss=0.7200]\u001b[A\n",
      "Training Epoch 4/25:  40%|████      | 118/292 [01:19<01:37,  1.78batch/s, auc=0.8604, loss=1.4338]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 119/292 [01:19<01:37,  1.78batch/s, auc=0.8604, loss=1.4338]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 119/292 [01:20<01:37,  1.78batch/s, auc=0.8608, loss=0.6247]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 120/292 [01:20<01:36,  1.78batch/s, auc=0.8608, loss=0.6247]\u001b[A\n",
      "Training Epoch 4/25:  41%|████      | 120/292 [01:20<01:36,  1.78batch/s, auc=0.8611, loss=0.8492]\u001b[A\n",
      "Training Epoch 4/25:  41%|████▏     | 121/292 [01:20<01:35,  1.79batch/s, auc=0.8611, loss=0.8492]\u001b[A\n",
      "Training Epoch 4/25:  41%|████▏     | 121/292 [01:21<01:35,  1.79batch/s, auc=0.8610, loss=0.8203]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 122/292 [01:21<01:35,  1.79batch/s, auc=0.8610, loss=0.8203]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 122/292 [01:22<01:35,  1.79batch/s, auc=0.8608, loss=0.7311]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 123/292 [01:22<02:00,  1.40batch/s, auc=0.8608, loss=0.7311]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 123/292 [01:22<02:00,  1.40batch/s, auc=0.8612, loss=0.7759]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 124/292 [01:22<01:52,  1.50batch/s, auc=0.8612, loss=0.7759]\u001b[A\n",
      "Training Epoch 4/25:  42%|████▏     | 124/292 [01:23<01:52,  1.50batch/s, auc=0.8618, loss=0.5783]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 125/292 [01:23<01:46,  1.58batch/s, auc=0.8618, loss=0.5783]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 125/292 [01:24<01:46,  1.58batch/s, auc=0.8603, loss=1.4242]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 126/292 [01:24<02:07,  1.30batch/s, auc=0.8603, loss=1.4242]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 126/292 [01:25<02:07,  1.30batch/s, auc=0.8604, loss=0.7173]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 127/292 [01:25<01:56,  1.42batch/s, auc=0.8604, loss=0.7173]\u001b[A\n",
      "Training Epoch 4/25:  43%|████▎     | 127/292 [01:25<01:56,  1.42batch/s, auc=0.8596, loss=1.0945]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 128/292 [01:25<01:48,  1.51batch/s, auc=0.8596, loss=1.0945]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 128/292 [01:26<01:48,  1.51batch/s, auc=0.8582, loss=1.3621]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 129/292 [01:26<02:07,  1.27batch/s, auc=0.8582, loss=1.3621]\u001b[A\n",
      "Training Epoch 4/25:  44%|████▍     | 129/292 [01:27<02:07,  1.27batch/s, auc=0.8579, loss=0.7988]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 130/292 [01:27<01:56,  1.39batch/s, auc=0.8579, loss=0.7988]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 130/292 [01:27<01:56,  1.39batch/s, auc=0.8580, loss=0.7738]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 131/292 [01:27<01:47,  1.49batch/s, auc=0.8580, loss=0.7738]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▍     | 131/292 [01:28<01:47,  1.49batch/s, auc=0.8578, loss=0.6988]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▌     | 132/292 [01:28<01:41,  1.57batch/s, auc=0.8578, loss=0.6988]\u001b[A\n",
      "Training Epoch 4/25:  45%|████▌     | 132/292 [01:28<01:41,  1.57batch/s, auc=0.8578, loss=0.7672]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 133/292 [01:28<01:37,  1.63batch/s, auc=0.8578, loss=0.7672]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 133/292 [01:29<01:37,  1.63batch/s, auc=0.8580, loss=0.5560]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 134/292 [01:29<01:34,  1.68batch/s, auc=0.8580, loss=0.5560]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 134/292 [01:30<01:34,  1.68batch/s, auc=0.8580, loss=0.9420]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 135/292 [01:30<01:31,  1.71batch/s, auc=0.8580, loss=0.9420]\u001b[A\n",
      "Training Epoch 4/25:  46%|████▌     | 135/292 [01:30<01:31,  1.71batch/s, auc=0.8582, loss=0.6490]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 136/292 [01:30<01:29,  1.73batch/s, auc=0.8582, loss=0.6490]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 136/292 [01:31<01:29,  1.73batch/s, auc=0.8582, loss=0.6282]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 137/292 [01:31<01:28,  1.75batch/s, auc=0.8582, loss=0.6282]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 137/292 [01:31<01:28,  1.75batch/s, auc=0.8584, loss=0.6683]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 138/292 [01:31<01:27,  1.76batch/s, auc=0.8584, loss=0.6683]\u001b[A\n",
      "Training Epoch 4/25:  47%|████▋     | 138/292 [01:32<01:27,  1.76batch/s, auc=0.8591, loss=0.5052]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 139/292 [01:32<01:26,  1.77batch/s, auc=0.8591, loss=0.5052]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 139/292 [01:33<01:26,  1.77batch/s, auc=0.8583, loss=1.1862]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 140/292 [01:33<01:49,  1.39batch/s, auc=0.8583, loss=1.1862]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 140/292 [01:33<01:49,  1.39batch/s, auc=0.8580, loss=0.6332]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 141/292 [01:33<01:41,  1.49batch/s, auc=0.8580, loss=0.6332]\u001b[A\n",
      "Training Epoch 4/25:  48%|████▊     | 141/292 [01:34<01:41,  1.49batch/s, auc=0.8585, loss=0.5069]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▊     | 142/292 [01:34<01:35,  1.57batch/s, auc=0.8585, loss=0.5069]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▊     | 142/292 [01:35<01:35,  1.57batch/s, auc=0.8583, loss=0.8609]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 143/292 [01:35<01:54,  1.30batch/s, auc=0.8583, loss=0.8609]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 143/292 [01:36<01:54,  1.30batch/s, auc=0.8588, loss=0.6251]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 144/292 [01:36<01:44,  1.42batch/s, auc=0.8588, loss=0.6251]\u001b[A\n",
      "Training Epoch 4/25:  49%|████▉     | 144/292 [01:36<01:44,  1.42batch/s, auc=0.8593, loss=0.5712]\u001b[A\n",
      "Training Epoch 4/25:  50%|████▉     | 145/292 [01:36<01:37,  1.51batch/s, auc=0.8593, loss=0.5712]\u001b[A\n",
      "Training Epoch 4/25:  50%|████▉     | 145/292 [01:37<01:37,  1.51batch/s, auc=0.8593, loss=0.7088]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 146/292 [01:37<01:32,  1.58batch/s, auc=0.8593, loss=0.7088]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 146/292 [01:37<01:32,  1.58batch/s, auc=0.8602, loss=0.5697]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 147/292 [01:37<01:28,  1.64batch/s, auc=0.8602, loss=0.5697]\u001b[A\n",
      "Training Epoch 4/25:  50%|█████     | 147/292 [01:38<01:28,  1.64batch/s, auc=0.8606, loss=0.5614]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 148/292 [01:38<01:25,  1.68batch/s, auc=0.8606, loss=0.5614]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 148/292 [01:38<01:25,  1.68batch/s, auc=0.8605, loss=0.8108]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 149/292 [01:38<01:23,  1.71batch/s, auc=0.8605, loss=0.8108]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████     | 149/292 [01:39<01:23,  1.71batch/s, auc=0.8607, loss=0.7615]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████▏    | 150/292 [01:39<01:21,  1.73batch/s, auc=0.8607, loss=0.7615]\u001b[A\n",
      "Training Epoch 4/25:  51%|█████▏    | 150/292 [01:39<01:21,  1.73batch/s, auc=0.8603, loss=1.0846]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 151/292 [01:39<01:20,  1.75batch/s, auc=0.8603, loss=1.0846]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 151/292 [01:40<01:20,  1.75batch/s, auc=0.8602, loss=0.8829]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 152/292 [01:40<01:19,  1.76batch/s, auc=0.8602, loss=0.8829]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 152/292 [01:41<01:19,  1.76batch/s, auc=0.8597, loss=0.9361]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 153/292 [01:41<01:40,  1.39batch/s, auc=0.8597, loss=0.9361]\u001b[A\n",
      "Training Epoch 4/25:  52%|█████▏    | 153/292 [01:42<01:40,  1.39batch/s, auc=0.8596, loss=0.9781]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 154/292 [01:42<01:32,  1.49batch/s, auc=0.8596, loss=0.9781]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 154/292 [01:42<01:32,  1.49batch/s, auc=0.8592, loss=0.8780]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 155/292 [01:42<01:27,  1.57batch/s, auc=0.8592, loss=0.8780]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 155/292 [01:43<01:27,  1.57batch/s, auc=0.8591, loss=1.0591]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 156/292 [01:43<01:23,  1.63batch/s, auc=0.8591, loss=1.0591]\u001b[A\n",
      "Training Epoch 4/25:  53%|█████▎    | 156/292 [01:43<01:23,  1.63batch/s, auc=0.8590, loss=0.8211]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 157/292 [01:43<01:20,  1.67batch/s, auc=0.8590, loss=0.8211]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 157/292 [01:44<01:20,  1.67batch/s, auc=0.8588, loss=0.7755]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 158/292 [01:44<01:18,  1.70batch/s, auc=0.8588, loss=0.7755]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 158/292 [01:44<01:18,  1.70batch/s, auc=0.8584, loss=0.9199]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 159/292 [01:44<01:17,  1.73batch/s, auc=0.8584, loss=0.9199]\u001b[A\n",
      "Training Epoch 4/25:  54%|█████▍    | 159/292 [01:45<01:17,  1.73batch/s, auc=0.8585, loss=0.6570]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▍    | 160/292 [01:45<01:15,  1.74batch/s, auc=0.8585, loss=0.6570]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▍    | 160/292 [01:46<01:15,  1.74batch/s, auc=0.8586, loss=0.6945]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 161/292 [01:46<01:14,  1.76batch/s, auc=0.8586, loss=0.6945]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 161/292 [01:46<01:14,  1.76batch/s, auc=0.8581, loss=1.0407]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 162/292 [01:46<01:13,  1.76batch/s, auc=0.8581, loss=1.0407]\u001b[A\n",
      "Training Epoch 4/25:  55%|█████▌    | 162/292 [01:47<01:13,  1.76batch/s, auc=0.8580, loss=0.7486]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 163/292 [01:47<01:13,  1.77batch/s, auc=0.8580, loss=0.7486]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 163/292 [01:47<01:13,  1.77batch/s, auc=0.8583, loss=0.6260]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 164/292 [01:47<01:12,  1.77batch/s, auc=0.8583, loss=0.6260]\u001b[A\n",
      "Training Epoch 4/25:  56%|█████▌    | 164/292 [01:48<01:12,  1.77batch/s, auc=0.8590, loss=0.7246]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 165/292 [01:48<01:11,  1.77batch/s, auc=0.8590, loss=0.7246]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 165/292 [01:49<01:11,  1.77batch/s, auc=0.8588, loss=1.0393]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 166/292 [01:49<01:30,  1.39batch/s, auc=0.8588, loss=1.0393]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 166/292 [01:49<01:30,  1.39batch/s, auc=0.8590, loss=0.6453]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 167/292 [01:49<01:23,  1.49batch/s, auc=0.8590, loss=0.6453]\u001b[A\n",
      "Training Epoch 4/25:  57%|█████▋    | 167/292 [01:51<01:23,  1.49batch/s, auc=0.8580, loss=1.1989]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 168/292 [01:51<01:38,  1.26batch/s, auc=0.8580, loss=1.1989]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 168/292 [01:51<01:38,  1.26batch/s, auc=0.8585, loss=0.5848]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 169/292 [01:51<01:29,  1.38batch/s, auc=0.8585, loss=0.5848]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 169/292 [01:52<01:29,  1.38batch/s, auc=0.8588, loss=0.5983]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 170/292 [01:52<01:22,  1.48batch/s, auc=0.8588, loss=0.5983]\u001b[A\n",
      "Training Epoch 4/25:  58%|█████▊    | 170/292 [01:52<01:22,  1.48batch/s, auc=0.8592, loss=0.6098]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▊    | 171/292 [01:52<01:17,  1.56batch/s, auc=0.8592, loss=0.6098]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▊    | 171/292 [01:53<01:17,  1.56batch/s, auc=0.8592, loss=0.6518]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 172/292 [01:53<01:13,  1.62batch/s, auc=0.8592, loss=0.6518]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 172/292 [01:53<01:13,  1.62batch/s, auc=0.8597, loss=0.5396]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 173/292 [01:53<01:11,  1.67batch/s, auc=0.8597, loss=0.5396]\u001b[A\n",
      "Training Epoch 4/25:  59%|█████▉    | 173/292 [01:54<01:11,  1.67batch/s, auc=0.8599, loss=0.5379]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 174/292 [01:54<01:09,  1.70batch/s, auc=0.8599, loss=0.5379]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 174/292 [01:54<01:09,  1.70batch/s, auc=0.8600, loss=0.7178]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 175/292 [01:54<01:07,  1.73batch/s, auc=0.8600, loss=0.7178]\u001b[A\n",
      "Training Epoch 4/25:  60%|█████▉    | 175/292 [01:55<01:07,  1.73batch/s, auc=0.8600, loss=0.7996]\u001b[A\n",
      "Training Epoch 4/25:  60%|██████    | 176/292 [01:55<01:06,  1.74batch/s, auc=0.8600, loss=0.7996]\u001b[A\n",
      "Training Epoch 4/25:  60%|██████    | 176/292 [01:56<01:06,  1.74batch/s, auc=0.8598, loss=0.8185]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 177/292 [01:56<01:23,  1.38batch/s, auc=0.8598, loss=0.8185]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 177/292 [01:57<01:23,  1.38batch/s, auc=0.8602, loss=0.7089]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 178/292 [01:57<01:17,  1.48batch/s, auc=0.8602, loss=0.7089]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████    | 178/292 [01:57<01:17,  1.48batch/s, auc=0.8608, loss=0.5777]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████▏   | 179/292 [01:57<01:12,  1.56batch/s, auc=0.8608, loss=0.5777]\u001b[A\n",
      "Training Epoch 4/25:  61%|██████▏   | 179/292 [01:58<01:12,  1.56batch/s, auc=0.8609, loss=0.8661]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 180/292 [01:58<01:09,  1.62batch/s, auc=0.8609, loss=0.8661]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 180/292 [01:58<01:09,  1.62batch/s, auc=0.8607, loss=0.6691]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 181/292 [01:58<01:06,  1.67batch/s, auc=0.8607, loss=0.6691]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 181/292 [01:59<01:06,  1.67batch/s, auc=0.8607, loss=0.9672]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 182/292 [01:59<01:04,  1.70batch/s, auc=0.8607, loss=0.9672]\u001b[A\n",
      "Training Epoch 4/25:  62%|██████▏   | 182/292 [02:00<01:04,  1.70batch/s, auc=0.8605, loss=0.7804]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 183/292 [02:00<01:20,  1.36batch/s, auc=0.8605, loss=0.7804]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 183/292 [02:01<01:20,  1.36batch/s, auc=0.8611, loss=0.5842]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 184/292 [02:01<01:13,  1.46batch/s, auc=0.8611, loss=0.5842]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 184/292 [02:01<01:13,  1.46batch/s, auc=0.8608, loss=0.9770]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 185/292 [02:01<01:09,  1.55batch/s, auc=0.8608, loss=0.9770]\u001b[A\n",
      "Training Epoch 4/25:  63%|██████▎   | 185/292 [02:02<01:09,  1.55batch/s, auc=0.8610, loss=0.7476]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▎   | 186/292 [02:02<01:05,  1.61batch/s, auc=0.8610, loss=0.7476]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▎   | 186/292 [02:02<01:05,  1.61batch/s, auc=0.8613, loss=0.6914]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 187/292 [02:02<01:03,  1.66batch/s, auc=0.8613, loss=0.6914]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 187/292 [02:03<01:03,  1.66batch/s, auc=0.8612, loss=0.8531]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 188/292 [02:03<01:01,  1.69batch/s, auc=0.8612, loss=0.8531]\u001b[A\n",
      "Training Epoch 4/25:  64%|██████▍   | 188/292 [02:04<01:01,  1.69batch/s, auc=0.8608, loss=0.9635]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▍   | 189/292 [02:04<01:15,  1.36batch/s, auc=0.8608, loss=0.9635]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▍   | 189/292 [02:05<01:15,  1.36batch/s, auc=0.8603, loss=1.1669]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 190/292 [02:05<01:25,  1.19batch/s, auc=0.8603, loss=1.1669]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 190/292 [02:06<01:25,  1.19batch/s, auc=0.8603, loss=0.6659]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 191/292 [02:06<01:16,  1.32batch/s, auc=0.8603, loss=0.6659]\u001b[A\n",
      "Training Epoch 4/25:  65%|██████▌   | 191/292 [02:07<01:16,  1.32batch/s, auc=0.8597, loss=1.1819]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 192/292 [02:07<01:25,  1.17batch/s, auc=0.8597, loss=1.1819]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 192/292 [02:07<01:25,  1.17batch/s, auc=0.8600, loss=0.7309]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 193/292 [02:07<01:15,  1.31batch/s, auc=0.8600, loss=0.7309]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▌   | 193/292 [02:08<01:15,  1.31batch/s, auc=0.8591, loss=1.5801]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▋   | 194/292 [02:08<01:24,  1.16batch/s, auc=0.8591, loss=1.5801]\u001b[A\n",
      "Training Epoch 4/25:  66%|██████▋   | 194/292 [02:09<01:24,  1.16batch/s, auc=0.8591, loss=0.8176]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 195/292 [02:09<01:14,  1.30batch/s, auc=0.8591, loss=0.8176]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 195/292 [02:09<01:14,  1.30batch/s, auc=0.8594, loss=0.6163]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 196/292 [02:09<01:07,  1.41batch/s, auc=0.8594, loss=0.6163]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 196/292 [02:10<01:07,  1.41batch/s, auc=0.8586, loss=1.4352]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 197/292 [02:10<01:17,  1.22batch/s, auc=0.8586, loss=1.4352]\u001b[A\n",
      "Training Epoch 4/25:  67%|██████▋   | 197/292 [02:11<01:17,  1.22batch/s, auc=0.8588, loss=0.7860]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 198/292 [02:11<01:09,  1.35batch/s, auc=0.8588, loss=0.7860]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 198/292 [02:12<01:09,  1.35batch/s, auc=0.8594, loss=0.5972]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 199/292 [02:12<01:04,  1.45batch/s, auc=0.8594, loss=0.5972]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 199/292 [02:12<01:04,  1.45batch/s, auc=0.8597, loss=0.8101]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 200/292 [02:12<00:59,  1.54batch/s, auc=0.8597, loss=0.8101]\u001b[A\n",
      "Training Epoch 4/25:  68%|██████▊   | 200/292 [02:13<00:59,  1.54batch/s, auc=0.8597, loss=0.7267]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 201/292 [02:13<00:56,  1.60batch/s, auc=0.8597, loss=0.7267]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 201/292 [02:13<00:56,  1.60batch/s, auc=0.8599, loss=0.5975]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 202/292 [02:13<00:54,  1.65batch/s, auc=0.8599, loss=0.5975]\u001b[A\n",
      "Training Epoch 4/25:  69%|██████▉   | 202/292 [02:14<00:54,  1.65batch/s, auc=0.8605, loss=0.6638]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 203/292 [02:14<00:52,  1.69batch/s, auc=0.8605, loss=0.6638]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 203/292 [02:14<00:52,  1.69batch/s, auc=0.8605, loss=0.6459]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 204/292 [02:14<00:51,  1.72batch/s, auc=0.8605, loss=0.6459]\u001b[A\n",
      "Training Epoch 4/25:  70%|██████▉   | 204/292 [02:15<00:51,  1.72batch/s, auc=0.8604, loss=0.6567]\u001b[A\n",
      "Training Epoch 4/25:  70%|███████   | 205/292 [02:15<00:50,  1.73batch/s, auc=0.8604, loss=0.6567]\u001b[A\n",
      "Training Epoch 4/25:  70%|███████   | 205/292 [02:16<00:50,  1.73batch/s, auc=0.8605, loss=0.9451]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 206/292 [02:16<00:49,  1.75batch/s, auc=0.8605, loss=0.9451]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 206/292 [02:16<00:49,  1.75batch/s, auc=0.8608, loss=0.7255]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 207/292 [02:16<00:48,  1.76batch/s, auc=0.8608, loss=0.7255]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 207/292 [02:17<00:48,  1.76batch/s, auc=0.8604, loss=1.0761]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 208/292 [02:17<00:47,  1.76batch/s, auc=0.8604, loss=1.0761]\u001b[A\n",
      "Training Epoch 4/25:  71%|███████   | 208/292 [02:17<00:47,  1.76batch/s, auc=0.8603, loss=0.7666]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 209/292 [02:17<00:46,  1.77batch/s, auc=0.8603, loss=0.7666]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 209/292 [02:18<00:46,  1.77batch/s, auc=0.8605, loss=0.6304]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 210/292 [02:18<00:46,  1.77batch/s, auc=0.8605, loss=0.6304]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 210/292 [02:18<00:46,  1.77batch/s, auc=0.8605, loss=0.8065]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 211/292 [02:18<00:45,  1.78batch/s, auc=0.8605, loss=0.8065]\u001b[A\n",
      "Training Epoch 4/25:  72%|███████▏  | 211/292 [02:19<00:45,  1.78batch/s, auc=0.8594, loss=1.6538]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 212/292 [02:19<00:57,  1.39batch/s, auc=0.8594, loss=1.6538]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 212/292 [02:20<00:57,  1.39batch/s, auc=0.8592, loss=0.7239]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 213/292 [02:20<00:53,  1.49batch/s, auc=0.8592, loss=0.7239]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 213/292 [02:21<00:53,  1.49batch/s, auc=0.8592, loss=0.7439]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 214/292 [02:21<00:49,  1.56batch/s, auc=0.8592, loss=0.7439]\u001b[A\n",
      "Training Epoch 4/25:  73%|███████▎  | 214/292 [02:21<00:49,  1.56batch/s, auc=0.8588, loss=0.8971]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▎  | 215/292 [02:21<00:47,  1.62batch/s, auc=0.8588, loss=0.8971]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▎  | 215/292 [02:22<00:47,  1.62batch/s, auc=0.8590, loss=0.6731]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 216/292 [02:22<00:45,  1.66batch/s, auc=0.8590, loss=0.6731]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 216/292 [02:23<00:45,  1.66batch/s, auc=0.8586, loss=0.8505]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 217/292 [02:23<00:55,  1.34batch/s, auc=0.8586, loss=0.8505]\u001b[A\n",
      "Training Epoch 4/25:  74%|███████▍  | 217/292 [02:23<00:55,  1.34batch/s, auc=0.8588, loss=0.6517]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▍  | 218/292 [02:23<00:51,  1.45batch/s, auc=0.8588, loss=0.6517]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▍  | 218/292 [02:24<00:51,  1.45batch/s, auc=0.8583, loss=0.9513]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 219/292 [02:24<00:58,  1.24batch/s, auc=0.8583, loss=0.9513]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 219/292 [02:25<00:58,  1.24batch/s, auc=0.8584, loss=0.6629]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 220/292 [02:25<00:52,  1.36batch/s, auc=0.8584, loss=0.6629]\u001b[A\n",
      "Training Epoch 4/25:  75%|███████▌  | 220/292 [02:26<00:52,  1.36batch/s, auc=0.8587, loss=0.5886]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 221/292 [02:26<00:48,  1.47batch/s, auc=0.8587, loss=0.5886]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 221/292 [02:26<00:48,  1.47batch/s, auc=0.8589, loss=0.6915]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 222/292 [02:26<00:45,  1.55batch/s, auc=0.8589, loss=0.6915]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▌  | 222/292 [02:27<00:45,  1.55batch/s, auc=0.8592, loss=0.6047]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▋  | 223/292 [02:27<00:42,  1.61batch/s, auc=0.8592, loss=0.6047]\u001b[A\n",
      "Training Epoch 4/25:  76%|███████▋  | 223/292 [02:27<00:42,  1.61batch/s, auc=0.8598, loss=0.5545]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 224/292 [02:27<00:41,  1.66batch/s, auc=0.8598, loss=0.5545]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 224/292 [02:28<00:41,  1.66batch/s, auc=0.8595, loss=1.0140]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 225/292 [02:28<00:50,  1.34batch/s, auc=0.8595, loss=1.0140]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 225/292 [02:29<00:50,  1.34batch/s, auc=0.8597, loss=0.6849]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 226/292 [02:29<00:45,  1.45batch/s, auc=0.8597, loss=0.6849]\u001b[A\n",
      "Training Epoch 4/25:  77%|███████▋  | 226/292 [02:29<00:45,  1.45batch/s, auc=0.8600, loss=0.6772]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 227/292 [02:29<00:42,  1.53batch/s, auc=0.8600, loss=0.6772]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 227/292 [02:30<00:42,  1.53batch/s, auc=0.8601, loss=0.7630]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 228/292 [02:30<00:40,  1.59batch/s, auc=0.8601, loss=0.7630]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 228/292 [02:31<00:40,  1.59batch/s, auc=0.8601, loss=0.6975]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 229/292 [02:31<00:38,  1.64batch/s, auc=0.8601, loss=0.6975]\u001b[A\n",
      "Training Epoch 4/25:  78%|███████▊  | 229/292 [02:32<00:38,  1.64batch/s, auc=0.8588, loss=1.6456]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 230/292 [02:32<00:46,  1.33batch/s, auc=0.8588, loss=1.6456]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 230/292 [02:32<00:46,  1.33batch/s, auc=0.8593, loss=0.5696]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 231/292 [02:32<00:42,  1.44batch/s, auc=0.8593, loss=0.5696]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 231/292 [02:33<00:42,  1.44batch/s, auc=0.8596, loss=0.6100]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 232/292 [02:33<00:39,  1.53batch/s, auc=0.8596, loss=0.6100]\u001b[A\n",
      "Training Epoch 4/25:  79%|███████▉  | 232/292 [02:33<00:39,  1.53batch/s, auc=0.8593, loss=0.9457]\u001b[A\n",
      "Training Epoch 4/25:  80%|███████▉  | 233/292 [02:33<00:36,  1.60batch/s, auc=0.8593, loss=0.9457]\u001b[A\n",
      "Training Epoch 4/25:  80%|███████▉  | 233/292 [02:34<00:36,  1.60batch/s, auc=0.8596, loss=0.6412]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 234/292 [02:34<00:35,  1.64batch/s, auc=0.8596, loss=0.6412]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 234/292 [02:35<00:35,  1.64batch/s, auc=0.8594, loss=0.7415]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 235/292 [02:35<00:42,  1.34batch/s, auc=0.8594, loss=0.7415]\u001b[A\n",
      "Training Epoch 4/25:  80%|████████  | 235/292 [02:36<00:42,  1.34batch/s, auc=0.8593, loss=0.7730]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 236/292 [02:36<00:47,  1.18batch/s, auc=0.8593, loss=0.7730]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 236/292 [02:37<00:47,  1.18batch/s, auc=0.8597, loss=0.4628]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 237/292 [02:37<00:41,  1.31batch/s, auc=0.8597, loss=0.4628]\u001b[A\n",
      "Training Epoch 4/25:  81%|████████  | 237/292 [02:37<00:41,  1.31batch/s, auc=0.8598, loss=0.6610]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 238/292 [02:37<00:37,  1.42batch/s, auc=0.8598, loss=0.6610]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 238/292 [02:38<00:37,  1.42batch/s, auc=0.8595, loss=0.8767]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 239/292 [02:38<00:43,  1.23batch/s, auc=0.8595, loss=0.8767]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 239/292 [02:39<00:43,  1.23batch/s, auc=0.8599, loss=0.7764]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 240/292 [02:39<00:38,  1.35batch/s, auc=0.8599, loss=0.7764]\u001b[A\n",
      "Training Epoch 4/25:  82%|████████▏ | 240/292 [02:39<00:38,  1.35batch/s, auc=0.8603, loss=0.5660]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 241/292 [02:39<00:35,  1.45batch/s, auc=0.8603, loss=0.5660]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 241/292 [02:40<00:35,  1.45batch/s, auc=0.8607, loss=0.5473]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 242/292 [02:40<00:32,  1.54batch/s, auc=0.8607, loss=0.5473]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 242/292 [02:40<00:32,  1.54batch/s, auc=0.8610, loss=0.5702]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 243/292 [02:40<00:30,  1.60batch/s, auc=0.8610, loss=0.5702]\u001b[A\n",
      "Training Epoch 4/25:  83%|████████▎ | 243/292 [02:41<00:30,  1.60batch/s, auc=0.8611, loss=0.7062]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▎ | 244/292 [02:41<00:29,  1.65batch/s, auc=0.8611, loss=0.7062]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▎ | 244/292 [02:42<00:29,  1.65batch/s, auc=0.8608, loss=0.8730]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 245/292 [02:42<00:27,  1.68batch/s, auc=0.8608, loss=0.8730]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 245/292 [02:42<00:27,  1.68batch/s, auc=0.8609, loss=0.7382]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 246/292 [02:42<00:26,  1.71batch/s, auc=0.8609, loss=0.7382]\u001b[A\n",
      "Training Epoch 4/25:  84%|████████▍ | 246/292 [02:43<00:26,  1.71batch/s, auc=0.8610, loss=0.5935]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 247/292 [02:43<00:26,  1.73batch/s, auc=0.8610, loss=0.5935]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 247/292 [02:43<00:26,  1.73batch/s, auc=0.8609, loss=1.0764]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 248/292 [02:43<00:25,  1.74batch/s, auc=0.8609, loss=1.0764]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▍ | 248/292 [02:44<00:25,  1.74batch/s, auc=0.8608, loss=0.8594]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▌ | 249/292 [02:44<00:24,  1.75batch/s, auc=0.8608, loss=0.8594]\u001b[A\n",
      "Training Epoch 4/25:  85%|████████▌ | 249/292 [02:44<00:24,  1.75batch/s, auc=0.8608, loss=0.7999]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 250/292 [02:44<00:23,  1.76batch/s, auc=0.8608, loss=0.7999]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 250/292 [02:45<00:23,  1.76batch/s, auc=0.8609, loss=0.8142]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 251/292 [02:45<00:23,  1.77batch/s, auc=0.8609, loss=0.8142]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▌ | 251/292 [02:46<00:23,  1.77batch/s, auc=0.8603, loss=1.3617]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▋ | 252/292 [02:46<00:28,  1.39batch/s, auc=0.8603, loss=1.3617]\u001b[A\n",
      "Training Epoch 4/25:  86%|████████▋ | 252/292 [02:47<00:28,  1.39batch/s, auc=0.8602, loss=0.7205]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 253/292 [02:47<00:26,  1.48batch/s, auc=0.8602, loss=0.7205]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 253/292 [02:47<00:26,  1.48batch/s, auc=0.8601, loss=0.5808]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 254/292 [02:47<00:24,  1.56batch/s, auc=0.8601, loss=0.5808]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 254/292 [02:48<00:24,  1.56batch/s, auc=0.8603, loss=0.6707]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 255/292 [02:48<00:22,  1.62batch/s, auc=0.8603, loss=0.6707]\u001b[A\n",
      "Training Epoch 4/25:  87%|████████▋ | 255/292 [02:49<00:22,  1.62batch/s, auc=0.8600, loss=0.8700]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 256/292 [02:49<00:27,  1.32batch/s, auc=0.8600, loss=0.8700]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 256/292 [02:49<00:27,  1.32batch/s, auc=0.8601, loss=0.6243]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 257/292 [02:49<00:24,  1.43batch/s, auc=0.8601, loss=0.6243]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 257/292 [02:50<00:24,  1.43batch/s, auc=0.8601, loss=0.7609]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 258/292 [02:50<00:22,  1.52batch/s, auc=0.8601, loss=0.7609]\u001b[A\n",
      "Training Epoch 4/25:  88%|████████▊ | 258/292 [02:51<00:22,  1.52batch/s, auc=0.8597, loss=0.9681]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▊ | 259/292 [02:51<00:25,  1.28batch/s, auc=0.8597, loss=0.9681]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▊ | 259/292 [02:52<00:25,  1.28batch/s, auc=0.8590, loss=1.2121]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 260/292 [02:52<00:27,  1.15batch/s, auc=0.8590, loss=1.2121]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 260/292 [02:53<00:27,  1.15batch/s, auc=0.8584, loss=1.3043]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 261/292 [02:53<00:28,  1.07batch/s, auc=0.8584, loss=1.3043]\u001b[A\n",
      "Training Epoch 4/25:  89%|████████▉ | 261/292 [02:54<00:28,  1.07batch/s, auc=0.8588, loss=0.5143]\u001b[A\n",
      "Training Epoch 4/25:  90%|████████▉ | 262/292 [02:54<00:24,  1.21batch/s, auc=0.8588, loss=0.5143]\u001b[A\n",
      "Training Epoch 4/25:  90%|████████▉ | 262/292 [02:54<00:24,  1.21batch/s, auc=0.8589, loss=0.5653]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 263/292 [02:54<00:21,  1.34batch/s, auc=0.8589, loss=0.5653]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 263/292 [02:55<00:21,  1.34batch/s, auc=0.8589, loss=0.6529]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 264/292 [02:55<00:19,  1.45batch/s, auc=0.8589, loss=0.6529]\u001b[A\n",
      "Training Epoch 4/25:  90%|█████████ | 264/292 [02:55<00:19,  1.45batch/s, auc=0.8590, loss=0.7819]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 265/292 [02:55<00:17,  1.53batch/s, auc=0.8590, loss=0.7819]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 265/292 [02:56<00:17,  1.53batch/s, auc=0.8590, loss=0.8876]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 266/292 [02:56<00:16,  1.60batch/s, auc=0.8590, loss=0.8876]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████ | 266/292 [02:57<00:16,  1.60batch/s, auc=0.8587, loss=1.1979]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████▏| 267/292 [02:57<00:19,  1.31batch/s, auc=0.8587, loss=1.1979]\u001b[A\n",
      "Training Epoch 4/25:  91%|█████████▏| 267/292 [02:58<00:19,  1.31batch/s, auc=0.8585, loss=1.0594]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 268/292 [02:58<00:20,  1.16batch/s, auc=0.8585, loss=1.0594]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 268/292 [02:59<00:20,  1.16batch/s, auc=0.8577, loss=1.2841]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 269/292 [02:59<00:21,  1.08batch/s, auc=0.8577, loss=1.2841]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 269/292 [03:00<00:21,  1.08batch/s, auc=0.8579, loss=0.7995]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 270/292 [03:00<00:17,  1.22batch/s, auc=0.8579, loss=0.7995]\u001b[A\n",
      "Training Epoch 4/25:  92%|█████████▏| 270/292 [03:00<00:17,  1.22batch/s, auc=0.8581, loss=0.5787]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 271/292 [03:00<00:15,  1.35batch/s, auc=0.8581, loss=0.5787]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 271/292 [03:01<00:15,  1.35batch/s, auc=0.8582, loss=0.6778]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 272/292 [03:01<00:16,  1.19batch/s, auc=0.8582, loss=0.6778]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 272/292 [03:02<00:16,  1.19batch/s, auc=0.8583, loss=0.7080]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 273/292 [03:02<00:14,  1.32batch/s, auc=0.8583, loss=0.7080]\u001b[A\n",
      "Training Epoch 4/25:  93%|█████████▎| 273/292 [03:03<00:14,  1.32batch/s, auc=0.8581, loss=0.9911]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 274/292 [03:03<00:15,  1.17batch/s, auc=0.8581, loss=0.9911]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 274/292 [03:04<00:15,  1.17batch/s, auc=0.8584, loss=0.6727]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 275/292 [03:04<00:13,  1.30batch/s, auc=0.8584, loss=0.6727]\u001b[A\n",
      "Training Epoch 4/25:  94%|█████████▍| 275/292 [03:04<00:13,  1.30batch/s, auc=0.8584, loss=0.7286]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 276/292 [03:04<00:11,  1.41batch/s, auc=0.8584, loss=0.7286]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 276/292 [03:05<00:11,  1.41batch/s, auc=0.8585, loss=0.7799]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 277/292 [03:05<00:09,  1.50batch/s, auc=0.8585, loss=0.7799]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▍| 277/292 [03:05<00:09,  1.50batch/s, auc=0.8584, loss=0.8487]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▌| 278/292 [03:05<00:08,  1.57batch/s, auc=0.8584, loss=0.8487]\u001b[A\n",
      "Training Epoch 4/25:  95%|█████████▌| 278/292 [03:06<00:08,  1.57batch/s, auc=0.8583, loss=0.7178]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 279/292 [03:06<00:07,  1.63batch/s, auc=0.8583, loss=0.7178]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 279/292 [03:07<00:07,  1.63batch/s, auc=0.8584, loss=0.6821]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 280/292 [03:07<00:07,  1.67batch/s, auc=0.8584, loss=0.6821]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 280/292 [03:07<00:07,  1.67batch/s, auc=0.8584, loss=0.6826]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 281/292 [03:07<00:06,  1.70batch/s, auc=0.8584, loss=0.6826]\u001b[A\n",
      "Training Epoch 4/25:  96%|█████████▌| 281/292 [03:08<00:06,  1.70batch/s, auc=0.8587, loss=0.4691]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 282/292 [03:08<00:05,  1.72batch/s, auc=0.8587, loss=0.4691]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 282/292 [03:08<00:05,  1.72batch/s, auc=0.8587, loss=0.6347]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 283/292 [03:08<00:05,  1.73batch/s, auc=0.8587, loss=0.6347]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 283/292 [03:09<00:05,  1.73batch/s, auc=0.8590, loss=0.6635]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 284/292 [03:09<00:04,  1.74batch/s, auc=0.8590, loss=0.6635]\u001b[A\n",
      "Training Epoch 4/25:  97%|█████████▋| 284/292 [03:09<00:04,  1.74batch/s, auc=0.8590, loss=0.7463]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 285/292 [03:09<00:03,  1.75batch/s, auc=0.8590, loss=0.7463]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 285/292 [03:10<00:03,  1.75batch/s, auc=0.8594, loss=0.5925]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 286/292 [03:10<00:03,  1.76batch/s, auc=0.8594, loss=0.5925]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 286/292 [03:10<00:03,  1.76batch/s, auc=0.8592, loss=0.6452]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 287/292 [03:10<00:02,  1.76batch/s, auc=0.8592, loss=0.6452]\u001b[A\n",
      "Training Epoch 4/25:  98%|█████████▊| 287/292 [03:12<00:02,  1.76batch/s, auc=0.8589, loss=1.0377]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▊| 288/292 [03:12<00:02,  1.39batch/s, auc=0.8589, loss=1.0377]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▊| 288/292 [03:12<00:02,  1.39batch/s, auc=0.8588, loss=0.7857]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 289/292 [03:12<00:02,  1.48batch/s, auc=0.8588, loss=0.7857]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 289/292 [03:13<00:02,  1.48batch/s, auc=0.8587, loss=1.0522]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 290/292 [03:13<00:01,  1.56batch/s, auc=0.8587, loss=1.0522]\u001b[A\n",
      "Training Epoch 4/25:  99%|█████████▉| 290/292 [03:13<00:01,  1.56batch/s, auc=0.8588, loss=0.6447]\u001b[A\n",
      "Training Epoch 4/25: 100%|█████████▉| 291/292 [03:13<00:00,  1.62batch/s, auc=0.8588, loss=0.6447]\u001b[A\n",
      "Training Epoch 4/25: 100%|█████████▉| 291/292 [03:14<00:00,  1.62batch/s, auc=0.8588, loss=0.7034]\u001b[A\n",
      "Training Epoch 4/25: 100%|██████████| 292/292 [03:14<00:00,  1.50batch/s, auc=0.8588, loss=0.7034]\u001b[A\n",
      "Epochs:  16%|█▌        | 4/25 [14:01<1:13:48, 210.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25] Train Loss: 0.7938 | Train AUROC: 0.8588 Val Loss: 0.8504 | Val AUROC: 0.8326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 5/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8822, loss=0.6406]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 1/292 [00:01<07:24,  1.53s/batch, auc=0.8822, loss=0.6406]\u001b[A\n",
      "Training Epoch 5/25:   0%|          | 1/292 [00:02<07:24,  1.53s/batch, auc=0.9068, loss=0.6609]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 2/292 [00:02<04:37,  1.05batch/s, auc=0.9068, loss=0.6609]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 2/292 [00:03<04:37,  1.05batch/s, auc=0.8496, loss=1.1461]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 3/292 [00:03<04:51,  1.01s/batch, auc=0.8496, loss=1.1461]\u001b[A\n",
      "Training Epoch 5/25:   1%|          | 3/292 [00:03<04:51,  1.01s/batch, auc=0.8546, loss=0.7317]\u001b[A\n",
      "Training Epoch 5/25:   1%|▏         | 4/292 [00:03<03:59,  1.20batch/s, auc=0.8546, loss=0.7317]\u001b[A\n",
      "Training Epoch 5/25:   1%|▏         | 4/292 [00:04<03:59,  1.20batch/s, auc=0.8564, loss=0.8488]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 5/292 [00:04<03:29,  1.37batch/s, auc=0.8564, loss=0.8488]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 5/292 [00:05<03:29,  1.37batch/s, auc=0.8310, loss=1.3507]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 6/292 [00:05<04:02,  1.18batch/s, auc=0.8310, loss=1.3507]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 6/292 [00:06<04:02,  1.18batch/s, auc=0.8058, loss=1.2763]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 7/292 [00:06<04:22,  1.09batch/s, auc=0.8058, loss=1.2763]\u001b[A\n",
      "Training Epoch 5/25:   2%|▏         | 7/292 [00:07<04:22,  1.09batch/s, auc=0.7908, loss=1.0453]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 8/292 [00:07<04:34,  1.03batch/s, auc=0.7908, loss=1.0453]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 8/292 [00:08<04:34,  1.03batch/s, auc=0.8115, loss=0.5910]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 9/292 [00:08<03:57,  1.19batch/s, auc=0.8115, loss=0.5910]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 9/292 [00:08<03:57,  1.19batch/s, auc=0.8160, loss=0.6524]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 10/292 [00:08<03:31,  1.33batch/s, auc=0.8160, loss=0.6524]\u001b[A\n",
      "Training Epoch 5/25:   3%|▎         | 10/292 [00:09<03:31,  1.33batch/s, auc=0.8166, loss=0.7115]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 11/292 [00:09<03:14,  1.45batch/s, auc=0.8166, loss=0.7115]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 11/292 [00:09<03:14,  1.45batch/s, auc=0.8287, loss=0.7808]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 12/292 [00:09<03:02,  1.54batch/s, auc=0.8287, loss=0.7808]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 12/292 [00:10<03:02,  1.54batch/s, auc=0.8264, loss=1.0040]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 13/292 [00:10<03:36,  1.29batch/s, auc=0.8264, loss=1.0040]\u001b[A\n",
      "Training Epoch 5/25:   4%|▍         | 13/292 [00:11<03:36,  1.29batch/s, auc=0.8335, loss=0.6028]\u001b[A\n",
      "Training Epoch 5/25:   5%|▍         | 14/292 [00:11<03:17,  1.41batch/s, auc=0.8335, loss=0.6028]\u001b[A\n",
      "Training Epoch 5/25:   5%|▍         | 14/292 [00:11<03:17,  1.41batch/s, auc=0.8366, loss=0.6647]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 15/292 [00:11<03:03,  1.51batch/s, auc=0.8366, loss=0.6647]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 15/292 [00:12<03:03,  1.51batch/s, auc=0.8397, loss=0.5469]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 16/292 [00:12<02:54,  1.58batch/s, auc=0.8397, loss=0.5469]\u001b[A\n",
      "Training Epoch 5/25:   5%|▌         | 16/292 [00:12<02:54,  1.58batch/s, auc=0.8439, loss=0.5712]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 17/292 [00:12<02:47,  1.64batch/s, auc=0.8439, loss=0.5712]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 17/292 [00:13<02:47,  1.64batch/s, auc=0.8492, loss=0.6538]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 18/292 [00:13<02:42,  1.69batch/s, auc=0.8492, loss=0.6538]\u001b[A\n",
      "Training Epoch 5/25:   6%|▌         | 18/292 [00:14<02:42,  1.69batch/s, auc=0.8532, loss=0.6226]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 19/292 [00:14<02:38,  1.72batch/s, auc=0.8532, loss=0.6226]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 19/292 [00:14<02:38,  1.72batch/s, auc=0.8475, loss=1.1826]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 20/292 [00:14<02:36,  1.74batch/s, auc=0.8475, loss=1.1826]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 20/292 [00:15<02:36,  1.74batch/s, auc=0.8514, loss=0.5032]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 21/292 [00:15<02:34,  1.76batch/s, auc=0.8514, loss=0.5032]\u001b[A\n",
      "Training Epoch 5/25:   7%|▋         | 21/292 [00:15<02:34,  1.76batch/s, auc=0.8561, loss=0.6651]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 22/292 [00:15<02:32,  1.77batch/s, auc=0.8561, loss=0.6651]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 22/292 [00:16<02:32,  1.77batch/s, auc=0.8473, loss=1.3683]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 23/292 [00:16<03:12,  1.40batch/s, auc=0.8473, loss=1.3683]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 23/292 [00:17<03:12,  1.40batch/s, auc=0.8519, loss=0.6078]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 24/292 [00:17<02:59,  1.50batch/s, auc=0.8519, loss=0.6078]\u001b[A\n",
      "Training Epoch 5/25:   8%|▊         | 24/292 [00:17<02:59,  1.50batch/s, auc=0.8523, loss=0.7636]\u001b[A\n",
      "Training Epoch 5/25:   9%|▊         | 25/292 [00:17<02:49,  1.58batch/s, auc=0.8523, loss=0.7636]\u001b[A\n",
      "Training Epoch 5/25:   9%|▊         | 25/292 [00:18<02:49,  1.58batch/s, auc=0.8559, loss=0.6676]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 26/292 [00:18<02:42,  1.64batch/s, auc=0.8559, loss=0.6676]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 26/292 [00:19<02:42,  1.64batch/s, auc=0.8590, loss=0.6514]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 27/292 [00:19<02:37,  1.68batch/s, auc=0.8590, loss=0.6514]\u001b[A\n",
      "Training Epoch 5/25:   9%|▉         | 27/292 [00:20<02:37,  1.68batch/s, auc=0.8568, loss=0.9151]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 28/292 [00:20<03:14,  1.36batch/s, auc=0.8568, loss=0.9151]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 28/292 [00:20<03:14,  1.36batch/s, auc=0.8593, loss=0.7084]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 29/292 [00:20<02:59,  1.47batch/s, auc=0.8593, loss=0.7084]\u001b[A\n",
      "Training Epoch 5/25:  10%|▉         | 29/292 [00:21<02:59,  1.47batch/s, auc=0.8583, loss=0.6933]\u001b[A\n",
      "Training Epoch 5/25:  10%|█         | 30/292 [00:21<02:48,  1.55batch/s, auc=0.8583, loss=0.6933]\u001b[A\n",
      "Training Epoch 5/25:  10%|█         | 30/292 [00:21<02:48,  1.55batch/s, auc=0.8603, loss=0.6419]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 31/292 [00:21<02:41,  1.62batch/s, auc=0.8603, loss=0.6419]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 31/292 [00:22<02:41,  1.62batch/s, auc=0.8599, loss=0.8500]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 32/292 [00:22<02:35,  1.67batch/s, auc=0.8599, loss=0.8500]\u001b[A\n",
      "Training Epoch 5/25:  11%|█         | 32/292 [00:22<02:35,  1.67batch/s, auc=0.8602, loss=0.8350]\u001b[A\n",
      "Training Epoch 5/25:  11%|█▏        | 33/292 [00:22<02:31,  1.71batch/s, auc=0.8602, loss=0.8350]\u001b[A\n",
      "Training Epoch 5/25:  11%|█▏        | 33/292 [00:23<02:31,  1.71batch/s, auc=0.8624, loss=0.6820]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 34/292 [00:23<02:28,  1.74batch/s, auc=0.8624, loss=0.6820]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 34/292 [00:24<02:28,  1.74batch/s, auc=0.8591, loss=1.1826]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 35/292 [00:24<03:06,  1.38batch/s, auc=0.8591, loss=1.1826]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 35/292 [00:25<03:06,  1.38batch/s, auc=0.8601, loss=0.6022]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 36/292 [00:25<02:52,  1.48batch/s, auc=0.8601, loss=0.6022]\u001b[A\n",
      "Training Epoch 5/25:  12%|█▏        | 36/292 [00:25<02:52,  1.48batch/s, auc=0.8624, loss=0.6142]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 37/292 [00:25<02:43,  1.56batch/s, auc=0.8624, loss=0.6142]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 37/292 [00:26<02:43,  1.56batch/s, auc=0.8641, loss=0.5580]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 38/292 [00:26<02:36,  1.63batch/s, auc=0.8641, loss=0.5580]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 38/292 [00:26<02:36,  1.63batch/s, auc=0.8630, loss=0.6849]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 39/292 [00:26<02:31,  1.68batch/s, auc=0.8630, loss=0.6849]\u001b[A\n",
      "Training Epoch 5/25:  13%|█▎        | 39/292 [00:27<02:31,  1.68batch/s, auc=0.8647, loss=0.7000]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▎        | 40/292 [00:27<02:27,  1.71batch/s, auc=0.8647, loss=0.7000]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▎        | 40/292 [00:27<02:27,  1.71batch/s, auc=0.8639, loss=0.7372]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 41/292 [00:27<02:24,  1.74batch/s, auc=0.8639, loss=0.7372]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 41/292 [00:28<02:24,  1.74batch/s, auc=0.8673, loss=0.7067]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 42/292 [00:28<02:22,  1.75batch/s, auc=0.8673, loss=0.7067]\u001b[A\n",
      "Training Epoch 5/25:  14%|█▍        | 42/292 [00:29<02:22,  1.75batch/s, auc=0.8619, loss=1.4917]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▍        | 43/292 [00:29<02:59,  1.39batch/s, auc=0.8619, loss=1.4917]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▍        | 43/292 [00:30<02:59,  1.39batch/s, auc=0.8591, loss=1.1443]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 44/292 [00:30<03:24,  1.21batch/s, auc=0.8591, loss=1.1443]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 44/292 [00:31<03:24,  1.21batch/s, auc=0.8562, loss=1.3829]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 45/292 [00:31<03:42,  1.11batch/s, auc=0.8562, loss=1.3829]\u001b[A\n",
      "Training Epoch 5/25:  15%|█▌        | 45/292 [00:32<03:42,  1.11batch/s, auc=0.8554, loss=0.7212]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 46/292 [00:32<03:16,  1.25batch/s, auc=0.8554, loss=0.7212]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 46/292 [00:32<03:16,  1.25batch/s, auc=0.8549, loss=0.6636]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 47/292 [00:32<02:57,  1.38batch/s, auc=0.8549, loss=0.6636]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▌        | 47/292 [00:33<02:57,  1.38batch/s, auc=0.8546, loss=0.6863]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▋        | 48/292 [00:33<02:44,  1.48batch/s, auc=0.8546, loss=0.6863]\u001b[A\n",
      "Training Epoch 5/25:  16%|█▋        | 48/292 [00:33<02:44,  1.48batch/s, auc=0.8559, loss=0.6232]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 49/292 [00:33<02:35,  1.56batch/s, auc=0.8559, loss=0.6232]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 49/292 [00:34<02:35,  1.56batch/s, auc=0.8570, loss=0.6057]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 50/292 [00:34<02:28,  1.63batch/s, auc=0.8570, loss=0.6057]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 50/292 [00:34<02:28,  1.63batch/s, auc=0.8574, loss=0.7094]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 51/292 [00:34<02:23,  1.68batch/s, auc=0.8574, loss=0.7094]\u001b[A\n",
      "Training Epoch 5/25:  17%|█▋        | 51/292 [00:35<02:23,  1.68batch/s, auc=0.8580, loss=0.5962]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 52/292 [00:35<02:20,  1.71batch/s, auc=0.8580, loss=0.5962]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 52/292 [00:36<02:20,  1.71batch/s, auc=0.8593, loss=0.5791]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 53/292 [00:36<02:17,  1.73batch/s, auc=0.8593, loss=0.5791]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 53/292 [00:36<02:17,  1.73batch/s, auc=0.8603, loss=0.5647]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 54/292 [00:36<02:15,  1.75batch/s, auc=0.8603, loss=0.5647]\u001b[A\n",
      "Training Epoch 5/25:  18%|█▊        | 54/292 [00:37<02:15,  1.75batch/s, auc=0.8564, loss=1.3386]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 55/292 [00:37<02:51,  1.38batch/s, auc=0.8564, loss=1.3386]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 55/292 [00:38<02:51,  1.38batch/s, auc=0.8570, loss=0.7324]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 56/292 [00:38<02:38,  1.49batch/s, auc=0.8570, loss=0.7324]\u001b[A\n",
      "Training Epoch 5/25:  19%|█▉        | 56/292 [00:38<02:38,  1.49batch/s, auc=0.8583, loss=0.5158]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 57/292 [00:38<02:29,  1.57batch/s, auc=0.8583, loss=0.5158]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 57/292 [00:39<02:29,  1.57batch/s, auc=0.8589, loss=0.6167]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 58/292 [00:39<02:23,  1.63batch/s, auc=0.8589, loss=0.6167]\u001b[A\n",
      "Training Epoch 5/25:  20%|█▉        | 58/292 [00:39<02:23,  1.63batch/s, auc=0.8572, loss=1.0054]\u001b[A\n",
      "Training Epoch 5/25:  20%|██        | 59/292 [00:39<02:18,  1.68batch/s, auc=0.8572, loss=1.0054]\u001b[A\n",
      "Training Epoch 5/25:  20%|██        | 59/292 [00:40<02:18,  1.68batch/s, auc=0.8574, loss=0.8001]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 60/292 [00:40<02:15,  1.71batch/s, auc=0.8574, loss=0.8001]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 60/292 [00:41<02:15,  1.71batch/s, auc=0.8545, loss=1.2616]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 61/292 [00:41<02:48,  1.37batch/s, auc=0.8545, loss=1.2616]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 61/292 [00:42<02:48,  1.37batch/s, auc=0.8504, loss=1.5902]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 62/292 [00:42<03:11,  1.20batch/s, auc=0.8504, loss=1.5902]\u001b[A\n",
      "Training Epoch 5/25:  21%|██        | 62/292 [00:43<03:11,  1.20batch/s, auc=0.8505, loss=0.6262]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 63/292 [00:43<02:51,  1.33batch/s, auc=0.8505, loss=0.6262]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 63/292 [00:44<02:51,  1.33batch/s, auc=0.8484, loss=1.2308]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 64/292 [00:44<03:13,  1.18batch/s, auc=0.8484, loss=1.2308]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 64/292 [00:45<03:13,  1.18batch/s, auc=0.8451, loss=1.4321]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 65/292 [00:45<03:27,  1.09batch/s, auc=0.8451, loss=1.4321]\u001b[A\n",
      "Training Epoch 5/25:  22%|██▏       | 65/292 [00:45<03:27,  1.09batch/s, auc=0.8466, loss=0.5716]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 66/292 [00:45<03:02,  1.24batch/s, auc=0.8466, loss=0.5716]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 66/292 [00:46<03:02,  1.24batch/s, auc=0.8482, loss=0.6643]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 67/292 [00:46<02:44,  1.36batch/s, auc=0.8482, loss=0.6643]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 67/292 [00:47<02:44,  1.36batch/s, auc=0.8478, loss=0.9229]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 68/292 [00:47<02:32,  1.47batch/s, auc=0.8478, loss=0.9229]\u001b[A\n",
      "Training Epoch 5/25:  23%|██▎       | 68/292 [00:47<02:32,  1.47batch/s, auc=0.8484, loss=0.7250]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▎       | 69/292 [00:47<02:23,  1.55batch/s, auc=0.8484, loss=0.7250]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▎       | 69/292 [00:48<02:23,  1.55batch/s, auc=0.8486, loss=0.7316]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 70/292 [00:48<02:17,  1.62batch/s, auc=0.8486, loss=0.7316]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 70/292 [00:49<02:17,  1.62batch/s, auc=0.8467, loss=1.0558]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 71/292 [00:49<02:46,  1.33batch/s, auc=0.8467, loss=1.0558]\u001b[A\n",
      "Training Epoch 5/25:  24%|██▍       | 71/292 [00:50<02:46,  1.33batch/s, auc=0.8465, loss=0.9175]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▍       | 72/292 [00:50<03:06,  1.18batch/s, auc=0.8465, loss=0.9175]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▍       | 72/292 [00:51<03:06,  1.18batch/s, auc=0.8452, loss=1.1940]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 73/292 [00:51<03:20,  1.09batch/s, auc=0.8452, loss=1.1940]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 73/292 [00:52<03:20,  1.09batch/s, auc=0.8429, loss=1.2016]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 74/292 [00:52<03:30,  1.04batch/s, auc=0.8429, loss=1.2016]\u001b[A\n",
      "Training Epoch 5/25:  25%|██▌       | 74/292 [00:53<03:30,  1.04batch/s, auc=0.8435, loss=0.6725]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 75/292 [00:53<03:02,  1.19batch/s, auc=0.8435, loss=0.6725]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 75/292 [00:54<03:02,  1.19batch/s, auc=0.8417, loss=1.1342]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 76/292 [00:54<03:16,  1.10batch/s, auc=0.8417, loss=1.1342]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▌       | 76/292 [00:54<03:16,  1.10batch/s, auc=0.8425, loss=0.8262]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▋       | 77/292 [00:54<02:53,  1.24batch/s, auc=0.8425, loss=0.8262]\u001b[A\n",
      "Training Epoch 5/25:  26%|██▋       | 77/292 [00:55<02:53,  1.24batch/s, auc=0.8422, loss=0.8637]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 78/292 [00:55<02:36,  1.37batch/s, auc=0.8422, loss=0.8637]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 78/292 [00:55<02:36,  1.37batch/s, auc=0.8429, loss=0.6193]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 79/292 [00:55<02:24,  1.47batch/s, auc=0.8429, loss=0.6193]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 79/292 [00:56<02:24,  1.47batch/s, auc=0.8433, loss=0.7722]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 80/292 [00:56<02:16,  1.56batch/s, auc=0.8433, loss=0.7722]\u001b[A\n",
      "Training Epoch 5/25:  27%|██▋       | 80/292 [00:56<02:16,  1.56batch/s, auc=0.8440, loss=0.6189]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 81/292 [00:56<02:10,  1.62batch/s, auc=0.8440, loss=0.6189]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 81/292 [00:57<02:10,  1.62batch/s, auc=0.8435, loss=0.8784]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 82/292 [00:57<02:38,  1.33batch/s, auc=0.8435, loss=0.8784]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 82/292 [00:58<02:38,  1.33batch/s, auc=0.8441, loss=0.6587]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 83/292 [00:58<02:25,  1.44batch/s, auc=0.8441, loss=0.6587]\u001b[A\n",
      "Training Epoch 5/25:  28%|██▊       | 83/292 [00:59<02:25,  1.44batch/s, auc=0.8449, loss=0.6990]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 84/292 [00:59<02:16,  1.53batch/s, auc=0.8449, loss=0.6990]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 84/292 [00:59<02:16,  1.53batch/s, auc=0.8465, loss=0.6571]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 85/292 [00:59<02:09,  1.60batch/s, auc=0.8465, loss=0.6571]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 85/292 [01:00<02:09,  1.60batch/s, auc=0.8467, loss=0.6766]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 86/292 [01:00<02:04,  1.65batch/s, auc=0.8467, loss=0.6766]\u001b[A\n",
      "Training Epoch 5/25:  29%|██▉       | 86/292 [01:01<02:04,  1.65batch/s, auc=0.8461, loss=0.8457]\u001b[A\n",
      "Training Epoch 5/25:  30%|██▉       | 87/292 [01:01<02:32,  1.34batch/s, auc=0.8461, loss=0.8457]\u001b[A\n",
      "Training Epoch 5/25:  30%|██▉       | 87/292 [01:01<02:32,  1.34batch/s, auc=0.8480, loss=0.5991]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 88/292 [01:01<02:20,  1.45batch/s, auc=0.8480, loss=0.5991]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 88/292 [01:02<02:20,  1.45batch/s, auc=0.8488, loss=0.6016]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 89/292 [01:02<02:12,  1.54batch/s, auc=0.8488, loss=0.6016]\u001b[A\n",
      "Training Epoch 5/25:  30%|███       | 89/292 [01:02<02:12,  1.54batch/s, auc=0.8494, loss=0.6021]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 90/292 [01:02<02:05,  1.61batch/s, auc=0.8494, loss=0.6021]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 90/292 [01:04<02:05,  1.61batch/s, auc=0.8473, loss=1.3222]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 91/292 [01:04<02:32,  1.32batch/s, auc=0.8473, loss=1.3222]\u001b[A\n",
      "Training Epoch 5/25:  31%|███       | 91/292 [01:04<02:32,  1.32batch/s, auc=0.8473, loss=0.5444]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 92/292 [01:04<02:19,  1.43batch/s, auc=0.8473, loss=0.5444]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 92/292 [01:05<02:19,  1.43batch/s, auc=0.8476, loss=0.9129]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 93/292 [01:05<02:10,  1.52batch/s, auc=0.8476, loss=0.9129]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 93/292 [01:05<02:10,  1.52batch/s, auc=0.8481, loss=0.8360]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 94/292 [01:05<02:04,  1.60batch/s, auc=0.8481, loss=0.8360]\u001b[A\n",
      "Training Epoch 5/25:  32%|███▏      | 94/292 [01:06<02:04,  1.60batch/s, auc=0.8478, loss=0.7059]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 95/292 [01:06<01:59,  1.65batch/s, auc=0.8478, loss=0.7059]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 95/292 [01:06<01:59,  1.65batch/s, auc=0.8490, loss=0.5244]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 96/292 [01:06<01:55,  1.69batch/s, auc=0.8490, loss=0.5244]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 96/292 [01:07<01:55,  1.69batch/s, auc=0.8480, loss=1.4177]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 97/292 [01:07<01:53,  1.72batch/s, auc=0.8480, loss=1.4177]\u001b[A\n",
      "Training Epoch 5/25:  33%|███▎      | 97/292 [01:07<01:53,  1.72batch/s, auc=0.8486, loss=0.7081]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▎      | 98/292 [01:07<01:51,  1.74batch/s, auc=0.8486, loss=0.7081]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▎      | 98/292 [01:08<01:51,  1.74batch/s, auc=0.8495, loss=0.7305]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 99/292 [01:08<01:49,  1.76batch/s, auc=0.8495, loss=0.7305]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 99/292 [01:09<01:49,  1.76batch/s, auc=0.8494, loss=0.7552]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 100/292 [01:09<01:48,  1.77batch/s, auc=0.8494, loss=0.7552]\u001b[A\n",
      "Training Epoch 5/25:  34%|███▍      | 100/292 [01:09<01:48,  1.77batch/s, auc=0.8505, loss=0.6485]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 101/292 [01:09<01:47,  1.77batch/s, auc=0.8505, loss=0.6485]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 101/292 [01:10<01:47,  1.77batch/s, auc=0.8515, loss=0.5988]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 102/292 [01:10<01:46,  1.78batch/s, auc=0.8515, loss=0.5988]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▍      | 102/292 [01:11<01:46,  1.78batch/s, auc=0.8513, loss=0.7316]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▌      | 103/292 [01:11<02:15,  1.40batch/s, auc=0.8513, loss=0.7316]\u001b[A\n",
      "Training Epoch 5/25:  35%|███▌      | 103/292 [01:11<02:15,  1.40batch/s, auc=0.8512, loss=0.6330]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 104/292 [01:11<02:05,  1.49batch/s, auc=0.8512, loss=0.6330]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 104/292 [01:12<02:05,  1.49batch/s, auc=0.8506, loss=1.0487]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 105/292 [01:12<02:27,  1.26batch/s, auc=0.8506, loss=1.0487]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▌      | 105/292 [01:13<02:27,  1.26batch/s, auc=0.8512, loss=0.6654]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▋      | 106/292 [01:13<02:14,  1.39batch/s, auc=0.8512, loss=0.6654]\u001b[A\n",
      "Training Epoch 5/25:  36%|███▋      | 106/292 [01:13<02:14,  1.39batch/s, auc=0.8521, loss=0.5265]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 107/292 [01:13<02:04,  1.49batch/s, auc=0.8521, loss=0.5265]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 107/292 [01:14<02:04,  1.49batch/s, auc=0.8515, loss=1.0649]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 108/292 [01:14<01:57,  1.57batch/s, auc=0.8515, loss=1.0649]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 108/292 [01:15<01:57,  1.57batch/s, auc=0.8519, loss=0.5758]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 109/292 [01:15<01:52,  1.63batch/s, auc=0.8519, loss=0.5758]\u001b[A\n",
      "Training Epoch 5/25:  37%|███▋      | 109/292 [01:15<01:52,  1.63batch/s, auc=0.8520, loss=0.7158]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 110/292 [01:15<01:48,  1.67batch/s, auc=0.8520, loss=0.7158]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 110/292 [01:16<01:48,  1.67batch/s, auc=0.8516, loss=0.8372]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 111/292 [01:16<02:14,  1.35batch/s, auc=0.8516, loss=0.8372]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 111/292 [01:17<02:14,  1.35batch/s, auc=0.8515, loss=0.7403]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 112/292 [01:17<02:03,  1.46batch/s, auc=0.8515, loss=0.7403]\u001b[A\n",
      "Training Epoch 5/25:  38%|███▊      | 112/292 [01:17<02:03,  1.46batch/s, auc=0.8513, loss=0.8224]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▊      | 113/292 [01:17<01:56,  1.54batch/s, auc=0.8513, loss=0.8224]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▊      | 113/292 [01:18<01:56,  1.54batch/s, auc=0.8516, loss=0.7902]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 114/292 [01:18<01:50,  1.61batch/s, auc=0.8516, loss=0.7902]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 114/292 [01:18<01:50,  1.61batch/s, auc=0.8527, loss=0.4971]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 115/292 [01:18<01:46,  1.66batch/s, auc=0.8527, loss=0.4971]\u001b[A\n",
      "Training Epoch 5/25:  39%|███▉      | 115/292 [01:20<01:46,  1.66batch/s, auc=0.8527, loss=0.8242]\u001b[A\n",
      "Training Epoch 5/25:  40%|███▉      | 116/292 [01:20<02:10,  1.34batch/s, auc=0.8527, loss=0.8242]\u001b[A\n",
      "Training Epoch 5/25:  40%|███▉      | 116/292 [01:20<02:10,  1.34batch/s, auc=0.8533, loss=0.6677]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 117/292 [01:20<02:00,  1.45batch/s, auc=0.8533, loss=0.6677]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 117/292 [01:21<02:00,  1.45batch/s, auc=0.8535, loss=0.8548]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 118/292 [01:21<01:53,  1.54batch/s, auc=0.8535, loss=0.8548]\u001b[A\n",
      "Training Epoch 5/25:  40%|████      | 118/292 [01:21<01:53,  1.54batch/s, auc=0.8537, loss=0.5734]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 119/292 [01:21<01:47,  1.61batch/s, auc=0.8537, loss=0.5734]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 119/292 [01:22<01:47,  1.61batch/s, auc=0.8547, loss=0.6618]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 120/292 [01:22<01:43,  1.66batch/s, auc=0.8547, loss=0.6618]\u001b[A\n",
      "Training Epoch 5/25:  41%|████      | 120/292 [01:22<01:43,  1.66batch/s, auc=0.8557, loss=0.3654]\u001b[A\n",
      "Training Epoch 5/25:  41%|████▏     | 121/292 [01:22<01:40,  1.69batch/s, auc=0.8557, loss=0.3654]\u001b[A\n",
      "Training Epoch 5/25:  41%|████▏     | 121/292 [01:23<01:40,  1.69batch/s, auc=0.8560, loss=0.6364]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 122/292 [01:23<01:38,  1.72batch/s, auc=0.8560, loss=0.6364]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 122/292 [01:23<01:38,  1.72batch/s, auc=0.8563, loss=0.6492]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 123/292 [01:23<01:36,  1.74batch/s, auc=0.8563, loss=0.6492]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 123/292 [01:24<01:36,  1.74batch/s, auc=0.8567, loss=0.5022]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 124/292 [01:24<01:35,  1.76batch/s, auc=0.8567, loss=0.5022]\u001b[A\n",
      "Training Epoch 5/25:  42%|████▏     | 124/292 [01:25<01:35,  1.76batch/s, auc=0.8576, loss=0.6257]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 125/292 [01:25<01:34,  1.77batch/s, auc=0.8576, loss=0.6257]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 125/292 [01:25<01:34,  1.77batch/s, auc=0.8581, loss=0.7151]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 126/292 [01:25<01:33,  1.77batch/s, auc=0.8581, loss=0.7151]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 126/292 [01:26<01:33,  1.77batch/s, auc=0.8584, loss=0.8192]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 127/292 [01:26<01:32,  1.78batch/s, auc=0.8584, loss=0.8192]\u001b[A\n",
      "Training Epoch 5/25:  43%|████▎     | 127/292 [01:26<01:32,  1.78batch/s, auc=0.8584, loss=0.7737]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 128/292 [01:26<01:32,  1.78batch/s, auc=0.8584, loss=0.7737]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 128/292 [01:27<01:32,  1.78batch/s, auc=0.8590, loss=0.5375]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 129/292 [01:27<01:31,  1.78batch/s, auc=0.8590, loss=0.5375]\u001b[A\n",
      "Training Epoch 5/25:  44%|████▍     | 129/292 [01:27<01:31,  1.78batch/s, auc=0.8589, loss=0.5787]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 130/292 [01:27<01:30,  1.78batch/s, auc=0.8589, loss=0.5787]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 130/292 [01:28<01:30,  1.78batch/s, auc=0.8596, loss=0.5612]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 131/292 [01:28<01:30,  1.78batch/s, auc=0.8596, loss=0.5612]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▍     | 131/292 [01:28<01:30,  1.78batch/s, auc=0.8598, loss=0.7439]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▌     | 132/292 [01:28<01:29,  1.79batch/s, auc=0.8598, loss=0.7439]\u001b[A\n",
      "Training Epoch 5/25:  45%|████▌     | 132/292 [01:29<01:29,  1.79batch/s, auc=0.8604, loss=0.6020]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 133/292 [01:29<01:28,  1.79batch/s, auc=0.8604, loss=0.6020]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 133/292 [01:30<01:28,  1.79batch/s, auc=0.8607, loss=0.7274]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 134/292 [01:30<01:28,  1.79batch/s, auc=0.8607, loss=0.7274]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 134/292 [01:31<01:28,  1.79batch/s, auc=0.8593, loss=1.4419]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 135/292 [01:31<01:52,  1.40batch/s, auc=0.8593, loss=1.4419]\u001b[A\n",
      "Training Epoch 5/25:  46%|████▌     | 135/292 [01:31<01:52,  1.40batch/s, auc=0.8589, loss=0.8965]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 136/292 [01:31<01:44,  1.50batch/s, auc=0.8589, loss=0.8965]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 136/292 [01:32<01:44,  1.50batch/s, auc=0.8588, loss=0.8804]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 137/292 [01:32<02:02,  1.27batch/s, auc=0.8588, loss=0.8804]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 137/292 [01:33<02:02,  1.27batch/s, auc=0.8587, loss=0.6022]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 138/292 [01:33<01:51,  1.39batch/s, auc=0.8587, loss=0.6022]\u001b[A\n",
      "Training Epoch 5/25:  47%|████▋     | 138/292 [01:34<01:51,  1.39batch/s, auc=0.8587, loss=0.8764]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 139/292 [01:34<02:06,  1.21batch/s, auc=0.8587, loss=0.8764]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 139/292 [01:34<02:06,  1.21batch/s, auc=0.8603, loss=0.5856]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 140/292 [01:34<01:53,  1.34batch/s, auc=0.8603, loss=0.5856]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 140/292 [01:35<01:53,  1.34batch/s, auc=0.8604, loss=0.5864]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 141/292 [01:35<01:44,  1.45batch/s, auc=0.8604, loss=0.5864]\u001b[A\n",
      "Training Epoch 5/25:  48%|████▊     | 141/292 [01:36<01:44,  1.45batch/s, auc=0.8606, loss=0.7701]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▊     | 142/292 [01:36<01:37,  1.53batch/s, auc=0.8606, loss=0.7701]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▊     | 142/292 [01:36<01:37,  1.53batch/s, auc=0.8608, loss=0.6534]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 143/292 [01:36<01:33,  1.60batch/s, auc=0.8608, loss=0.6534]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 143/292 [01:37<01:33,  1.60batch/s, auc=0.8610, loss=0.5484]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 144/292 [01:37<01:29,  1.65batch/s, auc=0.8610, loss=0.5484]\u001b[A\n",
      "Training Epoch 5/25:  49%|████▉     | 144/292 [01:37<01:29,  1.65batch/s, auc=0.8611, loss=0.5592]\u001b[A\n",
      "Training Epoch 5/25:  50%|████▉     | 145/292 [01:37<01:26,  1.69batch/s, auc=0.8611, loss=0.5592]\u001b[A\n",
      "Training Epoch 5/25:  50%|████▉     | 145/292 [01:38<01:26,  1.69batch/s, auc=0.8594, loss=1.5629]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 146/292 [01:38<01:24,  1.72batch/s, auc=0.8594, loss=1.5629]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 146/292 [01:38<01:24,  1.72batch/s, auc=0.8595, loss=0.5638]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 147/292 [01:38<01:23,  1.74batch/s, auc=0.8595, loss=0.5638]\u001b[A\n",
      "Training Epoch 5/25:  50%|█████     | 147/292 [01:39<01:23,  1.74batch/s, auc=0.8604, loss=0.5658]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 148/292 [01:39<01:22,  1.75batch/s, auc=0.8604, loss=0.5658]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 148/292 [01:40<01:22,  1.75batch/s, auc=0.8599, loss=1.0989]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 149/292 [01:40<01:21,  1.76batch/s, auc=0.8599, loss=1.0989]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████     | 149/292 [01:40<01:21,  1.76batch/s, auc=0.8600, loss=0.7200]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████▏    | 150/292 [01:40<01:20,  1.77batch/s, auc=0.8600, loss=0.7200]\u001b[A\n",
      "Training Epoch 5/25:  51%|█████▏    | 150/292 [01:41<01:20,  1.77batch/s, auc=0.8604, loss=0.5358]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 151/292 [01:41<01:19,  1.77batch/s, auc=0.8604, loss=0.5358]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 151/292 [01:41<01:19,  1.77batch/s, auc=0.8606, loss=0.8256]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 152/292 [01:41<01:18,  1.77batch/s, auc=0.8606, loss=0.8256]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 152/292 [01:42<01:18,  1.77batch/s, auc=0.8582, loss=2.0267]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 153/292 [01:42<01:39,  1.39batch/s, auc=0.8582, loss=2.0267]\u001b[A\n",
      "Training Epoch 5/25:  52%|█████▏    | 153/292 [01:43<01:39,  1.39batch/s, auc=0.8582, loss=0.6768]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 154/292 [01:43<01:32,  1.49batch/s, auc=0.8582, loss=0.6768]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 154/292 [01:44<01:32,  1.49batch/s, auc=0.8577, loss=0.8638]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 155/292 [01:44<01:48,  1.26batch/s, auc=0.8577, loss=0.8638]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 155/292 [01:44<01:48,  1.26batch/s, auc=0.8587, loss=0.5643]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 156/292 [01:44<01:38,  1.38batch/s, auc=0.8587, loss=0.5643]\u001b[A\n",
      "Training Epoch 5/25:  53%|█████▎    | 156/292 [01:46<01:38,  1.38batch/s, auc=0.8584, loss=0.8956]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 157/292 [01:46<01:51,  1.21batch/s, auc=0.8584, loss=0.8956]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 157/292 [01:46<01:51,  1.21batch/s, auc=0.8584, loss=0.7948]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 158/292 [01:46<01:40,  1.34batch/s, auc=0.8584, loss=0.7948]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 158/292 [01:47<01:40,  1.34batch/s, auc=0.8585, loss=0.6448]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 159/292 [01:47<01:31,  1.45batch/s, auc=0.8585, loss=0.6448]\u001b[A\n",
      "Training Epoch 5/25:  54%|█████▍    | 159/292 [01:47<01:31,  1.45batch/s, auc=0.8588, loss=0.7563]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▍    | 160/292 [01:47<01:26,  1.53batch/s, auc=0.8588, loss=0.7563]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▍    | 160/292 [01:48<01:26,  1.53batch/s, auc=0.8590, loss=0.6075]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 161/292 [01:48<01:21,  1.60batch/s, auc=0.8590, loss=0.6075]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 161/292 [01:48<01:21,  1.60batch/s, auc=0.8587, loss=0.8324]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 162/292 [01:48<01:18,  1.65batch/s, auc=0.8587, loss=0.8324]\u001b[A\n",
      "Training Epoch 5/25:  55%|█████▌    | 162/292 [01:49<01:18,  1.65batch/s, auc=0.8588, loss=0.7207]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 163/292 [01:49<01:16,  1.69batch/s, auc=0.8588, loss=0.7207]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 163/292 [01:50<01:16,  1.69batch/s, auc=0.8589, loss=0.6936]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 164/292 [01:50<01:34,  1.35batch/s, auc=0.8589, loss=0.6936]\u001b[A\n",
      "Training Epoch 5/25:  56%|█████▌    | 164/292 [01:51<01:34,  1.35batch/s, auc=0.8592, loss=0.5079]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 165/292 [01:51<01:27,  1.46batch/s, auc=0.8592, loss=0.5079]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 165/292 [01:51<01:27,  1.46batch/s, auc=0.8594, loss=0.6179]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 166/292 [01:51<01:21,  1.54batch/s, auc=0.8594, loss=0.6179]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 166/292 [01:52<01:21,  1.54batch/s, auc=0.8590, loss=0.9442]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 167/292 [01:52<01:37,  1.29batch/s, auc=0.8590, loss=0.9442]\u001b[A\n",
      "Training Epoch 5/25:  57%|█████▋    | 167/292 [01:53<01:37,  1.29batch/s, auc=0.8591, loss=0.6553]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 168/292 [01:53<01:28,  1.41batch/s, auc=0.8591, loss=0.6553]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 168/292 [01:53<01:28,  1.41batch/s, auc=0.8595, loss=0.7788]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 169/292 [01:53<01:22,  1.50batch/s, auc=0.8595, loss=0.7788]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 169/292 [01:54<01:22,  1.50batch/s, auc=0.8579, loss=1.5915]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 170/292 [01:54<01:36,  1.27batch/s, auc=0.8579, loss=1.5915]\u001b[A\n",
      "Training Epoch 5/25:  58%|█████▊    | 170/292 [01:55<01:36,  1.27batch/s, auc=0.8570, loss=1.3816]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▊    | 171/292 [01:55<01:46,  1.14batch/s, auc=0.8570, loss=1.3816]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▊    | 171/292 [01:57<01:46,  1.14batch/s, auc=0.8558, loss=1.4372]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 172/292 [01:57<01:52,  1.07batch/s, auc=0.8558, loss=1.4372]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 172/292 [01:57<01:52,  1.07batch/s, auc=0.8563, loss=0.5883]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 173/292 [01:57<01:38,  1.21batch/s, auc=0.8563, loss=0.5883]\u001b[A\n",
      "Training Epoch 5/25:  59%|█████▉    | 173/292 [01:58<01:38,  1.21batch/s, auc=0.8565, loss=0.6426]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 174/292 [01:58<01:28,  1.34batch/s, auc=0.8565, loss=0.6426]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 174/292 [01:58<01:28,  1.34batch/s, auc=0.8568, loss=0.6323]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 175/292 [01:58<01:20,  1.45batch/s, auc=0.8568, loss=0.6323]\u001b[A\n",
      "Training Epoch 5/25:  60%|█████▉    | 175/292 [01:59<01:20,  1.45batch/s, auc=0.8568, loss=0.6144]\u001b[A\n",
      "Training Epoch 5/25:  60%|██████    | 176/292 [01:59<01:15,  1.53batch/s, auc=0.8568, loss=0.6144]\u001b[A\n",
      "Training Epoch 5/25:  60%|██████    | 176/292 [01:59<01:15,  1.53batch/s, auc=0.8576, loss=0.4853]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 177/292 [01:59<01:11,  1.60batch/s, auc=0.8576, loss=0.4853]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 177/292 [02:00<01:11,  1.60batch/s, auc=0.8576, loss=0.6482]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 178/292 [02:00<01:08,  1.65batch/s, auc=0.8576, loss=0.6482]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████    | 178/292 [02:00<01:08,  1.65batch/s, auc=0.8579, loss=0.7080]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████▏   | 179/292 [02:00<01:06,  1.69batch/s, auc=0.8579, loss=0.7080]\u001b[A\n",
      "Training Epoch 5/25:  61%|██████▏   | 179/292 [02:01<01:06,  1.69batch/s, auc=0.8577, loss=0.8377]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 180/292 [02:01<01:05,  1.72batch/s, auc=0.8577, loss=0.8377]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 180/292 [02:02<01:05,  1.72batch/s, auc=0.8580, loss=0.5752]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 181/292 [02:02<01:03,  1.74batch/s, auc=0.8580, loss=0.5752]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 181/292 [02:02<01:03,  1.74batch/s, auc=0.8583, loss=0.5883]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 182/292 [02:02<01:02,  1.75batch/s, auc=0.8583, loss=0.5883]\u001b[A\n",
      "Training Epoch 5/25:  62%|██████▏   | 182/292 [02:03<01:02,  1.75batch/s, auc=0.8589, loss=0.6075]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 183/292 [02:03<01:01,  1.76batch/s, auc=0.8589, loss=0.6075]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 183/292 [02:03<01:01,  1.76batch/s, auc=0.8588, loss=0.7916]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 184/292 [02:03<01:01,  1.77batch/s, auc=0.8588, loss=0.7916]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 184/292 [02:04<01:01,  1.77batch/s, auc=0.8584, loss=0.8679]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 185/292 [02:04<01:16,  1.39batch/s, auc=0.8584, loss=0.8679]\u001b[A\n",
      "Training Epoch 5/25:  63%|██████▎   | 185/292 [02:05<01:16,  1.39batch/s, auc=0.8576, loss=1.1269]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▎   | 186/292 [02:05<01:27,  1.21batch/s, auc=0.8576, loss=1.1269]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▎   | 186/292 [02:06<01:27,  1.21batch/s, auc=0.8579, loss=0.7253]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 187/292 [02:06<01:18,  1.34batch/s, auc=0.8579, loss=0.7253]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 187/292 [02:07<01:18,  1.34batch/s, auc=0.8583, loss=0.4399]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 188/292 [02:07<01:11,  1.45batch/s, auc=0.8583, loss=0.4399]\u001b[A\n",
      "Training Epoch 5/25:  64%|██████▍   | 188/292 [02:08<01:11,  1.45batch/s, auc=0.8580, loss=1.0316]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▍   | 189/292 [02:08<01:23,  1.24batch/s, auc=0.8580, loss=1.0316]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▍   | 189/292 [02:08<01:23,  1.24batch/s, auc=0.8581, loss=0.6677]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 190/292 [02:08<01:14,  1.36batch/s, auc=0.8581, loss=0.6677]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 190/292 [02:09<01:14,  1.36batch/s, auc=0.8582, loss=0.7932]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 191/292 [02:09<01:08,  1.47batch/s, auc=0.8582, loss=0.7932]\u001b[A\n",
      "Training Epoch 5/25:  65%|██████▌   | 191/292 [02:09<01:08,  1.47batch/s, auc=0.8586, loss=0.5653]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 192/292 [02:09<01:04,  1.55batch/s, auc=0.8586, loss=0.5653]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 192/292 [02:10<01:04,  1.55batch/s, auc=0.8580, loss=1.0294]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 193/292 [02:10<01:16,  1.29batch/s, auc=0.8580, loss=1.0294]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▌   | 193/292 [02:11<01:16,  1.29batch/s, auc=0.8581, loss=0.7613]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▋   | 194/292 [02:11<01:09,  1.41batch/s, auc=0.8581, loss=0.7613]\u001b[A\n",
      "Training Epoch 5/25:  66%|██████▋   | 194/292 [02:12<01:09,  1.41batch/s, auc=0.8581, loss=0.6016]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 195/292 [02:12<01:04,  1.50batch/s, auc=0.8581, loss=0.6016]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 195/292 [02:13<01:04,  1.50batch/s, auc=0.8577, loss=0.9693]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 196/292 [02:13<01:15,  1.26batch/s, auc=0.8577, loss=0.9693]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 196/292 [02:14<01:15,  1.26batch/s, auc=0.8574, loss=0.9957]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 197/292 [02:14<01:23,  1.14batch/s, auc=0.8574, loss=0.9957]\u001b[A\n",
      "Training Epoch 5/25:  67%|██████▋   | 197/292 [02:14<01:23,  1.14batch/s, auc=0.8580, loss=0.5488]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 198/292 [02:14<01:13,  1.28batch/s, auc=0.8580, loss=0.5488]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 198/292 [02:15<01:13,  1.28batch/s, auc=0.8580, loss=0.9415]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 199/292 [02:15<01:06,  1.40batch/s, auc=0.8580, loss=0.9415]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 199/292 [02:15<01:06,  1.40batch/s, auc=0.8583, loss=0.7283]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 200/292 [02:15<01:01,  1.49batch/s, auc=0.8583, loss=0.7283]\u001b[A\n",
      "Training Epoch 5/25:  68%|██████▊   | 200/292 [02:16<01:01,  1.49batch/s, auc=0.8586, loss=0.5533]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 201/292 [02:16<00:58,  1.57batch/s, auc=0.8586, loss=0.5533]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 201/292 [02:17<00:58,  1.57batch/s, auc=0.8582, loss=0.9925]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 202/292 [02:17<00:55,  1.63batch/s, auc=0.8582, loss=0.9925]\u001b[A\n",
      "Training Epoch 5/25:  69%|██████▉   | 202/292 [02:17<00:55,  1.63batch/s, auc=0.8582, loss=0.7094]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 203/292 [02:17<00:53,  1.67batch/s, auc=0.8582, loss=0.7094]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 203/292 [02:18<00:53,  1.67batch/s, auc=0.8584, loss=0.7270]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 204/292 [02:18<00:51,  1.70batch/s, auc=0.8584, loss=0.7270]\u001b[A\n",
      "Training Epoch 5/25:  70%|██████▉   | 204/292 [02:18<00:51,  1.70batch/s, auc=0.8584, loss=0.7007]\u001b[A\n",
      "Training Epoch 5/25:  70%|███████   | 205/292 [02:18<00:50,  1.72batch/s, auc=0.8584, loss=0.7007]\u001b[A\n",
      "Training Epoch 5/25:  70%|███████   | 205/292 [02:19<00:50,  1.72batch/s, auc=0.8589, loss=0.6462]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 206/292 [02:19<00:49,  1.74batch/s, auc=0.8589, loss=0.6462]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 206/292 [02:19<00:49,  1.74batch/s, auc=0.8585, loss=0.9397]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 207/292 [02:19<00:48,  1.75batch/s, auc=0.8585, loss=0.9397]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 207/292 [02:20<00:48,  1.75batch/s, auc=0.8587, loss=0.6476]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 208/292 [02:20<00:47,  1.76batch/s, auc=0.8587, loss=0.6476]\u001b[A\n",
      "Training Epoch 5/25:  71%|███████   | 208/292 [02:21<00:47,  1.76batch/s, auc=0.8580, loss=1.1366]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 209/292 [02:21<00:59,  1.39batch/s, auc=0.8580, loss=1.1366]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 209/292 [02:22<00:59,  1.39batch/s, auc=0.8583, loss=0.7426]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 210/292 [02:22<00:55,  1.48batch/s, auc=0.8583, loss=0.7426]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 210/292 [02:23<00:55,  1.48batch/s, auc=0.8578, loss=1.1199]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 211/292 [02:23<01:04,  1.26batch/s, auc=0.8578, loss=1.1199]\u001b[A\n",
      "Training Epoch 5/25:  72%|███████▏  | 211/292 [02:23<01:04,  1.26batch/s, auc=0.8580, loss=0.8196]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 212/292 [02:23<00:58,  1.38batch/s, auc=0.8580, loss=0.8196]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 212/292 [02:24<00:58,  1.38batch/s, auc=0.8582, loss=0.6215]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 213/292 [02:24<00:53,  1.48batch/s, auc=0.8582, loss=0.6215]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 213/292 [02:24<00:53,  1.48batch/s, auc=0.8587, loss=0.6024]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 214/292 [02:24<00:50,  1.56batch/s, auc=0.8587, loss=0.6024]\u001b[A\n",
      "Training Epoch 5/25:  73%|███████▎  | 214/292 [02:25<00:50,  1.56batch/s, auc=0.8590, loss=0.7340]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▎  | 215/292 [02:25<00:47,  1.62batch/s, auc=0.8590, loss=0.7340]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▎  | 215/292 [02:25<00:47,  1.62batch/s, auc=0.8594, loss=0.5727]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 216/292 [02:25<00:45,  1.66batch/s, auc=0.8594, loss=0.5727]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 216/292 [02:26<00:45,  1.66batch/s, auc=0.8592, loss=1.0354]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 217/292 [02:26<00:44,  1.70batch/s, auc=0.8592, loss=1.0354]\u001b[A\n",
      "Training Epoch 5/25:  74%|███████▍  | 217/292 [02:27<00:44,  1.70batch/s, auc=0.8593, loss=0.5376]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▍  | 218/292 [02:27<00:43,  1.72batch/s, auc=0.8593, loss=0.5376]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▍  | 218/292 [02:27<00:43,  1.72batch/s, auc=0.8598, loss=0.5571]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 219/292 [02:27<00:42,  1.74batch/s, auc=0.8598, loss=0.5571]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 219/292 [02:28<00:42,  1.74batch/s, auc=0.8601, loss=0.6627]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 220/292 [02:28<00:41,  1.75batch/s, auc=0.8601, loss=0.6627]\u001b[A\n",
      "Training Epoch 5/25:  75%|███████▌  | 220/292 [02:28<00:41,  1.75batch/s, auc=0.8597, loss=0.9517]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 221/292 [02:28<00:40,  1.76batch/s, auc=0.8597, loss=0.9517]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 221/292 [02:29<00:40,  1.76batch/s, auc=0.8602, loss=0.6760]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 222/292 [02:29<00:39,  1.77batch/s, auc=0.8602, loss=0.6760]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▌  | 222/292 [02:29<00:39,  1.77batch/s, auc=0.8601, loss=0.7870]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▋  | 223/292 [02:29<00:38,  1.77batch/s, auc=0.8601, loss=0.7870]\u001b[A\n",
      "Training Epoch 5/25:  76%|███████▋  | 223/292 [02:30<00:38,  1.77batch/s, auc=0.8599, loss=0.8776]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 224/292 [02:30<00:38,  1.77batch/s, auc=0.8599, loss=0.8776]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 224/292 [02:31<00:38,  1.77batch/s, auc=0.8600, loss=0.7319]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 225/292 [02:31<00:48,  1.39batch/s, auc=0.8600, loss=0.7319]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 225/292 [02:32<00:48,  1.39batch/s, auc=0.8604, loss=0.6385]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 226/292 [02:32<00:44,  1.49batch/s, auc=0.8604, loss=0.6385]\u001b[A\n",
      "Training Epoch 5/25:  77%|███████▋  | 226/292 [02:32<00:44,  1.49batch/s, auc=0.8604, loss=0.6283]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 227/292 [02:32<00:41,  1.56batch/s, auc=0.8604, loss=0.6283]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 227/292 [02:33<00:41,  1.56batch/s, auc=0.8603, loss=0.7941]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 228/292 [02:33<00:39,  1.62batch/s, auc=0.8603, loss=0.7941]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 228/292 [02:33<00:39,  1.62batch/s, auc=0.8605, loss=0.5988]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 229/292 [02:33<00:37,  1.66batch/s, auc=0.8605, loss=0.5988]\u001b[A\n",
      "Training Epoch 5/25:  78%|███████▊  | 229/292 [02:34<00:37,  1.66batch/s, auc=0.8601, loss=0.9608]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 230/292 [02:34<00:46,  1.34batch/s, auc=0.8601, loss=0.9608]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 230/292 [02:35<00:46,  1.34batch/s, auc=0.8604, loss=0.6589]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 231/292 [02:35<00:42,  1.45batch/s, auc=0.8604, loss=0.6589]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 231/292 [02:35<00:42,  1.45batch/s, auc=0.8604, loss=0.5840]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 232/292 [02:35<00:39,  1.54batch/s, auc=0.8604, loss=0.5840]\u001b[A\n",
      "Training Epoch 5/25:  79%|███████▉  | 232/292 [02:37<00:39,  1.54batch/s, auc=0.8600, loss=0.8672]\u001b[A\n",
      "Training Epoch 5/25:  80%|███████▉  | 233/292 [02:37<00:46,  1.28batch/s, auc=0.8600, loss=0.8672]\u001b[A\n",
      "Training Epoch 5/25:  80%|███████▉  | 233/292 [02:37<00:46,  1.28batch/s, auc=0.8599, loss=0.6915]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 234/292 [02:37<00:41,  1.40batch/s, auc=0.8599, loss=0.6915]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 234/292 [02:38<00:41,  1.40batch/s, auc=0.8604, loss=0.6834]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 235/292 [02:38<00:38,  1.49batch/s, auc=0.8604, loss=0.6834]\u001b[A\n",
      "Training Epoch 5/25:  80%|████████  | 235/292 [02:38<00:38,  1.49batch/s, auc=0.8604, loss=0.8137]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 236/292 [02:38<00:35,  1.57batch/s, auc=0.8604, loss=0.8137]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 236/292 [02:39<00:35,  1.57batch/s, auc=0.8601, loss=0.9770]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 237/292 [02:39<00:33,  1.63batch/s, auc=0.8601, loss=0.9770]\u001b[A\n",
      "Training Epoch 5/25:  81%|████████  | 237/292 [02:40<00:33,  1.63batch/s, auc=0.8598, loss=0.9459]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 238/292 [02:40<00:40,  1.33batch/s, auc=0.8598, loss=0.9459]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 238/292 [02:40<00:40,  1.33batch/s, auc=0.8602, loss=0.5395]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 239/292 [02:40<00:36,  1.43batch/s, auc=0.8602, loss=0.5395]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 239/292 [02:41<00:36,  1.43batch/s, auc=0.8598, loss=1.1002]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 240/292 [02:41<00:34,  1.52batch/s, auc=0.8598, loss=1.1002]\u001b[A\n",
      "Training Epoch 5/25:  82%|████████▏ | 240/292 [02:42<00:34,  1.52batch/s, auc=0.8590, loss=1.4830]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 241/292 [02:42<00:39,  1.28batch/s, auc=0.8590, loss=1.4830]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 241/292 [02:43<00:39,  1.28batch/s, auc=0.8592, loss=0.5484]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 242/292 [02:43<00:35,  1.39batch/s, auc=0.8592, loss=0.5484]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 242/292 [02:43<00:35,  1.39batch/s, auc=0.8593, loss=0.7108]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 243/292 [02:43<00:32,  1.49batch/s, auc=0.8593, loss=0.7108]\u001b[A\n",
      "Training Epoch 5/25:  83%|████████▎ | 243/292 [02:44<00:32,  1.49batch/s, auc=0.8591, loss=0.9044]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▎ | 244/292 [02:44<00:30,  1.57batch/s, auc=0.8591, loss=0.9044]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▎ | 244/292 [02:44<00:30,  1.57batch/s, auc=0.8591, loss=0.6664]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 245/292 [02:44<00:28,  1.62batch/s, auc=0.8591, loss=0.6664]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 245/292 [02:45<00:28,  1.62batch/s, auc=0.8591, loss=1.0090]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 246/292 [02:45<00:27,  1.67batch/s, auc=0.8591, loss=1.0090]\u001b[A\n",
      "Training Epoch 5/25:  84%|████████▍ | 246/292 [02:46<00:27,  1.67batch/s, auc=0.8581, loss=1.5248]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 247/292 [02:46<00:33,  1.34batch/s, auc=0.8581, loss=1.5248]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 247/292 [02:47<00:33,  1.34batch/s, auc=0.8585, loss=0.5583]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 248/292 [02:47<00:30,  1.45batch/s, auc=0.8585, loss=0.5583]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▍ | 248/292 [02:48<00:30,  1.45batch/s, auc=0.8582, loss=1.0445]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▌ | 249/292 [02:48<00:34,  1.24batch/s, auc=0.8582, loss=1.0445]\u001b[A\n",
      "Training Epoch 5/25:  85%|████████▌ | 249/292 [02:49<00:34,  1.24batch/s, auc=0.8581, loss=0.9500]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 250/292 [02:49<00:37,  1.12batch/s, auc=0.8581, loss=0.9500]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 250/292 [02:50<00:37,  1.12batch/s, auc=0.8575, loss=1.2369]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 251/292 [02:50<00:38,  1.05batch/s, auc=0.8575, loss=1.2369]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▌ | 251/292 [02:50<00:38,  1.05batch/s, auc=0.8578, loss=0.6495]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▋ | 252/292 [02:50<00:33,  1.20batch/s, auc=0.8578, loss=0.6495]\u001b[A\n",
      "Training Epoch 5/25:  86%|████████▋ | 252/292 [02:51<00:33,  1.20batch/s, auc=0.8576, loss=0.8291]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 253/292 [02:51<00:35,  1.10batch/s, auc=0.8576, loss=0.8291]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 253/292 [02:52<00:35,  1.10batch/s, auc=0.8578, loss=0.7700]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 254/292 [02:52<00:30,  1.24batch/s, auc=0.8578, loss=0.7700]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 254/292 [02:53<00:30,  1.24batch/s, auc=0.8581, loss=0.6687]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 255/292 [02:53<00:27,  1.37batch/s, auc=0.8581, loss=0.6687]\u001b[A\n",
      "Training Epoch 5/25:  87%|████████▋ | 255/292 [02:53<00:27,  1.37batch/s, auc=0.8583, loss=0.7981]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 256/292 [02:53<00:24,  1.47batch/s, auc=0.8583, loss=0.7981]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 256/292 [02:54<00:24,  1.47batch/s, auc=0.8582, loss=0.7818]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 257/292 [02:54<00:22,  1.55batch/s, auc=0.8582, loss=0.7818]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 257/292 [02:54<00:22,  1.55batch/s, auc=0.8583, loss=0.5472]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 258/292 [02:54<00:21,  1.61batch/s, auc=0.8583, loss=0.5472]\u001b[A\n",
      "Training Epoch 5/25:  88%|████████▊ | 258/292 [02:55<00:21,  1.61batch/s, auc=0.8581, loss=0.8468]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▊ | 259/292 [02:55<00:19,  1.65batch/s, auc=0.8581, loss=0.8468]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▊ | 259/292 [02:55<00:19,  1.65batch/s, auc=0.8581, loss=0.7011]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 260/292 [02:55<00:18,  1.69batch/s, auc=0.8581, loss=0.7011]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 260/292 [02:56<00:18,  1.69batch/s, auc=0.8583, loss=0.7487]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 261/292 [02:56<00:18,  1.71batch/s, auc=0.8583, loss=0.7487]\u001b[A\n",
      "Training Epoch 5/25:  89%|████████▉ | 261/292 [02:57<00:18,  1.71batch/s, auc=0.8583, loss=0.7426]\u001b[A\n",
      "Training Epoch 5/25:  90%|████████▉ | 262/292 [02:57<00:17,  1.73batch/s, auc=0.8583, loss=0.7426]\u001b[A\n",
      "Training Epoch 5/25:  90%|████████▉ | 262/292 [02:57<00:17,  1.73batch/s, auc=0.8586, loss=0.6082]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 263/292 [02:57<00:16,  1.74batch/s, auc=0.8586, loss=0.6082]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 263/292 [02:58<00:16,  1.74batch/s, auc=0.8585, loss=0.8249]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 264/292 [02:58<00:20,  1.38batch/s, auc=0.8585, loss=0.8249]\u001b[A\n",
      "Training Epoch 5/25:  90%|█████████ | 264/292 [02:59<00:20,  1.38batch/s, auc=0.8586, loss=0.6276]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 265/292 [02:59<00:18,  1.48batch/s, auc=0.8586, loss=0.6276]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 265/292 [02:59<00:18,  1.48batch/s, auc=0.8588, loss=0.6081]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 266/292 [02:59<00:16,  1.55batch/s, auc=0.8588, loss=0.6081]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████ | 266/292 [03:00<00:16,  1.55batch/s, auc=0.8589, loss=0.7597]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████▏| 267/292 [03:00<00:15,  1.61batch/s, auc=0.8589, loss=0.7597]\u001b[A\n",
      "Training Epoch 5/25:  91%|█████████▏| 267/292 [03:00<00:15,  1.61batch/s, auc=0.8592, loss=0.8515]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 268/292 [03:00<00:14,  1.66batch/s, auc=0.8592, loss=0.8515]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 268/292 [03:01<00:14,  1.66batch/s, auc=0.8592, loss=0.7603]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 269/292 [03:01<00:13,  1.69batch/s, auc=0.8592, loss=0.7603]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 269/292 [03:02<00:13,  1.69batch/s, auc=0.8593, loss=0.5953]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 270/292 [03:02<00:12,  1.71batch/s, auc=0.8593, loss=0.5953]\u001b[A\n",
      "Training Epoch 5/25:  92%|█████████▏| 270/292 [03:02<00:12,  1.71batch/s, auc=0.8593, loss=0.8817]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 271/292 [03:02<00:12,  1.73batch/s, auc=0.8593, loss=0.8817]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 271/292 [03:03<00:12,  1.73batch/s, auc=0.8587, loss=1.4214]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 272/292 [03:03<00:14,  1.37batch/s, auc=0.8587, loss=1.4214]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 272/292 [03:04<00:14,  1.37batch/s, auc=0.8589, loss=0.7870]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 273/292 [03:04<00:12,  1.47batch/s, auc=0.8589, loss=0.7870]\u001b[A\n",
      "Training Epoch 5/25:  93%|█████████▎| 273/292 [03:04<00:12,  1.47batch/s, auc=0.8589, loss=0.7964]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 274/292 [03:04<00:11,  1.55batch/s, auc=0.8589, loss=0.7964]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 274/292 [03:05<00:11,  1.55batch/s, auc=0.8589, loss=0.8101]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 275/292 [03:05<00:13,  1.29batch/s, auc=0.8589, loss=0.8101]\u001b[A\n",
      "Training Epoch 5/25:  94%|█████████▍| 275/292 [03:06<00:13,  1.29batch/s, auc=0.8592, loss=0.5897]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 276/292 [03:06<00:11,  1.40batch/s, auc=0.8592, loss=0.5897]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 276/292 [03:07<00:11,  1.40batch/s, auc=0.8585, loss=1.3806]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 277/292 [03:07<00:12,  1.21batch/s, auc=0.8585, loss=1.3806]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▍| 277/292 [03:08<00:12,  1.21batch/s, auc=0.8587, loss=0.6250]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▌| 278/292 [03:08<00:10,  1.34batch/s, auc=0.8587, loss=0.6250]\u001b[A\n",
      "Training Epoch 5/25:  95%|█████████▌| 278/292 [03:08<00:10,  1.34batch/s, auc=0.8587, loss=0.8160]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 279/292 [03:08<00:09,  1.44batch/s, auc=0.8587, loss=0.8160]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 279/292 [03:09<00:09,  1.44batch/s, auc=0.8585, loss=0.8065]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 280/292 [03:09<00:09,  1.24batch/s, auc=0.8585, loss=0.8065]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 280/292 [03:10<00:09,  1.24batch/s, auc=0.8587, loss=0.5585]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 281/292 [03:10<00:08,  1.36batch/s, auc=0.8587, loss=0.5585]\u001b[A\n",
      "Training Epoch 5/25:  96%|█████████▌| 281/292 [03:10<00:08,  1.36batch/s, auc=0.8588, loss=0.7847]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 282/292 [03:10<00:06,  1.46batch/s, auc=0.8588, loss=0.7847]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 282/292 [03:11<00:06,  1.46batch/s, auc=0.8587, loss=0.8869]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 283/292 [03:11<00:07,  1.24batch/s, auc=0.8587, loss=0.8869]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 283/292 [03:12<00:07,  1.24batch/s, auc=0.8587, loss=0.8466]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.36batch/s, auc=0.8587, loss=0.8466]\u001b[A\n",
      "Training Epoch 5/25:  97%|█████████▋| 284/292 [03:13<00:05,  1.36batch/s, auc=0.8581, loss=1.2543]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 285/292 [03:13<00:05,  1.19batch/s, auc=0.8581, loss=1.2543]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 285/292 [03:14<00:05,  1.19batch/s, auc=0.8581, loss=0.6495]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 286/292 [03:14<00:04,  1.32batch/s, auc=0.8581, loss=0.6495]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 286/292 [03:14<00:04,  1.32batch/s, auc=0.8583, loss=0.6550]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.43batch/s, auc=0.8583, loss=0.6550]\u001b[A\n",
      "Training Epoch 5/25:  98%|█████████▊| 287/292 [03:15<00:03,  1.43batch/s, auc=0.8585, loss=0.7223]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▊| 288/292 [03:15<00:02,  1.52batch/s, auc=0.8585, loss=0.7223]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▊| 288/292 [03:15<00:02,  1.52batch/s, auc=0.8587, loss=0.5027]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 289/292 [03:15<00:01,  1.59batch/s, auc=0.8587, loss=0.5027]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 289/292 [03:16<00:01,  1.59batch/s, auc=0.8588, loss=0.8255]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 290/292 [03:16<00:01,  1.64batch/s, auc=0.8588, loss=0.8255]\u001b[A\n",
      "Training Epoch 5/25:  99%|█████████▉| 290/292 [03:17<00:01,  1.64batch/s, auc=0.8590, loss=0.6513]\u001b[A\n",
      "Training Epoch 5/25: 100%|█████████▉| 291/292 [03:17<00:00,  1.68batch/s, auc=0.8590, loss=0.6513]\u001b[A\n",
      "Training Epoch 5/25: 100%|█████████▉| 291/292 [03:17<00:00,  1.68batch/s, auc=0.8591, loss=0.5286]\u001b[A\n",
      "Training Epoch 5/25: 100%|██████████| 292/292 [03:17<00:00,  1.48batch/s, auc=0.8591, loss=0.5286]\u001b[A\n",
      "Epochs:  20%|██        | 5/25 [17:38<1:10:59, 212.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25] Train Loss: 0.7894 | Train AUROC: 0.8591 Val Loss: 0.7808 | Val AUROC: 0.8533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 6/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 0/292 [00:02<?, ?batch/s, auc=0.6381, loss=1.4204]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 1/292 [00:02<09:55,  2.05s/batch, auc=0.6381, loss=1.4204]\u001b[A\n",
      "Training Epoch 6/25:   0%|          | 1/292 [00:02<09:55,  2.05s/batch, auc=0.8095, loss=0.6492]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 2/292 [00:02<05:38,  1.17s/batch, auc=0.8095, loss=0.6492]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 2/292 [00:03<05:38,  1.17s/batch, auc=0.7726, loss=1.0975]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 3/292 [00:03<05:25,  1.12s/batch, auc=0.7726, loss=1.0975]\u001b[A\n",
      "Training Epoch 6/25:   1%|          | 3/292 [00:04<05:25,  1.12s/batch, auc=0.8182, loss=0.6745]\u001b[A\n",
      "Training Epoch 6/25:   1%|▏         | 4/292 [00:04<04:19,  1.11batch/s, auc=0.8182, loss=0.6745]\u001b[A\n",
      "Training Epoch 6/25:   1%|▏         | 4/292 [00:04<04:19,  1.11batch/s, auc=0.8385, loss=0.6348]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 5/292 [00:04<03:42,  1.29batch/s, auc=0.8385, loss=0.6348]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 5/292 [00:05<03:42,  1.29batch/s, auc=0.8281, loss=0.8599]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 6/292 [00:05<03:20,  1.43batch/s, auc=0.8281, loss=0.8599]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 6/292 [00:05<03:20,  1.43batch/s, auc=0.8296, loss=0.7680]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 7/292 [00:05<03:05,  1.53batch/s, auc=0.8296, loss=0.7680]\u001b[A\n",
      "Training Epoch 6/25:   2%|▏         | 7/292 [00:06<03:05,  1.53batch/s, auc=0.8421, loss=0.6689]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 8/292 [00:06<02:57,  1.60batch/s, auc=0.8421, loss=0.6689]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 8/292 [00:07<02:57,  1.60batch/s, auc=0.8307, loss=0.8904]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 9/292 [00:07<03:36,  1.31batch/s, auc=0.8307, loss=0.8904]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 9/292 [00:08<03:36,  1.31batch/s, auc=0.8425, loss=0.5530]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 10/292 [00:08<03:17,  1.43batch/s, auc=0.8425, loss=0.5530]\u001b[A\n",
      "Training Epoch 6/25:   3%|▎         | 10/292 [00:08<03:17,  1.43batch/s, auc=0.8423, loss=0.7432]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 11/292 [00:08<03:04,  1.53batch/s, auc=0.8423, loss=0.7432]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 11/292 [00:09<03:04,  1.53batch/s, auc=0.8479, loss=0.5644]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 12/292 [00:09<02:54,  1.60batch/s, auc=0.8479, loss=0.5644]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 12/292 [00:09<02:54,  1.60batch/s, auc=0.8492, loss=0.7348]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 13/292 [00:09<02:48,  1.66batch/s, auc=0.8492, loss=0.7348]\u001b[A\n",
      "Training Epoch 6/25:   4%|▍         | 13/292 [00:10<02:48,  1.66batch/s, auc=0.8520, loss=0.6090]\u001b[A\n",
      "Training Epoch 6/25:   5%|▍         | 14/292 [00:10<02:43,  1.70batch/s, auc=0.8520, loss=0.6090]\u001b[A\n",
      "Training Epoch 6/25:   5%|▍         | 14/292 [00:10<02:43,  1.70batch/s, auc=0.8482, loss=0.9693]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 15/292 [00:10<02:40,  1.73batch/s, auc=0.8482, loss=0.9693]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 15/292 [00:11<02:40,  1.73batch/s, auc=0.8539, loss=0.5937]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 16/292 [00:11<02:37,  1.75batch/s, auc=0.8539, loss=0.5937]\u001b[A\n",
      "Training Epoch 6/25:   5%|▌         | 16/292 [00:11<02:37,  1.75batch/s, auc=0.8592, loss=0.6126]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 17/292 [00:11<02:35,  1.77batch/s, auc=0.8592, loss=0.6126]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 17/292 [00:12<02:35,  1.77batch/s, auc=0.8628, loss=0.5283]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 18/292 [00:12<02:33,  1.78batch/s, auc=0.8628, loss=0.5283]\u001b[A\n",
      "Training Epoch 6/25:   6%|▌         | 18/292 [00:13<02:33,  1.78batch/s, auc=0.8662, loss=0.7516]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 19/292 [00:13<02:32,  1.79batch/s, auc=0.8662, loss=0.7516]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 19/292 [00:13<02:32,  1.79batch/s, auc=0.8691, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 20/292 [00:13<02:31,  1.79batch/s, auc=0.8691, loss=0.5727]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 20/292 [00:14<02:31,  1.79batch/s, auc=0.8710, loss=0.6110]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 21/292 [00:14<02:30,  1.80batch/s, auc=0.8710, loss=0.6110]\u001b[A\n",
      "Training Epoch 6/25:   7%|▋         | 21/292 [00:14<02:30,  1.80batch/s, auc=0.8707, loss=0.8371]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 22/292 [00:14<02:30,  1.80batch/s, auc=0.8707, loss=0.8371]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 22/292 [00:15<02:30,  1.80batch/s, auc=0.8676, loss=0.9577]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 23/292 [00:15<02:29,  1.80batch/s, auc=0.8676, loss=0.9577]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 23/292 [00:15<02:29,  1.80batch/s, auc=0.8683, loss=0.6587]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 24/292 [00:15<02:28,  1.80batch/s, auc=0.8683, loss=0.6587]\u001b[A\n",
      "Training Epoch 6/25:   8%|▊         | 24/292 [00:16<02:28,  1.80batch/s, auc=0.8714, loss=0.6824]\u001b[A\n",
      "Training Epoch 6/25:   9%|▊         | 25/292 [00:16<02:28,  1.80batch/s, auc=0.8714, loss=0.6824]\u001b[A\n",
      "Training Epoch 6/25:   9%|▊         | 25/292 [00:16<02:28,  1.80batch/s, auc=0.8729, loss=0.6213]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 26/292 [00:16<02:27,  1.80batch/s, auc=0.8729, loss=0.6213]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 26/292 [00:17<02:27,  1.80batch/s, auc=0.8724, loss=0.8357]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 27/292 [00:17<02:27,  1.80batch/s, auc=0.8724, loss=0.8357]\u001b[A\n",
      "Training Epoch 6/25:   9%|▉         | 27/292 [00:18<02:27,  1.80batch/s, auc=0.8747, loss=0.6838]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 28/292 [00:18<02:26,  1.80batch/s, auc=0.8747, loss=0.6838]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 28/292 [00:18<02:26,  1.80batch/s, auc=0.8800, loss=0.5116]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 29/292 [00:18<02:25,  1.80batch/s, auc=0.8800, loss=0.5116]\u001b[A\n",
      "Training Epoch 6/25:  10%|▉         | 29/292 [00:19<02:25,  1.80batch/s, auc=0.8797, loss=0.6300]\u001b[A\n",
      "Training Epoch 6/25:  10%|█         | 30/292 [00:19<02:25,  1.80batch/s, auc=0.8797, loss=0.6300]\u001b[A\n",
      "Training Epoch 6/25:  10%|█         | 30/292 [00:19<02:25,  1.80batch/s, auc=0.8809, loss=0.5552]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 31/292 [00:19<02:24,  1.80batch/s, auc=0.8809, loss=0.5552]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 31/292 [00:20<02:24,  1.80batch/s, auc=0.8830, loss=0.5379]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 32/292 [00:20<02:24,  1.81batch/s, auc=0.8830, loss=0.5379]\u001b[A\n",
      "Training Epoch 6/25:  11%|█         | 32/292 [00:20<02:24,  1.81batch/s, auc=0.8813, loss=0.8023]\u001b[A\n",
      "Training Epoch 6/25:  11%|█▏        | 33/292 [00:20<02:23,  1.80batch/s, auc=0.8813, loss=0.8023]\u001b[A\n",
      "Training Epoch 6/25:  11%|█▏        | 33/292 [00:21<02:23,  1.80batch/s, auc=0.8808, loss=0.5781]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 34/292 [00:21<02:23,  1.80batch/s, auc=0.8808, loss=0.5781]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 34/292 [00:21<02:23,  1.80batch/s, auc=0.8808, loss=0.7983]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 35/292 [00:21<02:22,  1.80batch/s, auc=0.8808, loss=0.7983]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 35/292 [00:22<02:22,  1.80batch/s, auc=0.8809, loss=0.7587]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 36/292 [00:22<02:21,  1.80batch/s, auc=0.8809, loss=0.7587]\u001b[A\n",
      "Training Epoch 6/25:  12%|█▏        | 36/292 [00:23<02:21,  1.80batch/s, auc=0.8831, loss=0.4858]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 37/292 [00:23<02:21,  1.81batch/s, auc=0.8831, loss=0.4858]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 37/292 [00:23<02:21,  1.81batch/s, auc=0.8834, loss=0.7143]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 38/292 [00:23<02:20,  1.80batch/s, auc=0.8834, loss=0.7143]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 38/292 [00:24<02:20,  1.80batch/s, auc=0.8842, loss=0.5839]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 39/292 [00:24<02:20,  1.80batch/s, auc=0.8842, loss=0.5839]\u001b[A\n",
      "Training Epoch 6/25:  13%|█▎        | 39/292 [00:24<02:20,  1.80batch/s, auc=0.8865, loss=0.5839]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▎        | 40/292 [00:24<02:19,  1.80batch/s, auc=0.8865, loss=0.5839]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▎        | 40/292 [00:25<02:19,  1.80batch/s, auc=0.8872, loss=0.4644]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 41/292 [00:25<02:19,  1.80batch/s, auc=0.8872, loss=0.4644]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 41/292 [00:25<02:19,  1.80batch/s, auc=0.8877, loss=0.5863]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 42/292 [00:25<02:18,  1.80batch/s, auc=0.8877, loss=0.5863]\u001b[A\n",
      "Training Epoch 6/25:  14%|█▍        | 42/292 [00:26<02:18,  1.80batch/s, auc=0.8870, loss=0.8587]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▍        | 43/292 [00:26<02:56,  1.41batch/s, auc=0.8870, loss=0.8587]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▍        | 43/292 [00:27<02:56,  1.41batch/s, auc=0.8835, loss=1.2674]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 44/292 [00:27<03:23,  1.22batch/s, auc=0.8835, loss=1.2674]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 44/292 [00:28<03:23,  1.22batch/s, auc=0.8828, loss=1.0064]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 45/292 [00:28<03:02,  1.35batch/s, auc=0.8828, loss=1.0064]\u001b[A\n",
      "Training Epoch 6/25:  15%|█▌        | 45/292 [00:29<03:02,  1.35batch/s, auc=0.8787, loss=1.5815]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 46/292 [00:29<03:26,  1.19batch/s, auc=0.8787, loss=1.5815]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 46/292 [00:30<03:26,  1.19batch/s, auc=0.8754, loss=1.2750]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 47/292 [00:30<03:42,  1.10batch/s, auc=0.8754, loss=1.2750]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▌        | 47/292 [00:31<03:42,  1.10batch/s, auc=0.8748, loss=0.7531]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▋        | 48/292 [00:31<03:15,  1.25batch/s, auc=0.8748, loss=0.7531]\u001b[A\n",
      "Training Epoch 6/25:  16%|█▋        | 48/292 [00:31<03:15,  1.25batch/s, auc=0.8752, loss=0.8198]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 49/292 [00:31<02:57,  1.37batch/s, auc=0.8752, loss=0.8198]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 49/292 [00:32<02:57,  1.37batch/s, auc=0.8753, loss=0.6791]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 50/292 [00:32<02:43,  1.48batch/s, auc=0.8753, loss=0.6791]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 50/292 [00:32<02:43,  1.48batch/s, auc=0.8772, loss=0.5947]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 51/292 [00:32<02:34,  1.56batch/s, auc=0.8772, loss=0.5947]\u001b[A\n",
      "Training Epoch 6/25:  17%|█▋        | 51/292 [00:33<02:34,  1.56batch/s, auc=0.8748, loss=1.1568]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 52/292 [00:33<03:04,  1.30batch/s, auc=0.8748, loss=1.1568]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 52/292 [00:34<03:04,  1.30batch/s, auc=0.8747, loss=0.7504]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 53/292 [00:34<02:48,  1.42batch/s, auc=0.8747, loss=0.7504]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 53/292 [00:35<02:48,  1.42batch/s, auc=0.8733, loss=1.1484]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 54/292 [00:35<02:37,  1.52batch/s, auc=0.8733, loss=1.1484]\u001b[A\n",
      "Training Epoch 6/25:  18%|█▊        | 54/292 [00:36<02:37,  1.52batch/s, auc=0.8713, loss=1.1135]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 55/292 [00:36<03:05,  1.28batch/s, auc=0.8713, loss=1.1135]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 55/292 [00:37<03:05,  1.28batch/s, auc=0.8672, loss=1.4713]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 56/292 [00:37<03:25,  1.15batch/s, auc=0.8672, loss=1.4713]\u001b[A\n",
      "Training Epoch 6/25:  19%|█▉        | 56/292 [00:38<03:25,  1.15batch/s, auc=0.8663, loss=0.9105]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 57/292 [00:38<03:38,  1.07batch/s, auc=0.8663, loss=0.9105]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 57/292 [00:39<03:38,  1.07batch/s, auc=0.8647, loss=1.1544]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 58/292 [00:39<03:47,  1.03batch/s, auc=0.8647, loss=1.1544]\u001b[A\n",
      "Training Epoch 6/25:  20%|█▉        | 58/292 [00:39<03:47,  1.03batch/s, auc=0.8650, loss=0.6409]\u001b[A\n",
      "Training Epoch 6/25:  20%|██        | 59/292 [00:39<03:17,  1.18batch/s, auc=0.8650, loss=0.6409]\u001b[A\n",
      "Training Epoch 6/25:  20%|██        | 59/292 [00:40<03:17,  1.18batch/s, auc=0.8648, loss=0.8374]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 60/292 [00:40<02:56,  1.32batch/s, auc=0.8648, loss=0.8374]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 60/292 [00:41<02:56,  1.32batch/s, auc=0.8616, loss=1.4857]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 61/292 [00:41<03:17,  1.17batch/s, auc=0.8616, loss=1.4857]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 61/292 [00:42<03:17,  1.17batch/s, auc=0.8626, loss=0.6492]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 62/292 [00:42<02:55,  1.31batch/s, auc=0.8626, loss=0.6492]\u001b[A\n",
      "Training Epoch 6/25:  21%|██        | 62/292 [00:42<02:55,  1.31batch/s, auc=0.8630, loss=0.6864]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 63/292 [00:42<02:40,  1.43batch/s, auc=0.8630, loss=0.6864]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 63/292 [00:43<02:40,  1.43batch/s, auc=0.8634, loss=0.6720]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 64/292 [00:43<02:29,  1.52batch/s, auc=0.8634, loss=0.6720]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 64/292 [00:43<02:29,  1.52batch/s, auc=0.8643, loss=0.6413]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 65/292 [00:43<02:22,  1.60batch/s, auc=0.8643, loss=0.6413]\u001b[A\n",
      "Training Epoch 6/25:  22%|██▏       | 65/292 [00:44<02:22,  1.60batch/s, auc=0.8655, loss=0.5354]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 66/292 [00:44<02:16,  1.65batch/s, auc=0.8655, loss=0.5354]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 66/292 [00:44<02:16,  1.65batch/s, auc=0.8648, loss=0.6088]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 67/292 [00:44<02:12,  1.69batch/s, auc=0.8648, loss=0.6088]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 67/292 [00:45<02:12,  1.69batch/s, auc=0.8654, loss=0.6974]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 68/292 [00:45<02:09,  1.73batch/s, auc=0.8654, loss=0.6974]\u001b[A\n",
      "Training Epoch 6/25:  23%|██▎       | 68/292 [00:45<02:09,  1.73batch/s, auc=0.8645, loss=0.8430]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▎       | 69/292 [00:45<02:07,  1.75batch/s, auc=0.8645, loss=0.8430]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▎       | 69/292 [00:46<02:07,  1.75batch/s, auc=0.8653, loss=0.7057]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 70/292 [00:46<02:05,  1.76batch/s, auc=0.8653, loss=0.7057]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 70/292 [00:47<02:05,  1.76batch/s, auc=0.8658, loss=0.7101]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 71/292 [00:47<02:04,  1.77batch/s, auc=0.8658, loss=0.7101]\u001b[A\n",
      "Training Epoch 6/25:  24%|██▍       | 71/292 [00:47<02:04,  1.77batch/s, auc=0.8661, loss=0.6305]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▍       | 72/292 [00:47<02:03,  1.78batch/s, auc=0.8661, loss=0.6305]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▍       | 72/292 [00:48<02:03,  1.78batch/s, auc=0.8659, loss=0.7047]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 73/292 [00:48<02:02,  1.79batch/s, auc=0.8659, loss=0.7047]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 73/292 [00:48<02:02,  1.79batch/s, auc=0.8658, loss=0.7442]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 74/292 [00:48<02:01,  1.79batch/s, auc=0.8658, loss=0.7442]\u001b[A\n",
      "Training Epoch 6/25:  25%|██▌       | 74/292 [00:49<02:01,  1.79batch/s, auc=0.8673, loss=0.5926]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 75/292 [00:49<02:00,  1.79batch/s, auc=0.8673, loss=0.5926]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 75/292 [00:49<02:00,  1.79batch/s, auc=0.8663, loss=0.8235]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 76/292 [00:49<02:00,  1.79batch/s, auc=0.8663, loss=0.8235]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▌       | 76/292 [00:50<02:00,  1.79batch/s, auc=0.8663, loss=0.8346]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▋       | 77/292 [00:50<01:59,  1.79batch/s, auc=0.8663, loss=0.8346]\u001b[A\n",
      "Training Epoch 6/25:  26%|██▋       | 77/292 [00:50<01:59,  1.79batch/s, auc=0.8671, loss=0.5736]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 78/292 [00:50<01:59,  1.79batch/s, auc=0.8671, loss=0.5736]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 78/292 [00:51<01:59,  1.79batch/s, auc=0.8682, loss=0.6370]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 79/292 [00:51<01:58,  1.79batch/s, auc=0.8682, loss=0.6370]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 79/292 [00:52<01:58,  1.79batch/s, auc=0.8679, loss=0.8578]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 80/292 [00:52<01:57,  1.80batch/s, auc=0.8679, loss=0.8578]\u001b[A\n",
      "Training Epoch 6/25:  27%|██▋       | 80/292 [00:52<01:57,  1.80batch/s, auc=0.8679, loss=0.8623]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 81/292 [00:52<01:57,  1.80batch/s, auc=0.8679, loss=0.8623]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 81/292 [00:53<01:57,  1.80batch/s, auc=0.8684, loss=0.5105]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 82/292 [00:53<01:56,  1.80batch/s, auc=0.8684, loss=0.5105]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 82/292 [00:54<01:56,  1.80batch/s, auc=0.8664, loss=1.2120]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 83/292 [00:54<02:28,  1.41batch/s, auc=0.8664, loss=1.2120]\u001b[A\n",
      "Training Epoch 6/25:  28%|██▊       | 83/292 [00:54<02:28,  1.41batch/s, auc=0.8662, loss=1.0080]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 84/292 [00:54<02:18,  1.50batch/s, auc=0.8662, loss=1.0080]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 84/292 [00:55<02:18,  1.50batch/s, auc=0.8669, loss=0.5691]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 85/292 [00:55<02:10,  1.58batch/s, auc=0.8669, loss=0.5691]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 85/292 [00:55<02:10,  1.58batch/s, auc=0.8667, loss=0.9530]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 86/292 [00:55<02:05,  1.64batch/s, auc=0.8667, loss=0.9530]\u001b[A\n",
      "Training Epoch 6/25:  29%|██▉       | 86/292 [00:57<02:05,  1.64batch/s, auc=0.8667, loss=0.7337]\u001b[A\n",
      "Training Epoch 6/25:  30%|██▉       | 87/292 [00:57<02:33,  1.34batch/s, auc=0.8667, loss=0.7337]\u001b[A\n",
      "Training Epoch 6/25:  30%|██▉       | 87/292 [00:57<02:33,  1.34batch/s, auc=0.8673, loss=0.5746]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 88/292 [00:57<02:20,  1.45batch/s, auc=0.8673, loss=0.5746]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 88/292 [00:58<02:20,  1.45batch/s, auc=0.8668, loss=0.8782]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 89/292 [00:58<02:43,  1.24batch/s, auc=0.8668, loss=0.8782]\u001b[A\n",
      "Training Epoch 6/25:  30%|███       | 89/292 [00:59<02:43,  1.24batch/s, auc=0.8672, loss=0.6964]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 90/292 [00:59<02:27,  1.37batch/s, auc=0.8672, loss=0.6964]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 90/292 [00:59<02:27,  1.37batch/s, auc=0.8676, loss=0.6246]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 91/292 [00:59<02:16,  1.47batch/s, auc=0.8676, loss=0.6246]\u001b[A\n",
      "Training Epoch 6/25:  31%|███       | 91/292 [01:00<02:16,  1.47batch/s, auc=0.8669, loss=0.9609]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 92/292 [01:00<02:39,  1.25batch/s, auc=0.8669, loss=0.9609]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 92/292 [01:01<02:39,  1.25batch/s, auc=0.8677, loss=0.6015]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 93/292 [01:01<02:24,  1.38batch/s, auc=0.8677, loss=0.6015]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 93/292 [01:01<02:24,  1.38batch/s, auc=0.8672, loss=0.7125]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 94/292 [01:01<02:13,  1.48batch/s, auc=0.8672, loss=0.7125]\u001b[A\n",
      "Training Epoch 6/25:  32%|███▏      | 94/292 [01:03<02:13,  1.48batch/s, auc=0.8668, loss=1.0401]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 95/292 [01:03<02:36,  1.26batch/s, auc=0.8668, loss=1.0401]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 95/292 [01:03<02:36,  1.26batch/s, auc=0.8664, loss=0.8584]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 96/292 [01:03<02:21,  1.38batch/s, auc=0.8664, loss=0.8584]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 96/292 [01:04<02:21,  1.38batch/s, auc=0.8666, loss=0.8236]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 97/292 [01:04<02:11,  1.48batch/s, auc=0.8666, loss=0.8236]\u001b[A\n",
      "Training Epoch 6/25:  33%|███▎      | 97/292 [01:04<02:11,  1.48batch/s, auc=0.8673, loss=0.5711]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▎      | 98/292 [01:04<02:03,  1.56batch/s, auc=0.8673, loss=0.5711]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▎      | 98/292 [01:05<02:03,  1.56batch/s, auc=0.8676, loss=0.6474]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 99/292 [01:05<01:58,  1.62batch/s, auc=0.8676, loss=0.6474]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 99/292 [01:05<01:58,  1.62batch/s, auc=0.8682, loss=0.6342]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 100/292 [01:05<01:54,  1.67batch/s, auc=0.8682, loss=0.6342]\u001b[A\n",
      "Training Epoch 6/25:  34%|███▍      | 100/292 [01:06<01:54,  1.67batch/s, auc=0.8688, loss=0.6651]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 101/292 [01:06<01:52,  1.70batch/s, auc=0.8688, loss=0.6651]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 101/292 [01:06<01:52,  1.70batch/s, auc=0.8690, loss=0.7729]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 102/292 [01:06<01:49,  1.73batch/s, auc=0.8690, loss=0.7729]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▍      | 102/292 [01:08<01:49,  1.73batch/s, auc=0.8685, loss=0.8255]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▌      | 103/292 [01:08<02:17,  1.38batch/s, auc=0.8685, loss=0.8255]\u001b[A\n",
      "Training Epoch 6/25:  35%|███▌      | 103/292 [01:08<02:17,  1.38batch/s, auc=0.8683, loss=0.7170]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 104/292 [01:08<02:07,  1.48batch/s, auc=0.8683, loss=0.7170]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 104/292 [01:09<02:07,  1.48batch/s, auc=0.8685, loss=0.6846]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 105/292 [01:09<01:59,  1.56batch/s, auc=0.8685, loss=0.6846]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▌      | 105/292 [01:09<01:59,  1.56batch/s, auc=0.8678, loss=0.8141]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▋      | 106/292 [01:09<01:54,  1.62batch/s, auc=0.8678, loss=0.8141]\u001b[A\n",
      "Training Epoch 6/25:  36%|███▋      | 106/292 [01:10<01:54,  1.62batch/s, auc=0.8678, loss=0.7013]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 107/292 [01:10<01:50,  1.67batch/s, auc=0.8678, loss=0.7013]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 107/292 [01:10<01:50,  1.67batch/s, auc=0.8684, loss=0.7117]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 108/292 [01:10<01:48,  1.70batch/s, auc=0.8684, loss=0.7117]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 108/292 [01:11<01:48,  1.70batch/s, auc=0.8695, loss=0.5794]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 109/292 [01:11<01:45,  1.73batch/s, auc=0.8695, loss=0.5794]\u001b[A\n",
      "Training Epoch 6/25:  37%|███▋      | 109/292 [01:11<01:45,  1.73batch/s, auc=0.8698, loss=0.6513]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 110/292 [01:11<01:44,  1.75batch/s, auc=0.8698, loss=0.6513]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 110/292 [01:12<01:44,  1.75batch/s, auc=0.8705, loss=0.5860]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 111/292 [01:12<01:42,  1.76batch/s, auc=0.8705, loss=0.5860]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 111/292 [01:13<01:42,  1.76batch/s, auc=0.8709, loss=0.7687]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 112/292 [01:13<01:41,  1.77batch/s, auc=0.8709, loss=0.7687]\u001b[A\n",
      "Training Epoch 6/25:  38%|███▊      | 112/292 [01:13<01:41,  1.77batch/s, auc=0.8701, loss=1.0944]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▊      | 113/292 [01:13<01:40,  1.78batch/s, auc=0.8701, loss=1.0944]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▊      | 113/292 [01:14<01:40,  1.78batch/s, auc=0.8705, loss=0.7101]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 114/292 [01:14<01:39,  1.78batch/s, auc=0.8705, loss=0.7101]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 114/292 [01:14<01:39,  1.78batch/s, auc=0.8710, loss=0.5545]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 115/292 [01:14<01:39,  1.79batch/s, auc=0.8710, loss=0.5545]\u001b[A\n",
      "Training Epoch 6/25:  39%|███▉      | 115/292 [01:15<01:39,  1.79batch/s, auc=0.8711, loss=0.7685]\u001b[A\n",
      "Training Epoch 6/25:  40%|███▉      | 116/292 [01:15<01:38,  1.79batch/s, auc=0.8711, loss=0.7685]\u001b[A\n",
      "Training Epoch 6/25:  40%|███▉      | 116/292 [01:16<01:38,  1.79batch/s, auc=0.8706, loss=0.9307]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 117/292 [01:16<02:05,  1.40batch/s, auc=0.8706, loss=0.9307]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 117/292 [01:17<02:05,  1.40batch/s, auc=0.8707, loss=0.6565]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 118/292 [01:17<02:23,  1.22batch/s, auc=0.8707, loss=0.6565]\u001b[A\n",
      "Training Epoch 6/25:  40%|████      | 118/292 [01:17<02:23,  1.22batch/s, auc=0.8717, loss=0.4753]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 119/292 [01:17<02:08,  1.35batch/s, auc=0.8717, loss=0.4753]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 119/292 [01:18<02:08,  1.35batch/s, auc=0.8714, loss=0.7520]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 120/292 [01:18<01:58,  1.45batch/s, auc=0.8714, loss=0.7520]\u001b[A\n",
      "Training Epoch 6/25:  41%|████      | 120/292 [01:19<01:58,  1.45batch/s, auc=0.8710, loss=0.9701]\u001b[A\n",
      "Training Epoch 6/25:  41%|████▏     | 121/292 [01:19<02:17,  1.24batch/s, auc=0.8710, loss=0.9701]\u001b[A\n",
      "Training Epoch 6/25:  41%|████▏     | 121/292 [01:20<02:17,  1.24batch/s, auc=0.8713, loss=0.5793]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 122/292 [01:20<02:04,  1.37batch/s, auc=0.8713, loss=0.5793]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 122/292 [01:20<02:04,  1.37batch/s, auc=0.8720, loss=0.5121]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 123/292 [01:20<01:54,  1.47batch/s, auc=0.8720, loss=0.5121]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 123/292 [01:21<01:54,  1.47batch/s, auc=0.8720, loss=0.6698]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 124/292 [01:21<01:48,  1.55batch/s, auc=0.8720, loss=0.6698]\u001b[A\n",
      "Training Epoch 6/25:  42%|████▏     | 124/292 [01:22<01:48,  1.55batch/s, auc=0.8715, loss=0.9786]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 125/292 [01:22<02:09,  1.29batch/s, auc=0.8715, loss=0.9786]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 125/292 [01:22<02:09,  1.29batch/s, auc=0.8719, loss=0.7160]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 126/292 [01:22<01:57,  1.41batch/s, auc=0.8719, loss=0.7160]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 126/292 [01:23<01:57,  1.41batch/s, auc=0.8712, loss=1.1731]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 127/292 [01:23<02:15,  1.22batch/s, auc=0.8712, loss=1.1731]\u001b[A\n",
      "Training Epoch 6/25:  43%|████▎     | 127/292 [01:24<02:15,  1.22batch/s, auc=0.8714, loss=0.6449]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 128/292 [01:24<02:01,  1.35batch/s, auc=0.8714, loss=0.6449]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 128/292 [01:25<02:01,  1.35batch/s, auc=0.8714, loss=0.8012]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 129/292 [01:25<01:51,  1.46batch/s, auc=0.8714, loss=0.8012]\u001b[A\n",
      "Training Epoch 6/25:  44%|████▍     | 129/292 [01:25<01:51,  1.46batch/s, auc=0.8719, loss=0.5901]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 130/292 [01:25<01:45,  1.54batch/s, auc=0.8719, loss=0.5901]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 130/292 [01:26<01:45,  1.54batch/s, auc=0.8719, loss=0.7393]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 131/292 [01:26<01:40,  1.61batch/s, auc=0.8719, loss=0.7393]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▍     | 131/292 [01:26<01:40,  1.61batch/s, auc=0.8721, loss=0.8474]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▌     | 132/292 [01:26<01:36,  1.66batch/s, auc=0.8721, loss=0.8474]\u001b[A\n",
      "Training Epoch 6/25:  45%|████▌     | 132/292 [01:27<01:36,  1.66batch/s, auc=0.8723, loss=0.7277]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 133/292 [01:27<01:33,  1.70batch/s, auc=0.8723, loss=0.7277]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 133/292 [01:28<01:33,  1.70batch/s, auc=0.8723, loss=0.6669]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 134/292 [01:28<01:56,  1.36batch/s, auc=0.8723, loss=0.6669]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 134/292 [01:28<01:56,  1.36batch/s, auc=0.8728, loss=0.5374]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 135/292 [01:28<01:47,  1.47batch/s, auc=0.8728, loss=0.5374]\u001b[A\n",
      "Training Epoch 6/25:  46%|████▌     | 135/292 [01:29<01:47,  1.47batch/s, auc=0.8735, loss=0.4643]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 136/292 [01:29<01:40,  1.55batch/s, auc=0.8735, loss=0.4643]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 136/292 [01:30<01:40,  1.55batch/s, auc=0.8730, loss=0.8090]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 137/292 [01:30<01:36,  1.61batch/s, auc=0.8730, loss=0.8090]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 137/292 [01:30<01:36,  1.61batch/s, auc=0.8733, loss=0.7777]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 138/292 [01:30<01:32,  1.66batch/s, auc=0.8733, loss=0.7777]\u001b[A\n",
      "Training Epoch 6/25:  47%|████▋     | 138/292 [01:31<01:32,  1.66batch/s, auc=0.8730, loss=0.8387]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 139/292 [01:31<01:53,  1.35batch/s, auc=0.8730, loss=0.8387]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 139/292 [01:32<01:53,  1.35batch/s, auc=0.8735, loss=0.5997]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 140/292 [01:32<01:44,  1.45batch/s, auc=0.8735, loss=0.5997]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 140/292 [01:32<01:44,  1.45batch/s, auc=0.8738, loss=0.5314]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 141/292 [01:32<01:38,  1.54batch/s, auc=0.8738, loss=0.5314]\u001b[A\n",
      "Training Epoch 6/25:  48%|████▊     | 141/292 [01:33<01:38,  1.54batch/s, auc=0.8738, loss=0.6124]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▊     | 142/292 [01:33<01:33,  1.61batch/s, auc=0.8738, loss=0.6124]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▊     | 142/292 [01:34<01:33,  1.61batch/s, auc=0.8733, loss=1.0949]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 143/292 [01:34<01:52,  1.32batch/s, auc=0.8733, loss=1.0949]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 143/292 [01:35<01:52,  1.32batch/s, auc=0.8735, loss=0.6459]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 144/292 [01:35<01:43,  1.43batch/s, auc=0.8735, loss=0.6459]\u001b[A\n",
      "Training Epoch 6/25:  49%|████▉     | 144/292 [01:35<01:43,  1.43batch/s, auc=0.8737, loss=0.5624]\u001b[A\n",
      "Training Epoch 6/25:  50%|████▉     | 145/292 [01:35<01:36,  1.52batch/s, auc=0.8737, loss=0.5624]\u001b[A\n",
      "Training Epoch 6/25:  50%|████▉     | 145/292 [01:36<01:36,  1.52batch/s, auc=0.8741, loss=0.5606]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 146/292 [01:36<01:54,  1.28batch/s, auc=0.8741, loss=0.5606]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 146/292 [01:37<01:54,  1.28batch/s, auc=0.8741, loss=0.7321]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 147/292 [01:37<01:43,  1.40batch/s, auc=0.8741, loss=0.7321]\u001b[A\n",
      "Training Epoch 6/25:  50%|█████     | 147/292 [01:37<01:43,  1.40batch/s, auc=0.8741, loss=0.7898]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 148/292 [01:37<01:36,  1.50batch/s, auc=0.8741, loss=0.7898]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 148/292 [01:38<01:36,  1.50batch/s, auc=0.8737, loss=0.7802]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 149/292 [01:38<01:30,  1.57batch/s, auc=0.8737, loss=0.7802]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████     | 149/292 [01:38<01:30,  1.57batch/s, auc=0.8738, loss=0.6305]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████▏    | 150/292 [01:38<01:26,  1.63batch/s, auc=0.8738, loss=0.6305]\u001b[A\n",
      "Training Epoch 6/25:  51%|█████▏    | 150/292 [01:39<01:26,  1.63batch/s, auc=0.8738, loss=0.5536]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 151/292 [01:39<01:24,  1.68batch/s, auc=0.8738, loss=0.5536]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 151/292 [01:40<01:24,  1.68batch/s, auc=0.8739, loss=0.7155]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 152/292 [01:40<01:21,  1.71batch/s, auc=0.8739, loss=0.7155]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 152/292 [01:41<01:21,  1.71batch/s, auc=0.8732, loss=1.1572]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 153/292 [01:41<01:41,  1.37batch/s, auc=0.8732, loss=1.1572]\u001b[A\n",
      "Training Epoch 6/25:  52%|█████▏    | 153/292 [01:41<01:41,  1.37batch/s, auc=0.8733, loss=0.5840]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 154/292 [01:41<01:33,  1.47batch/s, auc=0.8733, loss=0.5840]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 154/292 [01:42<01:33,  1.47batch/s, auc=0.8737, loss=0.6054]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 155/292 [01:42<01:28,  1.55batch/s, auc=0.8737, loss=0.6054]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 155/292 [01:42<01:28,  1.55batch/s, auc=0.8739, loss=0.5807]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 156/292 [01:42<01:24,  1.62batch/s, auc=0.8739, loss=0.5807]\u001b[A\n",
      "Training Epoch 6/25:  53%|█████▎    | 156/292 [01:43<01:24,  1.62batch/s, auc=0.8738, loss=0.6981]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 157/292 [01:43<01:21,  1.66batch/s, auc=0.8738, loss=0.6981]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 157/292 [01:43<01:21,  1.66batch/s, auc=0.8739, loss=0.5306]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 158/292 [01:43<01:18,  1.70batch/s, auc=0.8739, loss=0.5306]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 158/292 [01:44<01:18,  1.70batch/s, auc=0.8742, loss=0.4837]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 159/292 [01:44<01:17,  1.73batch/s, auc=0.8742, loss=0.4837]\u001b[A\n",
      "Training Epoch 6/25:  54%|█████▍    | 159/292 [01:45<01:17,  1.73batch/s, auc=0.8745, loss=0.5517]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▍    | 160/292 [01:45<01:15,  1.74batch/s, auc=0.8745, loss=0.5517]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▍    | 160/292 [01:45<01:15,  1.74batch/s, auc=0.8746, loss=0.6781]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 161/292 [01:45<01:14,  1.76batch/s, auc=0.8746, loss=0.6781]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 161/292 [01:46<01:14,  1.76batch/s, auc=0.8737, loss=1.3801]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 162/292 [01:46<01:13,  1.77batch/s, auc=0.8737, loss=1.3801]\u001b[A\n",
      "Training Epoch 6/25:  55%|█████▌    | 162/292 [01:46<01:13,  1.77batch/s, auc=0.8743, loss=0.5703]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 163/292 [01:46<01:12,  1.77batch/s, auc=0.8743, loss=0.5703]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 163/292 [01:47<01:12,  1.77batch/s, auc=0.8740, loss=0.9097]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 164/292 [01:47<01:31,  1.39batch/s, auc=0.8740, loss=0.9097]\u001b[A\n",
      "Training Epoch 6/25:  56%|█████▌    | 164/292 [01:48<01:31,  1.39batch/s, auc=0.8739, loss=0.8273]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 165/292 [01:48<01:25,  1.49batch/s, auc=0.8739, loss=0.8273]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 165/292 [01:49<01:25,  1.49batch/s, auc=0.8725, loss=1.6285]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 166/292 [01:49<01:39,  1.26batch/s, auc=0.8725, loss=1.6285]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 166/292 [01:49<01:39,  1.26batch/s, auc=0.8723, loss=0.9597]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 167/292 [01:49<01:30,  1.39batch/s, auc=0.8723, loss=0.9597]\u001b[A\n",
      "Training Epoch 6/25:  57%|█████▋    | 167/292 [01:50<01:30,  1.39batch/s, auc=0.8725, loss=0.6866]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 168/292 [01:50<01:23,  1.49batch/s, auc=0.8725, loss=0.6866]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 168/292 [01:51<01:23,  1.49batch/s, auc=0.8721, loss=1.0907]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 169/292 [01:51<01:18,  1.56batch/s, auc=0.8721, loss=1.0907]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 169/292 [01:51<01:18,  1.56batch/s, auc=0.8724, loss=0.5928]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 170/292 [01:51<01:15,  1.63batch/s, auc=0.8724, loss=0.5928]\u001b[A\n",
      "Training Epoch 6/25:  58%|█████▊    | 170/292 [01:52<01:15,  1.63batch/s, auc=0.8720, loss=1.0338]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▊    | 171/292 [01:52<01:12,  1.67batch/s, auc=0.8720, loss=1.0338]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▊    | 171/292 [01:53<01:12,  1.67batch/s, auc=0.8710, loss=1.3853]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 172/292 [01:53<01:29,  1.35batch/s, auc=0.8710, loss=1.3853]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 172/292 [01:53<01:29,  1.35batch/s, auc=0.8714, loss=0.6818]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 173/292 [01:53<01:21,  1.45batch/s, auc=0.8714, loss=0.6818]\u001b[A\n",
      "Training Epoch 6/25:  59%|█████▉    | 173/292 [01:54<01:21,  1.45batch/s, auc=0.8718, loss=0.6748]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 174/292 [01:54<01:16,  1.54batch/s, auc=0.8718, loss=0.6748]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 174/292 [01:54<01:16,  1.54batch/s, auc=0.8715, loss=0.7655]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 175/292 [01:54<01:12,  1.61batch/s, auc=0.8715, loss=0.7655]\u001b[A\n",
      "Training Epoch 6/25:  60%|█████▉    | 175/292 [01:56<01:12,  1.61batch/s, auc=0.8710, loss=0.9603]\u001b[A\n",
      "Training Epoch 6/25:  60%|██████    | 176/292 [01:56<01:28,  1.32batch/s, auc=0.8710, loss=0.9603]\u001b[A\n",
      "Training Epoch 6/25:  60%|██████    | 176/292 [01:57<01:28,  1.32batch/s, auc=0.8703, loss=0.9644]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 177/292 [01:57<01:38,  1.17batch/s, auc=0.8703, loss=0.9644]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 177/292 [01:57<01:38,  1.17batch/s, auc=0.8702, loss=0.9061]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 178/292 [01:57<01:27,  1.31batch/s, auc=0.8702, loss=0.9061]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████    | 178/292 [01:58<01:27,  1.31batch/s, auc=0.8703, loss=0.7147]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████▏   | 179/292 [01:58<01:19,  1.42batch/s, auc=0.8703, loss=0.7147]\u001b[A\n",
      "Training Epoch 6/25:  61%|██████▏   | 179/292 [01:58<01:19,  1.42batch/s, auc=0.8706, loss=0.6862]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 180/292 [01:58<01:13,  1.51batch/s, auc=0.8706, loss=0.6862]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 180/292 [01:59<01:13,  1.51batch/s, auc=0.8701, loss=1.0114]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 181/292 [01:59<01:27,  1.27batch/s, auc=0.8701, loss=1.0114]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 181/292 [02:00<01:27,  1.27batch/s, auc=0.8701, loss=0.6067]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 182/292 [02:00<01:18,  1.39batch/s, auc=0.8701, loss=0.6067]\u001b[A\n",
      "Training Epoch 6/25:  62%|██████▏   | 182/292 [02:00<01:18,  1.39batch/s, auc=0.8706, loss=0.6271]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 183/292 [02:00<01:13,  1.49batch/s, auc=0.8706, loss=0.6271]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 183/292 [02:01<01:13,  1.49batch/s, auc=0.8704, loss=0.7369]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 184/292 [02:01<01:08,  1.57batch/s, auc=0.8704, loss=0.7369]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 184/292 [02:02<01:08,  1.57batch/s, auc=0.8703, loss=0.7550]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 185/292 [02:02<01:05,  1.63batch/s, auc=0.8703, loss=0.7550]\u001b[A\n",
      "Training Epoch 6/25:  63%|██████▎   | 185/292 [02:02<01:05,  1.63batch/s, auc=0.8700, loss=0.8028]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▎   | 186/292 [02:02<01:03,  1.67batch/s, auc=0.8700, loss=0.8028]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▎   | 186/292 [02:03<01:03,  1.67batch/s, auc=0.8692, loss=1.2141]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 187/292 [02:03<01:17,  1.35batch/s, auc=0.8692, loss=1.2141]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 187/292 [02:04<01:17,  1.35batch/s, auc=0.8694, loss=0.5743]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 188/292 [02:04<01:11,  1.45batch/s, auc=0.8694, loss=0.5743]\u001b[A\n",
      "Training Epoch 6/25:  64%|██████▍   | 188/292 [02:04<01:11,  1.45batch/s, auc=0.8692, loss=0.9121]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▍   | 189/292 [02:04<01:06,  1.54batch/s, auc=0.8692, loss=0.9121]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▍   | 189/292 [02:05<01:06,  1.54batch/s, auc=0.8694, loss=0.7240]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 190/292 [02:05<01:03,  1.60batch/s, auc=0.8694, loss=0.7240]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 190/292 [02:05<01:03,  1.60batch/s, auc=0.8693, loss=0.7903]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 191/292 [02:05<01:01,  1.65batch/s, auc=0.8693, loss=0.7903]\u001b[A\n",
      "Training Epoch 6/25:  65%|██████▌   | 191/292 [02:06<01:01,  1.65batch/s, auc=0.8695, loss=0.7377]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 192/292 [02:06<00:59,  1.69batch/s, auc=0.8695, loss=0.7377]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 192/292 [02:07<00:59,  1.69batch/s, auc=0.8697, loss=0.7087]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 193/292 [02:07<00:57,  1.72batch/s, auc=0.8697, loss=0.7087]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▌   | 193/292 [02:08<00:57,  1.72batch/s, auc=0.8694, loss=0.8807]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▋   | 194/292 [02:08<01:11,  1.37batch/s, auc=0.8694, loss=0.8807]\u001b[A\n",
      "Training Epoch 6/25:  66%|██████▋   | 194/292 [02:08<01:11,  1.37batch/s, auc=0.8693, loss=0.7151]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 195/292 [02:08<01:05,  1.47batch/s, auc=0.8693, loss=0.7151]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 195/292 [02:09<01:05,  1.47batch/s, auc=0.8692, loss=0.7711]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 196/292 [02:09<01:01,  1.55batch/s, auc=0.8692, loss=0.7711]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 196/292 [02:09<01:01,  1.55batch/s, auc=0.8689, loss=0.6692]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 197/292 [02:09<00:58,  1.61batch/s, auc=0.8689, loss=0.6692]\u001b[A\n",
      "Training Epoch 6/25:  67%|██████▋   | 197/292 [02:10<00:58,  1.61batch/s, auc=0.8688, loss=0.9373]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 198/292 [02:10<00:56,  1.66batch/s, auc=0.8688, loss=0.9373]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 198/292 [02:11<00:56,  1.66batch/s, auc=0.8688, loss=0.6991]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 199/292 [02:11<00:54,  1.70batch/s, auc=0.8688, loss=0.6991]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 199/292 [02:11<00:54,  1.70batch/s, auc=0.8691, loss=0.7522]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 200/292 [02:11<00:53,  1.72batch/s, auc=0.8691, loss=0.7522]\u001b[A\n",
      "Training Epoch 6/25:  68%|██████▊   | 200/292 [02:12<00:53,  1.72batch/s, auc=0.8693, loss=0.5168]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 201/292 [02:12<00:52,  1.74batch/s, auc=0.8693, loss=0.5168]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 201/292 [02:12<00:52,  1.74batch/s, auc=0.8698, loss=0.6454]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 202/292 [02:12<00:51,  1.75batch/s, auc=0.8698, loss=0.6454]\u001b[A\n",
      "Training Epoch 6/25:  69%|██████▉   | 202/292 [02:13<00:51,  1.75batch/s, auc=0.8699, loss=0.7566]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 203/292 [02:13<00:50,  1.76batch/s, auc=0.8699, loss=0.7566]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 203/292 [02:13<00:50,  1.76batch/s, auc=0.8703, loss=0.5542]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 204/292 [02:13<00:49,  1.77batch/s, auc=0.8703, loss=0.5542]\u001b[A\n",
      "Training Epoch 6/25:  70%|██████▉   | 204/292 [02:14<00:49,  1.77batch/s, auc=0.8699, loss=0.7427]\u001b[A\n",
      "Training Epoch 6/25:  70%|███████   | 205/292 [02:14<00:49,  1.77batch/s, auc=0.8699, loss=0.7427]\u001b[A\n",
      "Training Epoch 6/25:  70%|███████   | 205/292 [02:14<00:49,  1.77batch/s, auc=0.8700, loss=0.5782]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 206/292 [02:14<00:48,  1.77batch/s, auc=0.8700, loss=0.5782]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 206/292 [02:15<00:48,  1.77batch/s, auc=0.8701, loss=0.6534]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 207/292 [02:15<00:47,  1.78batch/s, auc=0.8701, loss=0.6534]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 207/292 [02:16<00:47,  1.78batch/s, auc=0.8699, loss=0.8238]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 208/292 [02:16<01:00,  1.40batch/s, auc=0.8699, loss=0.8238]\u001b[A\n",
      "Training Epoch 6/25:  71%|███████   | 208/292 [02:17<01:00,  1.40batch/s, auc=0.8700, loss=0.6301]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 209/292 [02:17<00:55,  1.49batch/s, auc=0.8700, loss=0.6301]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 209/292 [02:17<00:55,  1.49batch/s, auc=0.8701, loss=0.6283]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 210/292 [02:17<00:52,  1.57batch/s, auc=0.8701, loss=0.6283]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 210/292 [02:18<00:52,  1.57batch/s, auc=0.8702, loss=0.7941]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 211/292 [02:18<01:02,  1.30batch/s, auc=0.8702, loss=0.7941]\u001b[A\n",
      "Training Epoch 6/25:  72%|███████▏  | 211/292 [02:19<01:02,  1.30batch/s, auc=0.8703, loss=0.5630]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 212/292 [02:19<00:56,  1.42batch/s, auc=0.8703, loss=0.5630]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 212/292 [02:19<00:56,  1.42batch/s, auc=0.8704, loss=0.5922]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 213/292 [02:19<00:52,  1.51batch/s, auc=0.8704, loss=0.5922]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 213/292 [02:20<00:52,  1.51batch/s, auc=0.8705, loss=0.7170]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 214/292 [02:20<00:49,  1.58batch/s, auc=0.8705, loss=0.7170]\u001b[A\n",
      "Training Epoch 6/25:  73%|███████▎  | 214/292 [02:21<00:49,  1.58batch/s, auc=0.8699, loss=1.2205]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▎  | 215/292 [02:21<00:58,  1.31batch/s, auc=0.8699, loss=1.2205]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▎  | 215/292 [02:22<00:58,  1.31batch/s, auc=0.8699, loss=0.6935]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 216/292 [02:22<00:53,  1.42batch/s, auc=0.8699, loss=0.6935]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 216/292 [02:23<00:53,  1.42batch/s, auc=0.8689, loss=1.3627]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 217/292 [02:23<01:01,  1.22batch/s, auc=0.8689, loss=1.3627]\u001b[A\n",
      "Training Epoch 6/25:  74%|███████▍  | 217/292 [02:23<01:01,  1.22batch/s, auc=0.8686, loss=1.0880]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▍  | 218/292 [02:23<00:54,  1.35batch/s, auc=0.8686, loss=1.0880]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▍  | 218/292 [02:24<00:54,  1.35batch/s, auc=0.8688, loss=0.5958]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 219/292 [02:24<00:50,  1.46batch/s, auc=0.8688, loss=0.5958]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 219/292 [02:24<00:50,  1.46batch/s, auc=0.8686, loss=0.7916]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 220/292 [02:24<00:46,  1.54batch/s, auc=0.8686, loss=0.7916]\u001b[A\n",
      "Training Epoch 6/25:  75%|███████▌  | 220/292 [02:25<00:46,  1.54batch/s, auc=0.8683, loss=0.7480]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 221/292 [02:25<00:44,  1.60batch/s, auc=0.8683, loss=0.7480]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 221/292 [02:25<00:44,  1.60batch/s, auc=0.8688, loss=0.5233]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 222/292 [02:25<00:42,  1.65batch/s, auc=0.8688, loss=0.5233]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▌  | 222/292 [02:26<00:42,  1.65batch/s, auc=0.8690, loss=0.6846]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▋  | 223/292 [02:26<00:40,  1.69batch/s, auc=0.8690, loss=0.6846]\u001b[A\n",
      "Training Epoch 6/25:  76%|███████▋  | 223/292 [02:27<00:40,  1.69batch/s, auc=0.8692, loss=0.6131]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 224/292 [02:27<00:39,  1.71batch/s, auc=0.8692, loss=0.6131]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 224/292 [02:27<00:39,  1.71batch/s, auc=0.8692, loss=0.6782]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 225/292 [02:27<00:38,  1.73batch/s, auc=0.8692, loss=0.6782]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 225/292 [02:28<00:38,  1.73batch/s, auc=0.8695, loss=0.7876]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 226/292 [02:28<00:37,  1.74batch/s, auc=0.8695, loss=0.7876]\u001b[A\n",
      "Training Epoch 6/25:  77%|███████▋  | 226/292 [02:28<00:37,  1.74batch/s, auc=0.8697, loss=0.6683]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 227/292 [02:28<00:37,  1.75batch/s, auc=0.8697, loss=0.6683]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 227/292 [02:29<00:37,  1.75batch/s, auc=0.8696, loss=0.6653]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 228/292 [02:29<00:36,  1.76batch/s, auc=0.8696, loss=0.6653]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 228/292 [02:29<00:36,  1.76batch/s, auc=0.8700, loss=0.4719]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 229/292 [02:29<00:35,  1.76batch/s, auc=0.8700, loss=0.4719]\u001b[A\n",
      "Training Epoch 6/25:  78%|███████▊  | 229/292 [02:30<00:35,  1.76batch/s, auc=0.8699, loss=0.8083]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 230/292 [02:30<00:35,  1.77batch/s, auc=0.8699, loss=0.8083]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 230/292 [02:31<00:35,  1.77batch/s, auc=0.8691, loss=1.3349]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 231/292 [02:31<00:43,  1.39batch/s, auc=0.8691, loss=1.3349]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 231/292 [02:32<00:43,  1.39batch/s, auc=0.8691, loss=0.5546]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 232/292 [02:32<00:40,  1.49batch/s, auc=0.8691, loss=0.5546]\u001b[A\n",
      "Training Epoch 6/25:  79%|███████▉  | 232/292 [02:32<00:40,  1.49batch/s, auc=0.8693, loss=0.5672]\u001b[A\n",
      "Training Epoch 6/25:  80%|███████▉  | 233/292 [02:32<00:37,  1.56batch/s, auc=0.8693, loss=0.5672]\u001b[A\n",
      "Training Epoch 6/25:  80%|███████▉  | 233/292 [02:33<00:37,  1.56batch/s, auc=0.8697, loss=0.6105]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 234/292 [02:33<00:35,  1.62batch/s, auc=0.8697, loss=0.6105]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 234/292 [02:33<00:35,  1.62batch/s, auc=0.8695, loss=0.9477]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 235/292 [02:33<00:34,  1.66batch/s, auc=0.8695, loss=0.9477]\u001b[A\n",
      "Training Epoch 6/25:  80%|████████  | 235/292 [02:34<00:34,  1.66batch/s, auc=0.8697, loss=0.5657]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 236/292 [02:34<00:33,  1.70batch/s, auc=0.8697, loss=0.5657]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 236/292 [02:34<00:33,  1.70batch/s, auc=0.8698, loss=0.6721]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 237/292 [02:34<00:32,  1.72batch/s, auc=0.8698, loss=0.6721]\u001b[A\n",
      "Training Epoch 6/25:  81%|████████  | 237/292 [02:36<00:32,  1.72batch/s, auc=0.8697, loss=0.8496]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 238/292 [02:36<00:39,  1.37batch/s, auc=0.8697, loss=0.8496]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 238/292 [02:36<00:39,  1.37batch/s, auc=0.8698, loss=0.7239]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 239/292 [02:36<00:36,  1.47batch/s, auc=0.8698, loss=0.7239]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 239/292 [02:37<00:36,  1.47batch/s, auc=0.8701, loss=0.5836]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 240/292 [02:37<00:33,  1.55batch/s, auc=0.8701, loss=0.5836]\u001b[A\n",
      "Training Epoch 6/25:  82%|████████▏ | 240/292 [02:38<00:33,  1.55batch/s, auc=0.8702, loss=0.6194]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 241/292 [02:38<00:39,  1.29batch/s, auc=0.8702, loss=0.6194]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 241/292 [02:38<00:39,  1.29batch/s, auc=0.8703, loss=0.5370]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 242/292 [02:38<00:35,  1.40batch/s, auc=0.8703, loss=0.5370]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 242/292 [02:39<00:35,  1.40batch/s, auc=0.8701, loss=0.6046]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 243/292 [02:39<00:32,  1.50batch/s, auc=0.8701, loss=0.6046]\u001b[A\n",
      "Training Epoch 6/25:  83%|████████▎ | 243/292 [02:39<00:32,  1.50batch/s, auc=0.8698, loss=0.8895]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▎ | 244/292 [02:39<00:30,  1.57batch/s, auc=0.8698, loss=0.8895]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▎ | 244/292 [02:40<00:30,  1.57batch/s, auc=0.8700, loss=0.7259]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 245/292 [02:40<00:28,  1.63batch/s, auc=0.8700, loss=0.7259]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 245/292 [02:41<00:28,  1.63batch/s, auc=0.8700, loss=0.8104]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 246/292 [02:41<00:34,  1.32batch/s, auc=0.8700, loss=0.8104]\u001b[A\n",
      "Training Epoch 6/25:  84%|████████▍ | 246/292 [02:42<00:34,  1.32batch/s, auc=0.8699, loss=0.6862]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 247/292 [02:42<00:31,  1.43batch/s, auc=0.8699, loss=0.6862]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 247/292 [02:42<00:31,  1.43batch/s, auc=0.8702, loss=0.6328]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 248/292 [02:42<00:28,  1.52batch/s, auc=0.8702, loss=0.6328]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▍ | 248/292 [02:43<00:28,  1.52batch/s, auc=0.8700, loss=0.9214]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▌ | 249/292 [02:43<00:27,  1.59batch/s, auc=0.8700, loss=0.9214]\u001b[A\n",
      "Training Epoch 6/25:  85%|████████▌ | 249/292 [02:43<00:27,  1.59batch/s, auc=0.8701, loss=0.7617]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 250/292 [02:43<00:25,  1.64batch/s, auc=0.8701, loss=0.7617]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 250/292 [02:44<00:25,  1.64batch/s, auc=0.8702, loss=0.5964]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 251/292 [02:44<00:24,  1.68batch/s, auc=0.8702, loss=0.5964]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▌ | 251/292 [02:44<00:24,  1.68batch/s, auc=0.8705, loss=0.5584]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▋ | 252/292 [02:44<00:23,  1.70batch/s, auc=0.8705, loss=0.5584]\u001b[A\n",
      "Training Epoch 6/25:  86%|████████▋ | 252/292 [02:45<00:23,  1.70batch/s, auc=0.8705, loss=0.7346]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 253/292 [02:45<00:22,  1.72batch/s, auc=0.8705, loss=0.7346]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 253/292 [02:46<00:22,  1.72batch/s, auc=0.8705, loss=0.8931]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 254/292 [02:46<00:21,  1.73batch/s, auc=0.8705, loss=0.8931]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 254/292 [02:46<00:21,  1.73batch/s, auc=0.8705, loss=0.7598]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 255/292 [02:46<00:21,  1.75batch/s, auc=0.8705, loss=0.7598]\u001b[A\n",
      "Training Epoch 6/25:  87%|████████▋ | 255/292 [02:47<00:21,  1.75batch/s, auc=0.8705, loss=0.8093]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 256/292 [02:47<00:20,  1.75batch/s, auc=0.8705, loss=0.8093]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 256/292 [02:47<00:20,  1.75batch/s, auc=0.8706, loss=0.6628]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 257/292 [02:47<00:19,  1.76batch/s, auc=0.8706, loss=0.6628]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 257/292 [02:48<00:19,  1.76batch/s, auc=0.8708, loss=0.6389]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 258/292 [02:48<00:19,  1.76batch/s, auc=0.8708, loss=0.6389]\u001b[A\n",
      "Training Epoch 6/25:  88%|████████▊ | 258/292 [02:48<00:19,  1.76batch/s, auc=0.8708, loss=0.6273]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▊ | 259/292 [02:48<00:18,  1.77batch/s, auc=0.8708, loss=0.6273]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▊ | 259/292 [02:50<00:18,  1.77batch/s, auc=0.8699, loss=1.6910]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 260/292 [02:50<00:23,  1.39batch/s, auc=0.8699, loss=1.6910]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 260/292 [02:50<00:23,  1.39batch/s, auc=0.8698, loss=0.7221]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 261/292 [02:50<00:20,  1.48batch/s, auc=0.8698, loss=0.7221]\u001b[A\n",
      "Training Epoch 6/25:  89%|████████▉ | 261/292 [02:51<00:20,  1.48batch/s, auc=0.8684, loss=2.2523]\u001b[A\n",
      "Training Epoch 6/25:  90%|████████▉ | 262/292 [02:51<00:23,  1.26batch/s, auc=0.8684, loss=2.2523]\u001b[A\n",
      "Training Epoch 6/25:  90%|████████▉ | 262/292 [02:52<00:23,  1.26batch/s, auc=0.8681, loss=0.9629]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 263/292 [02:52<00:25,  1.13batch/s, auc=0.8681, loss=0.9629]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 263/292 [02:53<00:25,  1.13batch/s, auc=0.8678, loss=0.9571]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 264/292 [02:53<00:26,  1.06batch/s, auc=0.8678, loss=0.9571]\u001b[A\n",
      "Training Epoch 6/25:  90%|█████████ | 264/292 [02:54<00:26,  1.06batch/s, auc=0.8675, loss=0.9040]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 265/292 [02:54<00:26,  1.02batch/s, auc=0.8675, loss=0.9040]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 265/292 [02:55<00:26,  1.02batch/s, auc=0.8675, loss=0.6443]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 266/292 [02:55<00:22,  1.16batch/s, auc=0.8675, loss=0.6443]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████ | 266/292 [02:56<00:22,  1.16batch/s, auc=0.8677, loss=0.6531]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████▏| 267/292 [02:56<00:19,  1.30batch/s, auc=0.8677, loss=0.6531]\u001b[A\n",
      "Training Epoch 6/25:  91%|█████████▏| 267/292 [02:56<00:19,  1.30batch/s, auc=0.8679, loss=0.5794]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 268/292 [02:56<00:17,  1.41batch/s, auc=0.8679, loss=0.5794]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 268/292 [02:57<00:17,  1.41batch/s, auc=0.8681, loss=0.6295]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 269/292 [02:57<00:15,  1.50batch/s, auc=0.8681, loss=0.6295]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 269/292 [02:57<00:15,  1.50batch/s, auc=0.8681, loss=0.6554]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 270/292 [02:57<00:13,  1.57batch/s, auc=0.8681, loss=0.6554]\u001b[A\n",
      "Training Epoch 6/25:  92%|█████████▏| 270/292 [02:58<00:13,  1.57batch/s, auc=0.8681, loss=0.6061]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 271/292 [02:58<00:16,  1.30batch/s, auc=0.8681, loss=0.6061]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 271/292 [02:59<00:16,  1.30batch/s, auc=0.8684, loss=0.5625]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 272/292 [02:59<00:14,  1.41batch/s, auc=0.8684, loss=0.5625]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 272/292 [02:59<00:14,  1.41batch/s, auc=0.8682, loss=0.7740]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 273/292 [02:59<00:12,  1.50batch/s, auc=0.8682, loss=0.7740]\u001b[A\n",
      "Training Epoch 6/25:  93%|█████████▎| 273/292 [03:01<00:12,  1.50batch/s, auc=0.8680, loss=1.0467]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 274/292 [03:01<00:14,  1.27batch/s, auc=0.8680, loss=1.0467]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 274/292 [03:01<00:14,  1.27batch/s, auc=0.8678, loss=0.7744]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 275/292 [03:01<00:12,  1.38batch/s, auc=0.8678, loss=0.7744]\u001b[A\n",
      "Training Epoch 6/25:  94%|█████████▍| 275/292 [03:02<00:12,  1.38batch/s, auc=0.8676, loss=0.9153]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 276/292 [03:02<00:10,  1.48batch/s, auc=0.8676, loss=0.9153]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 276/292 [03:03<00:10,  1.48batch/s, auc=0.8673, loss=0.9905]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 277/292 [03:03<00:11,  1.25batch/s, auc=0.8673, loss=0.9905]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▍| 277/292 [03:03<00:11,  1.25batch/s, auc=0.8675, loss=0.6205]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▌| 278/292 [03:03<00:10,  1.37batch/s, auc=0.8675, loss=0.6205]\u001b[A\n",
      "Training Epoch 6/25:  95%|█████████▌| 278/292 [03:04<00:10,  1.37batch/s, auc=0.8675, loss=0.7146]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 279/292 [03:04<00:08,  1.47batch/s, auc=0.8675, loss=0.7146]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 279/292 [03:04<00:08,  1.47batch/s, auc=0.8674, loss=0.9057]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 280/292 [03:04<00:07,  1.55batch/s, auc=0.8674, loss=0.9057]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 280/292 [03:05<00:07,  1.55batch/s, auc=0.8675, loss=0.7196]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 281/292 [03:05<00:06,  1.60batch/s, auc=0.8675, loss=0.7196]\u001b[A\n",
      "Training Epoch 6/25:  96%|█████████▌| 281/292 [03:06<00:06,  1.60batch/s, auc=0.8676, loss=0.6122]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 282/292 [03:06<00:06,  1.65batch/s, auc=0.8676, loss=0.6122]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 282/292 [03:06<00:06,  1.65batch/s, auc=0.8675, loss=0.8448]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 283/292 [03:06<00:05,  1.68batch/s, auc=0.8675, loss=0.8448]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.68batch/s, auc=0.8678, loss=0.5615]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.71batch/s, auc=0.8678, loss=0.5615]\u001b[A\n",
      "Training Epoch 6/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.71batch/s, auc=0.8679, loss=0.7229]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 285/292 [03:07<00:04,  1.73batch/s, auc=0.8679, loss=0.7229]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.73batch/s, auc=0.8683, loss=0.5668]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.74batch/s, auc=0.8683, loss=0.5668]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.74batch/s, auc=0.8684, loss=0.5942]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 287/292 [03:08<00:02,  1.75batch/s, auc=0.8684, loss=0.5942]\u001b[A\n",
      "Training Epoch 6/25:  98%|█████████▊| 287/292 [03:09<00:02,  1.75batch/s, auc=0.8685, loss=0.6963]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▊| 288/292 [03:09<00:02,  1.76batch/s, auc=0.8685, loss=0.6963]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.76batch/s, auc=0.8688, loss=0.7579]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.76batch/s, auc=0.8688, loss=0.7579]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.76batch/s, auc=0.8689, loss=0.6038]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 290/292 [03:10<00:01,  1.77batch/s, auc=0.8689, loss=0.6038]\u001b[A\n",
      "Training Epoch 6/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.77batch/s, auc=0.8692, loss=0.5355]\u001b[A\n",
      "Training Epoch 6/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.77batch/s, auc=0.8692, loss=0.5355]\u001b[A\n",
      "Training Epoch 6/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.77batch/s, auc=0.8689, loss=1.0759]\u001b[A\n",
      "Training Epoch 6/25: 100%|██████████| 292/292 [03:11<00:00,  1.52batch/s, auc=0.8689, loss=1.0759]\u001b[A\n",
      "Epochs:  24%|██▍       | 6/25 [21:09<1:07:12, 212.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25] Train Loss: 0.7637 | Train AUROC: 0.8689 Val Loss: 0.9047 | Val AUROC: 0.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 7/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9292, loss=0.5164]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 1/292 [00:01<07:14,  1.49s/batch, auc=0.9292, loss=0.5164]\u001b[A\n",
      "Training Epoch 7/25:   0%|          | 1/292 [00:02<07:14,  1.49s/batch, auc=0.8778, loss=0.7880]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 2/292 [00:02<04:32,  1.06batch/s, auc=0.8778, loss=0.7880]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 2/292 [00:03<04:32,  1.06batch/s, auc=0.8384, loss=1.1915]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 3/292 [00:03<04:48,  1.00batch/s, auc=0.8384, loss=1.1915]\u001b[A\n",
      "Training Epoch 7/25:   1%|          | 3/292 [00:03<04:48,  1.00batch/s, auc=0.8406, loss=0.8236]\u001b[A\n",
      "Training Epoch 7/25:   1%|▏         | 4/292 [00:03<03:57,  1.21batch/s, auc=0.8406, loss=0.8236]\u001b[A\n",
      "Training Epoch 7/25:   1%|▏         | 4/292 [00:04<03:57,  1.21batch/s, auc=0.8090, loss=1.4562]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 5/292 [00:04<04:22,  1.10batch/s, auc=0.8090, loss=1.4562]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 5/292 [00:05<04:22,  1.10batch/s, auc=0.7916, loss=1.2296]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 6/292 [00:05<04:36,  1.03batch/s, auc=0.7916, loss=1.2296]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 6/292 [00:06<04:36,  1.03batch/s, auc=0.7869, loss=1.1853]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 7/292 [00:06<03:56,  1.20batch/s, auc=0.7869, loss=1.1853]\u001b[A\n",
      "Training Epoch 7/25:   2%|▏         | 7/292 [00:07<03:56,  1.20batch/s, auc=0.7859, loss=0.7199]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 8/292 [00:07<04:17,  1.10batch/s, auc=0.7859, loss=0.7199]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 8/292 [00:07<04:17,  1.10batch/s, auc=0.7860, loss=0.7325]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 9/292 [00:07<03:45,  1.25batch/s, auc=0.7860, loss=0.7325]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 9/292 [00:08<03:45,  1.25batch/s, auc=0.7994, loss=0.5647]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 10/292 [00:08<03:23,  1.38batch/s, auc=0.7994, loss=0.5647]\u001b[A\n",
      "Training Epoch 7/25:   3%|▎         | 10/292 [00:09<03:23,  1.38batch/s, auc=0.8156, loss=0.5854]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 11/292 [00:09<03:08,  1.49batch/s, auc=0.8156, loss=0.5854]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 11/292 [00:09<03:08,  1.49batch/s, auc=0.8205, loss=0.7580]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 12/292 [00:09<02:57,  1.57batch/s, auc=0.8205, loss=0.7580]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 12/292 [00:10<02:57,  1.57batch/s, auc=0.8185, loss=1.0625]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 13/292 [00:10<02:50,  1.64batch/s, auc=0.8185, loss=1.0625]\u001b[A\n",
      "Training Epoch 7/25:   4%|▍         | 13/292 [00:10<02:50,  1.64batch/s, auc=0.8294, loss=0.4480]\u001b[A\n",
      "Training Epoch 7/25:   5%|▍         | 14/292 [00:10<02:45,  1.68batch/s, auc=0.8294, loss=0.4480]\u001b[A\n",
      "Training Epoch 7/25:   5%|▍         | 14/292 [00:11<02:45,  1.68batch/s, auc=0.8370, loss=0.5394]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 15/292 [00:11<02:42,  1.71batch/s, auc=0.8370, loss=0.5394]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 15/292 [00:11<02:42,  1.71batch/s, auc=0.8420, loss=0.5767]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 16/292 [00:11<02:39,  1.74batch/s, auc=0.8420, loss=0.5767]\u001b[A\n",
      "Training Epoch 7/25:   5%|▌         | 16/292 [00:12<02:39,  1.74batch/s, auc=0.8482, loss=0.5028]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 17/292 [00:12<02:36,  1.76batch/s, auc=0.8482, loss=0.5028]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 17/292 [00:12<02:36,  1.76batch/s, auc=0.8529, loss=0.5119]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 18/292 [00:12<02:34,  1.77batch/s, auc=0.8529, loss=0.5119]\u001b[A\n",
      "Training Epoch 7/25:   6%|▌         | 18/292 [00:13<02:34,  1.77batch/s, auc=0.8595, loss=0.5955]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 19/292 [00:13<02:33,  1.78batch/s, auc=0.8595, loss=0.5955]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 19/292 [00:14<02:33,  1.78batch/s, auc=0.8625, loss=0.5259]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 20/292 [00:14<02:32,  1.78batch/s, auc=0.8625, loss=0.5259]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 20/292 [00:14<02:32,  1.78batch/s, auc=0.8649, loss=0.6981]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 21/292 [00:14<02:31,  1.79batch/s, auc=0.8649, loss=0.6981]\u001b[A\n",
      "Training Epoch 7/25:   7%|▋         | 21/292 [00:15<02:31,  1.79batch/s, auc=0.8507, loss=1.7592]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 22/292 [00:15<03:12,  1.40batch/s, auc=0.8507, loss=1.7592]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 22/292 [00:16<03:12,  1.40batch/s, auc=0.8474, loss=1.2575]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 23/292 [00:16<03:40,  1.22batch/s, auc=0.8474, loss=1.2575]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 23/292 [00:17<03:40,  1.22batch/s, auc=0.8506, loss=0.5334]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 24/292 [00:17<03:18,  1.35batch/s, auc=0.8506, loss=0.5334]\u001b[A\n",
      "Training Epoch 7/25:   8%|▊         | 24/292 [00:17<03:18,  1.35batch/s, auc=0.8519, loss=0.7840]\u001b[A\n",
      "Training Epoch 7/25:   9%|▊         | 25/292 [00:17<03:02,  1.46batch/s, auc=0.8519, loss=0.7840]\u001b[A\n",
      "Training Epoch 7/25:   9%|▊         | 25/292 [00:18<03:02,  1.46batch/s, auc=0.8491, loss=1.2911]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 26/292 [00:18<02:51,  1.55batch/s, auc=0.8491, loss=1.2911]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 26/292 [00:19<02:51,  1.55batch/s, auc=0.8483, loss=0.5817]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 27/292 [00:19<02:43,  1.62batch/s, auc=0.8483, loss=0.5817]\u001b[A\n",
      "Training Epoch 7/25:   9%|▉         | 27/292 [00:19<02:43,  1.62batch/s, auc=0.8530, loss=0.5156]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 28/292 [00:19<02:37,  1.67batch/s, auc=0.8530, loss=0.5156]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 28/292 [00:20<02:37,  1.67batch/s, auc=0.8508, loss=0.8345]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 29/292 [00:20<03:14,  1.35batch/s, auc=0.8508, loss=0.8345]\u001b[A\n",
      "Training Epoch 7/25:  10%|▉         | 29/292 [00:21<03:14,  1.35batch/s, auc=0.8516, loss=0.6699]\u001b[A\n",
      "Training Epoch 7/25:  10%|█         | 30/292 [00:21<02:59,  1.46batch/s, auc=0.8516, loss=0.6699]\u001b[A\n",
      "Training Epoch 7/25:  10%|█         | 30/292 [00:21<02:59,  1.46batch/s, auc=0.8511, loss=0.6009]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 31/292 [00:21<02:48,  1.55batch/s, auc=0.8511, loss=0.6009]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 31/292 [00:22<02:48,  1.55batch/s, auc=0.8485, loss=1.0332]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 32/292 [00:22<02:41,  1.61batch/s, auc=0.8485, loss=1.0332]\u001b[A\n",
      "Training Epoch 7/25:  11%|█         | 32/292 [00:22<02:41,  1.61batch/s, auc=0.8520, loss=0.6000]\u001b[A\n",
      "Training Epoch 7/25:  11%|█▏        | 33/292 [00:22<02:35,  1.67batch/s, auc=0.8520, loss=0.6000]\u001b[A\n",
      "Training Epoch 7/25:  11%|█▏        | 33/292 [00:23<02:35,  1.67batch/s, auc=0.8508, loss=0.8230]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 34/292 [00:23<02:31,  1.70batch/s, auc=0.8508, loss=0.8230]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 34/292 [00:23<02:31,  1.70batch/s, auc=0.8522, loss=0.6683]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 35/292 [00:23<02:28,  1.73batch/s, auc=0.8522, loss=0.6683]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 35/292 [00:24<02:28,  1.73batch/s, auc=0.8531, loss=0.6649]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 36/292 [00:24<02:26,  1.75batch/s, auc=0.8531, loss=0.6649]\u001b[A\n",
      "Training Epoch 7/25:  12%|█▏        | 36/292 [00:25<02:26,  1.75batch/s, auc=0.8559, loss=0.5406]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 37/292 [00:25<02:24,  1.77batch/s, auc=0.8559, loss=0.5406]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 37/292 [00:25<02:24,  1.77batch/s, auc=0.8575, loss=0.7034]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 38/292 [00:25<02:23,  1.78batch/s, auc=0.8575, loss=0.7034]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 38/292 [00:26<02:23,  1.78batch/s, auc=0.8597, loss=0.4716]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 39/292 [00:26<02:21,  1.78batch/s, auc=0.8597, loss=0.4716]\u001b[A\n",
      "Training Epoch 7/25:  13%|█▎        | 39/292 [00:26<02:21,  1.78batch/s, auc=0.8610, loss=0.5720]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▎        | 40/292 [00:26<02:21,  1.79batch/s, auc=0.8610, loss=0.5720]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▎        | 40/292 [00:27<02:21,  1.79batch/s, auc=0.8599, loss=0.7913]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 41/292 [00:27<02:20,  1.79batch/s, auc=0.8599, loss=0.7913]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 41/292 [00:27<02:20,  1.79batch/s, auc=0.8607, loss=0.8299]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 42/292 [00:27<02:19,  1.79batch/s, auc=0.8607, loss=0.8299]\u001b[A\n",
      "Training Epoch 7/25:  14%|█▍        | 42/292 [00:28<02:19,  1.79batch/s, auc=0.8619, loss=0.5253]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▍        | 43/292 [00:28<02:18,  1.79batch/s, auc=0.8619, loss=0.5253]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▍        | 43/292 [00:28<02:18,  1.79batch/s, auc=0.8629, loss=0.6521]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 44/292 [00:28<02:18,  1.79batch/s, auc=0.8629, loss=0.6521]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 44/292 [00:29<02:18,  1.79batch/s, auc=0.8652, loss=0.7337]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 45/292 [00:29<02:17,  1.80batch/s, auc=0.8652, loss=0.7337]\u001b[A\n",
      "Training Epoch 7/25:  15%|█▌        | 45/292 [00:30<02:17,  1.80batch/s, auc=0.8646, loss=0.8607]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 46/292 [00:30<02:16,  1.80batch/s, auc=0.8646, loss=0.8607]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 46/292 [00:30<02:16,  1.80batch/s, auc=0.8646, loss=0.7525]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 47/292 [00:30<02:16,  1.80batch/s, auc=0.8646, loss=0.7525]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▌        | 47/292 [00:31<02:16,  1.80batch/s, auc=0.8654, loss=0.5786]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▋        | 48/292 [00:31<02:15,  1.80batch/s, auc=0.8654, loss=0.5786]\u001b[A\n",
      "Training Epoch 7/25:  16%|█▋        | 48/292 [00:31<02:15,  1.80batch/s, auc=0.8675, loss=0.5682]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 49/292 [00:31<02:15,  1.80batch/s, auc=0.8675, loss=0.5682]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 49/292 [00:32<02:15,  1.80batch/s, auc=0.8674, loss=0.7443]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 50/292 [00:32<02:14,  1.80batch/s, auc=0.8674, loss=0.7443]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 50/292 [00:32<02:14,  1.80batch/s, auc=0.8689, loss=0.5101]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 51/292 [00:32<02:14,  1.80batch/s, auc=0.8689, loss=0.5101]\u001b[A\n",
      "Training Epoch 7/25:  17%|█▋        | 51/292 [00:33<02:14,  1.80batch/s, auc=0.8663, loss=1.0288]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 52/292 [00:33<02:50,  1.41batch/s, auc=0.8663, loss=1.0288]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 52/292 [00:34<02:50,  1.41batch/s, auc=0.8670, loss=0.5730]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 53/292 [00:34<02:38,  1.50batch/s, auc=0.8670, loss=0.5730]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 53/292 [00:35<02:38,  1.50batch/s, auc=0.8682, loss=0.6339]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 54/292 [00:35<02:30,  1.58batch/s, auc=0.8682, loss=0.6339]\u001b[A\n",
      "Training Epoch 7/25:  18%|█▊        | 54/292 [00:35<02:30,  1.58batch/s, auc=0.8683, loss=0.5266]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 55/292 [00:35<02:24,  1.64batch/s, auc=0.8683, loss=0.5266]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 55/292 [00:36<02:24,  1.64batch/s, auc=0.8693, loss=0.5686]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 56/292 [00:36<02:20,  1.68batch/s, auc=0.8693, loss=0.5686]\u001b[A\n",
      "Training Epoch 7/25:  19%|█▉        | 56/292 [00:36<02:20,  1.68batch/s, auc=0.8698, loss=0.6345]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 57/292 [00:36<02:16,  1.72batch/s, auc=0.8698, loss=0.6345]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 57/292 [00:37<02:16,  1.72batch/s, auc=0.8691, loss=0.6960]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 58/292 [00:37<02:14,  1.74batch/s, auc=0.8691, loss=0.6960]\u001b[A\n",
      "Training Epoch 7/25:  20%|█▉        | 58/292 [00:37<02:14,  1.74batch/s, auc=0.8699, loss=0.4810]\u001b[A\n",
      "Training Epoch 7/25:  20%|██        | 59/292 [00:37<02:12,  1.76batch/s, auc=0.8699, loss=0.4810]\u001b[A\n",
      "Training Epoch 7/25:  20%|██        | 59/292 [00:38<02:12,  1.76batch/s, auc=0.8684, loss=0.8014]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 60/292 [00:38<02:10,  1.77batch/s, auc=0.8684, loss=0.8014]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 60/292 [00:38<02:10,  1.77batch/s, auc=0.8691, loss=0.6262]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 61/292 [00:38<02:09,  1.78batch/s, auc=0.8691, loss=0.6262]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 61/292 [00:40<02:09,  1.78batch/s, auc=0.8664, loss=1.4525]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 62/292 [00:40<02:44,  1.40batch/s, auc=0.8664, loss=1.4525]\u001b[A\n",
      "Training Epoch 7/25:  21%|██        | 62/292 [00:40<02:44,  1.40batch/s, auc=0.8677, loss=0.4831]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 63/292 [00:40<02:32,  1.50batch/s, auc=0.8677, loss=0.4831]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 63/292 [00:41<02:32,  1.50batch/s, auc=0.8686, loss=0.6069]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 64/292 [00:41<02:24,  1.58batch/s, auc=0.8686, loss=0.6069]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 64/292 [00:41<02:24,  1.58batch/s, auc=0.8704, loss=0.5834]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 65/292 [00:41<02:18,  1.64batch/s, auc=0.8704, loss=0.5834]\u001b[A\n",
      "Training Epoch 7/25:  22%|██▏       | 65/292 [00:42<02:18,  1.64batch/s, auc=0.8706, loss=0.6854]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 66/292 [00:42<02:14,  1.68batch/s, auc=0.8706, loss=0.6854]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 66/292 [00:42<02:14,  1.68batch/s, auc=0.8713, loss=0.6547]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 67/292 [00:42<02:11,  1.71batch/s, auc=0.8713, loss=0.6547]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 67/292 [00:43<02:11,  1.71batch/s, auc=0.8717, loss=0.6705]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 68/292 [00:43<02:08,  1.74batch/s, auc=0.8717, loss=0.6705]\u001b[A\n",
      "Training Epoch 7/25:  23%|██▎       | 68/292 [00:43<02:08,  1.74batch/s, auc=0.8715, loss=0.8073]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▎       | 69/292 [00:43<02:06,  1.76batch/s, auc=0.8715, loss=0.8073]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▎       | 69/292 [00:44<02:06,  1.76batch/s, auc=0.8703, loss=1.1374]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 70/292 [00:44<02:05,  1.77batch/s, auc=0.8703, loss=1.1374]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 70/292 [00:45<02:05,  1.77batch/s, auc=0.8702, loss=0.7111]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 71/292 [00:45<02:04,  1.78batch/s, auc=0.8702, loss=0.7111]\u001b[A\n",
      "Training Epoch 7/25:  24%|██▍       | 71/292 [00:45<02:04,  1.78batch/s, auc=0.8690, loss=1.0231]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▍       | 72/292 [00:45<02:03,  1.78batch/s, auc=0.8690, loss=1.0231]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▍       | 72/292 [00:46<02:03,  1.78batch/s, auc=0.8683, loss=0.9161]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 73/292 [00:46<02:36,  1.40batch/s, auc=0.8683, loss=0.9161]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 73/292 [00:47<02:36,  1.40batch/s, auc=0.8683, loss=0.7468]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 74/292 [00:47<02:25,  1.50batch/s, auc=0.8683, loss=0.7468]\u001b[A\n",
      "Training Epoch 7/25:  25%|██▌       | 74/292 [00:47<02:25,  1.50batch/s, auc=0.8695, loss=0.5404]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 75/292 [00:47<02:17,  1.58batch/s, auc=0.8695, loss=0.5404]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 75/292 [00:48<02:17,  1.58batch/s, auc=0.8694, loss=0.6006]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 76/292 [00:48<02:11,  1.64batch/s, auc=0.8694, loss=0.6006]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▌       | 76/292 [00:48<02:11,  1.64batch/s, auc=0.8706, loss=0.5930]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▋       | 77/292 [00:48<02:07,  1.68batch/s, auc=0.8706, loss=0.5930]\u001b[A\n",
      "Training Epoch 7/25:  26%|██▋       | 77/292 [00:49<02:07,  1.68batch/s, auc=0.8709, loss=0.6163]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 78/292 [00:49<02:04,  1.72batch/s, auc=0.8709, loss=0.6163]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 78/292 [00:50<02:04,  1.72batch/s, auc=0.8717, loss=0.5761]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 79/292 [00:50<02:02,  1.74batch/s, auc=0.8717, loss=0.5761]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 79/292 [00:50<02:02,  1.74batch/s, auc=0.8735, loss=0.5086]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 80/292 [00:50<02:00,  1.76batch/s, auc=0.8735, loss=0.5086]\u001b[A\n",
      "Training Epoch 7/25:  27%|██▋       | 80/292 [00:51<02:00,  1.76batch/s, auc=0.8736, loss=0.6553]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 81/292 [00:51<01:59,  1.77batch/s, auc=0.8736, loss=0.6553]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 81/292 [00:52<01:59,  1.77batch/s, auc=0.8716, loss=1.1965]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 82/292 [00:52<02:30,  1.39batch/s, auc=0.8716, loss=1.1965]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 82/292 [00:52<02:30,  1.39batch/s, auc=0.8720, loss=0.7543]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 83/292 [00:52<02:20,  1.49batch/s, auc=0.8720, loss=0.7543]\u001b[A\n",
      "Training Epoch 7/25:  28%|██▊       | 83/292 [00:53<02:20,  1.49batch/s, auc=0.8683, loss=1.8393]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 84/292 [00:53<02:44,  1.27batch/s, auc=0.8683, loss=1.8393]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 84/292 [00:54<02:44,  1.27batch/s, auc=0.8688, loss=0.5860]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 85/292 [00:54<02:29,  1.39batch/s, auc=0.8688, loss=0.5860]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 85/292 [00:54<02:29,  1.39batch/s, auc=0.8701, loss=0.6166]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 86/292 [00:54<02:18,  1.49batch/s, auc=0.8701, loss=0.6166]\u001b[A\n",
      "Training Epoch 7/25:  29%|██▉       | 86/292 [00:55<02:18,  1.49batch/s, auc=0.8704, loss=0.6953]\u001b[A\n",
      "Training Epoch 7/25:  30%|██▉       | 87/292 [00:55<02:10,  1.57batch/s, auc=0.8704, loss=0.6953]\u001b[A\n",
      "Training Epoch 7/25:  30%|██▉       | 87/292 [00:56<02:10,  1.57batch/s, auc=0.8697, loss=0.9036]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 88/292 [00:56<02:05,  1.63batch/s, auc=0.8697, loss=0.9036]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 88/292 [00:56<02:05,  1.63batch/s, auc=0.8691, loss=0.8578]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 89/292 [00:56<02:01,  1.67batch/s, auc=0.8691, loss=0.8578]\u001b[A\n",
      "Training Epoch 7/25:  30%|███       | 89/292 [00:57<02:01,  1.67batch/s, auc=0.8686, loss=0.8899]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 90/292 [00:57<01:58,  1.71batch/s, auc=0.8686, loss=0.8899]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 90/292 [00:57<01:58,  1.71batch/s, auc=0.8681, loss=0.8381]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 91/292 [00:57<01:56,  1.73batch/s, auc=0.8681, loss=0.8381]\u001b[A\n",
      "Training Epoch 7/25:  31%|███       | 91/292 [00:58<01:56,  1.73batch/s, auc=0.8675, loss=0.7940]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 92/292 [00:58<01:54,  1.75batch/s, auc=0.8675, loss=0.7940]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 92/292 [00:58<01:54,  1.75batch/s, auc=0.8683, loss=0.5568]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 93/292 [00:58<01:52,  1.76batch/s, auc=0.8683, loss=0.5568]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 93/292 [00:59<01:52,  1.76batch/s, auc=0.8673, loss=1.0395]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 94/292 [00:59<02:22,  1.39batch/s, auc=0.8673, loss=1.0395]\u001b[A\n",
      "Training Epoch 7/25:  32%|███▏      | 94/292 [01:00<02:22,  1.39batch/s, auc=0.8676, loss=0.7091]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 95/292 [01:00<02:12,  1.49batch/s, auc=0.8676, loss=0.7091]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 95/292 [01:01<02:12,  1.49batch/s, auc=0.8661, loss=1.1831]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 96/292 [01:01<02:35,  1.26batch/s, auc=0.8661, loss=1.1831]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 96/292 [01:02<02:35,  1.26batch/s, auc=0.8636, loss=1.6353]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 97/292 [01:02<02:50,  1.14batch/s, auc=0.8636, loss=1.6353]\u001b[A\n",
      "Training Epoch 7/25:  33%|███▎      | 97/292 [01:03<02:50,  1.14batch/s, auc=0.8643, loss=0.6368]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▎      | 98/292 [01:03<02:31,  1.28batch/s, auc=0.8643, loss=0.6368]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▎      | 98/292 [01:03<02:31,  1.28batch/s, auc=0.8645, loss=0.5924]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 99/292 [01:03<02:17,  1.40batch/s, auc=0.8645, loss=0.5924]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 99/292 [01:04<02:17,  1.40batch/s, auc=0.8653, loss=0.5093]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 100/292 [01:04<02:08,  1.50batch/s, auc=0.8653, loss=0.5093]\u001b[A\n",
      "Training Epoch 7/25:  34%|███▍      | 100/292 [01:05<02:08,  1.50batch/s, auc=0.8650, loss=0.8086]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 101/292 [01:05<02:30,  1.27batch/s, auc=0.8650, loss=0.8086]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 101/292 [01:06<02:30,  1.27batch/s, auc=0.8647, loss=0.8599]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 102/292 [01:06<02:46,  1.14batch/s, auc=0.8647, loss=0.8599]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▍      | 102/292 [01:07<02:46,  1.14batch/s, auc=0.8645, loss=0.7446]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▌      | 103/292 [01:07<02:27,  1.28batch/s, auc=0.8645, loss=0.7446]\u001b[A\n",
      "Training Epoch 7/25:  35%|███▌      | 103/292 [01:07<02:27,  1.28batch/s, auc=0.8645, loss=0.6337]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 104/292 [01:07<02:14,  1.40batch/s, auc=0.8645, loss=0.6337]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 104/292 [01:08<02:14,  1.40batch/s, auc=0.8655, loss=0.5809]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 105/292 [01:08<02:04,  1.50batch/s, auc=0.8655, loss=0.5809]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▌      | 105/292 [01:09<02:04,  1.50batch/s, auc=0.8650, loss=0.9626]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▋      | 106/292 [01:09<02:26,  1.27batch/s, auc=0.8650, loss=0.9626]\u001b[A\n",
      "Training Epoch 7/25:  36%|███▋      | 106/292 [01:10<02:26,  1.27batch/s, auc=0.8650, loss=0.7625]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 107/292 [01:10<02:41,  1.14batch/s, auc=0.8650, loss=0.7625]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 107/292 [01:11<02:41,  1.14batch/s, auc=0.8639, loss=1.0521]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 108/292 [01:11<02:52,  1.07batch/s, auc=0.8639, loss=1.0521]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 108/292 [01:11<02:52,  1.07batch/s, auc=0.8640, loss=0.6431]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 109/292 [01:11<02:30,  1.21batch/s, auc=0.8640, loss=0.6431]\u001b[A\n",
      "Training Epoch 7/25:  37%|███▋      | 109/292 [01:12<02:30,  1.21batch/s, auc=0.8640, loss=0.6881]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 110/292 [01:12<02:15,  1.34batch/s, auc=0.8640, loss=0.6881]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 110/292 [01:13<02:15,  1.34batch/s, auc=0.8641, loss=0.7955]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 111/292 [01:13<02:04,  1.45batch/s, auc=0.8641, loss=0.7955]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 111/292 [01:13<02:04,  1.45batch/s, auc=0.8640, loss=0.6565]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 112/292 [01:13<01:57,  1.54batch/s, auc=0.8640, loss=0.6565]\u001b[A\n",
      "Training Epoch 7/25:  38%|███▊      | 112/292 [01:14<01:57,  1.54batch/s, auc=0.8649, loss=0.4598]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▊      | 113/292 [01:14<01:51,  1.60batch/s, auc=0.8649, loss=0.4598]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▊      | 113/292 [01:14<01:51,  1.60batch/s, auc=0.8655, loss=0.6905]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 114/292 [01:14<01:47,  1.65batch/s, auc=0.8655, loss=0.6905]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 114/292 [01:15<01:47,  1.65batch/s, auc=0.8633, loss=1.4782]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 115/292 [01:15<02:12,  1.34batch/s, auc=0.8633, loss=1.4782]\u001b[A\n",
      "Training Epoch 7/25:  39%|███▉      | 115/292 [01:16<02:12,  1.34batch/s, auc=0.8639, loss=0.5420]\u001b[A\n",
      "Training Epoch 7/25:  40%|███▉      | 116/292 [01:16<02:01,  1.45batch/s, auc=0.8639, loss=0.5420]\u001b[A\n",
      "Training Epoch 7/25:  40%|███▉      | 116/292 [01:17<02:01,  1.45batch/s, auc=0.8629, loss=1.0716]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 117/292 [01:17<02:21,  1.24batch/s, auc=0.8629, loss=1.0716]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 117/292 [01:17<02:21,  1.24batch/s, auc=0.8623, loss=0.8997]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 118/292 [01:17<02:07,  1.36batch/s, auc=0.8623, loss=0.8997]\u001b[A\n",
      "Training Epoch 7/25:  40%|████      | 118/292 [01:18<02:07,  1.36batch/s, auc=0.8628, loss=0.7190]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 119/292 [01:18<01:57,  1.47batch/s, auc=0.8628, loss=0.7190]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 119/292 [01:19<01:57,  1.47batch/s, auc=0.8630, loss=0.8107]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 120/292 [01:19<01:50,  1.55batch/s, auc=0.8630, loss=0.8107]\u001b[A\n",
      "Training Epoch 7/25:  41%|████      | 120/292 [01:19<01:50,  1.55batch/s, auc=0.8637, loss=0.7007]\u001b[A\n",
      "Training Epoch 7/25:  41%|████▏     | 121/292 [01:19<01:45,  1.61batch/s, auc=0.8637, loss=0.7007]\u001b[A\n",
      "Training Epoch 7/25:  41%|████▏     | 121/292 [01:20<01:45,  1.61batch/s, auc=0.8640, loss=0.6034]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 122/292 [01:20<01:42,  1.66batch/s, auc=0.8640, loss=0.6034]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 122/292 [01:20<01:42,  1.66batch/s, auc=0.8646, loss=0.7160]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 123/292 [01:20<01:39,  1.70batch/s, auc=0.8646, loss=0.7160]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 123/292 [01:21<01:39,  1.70batch/s, auc=0.8645, loss=0.7404]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 124/292 [01:21<02:03,  1.36batch/s, auc=0.8645, loss=0.7404]\u001b[A\n",
      "Training Epoch 7/25:  42%|████▏     | 124/292 [01:22<02:03,  1.36batch/s, auc=0.8657, loss=0.4793]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 125/292 [01:22<01:54,  1.46batch/s, auc=0.8657, loss=0.4793]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 125/292 [01:23<01:54,  1.46batch/s, auc=0.8648, loss=0.9801]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 126/292 [01:23<02:13,  1.25batch/s, auc=0.8648, loss=0.9801]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 126/292 [01:24<02:13,  1.25batch/s, auc=0.8649, loss=0.6407]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 127/292 [01:24<02:00,  1.37batch/s, auc=0.8649, loss=0.6407]\u001b[A\n",
      "Training Epoch 7/25:  43%|████▎     | 127/292 [01:24<02:00,  1.37batch/s, auc=0.8649, loss=0.7906]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 128/292 [01:24<01:51,  1.47batch/s, auc=0.8649, loss=0.7906]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 128/292 [01:25<01:51,  1.47batch/s, auc=0.8658, loss=0.5355]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 129/292 [01:25<01:44,  1.56batch/s, auc=0.8658, loss=0.5355]\u001b[A\n",
      "Training Epoch 7/25:  44%|████▍     | 129/292 [01:25<01:44,  1.56batch/s, auc=0.8656, loss=0.8878]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 130/292 [01:25<01:40,  1.62batch/s, auc=0.8656, loss=0.8878]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 130/292 [01:26<01:40,  1.62batch/s, auc=0.8660, loss=0.6534]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 131/292 [01:26<01:36,  1.67batch/s, auc=0.8660, loss=0.6534]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▍     | 131/292 [01:26<01:36,  1.67batch/s, auc=0.8660, loss=0.8577]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▌     | 132/292 [01:26<01:34,  1.70batch/s, auc=0.8660, loss=0.8577]\u001b[A\n",
      "Training Epoch 7/25:  45%|████▌     | 132/292 [01:27<01:34,  1.70batch/s, auc=0.8665, loss=0.7196]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 133/292 [01:27<01:32,  1.72batch/s, auc=0.8665, loss=0.7196]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 133/292 [01:28<01:32,  1.72batch/s, auc=0.8658, loss=0.9290]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 134/292 [01:28<01:55,  1.37batch/s, auc=0.8658, loss=0.9290]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 134/292 [01:29<01:55,  1.37batch/s, auc=0.8645, loss=1.2772]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 135/292 [01:29<02:10,  1.20batch/s, auc=0.8645, loss=1.2772]\u001b[A\n",
      "Training Epoch 7/25:  46%|████▌     | 135/292 [01:30<02:10,  1.20batch/s, auc=0.8649, loss=0.5831]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 136/292 [01:30<01:57,  1.33batch/s, auc=0.8649, loss=0.5831]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 136/292 [01:30<01:57,  1.33batch/s, auc=0.8648, loss=0.7864]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 137/292 [01:30<01:47,  1.44batch/s, auc=0.8648, loss=0.7864]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 137/292 [01:31<01:47,  1.44batch/s, auc=0.8645, loss=0.9059]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 138/292 [01:31<01:40,  1.53batch/s, auc=0.8645, loss=0.9059]\u001b[A\n",
      "Training Epoch 7/25:  47%|████▋     | 138/292 [01:31<01:40,  1.53batch/s, auc=0.8651, loss=0.5902]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 139/292 [01:31<01:35,  1.60batch/s, auc=0.8651, loss=0.5902]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 139/292 [01:32<01:35,  1.60batch/s, auc=0.8647, loss=0.7327]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 140/292 [01:32<01:32,  1.65batch/s, auc=0.8647, loss=0.7327]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 140/292 [01:32<01:32,  1.65batch/s, auc=0.8643, loss=0.7372]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 141/292 [01:32<01:29,  1.69batch/s, auc=0.8643, loss=0.7372]\u001b[A\n",
      "Training Epoch 7/25:  48%|████▊     | 141/292 [01:33<01:29,  1.69batch/s, auc=0.8646, loss=0.5854]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▊     | 142/292 [01:33<01:27,  1.71batch/s, auc=0.8646, loss=0.5854]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▊     | 142/292 [01:34<01:27,  1.71batch/s, auc=0.8648, loss=0.7453]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 143/292 [01:34<01:25,  1.73batch/s, auc=0.8648, loss=0.7453]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 143/292 [01:34<01:25,  1.73batch/s, auc=0.8653, loss=0.5237]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 144/292 [01:34<01:24,  1.75batch/s, auc=0.8653, loss=0.5237]\u001b[A\n",
      "Training Epoch 7/25:  49%|████▉     | 144/292 [01:35<01:24,  1.75batch/s, auc=0.8652, loss=0.6133]\u001b[A\n",
      "Training Epoch 7/25:  50%|████▉     | 145/292 [01:35<01:23,  1.76batch/s, auc=0.8652, loss=0.6133]\u001b[A\n",
      "Training Epoch 7/25:  50%|████▉     | 145/292 [01:35<01:23,  1.76batch/s, auc=0.8651, loss=0.9034]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 146/292 [01:35<01:22,  1.77batch/s, auc=0.8651, loss=0.9034]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 146/292 [01:36<01:22,  1.77batch/s, auc=0.8652, loss=0.6630]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 147/292 [01:36<01:44,  1.39batch/s, auc=0.8652, loss=0.6630]\u001b[A\n",
      "Training Epoch 7/25:  50%|█████     | 147/292 [01:37<01:44,  1.39batch/s, auc=0.8653, loss=0.7547]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 148/292 [01:37<01:36,  1.49batch/s, auc=0.8653, loss=0.7547]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 148/292 [01:37<01:36,  1.49batch/s, auc=0.8655, loss=0.7244]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 149/292 [01:37<01:31,  1.57batch/s, auc=0.8655, loss=0.7244]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████     | 149/292 [01:38<01:31,  1.57batch/s, auc=0.8664, loss=0.4090]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████▏    | 150/292 [01:38<01:27,  1.63batch/s, auc=0.8664, loss=0.4090]\u001b[A\n",
      "Training Epoch 7/25:  51%|█████▏    | 150/292 [01:39<01:27,  1.63batch/s, auc=0.8666, loss=0.7021]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 151/292 [01:39<01:24,  1.67batch/s, auc=0.8666, loss=0.7021]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 151/292 [01:39<01:24,  1.67batch/s, auc=0.8665, loss=0.7592]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 152/292 [01:39<01:22,  1.71batch/s, auc=0.8665, loss=0.7592]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 152/292 [01:40<01:22,  1.71batch/s, auc=0.8670, loss=0.5388]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 153/292 [01:40<01:20,  1.73batch/s, auc=0.8670, loss=0.5388]\u001b[A\n",
      "Training Epoch 7/25:  52%|█████▏    | 153/292 [01:40<01:20,  1.73batch/s, auc=0.8677, loss=0.5559]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 154/292 [01:40<01:19,  1.74batch/s, auc=0.8677, loss=0.5559]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 154/292 [01:41<01:19,  1.74batch/s, auc=0.8680, loss=0.6435]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 155/292 [01:41<01:18,  1.75batch/s, auc=0.8680, loss=0.6435]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 155/292 [01:41<01:18,  1.75batch/s, auc=0.8680, loss=0.7624]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 156/292 [01:41<01:17,  1.76batch/s, auc=0.8680, loss=0.7624]\u001b[A\n",
      "Training Epoch 7/25:  53%|█████▎    | 156/292 [01:42<01:17,  1.76batch/s, auc=0.8678, loss=0.9491]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 157/292 [01:42<01:37,  1.39batch/s, auc=0.8678, loss=0.9491]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 157/292 [01:43<01:37,  1.39batch/s, auc=0.8678, loss=0.8172]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 158/292 [01:43<01:29,  1.49batch/s, auc=0.8678, loss=0.8172]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 158/292 [01:44<01:29,  1.49batch/s, auc=0.8683, loss=0.5779]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 159/292 [01:44<01:24,  1.57batch/s, auc=0.8683, loss=0.5779]\u001b[A\n",
      "Training Epoch 7/25:  54%|█████▍    | 159/292 [01:44<01:24,  1.57batch/s, auc=0.8684, loss=0.5151]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▍    | 160/292 [01:44<01:21,  1.63batch/s, auc=0.8684, loss=0.5151]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▍    | 160/292 [01:45<01:21,  1.63batch/s, auc=0.8679, loss=0.9332]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 161/292 [01:45<01:38,  1.33batch/s, auc=0.8679, loss=0.9332]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 161/292 [01:46<01:38,  1.33batch/s, auc=0.8679, loss=0.6867]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 162/292 [01:46<01:30,  1.44batch/s, auc=0.8679, loss=0.6867]\u001b[A\n",
      "Training Epoch 7/25:  55%|█████▌    | 162/292 [01:46<01:30,  1.44batch/s, auc=0.8680, loss=0.6825]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 163/292 [01:46<01:24,  1.53batch/s, auc=0.8680, loss=0.6825]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 163/292 [01:47<01:24,  1.53batch/s, auc=0.8683, loss=0.7204]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 164/292 [01:47<01:20,  1.59batch/s, auc=0.8683, loss=0.7204]\u001b[A\n",
      "Training Epoch 7/25:  56%|█████▌    | 164/292 [01:47<01:20,  1.59batch/s, auc=0.8680, loss=0.8406]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 165/292 [01:47<01:17,  1.65batch/s, auc=0.8680, loss=0.8406]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 165/292 [01:49<01:17,  1.65batch/s, auc=0.8680, loss=0.7987]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 166/292 [01:49<01:34,  1.34batch/s, auc=0.8680, loss=0.7987]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 166/292 [01:49<01:34,  1.34batch/s, auc=0.8679, loss=0.6413]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 167/292 [01:49<01:26,  1.45batch/s, auc=0.8679, loss=0.6413]\u001b[A\n",
      "Training Epoch 7/25:  57%|█████▋    | 167/292 [01:50<01:26,  1.45batch/s, auc=0.8681, loss=0.8639]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 168/292 [01:50<01:20,  1.53batch/s, auc=0.8681, loss=0.8639]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 168/292 [01:50<01:20,  1.53batch/s, auc=0.8683, loss=0.7015]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 169/292 [01:50<01:16,  1.60batch/s, auc=0.8683, loss=0.7015]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 169/292 [01:51<01:16,  1.60batch/s, auc=0.8681, loss=0.7191]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 170/292 [01:51<01:13,  1.65batch/s, auc=0.8681, loss=0.7191]\u001b[A\n",
      "Training Epoch 7/25:  58%|█████▊    | 170/292 [01:51<01:13,  1.65batch/s, auc=0.8682, loss=0.7834]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▊    | 171/292 [01:51<01:11,  1.69batch/s, auc=0.8682, loss=0.7834]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▊    | 171/292 [01:52<01:11,  1.69batch/s, auc=0.8678, loss=1.0378]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 172/292 [01:52<01:28,  1.35batch/s, auc=0.8678, loss=1.0378]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 172/292 [01:53<01:28,  1.35batch/s, auc=0.8680, loss=0.7382]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 173/292 [01:53<01:21,  1.46batch/s, auc=0.8680, loss=0.7382]\u001b[A\n",
      "Training Epoch 7/25:  59%|█████▉    | 173/292 [01:54<01:21,  1.46batch/s, auc=0.8672, loss=1.3228]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 174/292 [01:54<01:34,  1.24batch/s, auc=0.8672, loss=1.3228]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 174/292 [01:55<01:34,  1.24batch/s, auc=0.8667, loss=0.9725]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 175/292 [01:55<01:43,  1.13batch/s, auc=0.8667, loss=0.9725]\u001b[A\n",
      "Training Epoch 7/25:  60%|█████▉    | 175/292 [01:56<01:43,  1.13batch/s, auc=0.8662, loss=0.9871]\u001b[A\n",
      "Training Epoch 7/25:  60%|██████    | 176/292 [01:56<01:31,  1.27batch/s, auc=0.8662, loss=0.9871]\u001b[A\n",
      "Training Epoch 7/25:  60%|██████    | 176/292 [01:56<01:31,  1.27batch/s, auc=0.8667, loss=0.5134]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 177/292 [01:56<01:22,  1.39batch/s, auc=0.8667, loss=0.5134]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 177/292 [01:57<01:22,  1.39batch/s, auc=0.8664, loss=0.8438]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 178/292 [01:57<01:34,  1.21batch/s, auc=0.8664, loss=0.8438]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████    | 178/292 [01:58<01:34,  1.21batch/s, auc=0.8666, loss=0.7798]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████▏   | 179/292 [01:58<01:24,  1.34batch/s, auc=0.8666, loss=0.7798]\u001b[A\n",
      "Training Epoch 7/25:  61%|██████▏   | 179/292 [01:59<01:24,  1.34batch/s, auc=0.8660, loss=1.0242]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 180/292 [01:59<01:34,  1.18batch/s, auc=0.8660, loss=1.0242]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 180/292 [02:00<01:34,  1.18batch/s, auc=0.8660, loss=0.5961]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 181/292 [02:00<01:24,  1.31batch/s, auc=0.8660, loss=0.5961]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 181/292 [02:00<01:24,  1.31batch/s, auc=0.8662, loss=0.5832]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 182/292 [02:00<01:17,  1.43batch/s, auc=0.8662, loss=0.5832]\u001b[A\n",
      "Training Epoch 7/25:  62%|██████▏   | 182/292 [02:01<01:17,  1.43batch/s, auc=0.8662, loss=0.7139]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 183/292 [02:01<01:11,  1.52batch/s, auc=0.8662, loss=0.7139]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 183/292 [02:02<01:11,  1.52batch/s, auc=0.8659, loss=1.0926]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 184/292 [02:02<01:24,  1.27batch/s, auc=0.8659, loss=1.0926]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 184/292 [02:03<01:24,  1.27batch/s, auc=0.8651, loss=1.3061]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 185/292 [02:03<01:33,  1.15batch/s, auc=0.8651, loss=1.3061]\u001b[A\n",
      "Training Epoch 7/25:  63%|██████▎   | 185/292 [02:03<01:33,  1.15batch/s, auc=0.8652, loss=0.6871]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▎   | 186/292 [02:03<01:22,  1.28batch/s, auc=0.8652, loss=0.6871]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▎   | 186/292 [02:04<01:22,  1.28batch/s, auc=0.8651, loss=0.7890]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 187/292 [02:04<01:14,  1.40batch/s, auc=0.8651, loss=0.7890]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 187/292 [02:04<01:14,  1.40batch/s, auc=0.8655, loss=0.5602]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 188/292 [02:04<01:09,  1.50batch/s, auc=0.8655, loss=0.5602]\u001b[A\n",
      "Training Epoch 7/25:  64%|██████▍   | 188/292 [02:05<01:09,  1.50batch/s, auc=0.8656, loss=0.7749]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▍   | 189/292 [02:05<01:05,  1.57batch/s, auc=0.8656, loss=0.7749]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▍   | 189/292 [02:06<01:05,  1.57batch/s, auc=0.8656, loss=0.5407]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 190/292 [02:06<01:02,  1.63batch/s, auc=0.8656, loss=0.5407]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 190/292 [02:06<01:02,  1.63batch/s, auc=0.8660, loss=0.6455]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 191/292 [02:06<01:00,  1.67batch/s, auc=0.8660, loss=0.6455]\u001b[A\n",
      "Training Epoch 7/25:  65%|██████▌   | 191/292 [02:07<01:00,  1.67batch/s, auc=0.8659, loss=0.9606]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 192/292 [02:07<00:58,  1.70batch/s, auc=0.8659, loss=0.9606]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 192/292 [02:07<00:58,  1.70batch/s, auc=0.8663, loss=0.6580]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 193/292 [02:07<00:57,  1.72batch/s, auc=0.8663, loss=0.6580]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▌   | 193/292 [02:08<00:57,  1.72batch/s, auc=0.8661, loss=1.0752]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▋   | 194/292 [02:08<00:56,  1.74batch/s, auc=0.8661, loss=1.0752]\u001b[A\n",
      "Training Epoch 7/25:  66%|██████▋   | 194/292 [02:08<00:56,  1.74batch/s, auc=0.8665, loss=0.6745]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 195/292 [02:08<00:55,  1.75batch/s, auc=0.8665, loss=0.6745]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 195/292 [02:09<00:55,  1.75batch/s, auc=0.8662, loss=0.9176]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 196/292 [02:09<00:54,  1.76batch/s, auc=0.8662, loss=0.9176]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 196/292 [02:10<00:54,  1.76batch/s, auc=0.8660, loss=0.6608]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 197/292 [02:10<00:53,  1.77batch/s, auc=0.8660, loss=0.6608]\u001b[A\n",
      "Training Epoch 7/25:  67%|██████▋   | 197/292 [02:10<00:53,  1.77batch/s, auc=0.8663, loss=0.7802]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 198/292 [02:10<00:53,  1.77batch/s, auc=0.8663, loss=0.7802]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 198/292 [02:11<00:53,  1.77batch/s, auc=0.8667, loss=0.6540]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 199/292 [02:11<00:52,  1.77batch/s, auc=0.8667, loss=0.6540]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 199/292 [02:11<00:52,  1.77batch/s, auc=0.8670, loss=0.6205]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 200/292 [02:11<00:51,  1.78batch/s, auc=0.8670, loss=0.6205]\u001b[A\n",
      "Training Epoch 7/25:  68%|██████▊   | 200/292 [02:12<00:51,  1.78batch/s, auc=0.8668, loss=0.8309]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 201/292 [02:12<00:51,  1.78batch/s, auc=0.8668, loss=0.8309]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 201/292 [02:12<00:51,  1.78batch/s, auc=0.8673, loss=0.5575]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 202/292 [02:12<00:50,  1.78batch/s, auc=0.8673, loss=0.5575]\u001b[A\n",
      "Training Epoch 7/25:  69%|██████▉   | 202/292 [02:13<00:50,  1.78batch/s, auc=0.8676, loss=0.5694]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 203/292 [02:13<00:50,  1.78batch/s, auc=0.8676, loss=0.5694]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 203/292 [02:13<00:50,  1.78batch/s, auc=0.8676, loss=0.6195]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 204/292 [02:13<00:49,  1.78batch/s, auc=0.8676, loss=0.6195]\u001b[A\n",
      "Training Epoch 7/25:  70%|██████▉   | 204/292 [02:14<00:49,  1.78batch/s, auc=0.8676, loss=0.6024]\u001b[A\n",
      "Training Epoch 7/25:  70%|███████   | 205/292 [02:14<00:48,  1.78batch/s, auc=0.8676, loss=0.6024]\u001b[A\n",
      "Training Epoch 7/25:  70%|███████   | 205/292 [02:15<00:48,  1.78batch/s, auc=0.8674, loss=0.8012]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 206/292 [02:15<00:48,  1.78batch/s, auc=0.8674, loss=0.8012]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 206/292 [02:15<00:48,  1.78batch/s, auc=0.8680, loss=0.5643]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 207/292 [02:15<00:47,  1.78batch/s, auc=0.8680, loss=0.5643]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 207/292 [02:16<00:47,  1.78batch/s, auc=0.8670, loss=1.4259]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 208/292 [02:16<01:00,  1.40batch/s, auc=0.8670, loss=1.4259]\u001b[A\n",
      "Training Epoch 7/25:  71%|███████   | 208/292 [02:17<01:00,  1.40batch/s, auc=0.8674, loss=0.6859]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 209/292 [02:17<00:55,  1.49batch/s, auc=0.8674, loss=0.6859]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 209/292 [02:17<00:55,  1.49batch/s, auc=0.8677, loss=0.5855]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 210/292 [02:17<00:52,  1.57batch/s, auc=0.8677, loss=0.5855]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 210/292 [02:18<00:52,  1.57batch/s, auc=0.8679, loss=0.5348]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 211/292 [02:18<00:49,  1.63batch/s, auc=0.8679, loss=0.5348]\u001b[A\n",
      "Training Epoch 7/25:  72%|███████▏  | 211/292 [02:19<00:49,  1.63batch/s, auc=0.8676, loss=0.8109]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 212/292 [02:19<01:00,  1.33batch/s, auc=0.8676, loss=0.8109]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 212/292 [02:20<01:00,  1.33batch/s, auc=0.8671, loss=0.8580]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 213/292 [02:20<01:07,  1.18batch/s, auc=0.8671, loss=0.8580]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 213/292 [02:21<01:07,  1.18batch/s, auc=0.8670, loss=0.6946]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 214/292 [02:21<01:11,  1.09batch/s, auc=0.8670, loss=0.6946]\u001b[A\n",
      "Training Epoch 7/25:  73%|███████▎  | 214/292 [02:22<01:11,  1.09batch/s, auc=0.8672, loss=0.5598]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▎  | 215/292 [02:22<01:02,  1.23batch/s, auc=0.8672, loss=0.5598]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▎  | 215/292 [02:22<01:02,  1.23batch/s, auc=0.8672, loss=0.8587]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 216/292 [02:22<00:55,  1.36batch/s, auc=0.8672, loss=0.8587]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 216/292 [02:23<00:55,  1.36batch/s, auc=0.8674, loss=0.7205]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 217/292 [02:23<00:51,  1.46batch/s, auc=0.8674, loss=0.7205]\u001b[A\n",
      "Training Epoch 7/25:  74%|███████▍  | 217/292 [02:23<00:51,  1.46batch/s, auc=0.8674, loss=0.7042]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▍  | 218/292 [02:23<00:47,  1.54batch/s, auc=0.8674, loss=0.7042]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▍  | 218/292 [02:24<00:47,  1.54batch/s, auc=0.8668, loss=1.2835]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 219/292 [02:24<00:56,  1.29batch/s, auc=0.8668, loss=1.2835]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 219/292 [02:26<00:56,  1.29batch/s, auc=0.8666, loss=0.9282]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 220/292 [02:26<01:02,  1.15batch/s, auc=0.8666, loss=0.9282]\u001b[A\n",
      "Training Epoch 7/25:  75%|███████▌  | 220/292 [02:26<01:02,  1.15batch/s, auc=0.8666, loss=0.7479]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 221/292 [02:26<00:55,  1.29batch/s, auc=0.8666, loss=0.7479]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 221/292 [02:27<00:55,  1.29batch/s, auc=0.8669, loss=0.4952]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 222/292 [02:27<00:49,  1.40batch/s, auc=0.8669, loss=0.4952]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▌  | 222/292 [02:27<00:49,  1.40batch/s, auc=0.8666, loss=0.7937]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▋  | 223/292 [02:27<00:46,  1.50batch/s, auc=0.8666, loss=0.7937]\u001b[A\n",
      "Training Epoch 7/25:  76%|███████▋  | 223/292 [02:28<00:46,  1.50batch/s, auc=0.8671, loss=0.5258]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 224/292 [02:28<00:43,  1.57batch/s, auc=0.8671, loss=0.5258]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 224/292 [02:28<00:43,  1.57batch/s, auc=0.8670, loss=0.8185]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 225/292 [02:28<00:41,  1.63batch/s, auc=0.8670, loss=0.8185]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 225/292 [02:29<00:41,  1.63batch/s, auc=0.8669, loss=1.0217]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 226/292 [02:29<00:39,  1.67batch/s, auc=0.8669, loss=1.0217]\u001b[A\n",
      "Training Epoch 7/25:  77%|███████▋  | 226/292 [02:30<00:39,  1.67batch/s, auc=0.8671, loss=0.5397]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 227/292 [02:30<00:38,  1.70batch/s, auc=0.8671, loss=0.5397]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 227/292 [02:30<00:38,  1.70batch/s, auc=0.8673, loss=0.7006]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 228/292 [02:30<00:37,  1.72batch/s, auc=0.8673, loss=0.7006]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 228/292 [02:31<00:37,  1.72batch/s, auc=0.8678, loss=0.4885]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 229/292 [02:31<00:36,  1.74batch/s, auc=0.8678, loss=0.4885]\u001b[A\n",
      "Training Epoch 7/25:  78%|███████▊  | 229/292 [02:31<00:36,  1.74batch/s, auc=0.8678, loss=0.6526]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 230/292 [02:31<00:35,  1.75batch/s, auc=0.8678, loss=0.6526]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 230/292 [02:32<00:35,  1.75batch/s, auc=0.8677, loss=0.9528]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 231/292 [02:32<00:34,  1.74batch/s, auc=0.8677, loss=0.9528]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 231/292 [02:33<00:34,  1.74batch/s, auc=0.8671, loss=1.2064]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 232/292 [02:33<00:43,  1.38batch/s, auc=0.8671, loss=1.2064]\u001b[A\n",
      "Training Epoch 7/25:  79%|███████▉  | 232/292 [02:33<00:43,  1.38batch/s, auc=0.8672, loss=0.6064]\u001b[A\n",
      "Training Epoch 7/25:  80%|███████▉  | 233/292 [02:33<00:39,  1.48batch/s, auc=0.8672, loss=0.6064]\u001b[A\n",
      "Training Epoch 7/25:  80%|███████▉  | 233/292 [02:34<00:39,  1.48batch/s, auc=0.8673, loss=0.6637]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 234/292 [02:34<00:37,  1.55batch/s, auc=0.8673, loss=0.6637]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 234/292 [02:35<00:37,  1.55batch/s, auc=0.8671, loss=1.0266]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 235/292 [02:35<00:44,  1.29batch/s, auc=0.8671, loss=1.0266]\u001b[A\n",
      "Training Epoch 7/25:  80%|████████  | 235/292 [02:36<00:44,  1.29batch/s, auc=0.8673, loss=0.6329]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 236/292 [02:36<00:39,  1.41batch/s, auc=0.8673, loss=0.6329]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 236/292 [02:36<00:39,  1.41batch/s, auc=0.8674, loss=0.6817]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 237/292 [02:36<00:36,  1.50batch/s, auc=0.8674, loss=0.6817]\u001b[A\n",
      "Training Epoch 7/25:  81%|████████  | 237/292 [02:37<00:36,  1.50batch/s, auc=0.8673, loss=0.9671]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 238/292 [02:37<00:34,  1.57batch/s, auc=0.8673, loss=0.9671]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 238/292 [02:37<00:34,  1.57batch/s, auc=0.8672, loss=0.7267]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 239/292 [02:37<00:32,  1.63batch/s, auc=0.8672, loss=0.7267]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 239/292 [02:38<00:32,  1.63batch/s, auc=0.8678, loss=0.4605]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 240/292 [02:38<00:31,  1.67batch/s, auc=0.8678, loss=0.4605]\u001b[A\n",
      "Training Epoch 7/25:  82%|████████▏ | 240/292 [02:38<00:31,  1.67batch/s, auc=0.8680, loss=0.5160]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 241/292 [02:38<00:29,  1.70batch/s, auc=0.8680, loss=0.5160]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 241/292 [02:39<00:29,  1.70batch/s, auc=0.8682, loss=0.7944]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 242/292 [02:39<00:29,  1.72batch/s, auc=0.8682, loss=0.7944]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 242/292 [02:40<00:29,  1.72batch/s, auc=0.8686, loss=0.5454]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 243/292 [02:40<00:28,  1.74batch/s, auc=0.8686, loss=0.5454]\u001b[A\n",
      "Training Epoch 7/25:  83%|████████▎ | 243/292 [02:40<00:28,  1.74batch/s, auc=0.8686, loss=0.8233]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▎ | 244/292 [02:40<00:27,  1.75batch/s, auc=0.8686, loss=0.8233]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▎ | 244/292 [02:41<00:27,  1.75batch/s, auc=0.8691, loss=0.5363]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 245/292 [02:41<00:26,  1.75batch/s, auc=0.8691, loss=0.5363]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 245/292 [02:41<00:26,  1.75batch/s, auc=0.8691, loss=0.6187]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 246/292 [02:41<00:26,  1.75batch/s, auc=0.8691, loss=0.6187]\u001b[A\n",
      "Training Epoch 7/25:  84%|████████▍ | 246/292 [02:42<00:26,  1.75batch/s, auc=0.8684, loss=1.3671]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 247/292 [02:42<00:32,  1.38batch/s, auc=0.8684, loss=1.3671]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 247/292 [02:43<00:32,  1.38batch/s, auc=0.8685, loss=0.7982]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 248/292 [02:43<00:29,  1.48batch/s, auc=0.8685, loss=0.7982]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▍ | 248/292 [02:44<00:29,  1.48batch/s, auc=0.8677, loss=1.5117]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▌ | 249/292 [02:44<00:34,  1.25batch/s, auc=0.8677, loss=1.5117]\u001b[A\n",
      "Training Epoch 7/25:  85%|████████▌ | 249/292 [02:45<00:34,  1.25batch/s, auc=0.8679, loss=0.5864]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 250/292 [02:45<00:30,  1.37batch/s, auc=0.8679, loss=0.5864]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 250/292 [02:45<00:30,  1.37batch/s, auc=0.8684, loss=0.4761]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 251/292 [02:45<00:27,  1.47batch/s, auc=0.8684, loss=0.4761]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▌ | 251/292 [02:46<00:27,  1.47batch/s, auc=0.8687, loss=0.5167]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▋ | 252/292 [02:46<00:25,  1.55batch/s, auc=0.8687, loss=0.5167]\u001b[A\n",
      "Training Epoch 7/25:  86%|████████▋ | 252/292 [02:47<00:25,  1.55batch/s, auc=0.8683, loss=1.1315]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 253/292 [02:47<00:30,  1.29batch/s, auc=0.8683, loss=1.1315]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 253/292 [02:47<00:30,  1.29batch/s, auc=0.8683, loss=0.8306]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 254/292 [02:47<00:27,  1.40batch/s, auc=0.8683, loss=0.8306]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 254/292 [02:48<00:27,  1.40batch/s, auc=0.8681, loss=0.9846]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 255/292 [02:48<00:24,  1.50batch/s, auc=0.8681, loss=0.9846]\u001b[A\n",
      "Training Epoch 7/25:  87%|████████▋ | 255/292 [02:48<00:24,  1.50batch/s, auc=0.8683, loss=0.7364]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 256/292 [02:48<00:22,  1.57batch/s, auc=0.8683, loss=0.7364]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 256/292 [02:49<00:22,  1.57batch/s, auc=0.8683, loss=0.6243]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 257/292 [02:49<00:21,  1.62batch/s, auc=0.8683, loss=0.6243]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 257/292 [02:50<00:21,  1.62batch/s, auc=0.8684, loss=0.8464]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 258/292 [02:50<00:20,  1.66batch/s, auc=0.8684, loss=0.8464]\u001b[A\n",
      "Training Epoch 7/25:  88%|████████▊ | 258/292 [02:51<00:20,  1.66batch/s, auc=0.8682, loss=0.9148]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▊ | 259/292 [02:51<00:24,  1.34batch/s, auc=0.8682, loss=0.9148]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▊ | 259/292 [02:51<00:24,  1.34batch/s, auc=0.8682, loss=0.7361]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 260/292 [02:51<00:22,  1.45batch/s, auc=0.8682, loss=0.7361]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 260/292 [02:52<00:22,  1.45batch/s, auc=0.8681, loss=0.7828]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 261/292 [02:52<00:20,  1.53batch/s, auc=0.8681, loss=0.7828]\u001b[A\n",
      "Training Epoch 7/25:  89%|████████▉ | 261/292 [02:53<00:20,  1.53batch/s, auc=0.8676, loss=1.2183]\u001b[A\n",
      "Training Epoch 7/25:  90%|████████▉ | 262/292 [02:53<00:23,  1.28batch/s, auc=0.8676, loss=1.2183]\u001b[A\n",
      "Training Epoch 7/25:  90%|████████▉ | 262/292 [02:53<00:23,  1.28batch/s, auc=0.8677, loss=0.7023]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 263/292 [02:53<00:20,  1.39batch/s, auc=0.8677, loss=0.7023]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 263/292 [02:54<00:20,  1.39batch/s, auc=0.8677, loss=0.8201]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 264/292 [02:54<00:18,  1.49batch/s, auc=0.8677, loss=0.8201]\u001b[A\n",
      "Training Epoch 7/25:  90%|█████████ | 264/292 [02:55<00:18,  1.49batch/s, auc=0.8676, loss=0.6673]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 265/292 [02:55<00:17,  1.57batch/s, auc=0.8676, loss=0.6673]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 265/292 [02:55<00:17,  1.57batch/s, auc=0.8677, loss=0.6676]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 266/292 [02:55<00:16,  1.62batch/s, auc=0.8677, loss=0.6676]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████ | 266/292 [02:56<00:16,  1.62batch/s, auc=0.8676, loss=0.6092]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████▏| 267/292 [02:56<00:15,  1.66batch/s, auc=0.8676, loss=0.6092]\u001b[A\n",
      "Training Epoch 7/25:  91%|█████████▏| 267/292 [02:57<00:15,  1.66batch/s, auc=0.8670, loss=1.4471]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 268/292 [02:57<00:17,  1.34batch/s, auc=0.8670, loss=1.4471]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 268/292 [02:57<00:17,  1.34batch/s, auc=0.8671, loss=0.8473]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 269/292 [02:57<00:15,  1.44batch/s, auc=0.8671, loss=0.8473]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 269/292 [02:58<00:15,  1.44batch/s, auc=0.8671, loss=0.6168]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 270/292 [02:58<00:14,  1.53batch/s, auc=0.8671, loss=0.6168]\u001b[A\n",
      "Training Epoch 7/25:  92%|█████████▏| 270/292 [02:59<00:14,  1.53batch/s, auc=0.8672, loss=0.7374]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 271/292 [02:59<00:13,  1.59batch/s, auc=0.8672, loss=0.7374]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 271/292 [02:59<00:13,  1.59batch/s, auc=0.8673, loss=0.5913]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 272/292 [02:59<00:12,  1.63batch/s, auc=0.8673, loss=0.5913]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 272/292 [03:00<00:12,  1.63batch/s, auc=0.8671, loss=0.8266]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 273/292 [03:00<00:14,  1.33batch/s, auc=0.8671, loss=0.8266]\u001b[A\n",
      "Training Epoch 7/25:  93%|█████████▎| 273/292 [03:01<00:14,  1.33batch/s, auc=0.8671, loss=0.7685]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 274/292 [03:01<00:12,  1.43batch/s, auc=0.8671, loss=0.7685]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 274/292 [03:02<00:12,  1.43batch/s, auc=0.8670, loss=0.8811]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 275/292 [03:02<00:13,  1.23batch/s, auc=0.8670, loss=0.8811]\u001b[A\n",
      "Training Epoch 7/25:  94%|█████████▍| 275/292 [03:02<00:13,  1.23batch/s, auc=0.8668, loss=1.0277]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 276/292 [03:02<00:11,  1.35batch/s, auc=0.8668, loss=1.0277]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 276/292 [03:03<00:11,  1.35batch/s, auc=0.8668, loss=0.5709]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 277/292 [03:03<00:10,  1.45batch/s, auc=0.8668, loss=0.5709]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▍| 277/292 [03:04<00:10,  1.45batch/s, auc=0.8669, loss=0.8659]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▌| 278/292 [03:04<00:09,  1.53batch/s, auc=0.8669, loss=0.8659]\u001b[A\n",
      "Training Epoch 7/25:  95%|█████████▌| 278/292 [03:04<00:09,  1.53batch/s, auc=0.8671, loss=0.6181]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 279/292 [03:04<00:08,  1.60batch/s, auc=0.8671, loss=0.6181]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 279/292 [03:05<00:08,  1.60batch/s, auc=0.8673, loss=0.7725]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 280/292 [03:05<00:07,  1.64batch/s, auc=0.8673, loss=0.7725]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 280/292 [03:05<00:07,  1.64batch/s, auc=0.8676, loss=0.6716]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 281/292 [03:05<00:06,  1.68batch/s, auc=0.8676, loss=0.6716]\u001b[A\n",
      "Training Epoch 7/25:  96%|█████████▌| 281/292 [03:06<00:06,  1.68batch/s, auc=0.8676, loss=0.6068]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 282/292 [03:06<00:05,  1.70batch/s, auc=0.8676, loss=0.6068]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 282/292 [03:06<00:05,  1.70batch/s, auc=0.8674, loss=0.7188]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 283/292 [03:06<00:05,  1.72batch/s, auc=0.8674, loss=0.7188]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.72batch/s, auc=0.8673, loss=0.7563]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.74batch/s, auc=0.8673, loss=0.7563]\u001b[A\n",
      "Training Epoch 7/25:  97%|█████████▋| 284/292 [03:08<00:04,  1.74batch/s, auc=0.8671, loss=0.9975]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 285/292 [03:08<00:05,  1.37batch/s, auc=0.8671, loss=0.9975]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 285/292 [03:09<00:05,  1.37batch/s, auc=0.8670, loss=0.7713]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 286/292 [03:09<00:04,  1.47batch/s, auc=0.8670, loss=0.7713]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 286/292 [03:09<00:04,  1.47batch/s, auc=0.8670, loss=0.6864]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 287/292 [03:09<00:03,  1.55batch/s, auc=0.8670, loss=0.6864]\u001b[A\n",
      "Training Epoch 7/25:  98%|█████████▊| 287/292 [03:10<00:03,  1.55batch/s, auc=0.8671, loss=0.7426]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.61batch/s, auc=0.8671, loss=0.7426]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.61batch/s, auc=0.8672, loss=0.6190]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.65batch/s, auc=0.8672, loss=0.6190]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 289/292 [03:11<00:01,  1.65batch/s, auc=0.8673, loss=0.8121]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.69batch/s, auc=0.8673, loss=0.8121]\u001b[A\n",
      "Training Epoch 7/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.69batch/s, auc=0.8674, loss=0.6236]\u001b[A\n",
      "Training Epoch 7/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.71batch/s, auc=0.8674, loss=0.6236]\u001b[A\n",
      "Training Epoch 7/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.71batch/s, auc=0.8674, loss=0.5331]\u001b[A\n",
      "Training Epoch 7/25: 100%|██████████| 292/292 [03:12<00:00,  1.52batch/s, auc=0.8674, loss=0.5331]\u001b[A\n",
      "Epochs:  28%|██▊       | 7/25 [24:40<1:03:37, 212.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25] Train Loss: 0.7642 | Train AUROC: 0.8674 Val Loss: 0.8869 | Val AUROC: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 8/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.7759, loss=0.8509]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 1/292 [00:01<09:41,  2.00s/batch, auc=0.7759, loss=0.8509]\u001b[A\n",
      "Training Epoch 8/25:   0%|          | 1/292 [00:02<09:41,  2.00s/batch, auc=0.8025, loss=1.0093]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 2/292 [00:02<05:33,  1.15s/batch, auc=0.8025, loss=1.0093]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 2/292 [00:03<05:33,  1.15s/batch, auc=0.8401, loss=0.7219]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 3/292 [00:03<04:13,  1.14batch/s, auc=0.8401, loss=0.7219]\u001b[A\n",
      "Training Epoch 8/25:   1%|          | 3/292 [00:03<04:13,  1.14batch/s, auc=0.8493, loss=0.7528]\u001b[A\n",
      "Training Epoch 8/25:   1%|▏         | 4/292 [00:03<03:36,  1.33batch/s, auc=0.8493, loss=0.7528]\u001b[A\n",
      "Training Epoch 8/25:   1%|▏         | 4/292 [00:04<03:36,  1.33batch/s, auc=0.8493, loss=0.7529]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 5/292 [00:04<03:15,  1.47batch/s, auc=0.8493, loss=0.7529]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 5/292 [00:04<03:15,  1.47batch/s, auc=0.8474, loss=0.7264]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 6/292 [00:04<03:02,  1.57batch/s, auc=0.8474, loss=0.7264]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 6/292 [00:05<03:02,  1.57batch/s, auc=0.8528, loss=0.6352]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 7/292 [00:05<02:54,  1.64batch/s, auc=0.8528, loss=0.6352]\u001b[A\n",
      "Training Epoch 8/25:   2%|▏         | 7/292 [00:06<02:54,  1.64batch/s, auc=0.8528, loss=0.9486]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 8/292 [00:06<03:35,  1.32batch/s, auc=0.8528, loss=0.9486]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 8/292 [00:06<03:35,  1.32batch/s, auc=0.8571, loss=0.6188]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 9/292 [00:06<03:17,  1.43batch/s, auc=0.8571, loss=0.6188]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 9/292 [00:07<03:17,  1.43batch/s, auc=0.8694, loss=0.5118]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 10/292 [00:07<03:05,  1.52batch/s, auc=0.8694, loss=0.5118]\u001b[A\n",
      "Training Epoch 8/25:   3%|▎         | 10/292 [00:08<03:05,  1.52batch/s, auc=0.8792, loss=0.5729]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 11/292 [00:08<02:56,  1.59batch/s, auc=0.8792, loss=0.5729]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 11/292 [00:08<02:56,  1.59batch/s, auc=0.8774, loss=1.0646]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 12/292 [00:08<02:49,  1.65batch/s, auc=0.8774, loss=1.0646]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 12/292 [00:09<02:49,  1.65batch/s, auc=0.8739, loss=1.0053]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 13/292 [00:09<02:44,  1.69batch/s, auc=0.8739, loss=1.0053]\u001b[A\n",
      "Training Epoch 8/25:   4%|▍         | 13/292 [00:09<02:44,  1.69batch/s, auc=0.8745, loss=0.5404]\u001b[A\n",
      "Training Epoch 8/25:   5%|▍         | 14/292 [00:09<02:41,  1.72batch/s, auc=0.8745, loss=0.5404]\u001b[A\n",
      "Training Epoch 8/25:   5%|▍         | 14/292 [00:10<02:41,  1.72batch/s, auc=0.8774, loss=0.6409]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 15/292 [00:10<02:39,  1.74batch/s, auc=0.8774, loss=0.6409]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 15/292 [00:10<02:39,  1.74batch/s, auc=0.8825, loss=0.5180]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 16/292 [00:10<02:37,  1.76batch/s, auc=0.8825, loss=0.5180]\u001b[A\n",
      "Training Epoch 8/25:   5%|▌         | 16/292 [00:11<02:37,  1.76batch/s, auc=0.8853, loss=0.5553]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 17/292 [00:11<02:35,  1.77batch/s, auc=0.8853, loss=0.5553]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 17/292 [00:12<02:35,  1.77batch/s, auc=0.8853, loss=0.8091]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 18/292 [00:12<02:36,  1.76batch/s, auc=0.8853, loss=0.8091]\u001b[A\n",
      "Training Epoch 8/25:   6%|▌         | 18/292 [00:12<02:36,  1.76batch/s, auc=0.8842, loss=0.6157]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 19/292 [00:12<02:34,  1.77batch/s, auc=0.8842, loss=0.6157]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 19/292 [00:13<02:34,  1.77batch/s, auc=0.8783, loss=1.2503]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 20/292 [00:13<03:15,  1.39batch/s, auc=0.8783, loss=1.2503]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 20/292 [00:14<03:15,  1.39batch/s, auc=0.8743, loss=1.1702]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 21/292 [00:14<03:44,  1.21batch/s, auc=0.8743, loss=1.1702]\u001b[A\n",
      "Training Epoch 8/25:   7%|▋         | 21/292 [00:15<03:44,  1.21batch/s, auc=0.8721, loss=0.8742]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 22/292 [00:15<03:22,  1.33batch/s, auc=0.8721, loss=0.8742]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 22/292 [00:15<03:22,  1.33batch/s, auc=0.8735, loss=0.7287]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 23/292 [00:15<03:06,  1.44batch/s, auc=0.8735, loss=0.7287]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 23/292 [00:16<03:06,  1.44batch/s, auc=0.8776, loss=0.4972]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 24/292 [00:16<02:55,  1.53batch/s, auc=0.8776, loss=0.4972]\u001b[A\n",
      "Training Epoch 8/25:   8%|▊         | 24/292 [00:16<02:55,  1.53batch/s, auc=0.8793, loss=0.6023]\u001b[A\n",
      "Training Epoch 8/25:   9%|▊         | 25/292 [00:17<02:47,  1.59batch/s, auc=0.8793, loss=0.6023]\u001b[A\n",
      "Training Epoch 8/25:   9%|▊         | 25/292 [00:17<02:47,  1.59batch/s, auc=0.8835, loss=0.5486]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 26/292 [00:17<02:41,  1.65batch/s, auc=0.8835, loss=0.5486]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 26/292 [00:18<02:41,  1.65batch/s, auc=0.8828, loss=0.6467]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 27/292 [00:18<03:18,  1.33batch/s, auc=0.8828, loss=0.6467]\u001b[A\n",
      "Training Epoch 8/25:   9%|▉         | 27/292 [00:19<03:18,  1.33batch/s, auc=0.8857, loss=0.4974]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 28/292 [00:19<03:04,  1.43batch/s, auc=0.8857, loss=0.4974]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 28/292 [00:20<03:04,  1.43batch/s, auc=0.8818, loss=1.0534]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 29/292 [00:20<03:35,  1.22batch/s, auc=0.8818, loss=1.0534]\u001b[A\n",
      "Training Epoch 8/25:  10%|▉         | 29/292 [00:20<03:35,  1.22batch/s, auc=0.8839, loss=0.6653]\u001b[A\n",
      "Training Epoch 8/25:  10%|█         | 30/292 [00:20<03:14,  1.35batch/s, auc=0.8839, loss=0.6653]\u001b[A\n",
      "Training Epoch 8/25:  10%|█         | 30/292 [00:21<03:14,  1.35batch/s, auc=0.8827, loss=0.9615]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 31/292 [00:21<03:41,  1.18batch/s, auc=0.8827, loss=0.9615]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 31/292 [00:22<03:41,  1.18batch/s, auc=0.8845, loss=0.6337]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 32/292 [00:22<03:18,  1.31batch/s, auc=0.8845, loss=0.6337]\u001b[A\n",
      "Training Epoch 8/25:  11%|█         | 32/292 [00:23<03:18,  1.31batch/s, auc=0.8852, loss=0.6106]\u001b[A\n",
      "Training Epoch 8/25:  11%|█▏        | 33/292 [00:23<03:02,  1.42batch/s, auc=0.8852, loss=0.6106]\u001b[A\n",
      "Training Epoch 8/25:  11%|█▏        | 33/292 [00:23<03:02,  1.42batch/s, auc=0.8875, loss=0.4949]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 34/292 [00:23<02:51,  1.51batch/s, auc=0.8875, loss=0.4949]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 34/292 [00:24<02:51,  1.51batch/s, auc=0.8885, loss=0.5038]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 35/292 [00:24<02:42,  1.58batch/s, auc=0.8885, loss=0.5038]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 35/292 [00:24<02:42,  1.58batch/s, auc=0.8900, loss=0.6050]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 36/292 [00:24<02:36,  1.63batch/s, auc=0.8900, loss=0.6050]\u001b[A\n",
      "Training Epoch 8/25:  12%|█▏        | 36/292 [00:25<02:36,  1.63batch/s, auc=0.8913, loss=0.5351]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 37/292 [00:25<02:32,  1.68batch/s, auc=0.8913, loss=0.5351]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 37/292 [00:25<02:32,  1.68batch/s, auc=0.8916, loss=0.5166]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 38/292 [00:25<02:29,  1.70batch/s, auc=0.8916, loss=0.5166]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 38/292 [00:27<02:29,  1.70batch/s, auc=0.8878, loss=1.2286]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 39/292 [00:27<03:07,  1.35batch/s, auc=0.8878, loss=1.2286]\u001b[A\n",
      "Training Epoch 8/25:  13%|█▎        | 39/292 [00:27<03:07,  1.35batch/s, auc=0.8878, loss=0.5555]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▎        | 40/292 [00:27<02:54,  1.45batch/s, auc=0.8878, loss=0.5555]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▎        | 40/292 [00:28<02:54,  1.45batch/s, auc=0.8865, loss=0.9378]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 41/292 [00:28<03:23,  1.23batch/s, auc=0.8865, loss=0.9378]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 41/292 [00:29<03:23,  1.23batch/s, auc=0.8846, loss=1.0299]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 42/292 [00:29<03:04,  1.35batch/s, auc=0.8846, loss=1.0299]\u001b[A\n",
      "Training Epoch 8/25:  14%|█▍        | 42/292 [00:30<03:04,  1.35batch/s, auc=0.8836, loss=1.0390]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▍        | 43/292 [00:30<03:29,  1.19batch/s, auc=0.8836, loss=1.0390]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▍        | 43/292 [00:30<03:29,  1.19batch/s, auc=0.8831, loss=0.7561]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 44/292 [00:30<03:08,  1.32batch/s, auc=0.8831, loss=0.7561]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 44/292 [00:31<03:08,  1.32batch/s, auc=0.8838, loss=0.6287]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 45/292 [00:31<02:53,  1.43batch/s, auc=0.8838, loss=0.6287]\u001b[A\n",
      "Training Epoch 8/25:  15%|█▌        | 45/292 [00:32<02:53,  1.43batch/s, auc=0.8837, loss=0.5914]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 46/292 [00:32<02:41,  1.52batch/s, auc=0.8837, loss=0.5914]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 46/292 [00:32<02:41,  1.52batch/s, auc=0.8840, loss=0.5932]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 47/292 [00:32<02:33,  1.59batch/s, auc=0.8840, loss=0.5932]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▌        | 47/292 [00:33<02:33,  1.59batch/s, auc=0.8825, loss=1.0017]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▋        | 48/292 [00:33<03:06,  1.31batch/s, auc=0.8825, loss=1.0017]\u001b[A\n",
      "Training Epoch 8/25:  16%|█▋        | 48/292 [00:34<03:06,  1.31batch/s, auc=0.8835, loss=0.7014]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 49/292 [00:34<02:50,  1.42batch/s, auc=0.8835, loss=0.7014]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 49/292 [00:34<02:50,  1.42batch/s, auc=0.8811, loss=1.1567]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 50/292 [00:34<02:39,  1.51batch/s, auc=0.8811, loss=1.1567]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 50/292 [00:35<02:39,  1.51batch/s, auc=0.8812, loss=0.8232]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 51/292 [00:35<02:32,  1.58batch/s, auc=0.8812, loss=0.8232]\u001b[A\n",
      "Training Epoch 8/25:  17%|█▋        | 51/292 [00:35<02:32,  1.58batch/s, auc=0.8818, loss=0.5312]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 52/292 [00:35<02:26,  1.64batch/s, auc=0.8818, loss=0.5312]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 52/292 [00:36<02:26,  1.64batch/s, auc=0.8807, loss=0.8936]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 53/292 [00:36<02:21,  1.68batch/s, auc=0.8807, loss=0.8936]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 53/292 [00:37<02:21,  1.68batch/s, auc=0.8805, loss=0.7446]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 54/292 [00:37<02:18,  1.71batch/s, auc=0.8805, loss=0.7446]\u001b[A\n",
      "Training Epoch 8/25:  18%|█▊        | 54/292 [00:37<02:18,  1.71batch/s, auc=0.8818, loss=0.6440]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 55/292 [00:37<02:16,  1.73batch/s, auc=0.8818, loss=0.6440]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 55/292 [00:38<02:16,  1.73batch/s, auc=0.8810, loss=0.7314]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 56/292 [00:38<02:15,  1.75batch/s, auc=0.8810, loss=0.7314]\u001b[A\n",
      "Training Epoch 8/25:  19%|█▉        | 56/292 [00:38<02:15,  1.75batch/s, auc=0.8815, loss=0.5940]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 57/292 [00:38<02:13,  1.76batch/s, auc=0.8815, loss=0.5940]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 57/292 [00:39<02:13,  1.76batch/s, auc=0.8776, loss=1.4322]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 58/292 [00:39<02:50,  1.37batch/s, auc=0.8776, loss=1.4322]\u001b[A\n",
      "Training Epoch 8/25:  20%|█▉        | 58/292 [00:40<02:50,  1.37batch/s, auc=0.8775, loss=0.9383]\u001b[A\n",
      "Training Epoch 8/25:  20%|██        | 59/292 [00:40<02:37,  1.47batch/s, auc=0.8775, loss=0.9383]\u001b[A\n",
      "Training Epoch 8/25:  20%|██        | 59/292 [00:41<02:37,  1.47batch/s, auc=0.8734, loss=1.6788]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 60/292 [00:41<03:05,  1.25batch/s, auc=0.8734, loss=1.6788]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 60/292 [00:42<03:05,  1.25batch/s, auc=0.8741, loss=0.5999]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 61/292 [00:42<02:48,  1.37batch/s, auc=0.8741, loss=0.5999]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 61/292 [00:42<02:48,  1.37batch/s, auc=0.8747, loss=0.7141]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 62/292 [00:42<02:35,  1.48batch/s, auc=0.8747, loss=0.7141]\u001b[A\n",
      "Training Epoch 8/25:  21%|██        | 62/292 [00:43<02:35,  1.48batch/s, auc=0.8760, loss=0.5269]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 63/292 [00:43<02:27,  1.55batch/s, auc=0.8760, loss=0.5269]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 63/292 [00:43<02:27,  1.55batch/s, auc=0.8761, loss=0.7999]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 64/292 [00:43<02:20,  1.62batch/s, auc=0.8761, loss=0.7999]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 64/292 [00:44<02:20,  1.62batch/s, auc=0.8768, loss=0.6713]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 65/292 [00:44<02:16,  1.67batch/s, auc=0.8768, loss=0.6713]\u001b[A\n",
      "Training Epoch 8/25:  22%|██▏       | 65/292 [00:45<02:16,  1.67batch/s, auc=0.8751, loss=1.1102]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 66/292 [00:45<02:49,  1.34batch/s, auc=0.8751, loss=1.1102]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 66/292 [00:45<02:49,  1.34batch/s, auc=0.8762, loss=0.7105]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 67/292 [00:45<02:36,  1.44batch/s, auc=0.8762, loss=0.7105]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 67/292 [00:46<02:36,  1.44batch/s, auc=0.8772, loss=0.5680]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 68/292 [00:46<02:27,  1.52batch/s, auc=0.8772, loss=0.5680]\u001b[A\n",
      "Training Epoch 8/25:  23%|██▎       | 68/292 [00:47<02:27,  1.52batch/s, auc=0.8772, loss=0.7965]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▎       | 69/292 [00:47<02:20,  1.59batch/s, auc=0.8772, loss=0.7965]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▎       | 69/292 [00:47<02:20,  1.59batch/s, auc=0.8782, loss=0.5372]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 70/292 [00:47<02:15,  1.64batch/s, auc=0.8782, loss=0.5372]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 70/292 [00:48<02:15,  1.64batch/s, auc=0.8775, loss=0.8277]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 71/292 [00:48<02:12,  1.67batch/s, auc=0.8775, loss=0.8277]\u001b[A\n",
      "Training Epoch 8/25:  24%|██▍       | 71/292 [00:48<02:12,  1.67batch/s, auc=0.8767, loss=0.8616]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▍       | 72/292 [00:48<02:09,  1.70batch/s, auc=0.8767, loss=0.8616]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▍       | 72/292 [00:49<02:09,  1.70batch/s, auc=0.8764, loss=0.7753]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 73/292 [00:49<02:41,  1.35batch/s, auc=0.8764, loss=0.7753]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 73/292 [00:50<02:41,  1.35batch/s, auc=0.8760, loss=0.6643]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 74/292 [00:50<02:30,  1.45batch/s, auc=0.8760, loss=0.6643]\u001b[A\n",
      "Training Epoch 8/25:  25%|██▌       | 74/292 [00:51<02:30,  1.45batch/s, auc=0.8748, loss=1.0625]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 75/292 [00:51<02:56,  1.23batch/s, auc=0.8748, loss=1.0625]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 75/292 [00:52<02:56,  1.23batch/s, auc=0.8757, loss=0.6392]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 76/292 [00:52<02:40,  1.35batch/s, auc=0.8757, loss=0.6392]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▌       | 76/292 [00:52<02:40,  1.35batch/s, auc=0.8754, loss=0.7164]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▋       | 77/292 [00:52<02:27,  1.46batch/s, auc=0.8754, loss=0.7164]\u001b[A\n",
      "Training Epoch 8/25:  26%|██▋       | 77/292 [00:53<02:27,  1.46batch/s, auc=0.8760, loss=0.5722]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 78/292 [00:53<02:18,  1.54batch/s, auc=0.8760, loss=0.5722]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 78/292 [00:53<02:18,  1.54batch/s, auc=0.8750, loss=1.0158]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 79/292 [00:53<02:12,  1.60batch/s, auc=0.8750, loss=1.0158]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 79/292 [00:54<02:12,  1.60batch/s, auc=0.8747, loss=0.6115]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 80/292 [00:54<02:08,  1.65batch/s, auc=0.8747, loss=0.6115]\u001b[A\n",
      "Training Epoch 8/25:  27%|██▋       | 80/292 [00:54<02:08,  1.65batch/s, auc=0.8737, loss=1.0437]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 81/292 [00:54<02:05,  1.69batch/s, auc=0.8737, loss=1.0437]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 81/292 [00:55<02:05,  1.69batch/s, auc=0.8739, loss=0.7368]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 82/292 [00:55<02:02,  1.71batch/s, auc=0.8739, loss=0.7368]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 82/292 [00:56<02:02,  1.71batch/s, auc=0.8748, loss=0.6813]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 83/292 [00:56<02:00,  1.73batch/s, auc=0.8748, loss=0.6813]\u001b[A\n",
      "Training Epoch 8/25:  28%|██▊       | 83/292 [00:56<02:00,  1.73batch/s, auc=0.8747, loss=0.7596]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 84/292 [00:56<01:59,  1.75batch/s, auc=0.8747, loss=0.7596]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 84/292 [00:57<01:59,  1.75batch/s, auc=0.8755, loss=0.7155]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 85/292 [00:57<01:57,  1.76batch/s, auc=0.8755, loss=0.7155]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 85/292 [00:57<01:57,  1.76batch/s, auc=0.8769, loss=0.4905]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 86/292 [00:57<01:56,  1.76batch/s, auc=0.8769, loss=0.4905]\u001b[A\n",
      "Training Epoch 8/25:  29%|██▉       | 86/292 [00:58<01:56,  1.76batch/s, auc=0.8769, loss=0.7204]\u001b[A\n",
      "Training Epoch 8/25:  30%|██▉       | 87/292 [00:58<01:56,  1.76batch/s, auc=0.8769, loss=0.7204]\u001b[A\n",
      "Training Epoch 8/25:  30%|██▉       | 87/292 [00:58<01:56,  1.76batch/s, auc=0.8778, loss=0.5293]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 88/292 [00:58<01:55,  1.77batch/s, auc=0.8778, loss=0.5293]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 88/292 [00:59<01:55,  1.77batch/s, auc=0.8782, loss=0.6946]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 89/292 [00:59<01:54,  1.77batch/s, auc=0.8782, loss=0.6946]\u001b[A\n",
      "Training Epoch 8/25:  30%|███       | 89/292 [00:59<01:54,  1.77batch/s, auc=0.8789, loss=0.5512]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 90/292 [01:00<01:54,  1.77batch/s, auc=0.8789, loss=0.5512]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 90/292 [01:00<01:54,  1.77batch/s, auc=0.8787, loss=0.5748]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 91/292 [01:00<01:54,  1.75batch/s, auc=0.8787, loss=0.5748]\u001b[A\n",
      "Training Epoch 8/25:  31%|███       | 91/292 [01:01<01:54,  1.75batch/s, auc=0.8784, loss=0.7834]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 92/292 [01:01<02:24,  1.38batch/s, auc=0.8784, loss=0.7834]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 92/292 [01:02<02:24,  1.38batch/s, auc=0.8767, loss=1.2864]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 93/292 [01:02<02:46,  1.19batch/s, auc=0.8767, loss=1.2864]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 93/292 [01:03<02:46,  1.19batch/s, auc=0.8767, loss=0.7107]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 94/292 [01:03<02:29,  1.33batch/s, auc=0.8767, loss=0.7107]\u001b[A\n",
      "Training Epoch 8/25:  32%|███▏      | 94/292 [01:03<02:29,  1.33batch/s, auc=0.8771, loss=0.6258]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 95/292 [01:03<02:17,  1.44batch/s, auc=0.8771, loss=0.6258]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 95/292 [01:04<02:17,  1.44batch/s, auc=0.8770, loss=0.8290]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 96/292 [01:04<02:08,  1.52batch/s, auc=0.8770, loss=0.8290]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 96/292 [01:05<02:08,  1.52batch/s, auc=0.8769, loss=0.7529]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 97/292 [01:05<02:02,  1.59batch/s, auc=0.8769, loss=0.7529]\u001b[A\n",
      "Training Epoch 8/25:  33%|███▎      | 97/292 [01:05<02:02,  1.59batch/s, auc=0.8770, loss=0.7073]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▎      | 98/292 [01:05<01:58,  1.64batch/s, auc=0.8770, loss=0.7073]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▎      | 98/292 [01:06<01:58,  1.64batch/s, auc=0.8776, loss=0.5253]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 99/292 [01:06<01:54,  1.68batch/s, auc=0.8776, loss=0.5253]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 99/292 [01:06<01:54,  1.68batch/s, auc=0.8775, loss=0.6594]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 100/292 [01:06<01:52,  1.71batch/s, auc=0.8775, loss=0.6594]\u001b[A\n",
      "Training Epoch 8/25:  34%|███▍      | 100/292 [01:07<01:52,  1.71batch/s, auc=0.8778, loss=0.6440]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 101/292 [01:07<01:50,  1.74batch/s, auc=0.8778, loss=0.6440]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 101/292 [01:07<01:50,  1.74batch/s, auc=0.8776, loss=0.8156]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 102/292 [01:07<01:48,  1.75batch/s, auc=0.8776, loss=0.8156]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▍      | 102/292 [01:08<01:48,  1.75batch/s, auc=0.8775, loss=0.7446]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▌      | 103/292 [01:08<01:47,  1.76batch/s, auc=0.8775, loss=0.7446]\u001b[A\n",
      "Training Epoch 8/25:  35%|███▌      | 103/292 [01:09<01:47,  1.76batch/s, auc=0.8771, loss=0.7256]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 104/292 [01:09<02:16,  1.38batch/s, auc=0.8771, loss=0.7256]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 104/292 [01:10<02:16,  1.38batch/s, auc=0.8772, loss=0.6506]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 105/292 [01:10<02:06,  1.48batch/s, auc=0.8772, loss=0.6506]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▌      | 105/292 [01:10<02:06,  1.48batch/s, auc=0.8764, loss=1.2269]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▋      | 106/292 [01:10<01:59,  1.55batch/s, auc=0.8764, loss=1.2269]\u001b[A\n",
      "Training Epoch 8/25:  36%|███▋      | 106/292 [01:11<01:59,  1.55batch/s, auc=0.8768, loss=0.4752]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 107/292 [01:11<01:54,  1.62batch/s, auc=0.8768, loss=0.4752]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 107/292 [01:11<01:54,  1.62batch/s, auc=0.8765, loss=0.6988]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 108/292 [01:11<01:50,  1.66batch/s, auc=0.8765, loss=0.6988]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 108/292 [01:12<01:50,  1.66batch/s, auc=0.8768, loss=0.5892]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 109/292 [01:12<01:47,  1.70batch/s, auc=0.8768, loss=0.5892]\u001b[A\n",
      "Training Epoch 8/25:  37%|███▋      | 109/292 [01:12<01:47,  1.70batch/s, auc=0.8772, loss=0.5757]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 110/292 [01:12<01:45,  1.72batch/s, auc=0.8772, loss=0.5757]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 110/292 [01:13<01:45,  1.72batch/s, auc=0.8769, loss=0.7796]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 111/292 [01:13<01:44,  1.74batch/s, auc=0.8769, loss=0.7796]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 111/292 [01:13<01:44,  1.74batch/s, auc=0.8768, loss=0.8012]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 112/292 [01:13<01:42,  1.75batch/s, auc=0.8768, loss=0.8012]\u001b[A\n",
      "Training Epoch 8/25:  38%|███▊      | 112/292 [01:14<01:42,  1.75batch/s, auc=0.8759, loss=1.0454]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▊      | 113/292 [01:14<01:41,  1.76batch/s, auc=0.8759, loss=1.0454]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▊      | 113/292 [01:15<01:41,  1.76batch/s, auc=0.8756, loss=0.9619]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 114/292 [01:15<02:08,  1.38batch/s, auc=0.8756, loss=0.9619]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 114/292 [01:16<02:08,  1.38batch/s, auc=0.8751, loss=0.9322]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 115/292 [01:16<01:59,  1.48batch/s, auc=0.8751, loss=0.9322]\u001b[A\n",
      "Training Epoch 8/25:  39%|███▉      | 115/292 [01:16<01:59,  1.48batch/s, auc=0.8754, loss=0.6413]\u001b[A\n",
      "Training Epoch 8/25:  40%|███▉      | 116/292 [01:16<01:53,  1.56batch/s, auc=0.8754, loss=0.6413]\u001b[A\n",
      "Training Epoch 8/25:  40%|███▉      | 116/292 [01:17<01:53,  1.56batch/s, auc=0.8748, loss=0.9213]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 117/292 [01:17<01:48,  1.62batch/s, auc=0.8748, loss=0.9213]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 117/292 [01:18<01:48,  1.62batch/s, auc=0.8737, loss=1.0668]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 118/292 [01:18<02:11,  1.32batch/s, auc=0.8737, loss=1.0668]\u001b[A\n",
      "Training Epoch 8/25:  40%|████      | 118/292 [01:18<02:11,  1.32batch/s, auc=0.8737, loss=0.7239]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 119/292 [01:18<02:01,  1.43batch/s, auc=0.8737, loss=0.7239]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 119/292 [01:19<02:01,  1.43batch/s, auc=0.8733, loss=0.7894]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 120/292 [01:19<01:53,  1.52batch/s, auc=0.8733, loss=0.7894]\u001b[A\n",
      "Training Epoch 8/25:  41%|████      | 120/292 [01:20<01:53,  1.52batch/s, auc=0.8736, loss=0.6191]\u001b[A\n",
      "Training Epoch 8/25:  41%|████▏     | 121/292 [01:20<01:47,  1.59batch/s, auc=0.8736, loss=0.6191]\u001b[A\n",
      "Training Epoch 8/25:  41%|████▏     | 121/292 [01:20<01:47,  1.59batch/s, auc=0.8737, loss=0.7594]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 122/292 [01:20<01:43,  1.64batch/s, auc=0.8737, loss=0.7594]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 122/292 [01:21<01:43,  1.64batch/s, auc=0.8743, loss=0.5506]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 123/292 [01:21<01:40,  1.68batch/s, auc=0.8743, loss=0.5506]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 123/292 [01:21<01:40,  1.68batch/s, auc=0.8749, loss=0.4957]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 124/292 [01:21<01:38,  1.71batch/s, auc=0.8749, loss=0.4957]\u001b[A\n",
      "Training Epoch 8/25:  42%|████▏     | 124/292 [01:22<01:38,  1.71batch/s, auc=0.8752, loss=0.6315]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 125/292 [01:22<01:36,  1.73batch/s, auc=0.8752, loss=0.6315]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 125/292 [01:22<01:36,  1.73batch/s, auc=0.8749, loss=0.8043]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 126/292 [01:22<01:35,  1.74batch/s, auc=0.8749, loss=0.8043]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 126/292 [01:23<01:35,  1.74batch/s, auc=0.8747, loss=0.7072]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 127/292 [01:23<01:34,  1.75batch/s, auc=0.8747, loss=0.7072]\u001b[A\n",
      "Training Epoch 8/25:  43%|████▎     | 127/292 [01:24<01:34,  1.75batch/s, auc=0.8749, loss=0.7011]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 128/292 [01:24<01:58,  1.38batch/s, auc=0.8749, loss=0.7011]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 128/292 [01:25<01:58,  1.38batch/s, auc=0.8737, loss=1.2631]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 129/292 [01:25<02:15,  1.21batch/s, auc=0.8737, loss=1.2631]\u001b[A\n",
      "Training Epoch 8/25:  44%|████▍     | 129/292 [01:26<02:15,  1.21batch/s, auc=0.8731, loss=0.9420]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 130/292 [01:26<02:26,  1.10batch/s, auc=0.8731, loss=0.9420]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 130/292 [01:27<02:26,  1.10batch/s, auc=0.8724, loss=1.1856]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 131/292 [01:27<02:35,  1.04batch/s, auc=0.8724, loss=1.1856]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▍     | 131/292 [01:28<02:35,  1.04batch/s, auc=0.8722, loss=0.8902]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▌     | 132/292 [01:28<02:40,  1.00s/batch, auc=0.8722, loss=0.8902]\u001b[A\n",
      "Training Epoch 8/25:  45%|████▌     | 132/292 [01:29<02:40,  1.00s/batch, auc=0.8723, loss=0.7411]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 133/292 [01:29<02:18,  1.15batch/s, auc=0.8723, loss=0.7411]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 133/292 [01:30<02:18,  1.15batch/s, auc=0.8729, loss=0.6131]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 134/292 [01:30<02:02,  1.29batch/s, auc=0.8729, loss=0.6131]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 134/292 [01:31<02:02,  1.29batch/s, auc=0.8730, loss=0.8015]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 135/292 [01:31<02:15,  1.15batch/s, auc=0.8730, loss=0.8015]\u001b[A\n",
      "Training Epoch 8/25:  46%|████▌     | 135/292 [01:31<02:15,  1.15batch/s, auc=0.8732, loss=0.5804]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 136/292 [01:31<02:00,  1.29batch/s, auc=0.8732, loss=0.5804]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 136/292 [01:32<02:00,  1.29batch/s, auc=0.8721, loss=1.0837]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 137/292 [01:32<02:14,  1.15batch/s, auc=0.8721, loss=1.0837]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 137/292 [01:33<02:14,  1.15batch/s, auc=0.8721, loss=0.7665]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 138/292 [01:33<01:59,  1.29batch/s, auc=0.8721, loss=0.7665]\u001b[A\n",
      "Training Epoch 8/25:  47%|████▋     | 138/292 [01:33<01:59,  1.29batch/s, auc=0.8722, loss=0.6283]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 139/292 [01:33<01:48,  1.41batch/s, auc=0.8722, loss=0.6283]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 139/292 [01:34<01:48,  1.41batch/s, auc=0.8724, loss=0.6322]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 140/292 [01:34<01:41,  1.50batch/s, auc=0.8724, loss=0.6322]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 140/292 [01:34<01:41,  1.50batch/s, auc=0.8719, loss=0.9182]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 141/292 [01:34<01:35,  1.58batch/s, auc=0.8719, loss=0.9182]\u001b[A\n",
      "Training Epoch 8/25:  48%|████▊     | 141/292 [01:35<01:35,  1.58batch/s, auc=0.8723, loss=0.6235]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▊     | 142/292 [01:35<01:31,  1.63batch/s, auc=0.8723, loss=0.6235]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▊     | 142/292 [01:36<01:31,  1.63batch/s, auc=0.8726, loss=0.7380]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 143/292 [01:36<01:28,  1.68batch/s, auc=0.8726, loss=0.7380]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 143/292 [01:36<01:28,  1.68batch/s, auc=0.8723, loss=0.7096]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 144/292 [01:36<01:26,  1.71batch/s, auc=0.8723, loss=0.7096]\u001b[A\n",
      "Training Epoch 8/25:  49%|████▉     | 144/292 [01:37<01:26,  1.71batch/s, auc=0.8724, loss=0.7079]\u001b[A\n",
      "Training Epoch 8/25:  50%|████▉     | 145/292 [01:37<01:24,  1.73batch/s, auc=0.8724, loss=0.7079]\u001b[A\n",
      "Training Epoch 8/25:  50%|████▉     | 145/292 [01:37<01:24,  1.73batch/s, auc=0.8730, loss=0.5992]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 146/292 [01:37<01:23,  1.74batch/s, auc=0.8730, loss=0.5992]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 146/292 [01:38<01:23,  1.74batch/s, auc=0.8737, loss=0.6237]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 147/292 [01:38<01:22,  1.76batch/s, auc=0.8737, loss=0.6237]\u001b[A\n",
      "Training Epoch 8/25:  50%|█████     | 147/292 [01:38<01:22,  1.76batch/s, auc=0.8736, loss=0.5401]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 148/292 [01:38<01:21,  1.76batch/s, auc=0.8736, loss=0.5401]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 148/292 [01:39<01:21,  1.76batch/s, auc=0.8737, loss=0.6982]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 149/292 [01:39<01:20,  1.77batch/s, auc=0.8737, loss=0.6982]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████     | 149/292 [01:40<01:20,  1.77batch/s, auc=0.8737, loss=0.6265]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████▏    | 150/292 [01:40<01:19,  1.78batch/s, auc=0.8737, loss=0.6265]\u001b[A\n",
      "Training Epoch 8/25:  51%|█████▏    | 150/292 [01:40<01:19,  1.78batch/s, auc=0.8739, loss=0.5671]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 151/292 [01:40<01:19,  1.78batch/s, auc=0.8739, loss=0.5671]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 151/292 [01:41<01:19,  1.78batch/s, auc=0.8735, loss=0.7808]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 152/292 [01:41<01:40,  1.39batch/s, auc=0.8735, loss=0.7808]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 152/292 [01:42<01:40,  1.39batch/s, auc=0.8737, loss=0.5746]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 153/292 [01:42<01:33,  1.49batch/s, auc=0.8737, loss=0.5746]\u001b[A\n",
      "Training Epoch 8/25:  52%|█████▏    | 153/292 [01:42<01:33,  1.49batch/s, auc=0.8736, loss=0.8398]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 154/292 [01:42<01:28,  1.57batch/s, auc=0.8736, loss=0.8398]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 154/292 [01:43<01:28,  1.57batch/s, auc=0.8736, loss=0.7633]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 155/292 [01:43<01:24,  1.63batch/s, auc=0.8736, loss=0.7633]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 155/292 [01:43<01:24,  1.63batch/s, auc=0.8735, loss=0.7104]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 156/292 [01:43<01:21,  1.67batch/s, auc=0.8735, loss=0.7104]\u001b[A\n",
      "Training Epoch 8/25:  53%|█████▎    | 156/292 [01:44<01:21,  1.67batch/s, auc=0.8741, loss=0.5823]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 157/292 [01:44<01:19,  1.70batch/s, auc=0.8741, loss=0.5823]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 157/292 [01:45<01:19,  1.70batch/s, auc=0.8744, loss=0.4693]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 158/292 [01:45<01:17,  1.72batch/s, auc=0.8744, loss=0.4693]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 158/292 [01:45<01:17,  1.72batch/s, auc=0.8747, loss=0.6630]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 159/292 [01:45<01:16,  1.74batch/s, auc=0.8747, loss=0.6630]\u001b[A\n",
      "Training Epoch 8/25:  54%|█████▍    | 159/292 [01:46<01:16,  1.74batch/s, auc=0.8744, loss=0.7348]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▍    | 160/292 [01:46<01:15,  1.75batch/s, auc=0.8744, loss=0.7348]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▍    | 160/292 [01:46<01:15,  1.75batch/s, auc=0.8747, loss=0.6027]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 161/292 [01:46<01:14,  1.76batch/s, auc=0.8747, loss=0.6027]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 161/292 [01:47<01:14,  1.76batch/s, auc=0.8745, loss=0.8476]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 162/292 [01:47<01:13,  1.77batch/s, auc=0.8745, loss=0.8476]\u001b[A\n",
      "Training Epoch 8/25:  55%|█████▌    | 162/292 [01:47<01:13,  1.77batch/s, auc=0.8739, loss=1.1918]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 163/292 [01:47<01:12,  1.77batch/s, auc=0.8739, loss=1.1918]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 163/292 [01:48<01:12,  1.77batch/s, auc=0.8739, loss=0.7576]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 164/292 [01:48<01:12,  1.77batch/s, auc=0.8739, loss=0.7576]\u001b[A\n",
      "Training Epoch 8/25:  56%|█████▌    | 164/292 [01:48<01:12,  1.77batch/s, auc=0.8743, loss=0.7060]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 165/292 [01:48<01:11,  1.78batch/s, auc=0.8743, loss=0.7060]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 165/292 [01:50<01:11,  1.78batch/s, auc=0.8740, loss=0.8000]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 166/292 [01:50<01:30,  1.40batch/s, auc=0.8740, loss=0.8000]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 166/292 [01:50<01:30,  1.40batch/s, auc=0.8741, loss=0.5713]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 167/292 [01:50<01:23,  1.49batch/s, auc=0.8741, loss=0.5713]\u001b[A\n",
      "Training Epoch 8/25:  57%|█████▋    | 167/292 [01:51<01:23,  1.49batch/s, auc=0.8743, loss=0.5369]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 168/292 [01:51<01:18,  1.57batch/s, auc=0.8743, loss=0.5369]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 168/292 [01:51<01:18,  1.57batch/s, auc=0.8741, loss=0.8510]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 169/292 [01:51<01:15,  1.63batch/s, auc=0.8741, loss=0.8510]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 169/292 [01:52<01:15,  1.63batch/s, auc=0.8744, loss=0.5690]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 170/292 [01:52<01:13,  1.67batch/s, auc=0.8744, loss=0.5690]\u001b[A\n",
      "Training Epoch 8/25:  58%|█████▊    | 170/292 [01:52<01:13,  1.67batch/s, auc=0.8750, loss=0.4425]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▊    | 171/292 [01:52<01:11,  1.70batch/s, auc=0.8750, loss=0.4425]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▊    | 171/292 [01:53<01:11,  1.70batch/s, auc=0.8750, loss=0.7691]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 172/292 [01:53<01:28,  1.36batch/s, auc=0.8750, loss=0.7691]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 172/292 [01:54<01:28,  1.36batch/s, auc=0.8747, loss=0.7143]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 173/292 [01:54<01:21,  1.46batch/s, auc=0.8747, loss=0.7143]\u001b[A\n",
      "Training Epoch 8/25:  59%|█████▉    | 173/292 [01:55<01:21,  1.46batch/s, auc=0.8750, loss=0.5730]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 174/292 [01:55<01:16,  1.55batch/s, auc=0.8750, loss=0.5730]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 174/292 [01:55<01:16,  1.55batch/s, auc=0.8752, loss=0.5756]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 175/292 [01:55<01:12,  1.61batch/s, auc=0.8752, loss=0.5756]\u001b[A\n",
      "Training Epoch 8/25:  60%|█████▉    | 175/292 [01:56<01:12,  1.61batch/s, auc=0.8742, loss=1.3230]\u001b[A\n",
      "Training Epoch 8/25:  60%|██████    | 176/292 [01:56<01:27,  1.32batch/s, auc=0.8742, loss=1.3230]\u001b[A\n",
      "Training Epoch 8/25:  60%|██████    | 176/292 [01:57<01:27,  1.32batch/s, auc=0.8739, loss=0.7126]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 177/292 [01:57<01:20,  1.43batch/s, auc=0.8739, loss=0.7126]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 177/292 [01:57<01:20,  1.43batch/s, auc=0.8736, loss=0.9882]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 178/292 [01:57<01:15,  1.52batch/s, auc=0.8736, loss=0.9882]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████    | 178/292 [01:58<01:15,  1.52batch/s, auc=0.8740, loss=0.6477]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████▏   | 179/292 [01:58<01:11,  1.58batch/s, auc=0.8740, loss=0.6477]\u001b[A\n",
      "Training Epoch 8/25:  61%|██████▏   | 179/292 [01:58<01:11,  1.58batch/s, auc=0.8741, loss=0.5907]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 180/292 [01:58<01:08,  1.64batch/s, auc=0.8741, loss=0.5907]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 180/292 [02:00<01:08,  1.64batch/s, auc=0.8734, loss=1.2190]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 181/292 [02:00<01:23,  1.33batch/s, auc=0.8734, loss=1.2190]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 181/292 [02:00<01:23,  1.33batch/s, auc=0.8736, loss=0.6498]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 182/292 [02:00<01:16,  1.44batch/s, auc=0.8736, loss=0.6498]\u001b[A\n",
      "Training Epoch 8/25:  62%|██████▏   | 182/292 [02:01<01:16,  1.44batch/s, auc=0.8742, loss=0.4770]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 183/292 [02:01<01:11,  1.53batch/s, auc=0.8742, loss=0.4770]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 183/292 [02:01<01:11,  1.53batch/s, auc=0.8742, loss=0.6284]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 184/292 [02:01<01:07,  1.59batch/s, auc=0.8742, loss=0.6284]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 184/292 [02:02<01:07,  1.59batch/s, auc=0.8743, loss=0.5591]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 185/292 [02:02<01:05,  1.65batch/s, auc=0.8743, loss=0.5591]\u001b[A\n",
      "Training Epoch 8/25:  63%|██████▎   | 185/292 [02:02<01:05,  1.65batch/s, auc=0.8743, loss=0.6327]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▎   | 186/292 [02:02<01:02,  1.68batch/s, auc=0.8743, loss=0.6327]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▎   | 186/292 [02:03<01:02,  1.68batch/s, auc=0.8745, loss=0.5708]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 187/292 [02:03<01:01,  1.71batch/s, auc=0.8745, loss=0.5708]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 187/292 [02:03<01:01,  1.71batch/s, auc=0.8745, loss=0.7617]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 188/292 [02:03<01:00,  1.73batch/s, auc=0.8745, loss=0.7617]\u001b[A\n",
      "Training Epoch 8/25:  64%|██████▍   | 188/292 [02:04<01:00,  1.73batch/s, auc=0.8744, loss=0.7480]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▍   | 189/292 [02:04<00:59,  1.74batch/s, auc=0.8744, loss=0.7480]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▍   | 189/292 [02:05<00:59,  1.74batch/s, auc=0.8742, loss=0.8359]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 190/292 [02:05<00:58,  1.75batch/s, auc=0.8742, loss=0.8359]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 190/292 [02:05<00:58,  1.75batch/s, auc=0.8737, loss=0.9682]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 191/292 [02:05<00:57,  1.76batch/s, auc=0.8737, loss=0.9682]\u001b[A\n",
      "Training Epoch 8/25:  65%|██████▌   | 191/292 [02:06<00:57,  1.76batch/s, auc=0.8741, loss=0.6515]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 192/292 [02:06<00:56,  1.77batch/s, auc=0.8741, loss=0.6515]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 192/292 [02:07<00:56,  1.77batch/s, auc=0.8733, loss=1.2665]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 193/292 [02:07<01:11,  1.39batch/s, auc=0.8733, loss=1.2665]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▌   | 193/292 [02:07<01:11,  1.39batch/s, auc=0.8738, loss=0.5355]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▋   | 194/292 [02:07<01:05,  1.48batch/s, auc=0.8738, loss=0.5355]\u001b[A\n",
      "Training Epoch 8/25:  66%|██████▋   | 194/292 [02:08<01:05,  1.48batch/s, auc=0.8734, loss=0.7879]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 195/292 [02:08<01:17,  1.26batch/s, auc=0.8734, loss=0.7879]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 195/292 [02:09<01:17,  1.26batch/s, auc=0.8739, loss=0.5885]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 196/292 [02:09<01:09,  1.38batch/s, auc=0.8739, loss=0.5885]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 196/292 [02:10<01:09,  1.38batch/s, auc=0.8736, loss=0.8680]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 197/292 [02:10<01:19,  1.20batch/s, auc=0.8736, loss=0.8680]\u001b[A\n",
      "Training Epoch 8/25:  67%|██████▋   | 197/292 [02:11<01:19,  1.20batch/s, auc=0.8733, loss=0.9119]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 198/292 [02:11<01:10,  1.33batch/s, auc=0.8733, loss=0.9119]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 198/292 [02:12<01:10,  1.33batch/s, auc=0.8730, loss=0.8119]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 199/292 [02:12<01:19,  1.18batch/s, auc=0.8730, loss=0.8119]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 199/292 [02:13<01:19,  1.18batch/s, auc=0.8729, loss=0.6533]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 200/292 [02:13<01:24,  1.09batch/s, auc=0.8729, loss=0.6533]\u001b[A\n",
      "Training Epoch 8/25:  68%|██████▊   | 200/292 [02:13<01:24,  1.09batch/s, auc=0.8730, loss=0.5321]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 201/292 [02:13<01:13,  1.23batch/s, auc=0.8730, loss=0.5321]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 201/292 [02:14<01:13,  1.23batch/s, auc=0.8726, loss=0.8357]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 202/292 [02:14<01:06,  1.36batch/s, auc=0.8726, loss=0.8357]\u001b[A\n",
      "Training Epoch 8/25:  69%|██████▉   | 202/292 [02:14<01:06,  1.36batch/s, auc=0.8725, loss=0.9550]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 203/292 [02:14<01:00,  1.46batch/s, auc=0.8725, loss=0.9550]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 203/292 [02:16<01:00,  1.46batch/s, auc=0.8716, loss=1.4456]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 204/292 [02:16<01:10,  1.24batch/s, auc=0.8716, loss=1.4456]\u001b[A\n",
      "Training Epoch 8/25:  70%|██████▉   | 204/292 [02:16<01:10,  1.24batch/s, auc=0.8716, loss=0.6728]\u001b[A\n",
      "Training Epoch 8/25:  70%|███████   | 205/292 [02:16<01:03,  1.36batch/s, auc=0.8716, loss=0.6728]\u001b[A\n",
      "Training Epoch 8/25:  70%|███████   | 205/292 [02:17<01:03,  1.36batch/s, auc=0.8716, loss=0.6782]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 206/292 [02:17<00:58,  1.47batch/s, auc=0.8716, loss=0.6782]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 206/292 [02:17<00:58,  1.47batch/s, auc=0.8718, loss=0.5628]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 207/292 [02:17<00:54,  1.55batch/s, auc=0.8718, loss=0.5628]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 207/292 [02:18<00:54,  1.55batch/s, auc=0.8721, loss=0.4724]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 208/292 [02:18<00:52,  1.61batch/s, auc=0.8721, loss=0.4724]\u001b[A\n",
      "Training Epoch 8/25:  71%|███████   | 208/292 [02:18<00:52,  1.61batch/s, auc=0.8721, loss=0.8018]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 209/292 [02:18<00:50,  1.65batch/s, auc=0.8721, loss=0.8018]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 209/292 [02:19<00:50,  1.65batch/s, auc=0.8722, loss=0.6613]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 210/292 [02:19<00:48,  1.69batch/s, auc=0.8722, loss=0.6613]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 210/292 [02:20<00:48,  1.69batch/s, auc=0.8715, loss=1.2170]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 211/292 [02:20<00:59,  1.35batch/s, auc=0.8715, loss=1.2170]\u001b[A\n",
      "Training Epoch 8/25:  72%|███████▏  | 211/292 [02:21<00:59,  1.35batch/s, auc=0.8714, loss=0.7935]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 212/292 [02:21<00:54,  1.45batch/s, auc=0.8714, loss=0.7935]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 212/292 [02:22<00:54,  1.45batch/s, auc=0.8712, loss=0.7369]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 213/292 [02:22<01:03,  1.24batch/s, auc=0.8712, loss=0.7369]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 213/292 [02:23<01:03,  1.24batch/s, auc=0.8704, loss=1.4581]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 214/292 [02:23<01:09,  1.13batch/s, auc=0.8704, loss=1.4581]\u001b[A\n",
      "Training Epoch 8/25:  73%|███████▎  | 214/292 [02:23<01:09,  1.13batch/s, auc=0.8704, loss=0.7762]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▎  | 215/292 [02:23<01:00,  1.26batch/s, auc=0.8704, loss=0.7762]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▎  | 215/292 [02:24<01:00,  1.26batch/s, auc=0.8705, loss=0.8052]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 216/292 [02:24<00:55,  1.38batch/s, auc=0.8705, loss=0.8052]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 216/292 [02:24<00:55,  1.38batch/s, auc=0.8706, loss=0.5579]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 217/292 [02:24<00:50,  1.48batch/s, auc=0.8706, loss=0.5579]\u001b[A\n",
      "Training Epoch 8/25:  74%|███████▍  | 217/292 [02:26<00:50,  1.48batch/s, auc=0.8700, loss=1.2176]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▍  | 218/292 [02:26<00:59,  1.25batch/s, auc=0.8700, loss=1.2176]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▍  | 218/292 [02:26<00:59,  1.25batch/s, auc=0.8703, loss=0.6244]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 219/292 [02:26<00:53,  1.37batch/s, auc=0.8703, loss=0.6244]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 219/292 [02:27<00:53,  1.37batch/s, auc=0.8701, loss=0.9386]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 220/292 [02:27<00:48,  1.47batch/s, auc=0.8701, loss=0.9386]\u001b[A\n",
      "Training Epoch 8/25:  75%|███████▌  | 220/292 [02:27<00:48,  1.47batch/s, auc=0.8701, loss=0.6829]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 221/292 [02:27<00:45,  1.55batch/s, auc=0.8701, loss=0.6829]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 221/292 [02:28<00:45,  1.55batch/s, auc=0.8703, loss=0.7161]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 222/292 [02:28<00:43,  1.61batch/s, auc=0.8703, loss=0.7161]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▌  | 222/292 [02:28<00:43,  1.61batch/s, auc=0.8699, loss=1.0330]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▋  | 223/292 [02:28<00:41,  1.66batch/s, auc=0.8699, loss=1.0330]\u001b[A\n",
      "Training Epoch 8/25:  76%|███████▋  | 223/292 [02:29<00:41,  1.66batch/s, auc=0.8702, loss=0.5422]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 224/292 [02:29<00:40,  1.69batch/s, auc=0.8702, loss=0.5422]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 224/292 [02:29<00:40,  1.69batch/s, auc=0.8700, loss=0.7209]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 225/292 [02:29<00:39,  1.71batch/s, auc=0.8700, loss=0.7209]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 225/292 [02:30<00:39,  1.71batch/s, auc=0.8700, loss=0.7224]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 226/292 [02:30<00:38,  1.73batch/s, auc=0.8700, loss=0.7224]\u001b[A\n",
      "Training Epoch 8/25:  77%|███████▋  | 226/292 [02:31<00:38,  1.73batch/s, auc=0.8703, loss=0.6969]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 227/292 [02:31<00:37,  1.74batch/s, auc=0.8703, loss=0.6969]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 227/292 [02:31<00:37,  1.74batch/s, auc=0.8705, loss=0.5797]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 228/292 [02:31<00:36,  1.75batch/s, auc=0.8705, loss=0.5797]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 228/292 [02:32<00:36,  1.75batch/s, auc=0.8706, loss=0.6122]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 229/292 [02:32<00:35,  1.76batch/s, auc=0.8706, loss=0.6122]\u001b[A\n",
      "Training Epoch 8/25:  78%|███████▊  | 229/292 [02:32<00:35,  1.76batch/s, auc=0.8709, loss=0.5452]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 230/292 [02:32<00:35,  1.76batch/s, auc=0.8709, loss=0.5452]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 230/292 [02:33<00:35,  1.76batch/s, auc=0.8705, loss=0.9186]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 231/292 [02:33<00:44,  1.39batch/s, auc=0.8705, loss=0.9186]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 231/292 [02:34<00:44,  1.39batch/s, auc=0.8703, loss=1.0059]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 232/292 [02:34<00:40,  1.48batch/s, auc=0.8703, loss=1.0059]\u001b[A\n",
      "Training Epoch 8/25:  79%|███████▉  | 232/292 [02:35<00:40,  1.48batch/s, auc=0.8701, loss=0.7789]\u001b[A\n",
      "Training Epoch 8/25:  80%|███████▉  | 233/292 [02:35<00:37,  1.56batch/s, auc=0.8701, loss=0.7789]\u001b[A\n",
      "Training Epoch 8/25:  80%|███████▉  | 233/292 [02:36<00:37,  1.56batch/s, auc=0.8699, loss=0.9634]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 234/292 [02:36<00:44,  1.29batch/s, auc=0.8699, loss=0.9634]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 234/292 [02:36<00:44,  1.29batch/s, auc=0.8700, loss=0.6143]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 235/292 [02:36<00:40,  1.41batch/s, auc=0.8700, loss=0.6143]\u001b[A\n",
      "Training Epoch 8/25:  80%|████████  | 235/292 [02:37<00:40,  1.41batch/s, auc=0.8697, loss=0.9545]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 236/292 [02:37<00:37,  1.50batch/s, auc=0.8697, loss=0.9545]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 236/292 [02:37<00:37,  1.50batch/s, auc=0.8697, loss=0.6979]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 237/292 [02:37<00:34,  1.57batch/s, auc=0.8697, loss=0.6979]\u001b[A\n",
      "Training Epoch 8/25:  81%|████████  | 237/292 [02:38<00:34,  1.57batch/s, auc=0.8697, loss=0.6011]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 238/292 [02:38<00:33,  1.63batch/s, auc=0.8697, loss=0.6011]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 238/292 [02:39<00:33,  1.63batch/s, auc=0.8690, loss=1.1293]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 239/292 [02:39<00:40,  1.32batch/s, auc=0.8690, loss=1.1293]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 239/292 [02:40<00:40,  1.32batch/s, auc=0.8690, loss=0.5494]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 240/292 [02:40<00:36,  1.43batch/s, auc=0.8690, loss=0.5494]\u001b[A\n",
      "Training Epoch 8/25:  82%|████████▏ | 240/292 [02:40<00:36,  1.43batch/s, auc=0.8692, loss=0.5591]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 241/292 [02:40<00:33,  1.52batch/s, auc=0.8692, loss=0.5591]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 241/292 [02:41<00:33,  1.52batch/s, auc=0.8686, loss=1.2349]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 242/292 [02:41<00:39,  1.27batch/s, auc=0.8686, loss=1.2349]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 242/292 [02:42<00:39,  1.27batch/s, auc=0.8682, loss=1.1313]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 243/292 [02:42<00:42,  1.14batch/s, auc=0.8682, loss=1.1313]\u001b[A\n",
      "Training Epoch 8/25:  83%|████████▎ | 243/292 [02:43<00:42,  1.14batch/s, auc=0.8681, loss=0.7209]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▎ | 244/292 [02:43<00:37,  1.28batch/s, auc=0.8681, loss=0.7209]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▎ | 244/292 [02:44<00:37,  1.28batch/s, auc=0.8677, loss=1.0236]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 245/292 [02:44<00:40,  1.15batch/s, auc=0.8677, loss=1.0236]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 245/292 [02:44<00:40,  1.15batch/s, auc=0.8679, loss=0.6855]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 246/292 [02:44<00:35,  1.28batch/s, auc=0.8679, loss=0.6855]\u001b[A\n",
      "Training Epoch 8/25:  84%|████████▍ | 246/292 [02:45<00:35,  1.28batch/s, auc=0.8678, loss=0.6365]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 247/292 [02:45<00:32,  1.40batch/s, auc=0.8678, loss=0.6365]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 247/292 [02:46<00:32,  1.40batch/s, auc=0.8678, loss=0.7746]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 248/292 [02:46<00:29,  1.49batch/s, auc=0.8678, loss=0.7746]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▍ | 248/292 [02:46<00:29,  1.49batch/s, auc=0.8680, loss=0.6361]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▌ | 249/292 [02:46<00:27,  1.56batch/s, auc=0.8680, loss=0.6361]\u001b[A\n",
      "Training Epoch 8/25:  85%|████████▌ | 249/292 [02:47<00:27,  1.56batch/s, auc=0.8674, loss=1.2458]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 250/292 [02:47<00:32,  1.29batch/s, auc=0.8674, loss=1.2458]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 250/292 [02:48<00:32,  1.29batch/s, auc=0.8672, loss=1.0628]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 251/292 [02:48<00:35,  1.16batch/s, auc=0.8672, loss=1.0628]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▌ | 251/292 [02:49<00:35,  1.16batch/s, auc=0.8673, loss=0.6924]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▋ | 252/292 [02:49<00:31,  1.29batch/s, auc=0.8673, loss=0.6924]\u001b[A\n",
      "Training Epoch 8/25:  86%|████████▋ | 252/292 [02:49<00:31,  1.29batch/s, auc=0.8670, loss=1.0522]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 253/292 [02:49<00:27,  1.40batch/s, auc=0.8670, loss=1.0522]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 253/292 [02:50<00:27,  1.40batch/s, auc=0.8671, loss=0.5259]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 254/292 [02:50<00:25,  1.49batch/s, auc=0.8671, loss=0.5259]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 254/292 [02:51<00:25,  1.49batch/s, auc=0.8671, loss=0.8170]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 255/292 [02:51<00:23,  1.57batch/s, auc=0.8671, loss=0.8170]\u001b[A\n",
      "Training Epoch 8/25:  87%|████████▋ | 255/292 [02:51<00:23,  1.57batch/s, auc=0.8672, loss=0.7262]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 256/292 [02:51<00:22,  1.62batch/s, auc=0.8672, loss=0.7262]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 256/292 [02:52<00:22,  1.62batch/s, auc=0.8669, loss=0.8461]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 257/292 [02:52<00:26,  1.32batch/s, auc=0.8669, loss=0.8461]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 257/292 [02:53<00:26,  1.32batch/s, auc=0.8664, loss=1.1373]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 258/292 [02:53<00:29,  1.17batch/s, auc=0.8664, loss=1.1373]\u001b[A\n",
      "Training Epoch 8/25:  88%|████████▊ | 258/292 [02:54<00:29,  1.17batch/s, auc=0.8663, loss=0.9721]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▊ | 259/292 [02:54<00:25,  1.30batch/s, auc=0.8663, loss=0.9721]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▊ | 259/292 [02:54<00:25,  1.30batch/s, auc=0.8666, loss=0.6655]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 260/292 [02:54<00:22,  1.41batch/s, auc=0.8666, loss=0.6655]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 260/292 [02:56<00:22,  1.41batch/s, auc=0.8663, loss=1.0713]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 261/292 [02:56<00:25,  1.22batch/s, auc=0.8663, loss=1.0713]\u001b[A\n",
      "Training Epoch 8/25:  89%|████████▉ | 261/292 [02:56<00:25,  1.22batch/s, auc=0.8666, loss=0.5967]\u001b[A\n",
      "Training Epoch 8/25:  90%|████████▉ | 262/292 [02:56<00:22,  1.34batch/s, auc=0.8666, loss=0.5967]\u001b[A\n",
      "Training Epoch 8/25:  90%|████████▉ | 262/292 [02:57<00:22,  1.34batch/s, auc=0.8667, loss=0.5505]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 263/292 [02:57<00:20,  1.45batch/s, auc=0.8667, loss=0.5505]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 263/292 [02:58<00:20,  1.45batch/s, auc=0.8665, loss=0.8714]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 264/292 [02:58<00:22,  1.24batch/s, auc=0.8665, loss=0.8714]\u001b[A\n",
      "Training Epoch 8/25:  90%|█████████ | 264/292 [02:58<00:22,  1.24batch/s, auc=0.8668, loss=0.7217]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 265/292 [02:58<00:19,  1.36batch/s, auc=0.8668, loss=0.7217]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 265/292 [02:59<00:19,  1.36batch/s, auc=0.8668, loss=0.4926]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 266/292 [02:59<00:17,  1.46batch/s, auc=0.8668, loss=0.4926]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████ | 266/292 [02:59<00:17,  1.46batch/s, auc=0.8670, loss=0.5787]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████▏| 267/292 [02:59<00:16,  1.54batch/s, auc=0.8670, loss=0.5787]\u001b[A\n",
      "Training Epoch 8/25:  91%|█████████▏| 267/292 [03:00<00:16,  1.54batch/s, auc=0.8672, loss=0.5615]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 268/292 [03:00<00:14,  1.60batch/s, auc=0.8672, loss=0.5615]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 268/292 [03:01<00:14,  1.60batch/s, auc=0.8670, loss=0.9067]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 269/292 [03:01<00:17,  1.31batch/s, auc=0.8670, loss=0.9067]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 269/292 [03:02<00:17,  1.31batch/s, auc=0.8670, loss=0.6599]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 270/292 [03:02<00:15,  1.42batch/s, auc=0.8670, loss=0.6599]\u001b[A\n",
      "Training Epoch 8/25:  92%|█████████▏| 270/292 [03:02<00:15,  1.42batch/s, auc=0.8672, loss=0.5473]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 271/292 [03:02<00:13,  1.51batch/s, auc=0.8672, loss=0.5473]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 271/292 [03:03<00:13,  1.51batch/s, auc=0.8667, loss=1.1497]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 272/292 [03:03<00:15,  1.27batch/s, auc=0.8667, loss=1.1497]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 272/292 [03:04<00:15,  1.27batch/s, auc=0.8664, loss=0.9943]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 273/292 [03:04<00:16,  1.14batch/s, auc=0.8664, loss=0.9943]\u001b[A\n",
      "Training Epoch 8/25:  93%|█████████▎| 273/292 [03:05<00:16,  1.14batch/s, auc=0.8659, loss=1.0003]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 274/292 [03:05<00:16,  1.07batch/s, auc=0.8659, loss=1.0003]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 274/292 [03:06<00:16,  1.07batch/s, auc=0.8659, loss=0.6305]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 275/292 [03:06<00:14,  1.21batch/s, auc=0.8659, loss=0.6305]\u001b[A\n",
      "Training Epoch 8/25:  94%|█████████▍| 275/292 [03:07<00:14,  1.21batch/s, auc=0.8660, loss=0.5674]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 276/292 [03:07<00:11,  1.34batch/s, auc=0.8660, loss=0.5674]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 276/292 [03:07<00:11,  1.34batch/s, auc=0.8663, loss=0.5048]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 277/292 [03:07<00:10,  1.44batch/s, auc=0.8663, loss=0.5048]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▍| 277/292 [03:08<00:10,  1.44batch/s, auc=0.8669, loss=0.3872]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▌| 278/292 [03:08<00:09,  1.52batch/s, auc=0.8669, loss=0.3872]\u001b[A\n",
      "Training Epoch 8/25:  95%|█████████▌| 278/292 [03:08<00:09,  1.52batch/s, auc=0.8672, loss=0.4995]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 279/292 [03:08<00:08,  1.59batch/s, auc=0.8672, loss=0.4995]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 279/292 [03:09<00:08,  1.59batch/s, auc=0.8673, loss=0.5723]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 280/292 [03:09<00:07,  1.64batch/s, auc=0.8673, loss=0.5723]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 280/292 [03:09<00:07,  1.64batch/s, auc=0.8677, loss=0.5011]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 281/292 [03:09<00:06,  1.68batch/s, auc=0.8677, loss=0.5011]\u001b[A\n",
      "Training Epoch 8/25:  96%|█████████▌| 281/292 [03:10<00:06,  1.68batch/s, auc=0.8675, loss=1.0578]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 282/292 [03:10<00:05,  1.70batch/s, auc=0.8675, loss=1.0578]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 282/292 [03:11<00:05,  1.70batch/s, auc=0.8676, loss=0.5971]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 283/292 [03:11<00:05,  1.72batch/s, auc=0.8676, loss=0.5971]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 283/292 [03:12<00:05,  1.72batch/s, auc=0.8671, loss=1.2071]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.37batch/s, auc=0.8671, loss=1.2071]\u001b[A\n",
      "Training Epoch 8/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.37batch/s, auc=0.8673, loss=0.6079]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 285/292 [03:12<00:04,  1.47batch/s, auc=0.8673, loss=0.6079]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 285/292 [03:13<00:04,  1.47batch/s, auc=0.8672, loss=0.9253]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 286/292 [03:13<00:03,  1.54batch/s, auc=0.8672, loss=0.9253]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 286/292 [03:13<00:03,  1.54batch/s, auc=0.8673, loss=0.6689]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 287/292 [03:13<00:03,  1.60batch/s, auc=0.8673, loss=0.6689]\u001b[A\n",
      "Training Epoch 8/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.60batch/s, auc=0.8675, loss=0.7711]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▊| 288/292 [03:14<00:02,  1.65batch/s, auc=0.8675, loss=0.7711]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▊| 288/292 [03:14<00:02,  1.65batch/s, auc=0.8675, loss=0.6977]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 289/292 [03:14<00:01,  1.68batch/s, auc=0.8675, loss=0.6977]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 289/292 [03:15<00:01,  1.68batch/s, auc=0.8675, loss=1.0241]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 290/292 [03:15<00:01,  1.71batch/s, auc=0.8675, loss=1.0241]\u001b[A\n",
      "Training Epoch 8/25:  99%|█████████▉| 290/292 [03:16<00:01,  1.71batch/s, auc=0.8676, loss=0.6983]\u001b[A\n",
      "Training Epoch 8/25: 100%|█████████▉| 291/292 [03:16<00:00,  1.73batch/s, auc=0.8676, loss=0.6983]\u001b[A\n",
      "Training Epoch 8/25: 100%|█████████▉| 291/292 [03:16<00:00,  1.73batch/s, auc=0.8676, loss=0.7955]\u001b[A\n",
      "Training Epoch 8/25: 100%|██████████| 292/292 [03:16<00:00,  1.49batch/s, auc=0.8676, loss=0.7955]\u001b[A\n",
      "Epochs:  32%|███▏      | 8/25 [28:16<1:00:26, 213.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25] Train Loss: 0.7623 | Train AUROC: 0.8676 Val Loss: 0.8212 | Val AUROC: 0.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 9/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9049, loss=0.8760]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 1/292 [00:01<07:01,  1.45s/batch, auc=0.9049, loss=0.8760]\u001b[A\n",
      "Training Epoch 9/25:   0%|          | 1/292 [00:02<07:01,  1.45s/batch, auc=0.8260, loss=1.2571]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 2/292 [00:02<05:55,  1.23s/batch, auc=0.8260, loss=1.2571]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 2/292 [00:03<05:55,  1.23s/batch, auc=0.8537, loss=0.6831]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 3/292 [00:03<04:26,  1.09batch/s, auc=0.8537, loss=0.6831]\u001b[A\n",
      "Training Epoch 9/25:   1%|          | 3/292 [00:03<04:26,  1.09batch/s, auc=0.8639, loss=0.5387]\u001b[A\n",
      "Training Epoch 9/25:   1%|▏         | 4/292 [00:03<03:43,  1.29batch/s, auc=0.8639, loss=0.5387]\u001b[A\n",
      "Training Epoch 9/25:   1%|▏         | 4/292 [00:04<03:43,  1.29batch/s, auc=0.8757, loss=0.7197]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 5/292 [00:04<03:19,  1.44batch/s, auc=0.8757, loss=0.7197]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 5/292 [00:04<03:19,  1.44batch/s, auc=0.8805, loss=0.5534]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 6/292 [00:04<03:05,  1.54batch/s, auc=0.8805, loss=0.5534]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 6/292 [00:05<03:05,  1.54batch/s, auc=0.8821, loss=0.6181]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 7/292 [00:05<02:56,  1.62batch/s, auc=0.8821, loss=0.6181]\u001b[A\n",
      "Training Epoch 9/25:   2%|▏         | 7/292 [00:05<02:56,  1.62batch/s, auc=0.8820, loss=0.7351]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 8/292 [00:05<02:49,  1.67batch/s, auc=0.8820, loss=0.7351]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 8/292 [00:06<02:49,  1.67batch/s, auc=0.8884, loss=0.6404]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 9/292 [00:06<02:45,  1.71batch/s, auc=0.8884, loss=0.6404]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 9/292 [00:06<02:45,  1.71batch/s, auc=0.8921, loss=0.5173]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 10/292 [00:06<02:42,  1.74batch/s, auc=0.8921, loss=0.5173]\u001b[A\n",
      "Training Epoch 9/25:   3%|▎         | 10/292 [00:07<02:42,  1.74batch/s, auc=0.8970, loss=0.5308]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 11/292 [00:07<02:40,  1.75batch/s, auc=0.8970, loss=0.5308]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 11/292 [00:08<02:40,  1.75batch/s, auc=0.8941, loss=0.6090]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 12/292 [00:08<02:38,  1.77batch/s, auc=0.8941, loss=0.6090]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 12/292 [00:09<02:38,  1.77batch/s, auc=0.8825, loss=1.3349]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 13/292 [00:09<03:20,  1.39batch/s, auc=0.8825, loss=1.3349]\u001b[A\n",
      "Training Epoch 9/25:   4%|▍         | 13/292 [00:09<03:20,  1.39batch/s, auc=0.8822, loss=0.6262]\u001b[A\n",
      "Training Epoch 9/25:   5%|▍         | 14/292 [00:09<03:06,  1.49batch/s, auc=0.8822, loss=0.6262]\u001b[A\n",
      "Training Epoch 9/25:   5%|▍         | 14/292 [00:10<03:06,  1.49batch/s, auc=0.8865, loss=0.5477]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 15/292 [00:10<02:55,  1.58batch/s, auc=0.8865, loss=0.5477]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 15/292 [00:10<02:55,  1.58batch/s, auc=0.8845, loss=0.7218]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 16/292 [00:10<02:48,  1.64batch/s, auc=0.8845, loss=0.7218]\u001b[A\n",
      "Training Epoch 9/25:   5%|▌         | 16/292 [00:11<02:48,  1.64batch/s, auc=0.8731, loss=1.4461]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 17/292 [00:11<03:26,  1.33batch/s, auc=0.8731, loss=1.4461]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 17/292 [00:12<03:26,  1.33batch/s, auc=0.8735, loss=0.6779]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 18/292 [00:12<03:09,  1.45batch/s, auc=0.8735, loss=0.6779]\u001b[A\n",
      "Training Epoch 9/25:   6%|▌         | 18/292 [00:12<03:09,  1.45batch/s, auc=0.8744, loss=0.6431]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 19/292 [00:12<02:57,  1.54batch/s, auc=0.8744, loss=0.6431]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 19/292 [00:13<02:57,  1.54batch/s, auc=0.8708, loss=1.1023]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 20/292 [00:13<02:48,  1.61batch/s, auc=0.8708, loss=1.1023]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 20/292 [00:14<02:48,  1.61batch/s, auc=0.8712, loss=0.5952]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 21/292 [00:14<02:42,  1.66batch/s, auc=0.8712, loss=0.5952]\u001b[A\n",
      "Training Epoch 9/25:   7%|▋         | 21/292 [00:14<02:42,  1.66batch/s, auc=0.8708, loss=0.6720]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 22/292 [00:14<02:38,  1.70batch/s, auc=0.8708, loss=0.6720]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 22/292 [00:15<02:38,  1.70batch/s, auc=0.8653, loss=1.1454]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 23/292 [00:15<02:35,  1.73batch/s, auc=0.8653, loss=1.1454]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 23/292 [00:15<02:35,  1.73batch/s, auc=0.8662, loss=0.7601]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 24/292 [00:15<02:33,  1.75batch/s, auc=0.8662, loss=0.7601]\u001b[A\n",
      "Training Epoch 9/25:   8%|▊         | 24/292 [00:16<02:33,  1.75batch/s, auc=0.8635, loss=0.9133]\u001b[A\n",
      "Training Epoch 9/25:   9%|▊         | 25/292 [00:16<02:31,  1.77batch/s, auc=0.8635, loss=0.9133]\u001b[A\n",
      "Training Epoch 9/25:   9%|▊         | 25/292 [00:16<02:31,  1.77batch/s, auc=0.8652, loss=0.6324]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 26/292 [00:16<02:29,  1.77batch/s, auc=0.8652, loss=0.6324]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 26/292 [00:17<02:29,  1.77batch/s, auc=0.8660, loss=0.8299]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 27/292 [00:17<02:28,  1.78batch/s, auc=0.8660, loss=0.8299]\u001b[A\n",
      "Training Epoch 9/25:   9%|▉         | 27/292 [00:17<02:28,  1.78batch/s, auc=0.8685, loss=0.6888]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 28/292 [00:17<02:27,  1.78batch/s, auc=0.8685, loss=0.6888]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 28/292 [00:18<02:27,  1.78batch/s, auc=0.8679, loss=0.7681]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 29/292 [00:18<02:27,  1.79batch/s, auc=0.8679, loss=0.7681]\u001b[A\n",
      "Training Epoch 9/25:  10%|▉         | 29/292 [00:19<02:27,  1.79batch/s, auc=0.8638, loss=1.1362]\u001b[A\n",
      "Training Epoch 9/25:  10%|█         | 30/292 [00:19<03:06,  1.40batch/s, auc=0.8638, loss=1.1362]\u001b[A\n",
      "Training Epoch 9/25:  10%|█         | 30/292 [00:20<03:06,  1.40batch/s, auc=0.8660, loss=0.6484]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 31/292 [00:20<02:54,  1.50batch/s, auc=0.8660, loss=0.6484]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 31/292 [00:20<02:54,  1.50batch/s, auc=0.8682, loss=0.6628]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 32/292 [00:20<02:44,  1.58batch/s, auc=0.8682, loss=0.6628]\u001b[A\n",
      "Training Epoch 9/25:  11%|█         | 32/292 [00:21<02:44,  1.58batch/s, auc=0.8704, loss=0.6311]\u001b[A\n",
      "Training Epoch 9/25:  11%|█▏        | 33/292 [00:21<02:38,  1.64batch/s, auc=0.8704, loss=0.6311]\u001b[A\n",
      "Training Epoch 9/25:  11%|█▏        | 33/292 [00:21<02:38,  1.64batch/s, auc=0.8709, loss=0.7470]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 34/292 [00:21<02:33,  1.68batch/s, auc=0.8709, loss=0.7470]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 34/292 [00:22<02:33,  1.68batch/s, auc=0.8729, loss=0.6956]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 35/292 [00:22<02:29,  1.71batch/s, auc=0.8729, loss=0.6956]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 35/292 [00:22<02:29,  1.71batch/s, auc=0.8740, loss=0.6223]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 36/292 [00:22<02:27,  1.74batch/s, auc=0.8740, loss=0.6223]\u001b[A\n",
      "Training Epoch 9/25:  12%|█▏        | 36/292 [00:23<02:27,  1.74batch/s, auc=0.8769, loss=0.4935]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 37/292 [00:23<02:25,  1.75batch/s, auc=0.8769, loss=0.4935]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 37/292 [00:24<02:25,  1.75batch/s, auc=0.8730, loss=1.0486]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 38/292 [00:24<03:03,  1.39batch/s, auc=0.8730, loss=1.0486]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 38/292 [00:25<03:03,  1.39batch/s, auc=0.8709, loss=1.1194]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 39/292 [00:25<03:29,  1.21batch/s, auc=0.8709, loss=1.1194]\u001b[A\n",
      "Training Epoch 9/25:  13%|█▎        | 39/292 [00:26<03:29,  1.21batch/s, auc=0.8708, loss=0.6946]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▎        | 40/292 [00:26<03:08,  1.34batch/s, auc=0.8708, loss=0.6946]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▎        | 40/292 [00:26<03:08,  1.34batch/s, auc=0.8714, loss=0.5415]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 41/292 [00:26<02:53,  1.45batch/s, auc=0.8714, loss=0.5415]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 41/292 [00:27<02:53,  1.45batch/s, auc=0.8723, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 42/292 [00:27<02:42,  1.54batch/s, auc=0.8723, loss=0.5546]\u001b[A\n",
      "Training Epoch 9/25:  14%|█▍        | 42/292 [00:27<02:42,  1.54batch/s, auc=0.8720, loss=0.6558]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▍        | 43/292 [00:27<02:34,  1.61batch/s, auc=0.8720, loss=0.6558]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▍        | 43/292 [00:28<02:34,  1.61batch/s, auc=0.8734, loss=0.6758]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 44/292 [00:28<02:29,  1.66batch/s, auc=0.8734, loss=0.6758]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 44/292 [00:29<02:29,  1.66batch/s, auc=0.8737, loss=0.6591]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 45/292 [00:29<02:25,  1.70batch/s, auc=0.8737, loss=0.6591]\u001b[A\n",
      "Training Epoch 9/25:  15%|█▌        | 45/292 [00:30<02:25,  1.70batch/s, auc=0.8728, loss=0.7125]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 46/292 [00:30<03:00,  1.36batch/s, auc=0.8728, loss=0.7125]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 46/292 [00:30<03:00,  1.36batch/s, auc=0.8740, loss=0.6037]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 47/292 [00:30<02:46,  1.47batch/s, auc=0.8740, loss=0.6037]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▌        | 47/292 [00:31<02:46,  1.47batch/s, auc=0.8758, loss=0.5206]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▋        | 48/292 [00:31<02:37,  1.55batch/s, auc=0.8758, loss=0.5206]\u001b[A\n",
      "Training Epoch 9/25:  16%|█▋        | 48/292 [00:31<02:37,  1.55batch/s, auc=0.8763, loss=0.6491]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 49/292 [00:31<02:30,  1.62batch/s, auc=0.8763, loss=0.6491]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 49/292 [00:32<02:30,  1.62batch/s, auc=0.8731, loss=1.1193]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 50/292 [00:32<03:02,  1.32batch/s, auc=0.8731, loss=1.1193]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 50/292 [00:33<03:02,  1.32batch/s, auc=0.8736, loss=0.6534]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 51/292 [00:33<02:47,  1.44batch/s, auc=0.8736, loss=0.6534]\u001b[A\n",
      "Training Epoch 9/25:  17%|█▋        | 51/292 [00:33<02:47,  1.44batch/s, auc=0.8731, loss=0.6708]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 52/292 [00:33<02:36,  1.53batch/s, auc=0.8731, loss=0.6708]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 52/292 [00:34<02:36,  1.53batch/s, auc=0.8739, loss=0.6136]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 53/292 [00:34<02:29,  1.60batch/s, auc=0.8739, loss=0.6136]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 53/292 [00:35<02:29,  1.60batch/s, auc=0.8745, loss=0.6234]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 54/292 [00:35<02:23,  1.65batch/s, auc=0.8745, loss=0.6234]\u001b[A\n",
      "Training Epoch 9/25:  18%|█▊        | 54/292 [00:36<02:23,  1.65batch/s, auc=0.8719, loss=1.1381]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 55/292 [00:36<02:56,  1.34batch/s, auc=0.8719, loss=1.1381]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 55/292 [00:36<02:56,  1.34batch/s, auc=0.8722, loss=0.7065]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 56/292 [00:36<02:42,  1.45batch/s, auc=0.8722, loss=0.7065]\u001b[A\n",
      "Training Epoch 9/25:  19%|█▉        | 56/292 [00:37<02:42,  1.45batch/s, auc=0.8736, loss=0.4778]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 57/292 [00:37<02:32,  1.54batch/s, auc=0.8736, loss=0.4778]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 57/292 [00:37<02:32,  1.54batch/s, auc=0.8746, loss=0.6955]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 58/292 [00:37<02:25,  1.61batch/s, auc=0.8746, loss=0.6955]\u001b[A\n",
      "Training Epoch 9/25:  20%|█▉        | 58/292 [00:38<02:25,  1.61batch/s, auc=0.8762, loss=0.6213]\u001b[A\n",
      "Training Epoch 9/25:  20%|██        | 59/292 [00:38<02:20,  1.66batch/s, auc=0.8762, loss=0.6213]\u001b[A\n",
      "Training Epoch 9/25:  20%|██        | 59/292 [00:38<02:20,  1.66batch/s, auc=0.8766, loss=0.6547]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 60/292 [00:38<02:16,  1.70batch/s, auc=0.8766, loss=0.6547]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 60/292 [00:39<02:16,  1.70batch/s, auc=0.8765, loss=0.8208]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 61/292 [00:39<02:13,  1.73batch/s, auc=0.8765, loss=0.8208]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 61/292 [00:40<02:13,  1.73batch/s, auc=0.8764, loss=0.7625]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 62/292 [00:40<02:11,  1.75batch/s, auc=0.8764, loss=0.7625]\u001b[A\n",
      "Training Epoch 9/25:  21%|██        | 62/292 [00:40<02:11,  1.75batch/s, auc=0.8764, loss=0.8218]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 63/292 [00:40<02:10,  1.76batch/s, auc=0.8764, loss=0.8218]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 63/292 [00:41<02:10,  1.76batch/s, auc=0.8745, loss=0.9598]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 64/292 [00:41<02:44,  1.39batch/s, auc=0.8745, loss=0.9598]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 64/292 [00:42<02:44,  1.39batch/s, auc=0.8757, loss=0.5367]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 65/292 [00:42<02:32,  1.49batch/s, auc=0.8757, loss=0.5367]\u001b[A\n",
      "Training Epoch 9/25:  22%|██▏       | 65/292 [00:42<02:32,  1.49batch/s, auc=0.8754, loss=0.8591]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 66/292 [00:42<02:23,  1.57batch/s, auc=0.8754, loss=0.8591]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 66/292 [00:43<02:23,  1.57batch/s, auc=0.8732, loss=1.0858]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 67/292 [00:43<02:52,  1.30batch/s, auc=0.8732, loss=1.0858]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 67/292 [00:44<02:52,  1.30batch/s, auc=0.8734, loss=0.7668]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 68/292 [00:44<02:37,  1.42batch/s, auc=0.8734, loss=0.7668]\u001b[A\n",
      "Training Epoch 9/25:  23%|██▎       | 68/292 [00:45<02:37,  1.42batch/s, auc=0.8725, loss=0.8839]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▎       | 69/292 [00:45<03:01,  1.23batch/s, auc=0.8725, loss=0.8839]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▎       | 69/292 [00:46<03:01,  1.23batch/s, auc=0.8738, loss=0.5565]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 70/292 [00:46<02:44,  1.35batch/s, auc=0.8738, loss=0.5565]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 70/292 [00:46<02:44,  1.35batch/s, auc=0.8743, loss=0.6094]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 71/292 [00:46<02:31,  1.46batch/s, auc=0.8743, loss=0.6094]\u001b[A\n",
      "Training Epoch 9/25:  24%|██▍       | 71/292 [00:47<02:31,  1.46batch/s, auc=0.8737, loss=0.9771]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▍       | 72/292 [00:47<02:22,  1.55batch/s, auc=0.8737, loss=0.9771]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▍       | 72/292 [00:47<02:22,  1.55batch/s, auc=0.8744, loss=0.5839]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 73/292 [00:47<02:15,  1.61batch/s, auc=0.8744, loss=0.5839]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 73/292 [00:48<02:15,  1.61batch/s, auc=0.8748, loss=0.6627]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 74/292 [00:48<02:11,  1.66batch/s, auc=0.8748, loss=0.6627]\u001b[A\n",
      "Training Epoch 9/25:  25%|██▌       | 74/292 [00:48<02:11,  1.66batch/s, auc=0.8752, loss=0.6491]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 75/292 [00:48<02:07,  1.70batch/s, auc=0.8752, loss=0.6491]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 75/292 [00:49<02:07,  1.70batch/s, auc=0.8747, loss=0.8719]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 76/292 [00:49<02:38,  1.36batch/s, auc=0.8747, loss=0.8719]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▌       | 76/292 [00:50<02:38,  1.36batch/s, auc=0.8755, loss=0.5469]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▋       | 77/292 [00:50<02:26,  1.47batch/s, auc=0.8755, loss=0.5469]\u001b[A\n",
      "Training Epoch 9/25:  26%|██▋       | 77/292 [00:51<02:26,  1.47batch/s, auc=0.8749, loss=0.7419]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 78/292 [00:51<02:17,  1.55batch/s, auc=0.8749, loss=0.7419]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 78/292 [00:51<02:17,  1.55batch/s, auc=0.8752, loss=0.6465]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 79/292 [00:51<02:11,  1.62batch/s, auc=0.8752, loss=0.6465]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 79/292 [00:52<02:11,  1.62batch/s, auc=0.8747, loss=0.8028]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 80/292 [00:52<02:07,  1.67batch/s, auc=0.8747, loss=0.8028]\u001b[A\n",
      "Training Epoch 9/25:  27%|██▋       | 80/292 [00:53<02:07,  1.67batch/s, auc=0.8730, loss=1.1209]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 81/292 [00:53<02:36,  1.35batch/s, auc=0.8730, loss=1.1209]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 81/292 [00:53<02:36,  1.35batch/s, auc=0.8724, loss=0.8704]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 82/292 [00:53<02:24,  1.46batch/s, auc=0.8724, loss=0.8704]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 82/292 [00:54<02:24,  1.46batch/s, auc=0.8732, loss=0.5964]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 83/292 [00:54<02:15,  1.54batch/s, auc=0.8732, loss=0.5964]\u001b[A\n",
      "Training Epoch 9/25:  28%|██▊       | 83/292 [00:54<02:15,  1.54batch/s, auc=0.8732, loss=0.5984]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 84/292 [00:54<02:09,  1.61batch/s, auc=0.8732, loss=0.5984]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 84/292 [00:55<02:09,  1.61batch/s, auc=0.8741, loss=0.4795]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 85/292 [00:55<02:04,  1.66batch/s, auc=0.8741, loss=0.4795]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 85/292 [00:56<02:04,  1.66batch/s, auc=0.8751, loss=0.5403]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 86/292 [00:56<02:01,  1.70batch/s, auc=0.8751, loss=0.5403]\u001b[A\n",
      "Training Epoch 9/25:  29%|██▉       | 86/292 [00:56<02:01,  1.70batch/s, auc=0.8751, loss=0.7460]\u001b[A\n",
      "Training Epoch 9/25:  30%|██▉       | 87/292 [00:56<01:59,  1.72batch/s, auc=0.8751, loss=0.7460]\u001b[A\n",
      "Training Epoch 9/25:  30%|██▉       | 87/292 [00:57<01:59,  1.72batch/s, auc=0.8760, loss=0.5859]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 88/292 [00:57<01:57,  1.74batch/s, auc=0.8760, loss=0.5859]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 88/292 [00:57<01:57,  1.74batch/s, auc=0.8761, loss=0.8805]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 89/292 [00:57<01:55,  1.76batch/s, auc=0.8761, loss=0.8805]\u001b[A\n",
      "Training Epoch 9/25:  30%|███       | 89/292 [00:58<01:55,  1.76batch/s, auc=0.8763, loss=0.5080]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 90/292 [00:58<01:54,  1.77batch/s, auc=0.8763, loss=0.5080]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 90/292 [00:59<01:54,  1.77batch/s, auc=0.8743, loss=1.3587]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 91/292 [00:59<02:24,  1.39batch/s, auc=0.8743, loss=1.3587]\u001b[A\n",
      "Training Epoch 9/25:  31%|███       | 91/292 [00:59<02:24,  1.39batch/s, auc=0.8743, loss=0.6394]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 92/292 [00:59<02:14,  1.49batch/s, auc=0.8743, loss=0.6394]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 92/292 [01:00<02:14,  1.49batch/s, auc=0.8748, loss=0.5177]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 93/292 [01:00<02:06,  1.57batch/s, auc=0.8748, loss=0.5177]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 93/292 [01:00<02:06,  1.57batch/s, auc=0.8755, loss=0.6704]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 94/292 [01:00<02:01,  1.63batch/s, auc=0.8755, loss=0.6704]\u001b[A\n",
      "Training Epoch 9/25:  32%|███▏      | 94/292 [01:01<02:01,  1.63batch/s, auc=0.8749, loss=0.8252]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 95/292 [01:01<01:57,  1.68batch/s, auc=0.8749, loss=0.8252]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 95/292 [01:02<01:57,  1.68batch/s, auc=0.8755, loss=0.6813]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 96/292 [01:02<01:54,  1.71batch/s, auc=0.8755, loss=0.6813]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 96/292 [01:02<01:54,  1.71batch/s, auc=0.8759, loss=0.5626]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 97/292 [01:02<01:52,  1.73batch/s, auc=0.8759, loss=0.5626]\u001b[A\n",
      "Training Epoch 9/25:  33%|███▎      | 97/292 [01:03<01:52,  1.73batch/s, auc=0.8757, loss=0.7878]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▎      | 98/292 [01:03<01:50,  1.75batch/s, auc=0.8757, loss=0.7878]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▎      | 98/292 [01:03<01:50,  1.75batch/s, auc=0.8761, loss=0.5962]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 99/292 [01:03<01:49,  1.76batch/s, auc=0.8761, loss=0.5962]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 99/292 [01:04<01:49,  1.76batch/s, auc=0.8761, loss=0.6264]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 100/292 [01:04<01:48,  1.77batch/s, auc=0.8761, loss=0.6264]\u001b[A\n",
      "Training Epoch 9/25:  34%|███▍      | 100/292 [01:04<01:48,  1.77batch/s, auc=0.8752, loss=0.8956]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 101/292 [01:04<01:47,  1.77batch/s, auc=0.8752, loss=0.8956]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 101/292 [01:05<01:47,  1.77batch/s, auc=0.8757, loss=0.5351]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 102/292 [01:05<01:46,  1.78batch/s, auc=0.8757, loss=0.5351]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▍      | 102/292 [01:06<01:46,  1.78batch/s, auc=0.8755, loss=0.6959]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▌      | 103/292 [01:06<02:15,  1.40batch/s, auc=0.8755, loss=0.6959]\u001b[A\n",
      "Training Epoch 9/25:  35%|███▌      | 103/292 [01:07<02:15,  1.40batch/s, auc=0.8752, loss=0.8546]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 104/292 [01:07<02:05,  1.49batch/s, auc=0.8752, loss=0.8546]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 104/292 [01:07<02:05,  1.49batch/s, auc=0.8754, loss=0.8310]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 105/292 [01:07<01:58,  1.57batch/s, auc=0.8754, loss=0.8310]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▌      | 105/292 [01:08<01:58,  1.57batch/s, auc=0.8759, loss=0.5833]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▋      | 106/292 [01:08<01:54,  1.63batch/s, auc=0.8759, loss=0.5833]\u001b[A\n",
      "Training Epoch 9/25:  36%|███▋      | 106/292 [01:08<01:54,  1.63batch/s, auc=0.8762, loss=0.7307]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 107/292 [01:08<01:50,  1.67batch/s, auc=0.8762, loss=0.7307]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 107/292 [01:09<01:50,  1.67batch/s, auc=0.8768, loss=0.5554]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 108/292 [01:09<01:47,  1.71batch/s, auc=0.8768, loss=0.5554]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 108/292 [01:10<01:47,  1.71batch/s, auc=0.8758, loss=1.1571]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 109/292 [01:10<02:13,  1.37batch/s, auc=0.8758, loss=1.1571]\u001b[A\n",
      "Training Epoch 9/25:  37%|███▋      | 109/292 [01:10<02:13,  1.37batch/s, auc=0.8761, loss=0.5055]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 110/292 [01:10<02:03,  1.47batch/s, auc=0.8761, loss=0.5055]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 110/292 [01:12<02:03,  1.47batch/s, auc=0.8736, loss=1.5456]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 111/292 [01:12<02:24,  1.25batch/s, auc=0.8736, loss=1.5456]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 111/292 [01:12<02:24,  1.25batch/s, auc=0.8741, loss=0.6027]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 112/292 [01:12<02:10,  1.38batch/s, auc=0.8741, loss=0.6027]\u001b[A\n",
      "Training Epoch 9/25:  38%|███▊      | 112/292 [01:13<02:10,  1.38batch/s, auc=0.8742, loss=0.7454]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▊      | 113/292 [01:13<02:01,  1.48batch/s, auc=0.8742, loss=0.7454]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▊      | 113/292 [01:13<02:01,  1.48batch/s, auc=0.8745, loss=0.6710]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 114/292 [01:13<01:54,  1.56batch/s, auc=0.8745, loss=0.6710]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 114/292 [01:14<01:54,  1.56batch/s, auc=0.8743, loss=0.6873]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 115/292 [01:14<01:49,  1.62batch/s, auc=0.8743, loss=0.6873]\u001b[A\n",
      "Training Epoch 9/25:  39%|███▉      | 115/292 [01:15<01:49,  1.62batch/s, auc=0.8743, loss=0.8545]\u001b[A\n",
      "Training Epoch 9/25:  40%|███▉      | 116/292 [01:15<02:12,  1.32batch/s, auc=0.8743, loss=0.8545]\u001b[A\n",
      "Training Epoch 9/25:  40%|███▉      | 116/292 [01:16<02:12,  1.32batch/s, auc=0.8735, loss=1.1018]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 117/292 [01:16<02:28,  1.17batch/s, auc=0.8735, loss=1.1018]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 117/292 [01:16<02:28,  1.17batch/s, auc=0.8742, loss=0.6192]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 118/292 [01:16<02:12,  1.31batch/s, auc=0.8742, loss=0.6192]\u001b[A\n",
      "Training Epoch 9/25:  40%|████      | 118/292 [01:18<02:12,  1.31batch/s, auc=0.8736, loss=1.1742]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 119/292 [01:18<02:28,  1.17batch/s, auc=0.8736, loss=1.1742]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 119/292 [01:18<02:28,  1.17batch/s, auc=0.8743, loss=0.4876]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 120/292 [01:18<02:12,  1.30batch/s, auc=0.8743, loss=0.4876]\u001b[A\n",
      "Training Epoch 9/25:  41%|████      | 120/292 [01:19<02:12,  1.30batch/s, auc=0.8740, loss=0.8048]\u001b[A\n",
      "Training Epoch 9/25:  41%|████▏     | 121/292 [01:19<02:00,  1.42batch/s, auc=0.8740, loss=0.8048]\u001b[A\n",
      "Training Epoch 9/25:  41%|████▏     | 121/292 [01:19<02:00,  1.42batch/s, auc=0.8746, loss=0.4693]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 122/292 [01:19<01:52,  1.51batch/s, auc=0.8746, loss=0.4693]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 122/292 [01:20<01:52,  1.51batch/s, auc=0.8750, loss=0.6880]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 123/292 [01:20<01:46,  1.58batch/s, auc=0.8750, loss=0.6880]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 123/292 [01:20<01:46,  1.58batch/s, auc=0.8747, loss=0.7113]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 124/292 [01:20<01:42,  1.64batch/s, auc=0.8747, loss=0.7113]\u001b[A\n",
      "Training Epoch 9/25:  42%|████▏     | 124/292 [01:21<01:42,  1.64batch/s, auc=0.8748, loss=0.5842]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 125/292 [01:21<01:39,  1.68batch/s, auc=0.8748, loss=0.5842]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 125/292 [01:21<01:39,  1.68batch/s, auc=0.8751, loss=0.5752]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 126/292 [01:21<01:37,  1.71batch/s, auc=0.8751, loss=0.5752]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 126/292 [01:22<01:37,  1.71batch/s, auc=0.8754, loss=0.6796]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 127/292 [01:22<01:35,  1.73batch/s, auc=0.8754, loss=0.6796]\u001b[A\n",
      "Training Epoch 9/25:  43%|████▎     | 127/292 [01:23<01:35,  1.73batch/s, auc=0.8744, loss=1.0834]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 128/292 [01:23<01:59,  1.37batch/s, auc=0.8744, loss=1.0834]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 128/292 [01:24<01:59,  1.37batch/s, auc=0.8746, loss=0.4977]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 129/292 [01:24<01:50,  1.48batch/s, auc=0.8746, loss=0.4977]\u001b[A\n",
      "Training Epoch 9/25:  44%|████▍     | 129/292 [01:24<01:50,  1.48batch/s, auc=0.8748, loss=0.7902]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 130/292 [01:24<01:43,  1.56batch/s, auc=0.8748, loss=0.7902]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 130/292 [01:25<01:43,  1.56batch/s, auc=0.8741, loss=1.1076]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 131/292 [01:25<01:39,  1.62batch/s, auc=0.8741, loss=1.1076]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▍     | 131/292 [01:25<01:39,  1.62batch/s, auc=0.8741, loss=0.8355]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▌     | 132/292 [01:25<01:36,  1.67batch/s, auc=0.8741, loss=0.8355]\u001b[A\n",
      "Training Epoch 9/25:  45%|████▌     | 132/292 [01:26<01:36,  1.67batch/s, auc=0.8744, loss=0.5258]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 133/292 [01:26<01:33,  1.70batch/s, auc=0.8744, loss=0.5258]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 133/292 [01:26<01:33,  1.70batch/s, auc=0.8744, loss=0.6253]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 134/292 [01:26<01:31,  1.73batch/s, auc=0.8744, loss=0.6253]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 134/292 [01:28<01:31,  1.73batch/s, auc=0.8730, loss=1.1477]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 135/292 [01:28<01:54,  1.37batch/s, auc=0.8730, loss=1.1477]\u001b[A\n",
      "Training Epoch 9/25:  46%|████▌     | 135/292 [01:28<01:54,  1.37batch/s, auc=0.8731, loss=0.7434]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 136/292 [01:28<01:45,  1.48batch/s, auc=0.8731, loss=0.7434]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 136/292 [01:29<01:45,  1.48batch/s, auc=0.8730, loss=0.6782]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 137/292 [01:29<01:39,  1.56batch/s, auc=0.8730, loss=0.6782]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 137/292 [01:29<01:39,  1.56batch/s, auc=0.8732, loss=0.6834]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 138/292 [01:29<01:35,  1.62batch/s, auc=0.8732, loss=0.6834]\u001b[A\n",
      "Training Epoch 9/25:  47%|████▋     | 138/292 [01:30<01:35,  1.62batch/s, auc=0.8735, loss=0.5621]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 139/292 [01:30<01:32,  1.66batch/s, auc=0.8735, loss=0.5621]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 139/292 [01:30<01:32,  1.66batch/s, auc=0.8736, loss=0.7678]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 140/292 [01:30<01:29,  1.70batch/s, auc=0.8736, loss=0.7678]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 140/292 [01:31<01:29,  1.70batch/s, auc=0.8735, loss=0.7741]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 141/292 [01:31<01:51,  1.36batch/s, auc=0.8735, loss=0.7741]\u001b[A\n",
      "Training Epoch 9/25:  48%|████▊     | 141/292 [01:32<01:51,  1.36batch/s, auc=0.8739, loss=0.4990]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▊     | 142/292 [01:32<01:42,  1.46batch/s, auc=0.8739, loss=0.4990]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▊     | 142/292 [01:33<01:42,  1.46batch/s, auc=0.8720, loss=1.5513]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 143/292 [01:33<01:59,  1.25batch/s, auc=0.8720, loss=1.5513]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 143/292 [01:34<01:59,  1.25batch/s, auc=0.8719, loss=0.9872]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 144/292 [01:34<01:47,  1.37batch/s, auc=0.8719, loss=0.9872]\u001b[A\n",
      "Training Epoch 9/25:  49%|████▉     | 144/292 [01:34<01:47,  1.37batch/s, auc=0.8721, loss=0.6595]\u001b[A\n",
      "Training Epoch 9/25:  50%|████▉     | 145/292 [01:34<01:39,  1.47batch/s, auc=0.8721, loss=0.6595]\u001b[A\n",
      "Training Epoch 9/25:  50%|████▉     | 145/292 [01:35<01:39,  1.47batch/s, auc=0.8724, loss=0.6148]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 146/292 [01:35<01:33,  1.55batch/s, auc=0.8724, loss=0.6148]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 146/292 [01:35<01:33,  1.55batch/s, auc=0.8724, loss=0.6315]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 147/292 [01:35<01:29,  1.62batch/s, auc=0.8724, loss=0.6315]\u001b[A\n",
      "Training Epoch 9/25:  50%|█████     | 147/292 [01:36<01:29,  1.62batch/s, auc=0.8727, loss=0.5457]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 148/292 [01:36<01:26,  1.66batch/s, auc=0.8727, loss=0.5457]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 148/292 [01:36<01:26,  1.66batch/s, auc=0.8728, loss=0.5052]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 149/292 [01:36<01:24,  1.70batch/s, auc=0.8728, loss=0.5052]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████     | 149/292 [01:37<01:24,  1.70batch/s, auc=0.8735, loss=0.5302]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████▏    | 150/292 [01:37<01:22,  1.72batch/s, auc=0.8735, loss=0.5302]\u001b[A\n",
      "Training Epoch 9/25:  51%|█████▏    | 150/292 [01:38<01:22,  1.72batch/s, auc=0.8742, loss=0.4757]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 151/292 [01:38<01:20,  1.74batch/s, auc=0.8742, loss=0.4757]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 151/292 [01:39<01:20,  1.74batch/s, auc=0.8742, loss=0.7670]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 152/292 [01:39<01:41,  1.38batch/s, auc=0.8742, loss=0.7670]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 152/292 [01:39<01:41,  1.38batch/s, auc=0.8741, loss=0.7777]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 153/292 [01:39<01:33,  1.48batch/s, auc=0.8741, loss=0.7777]\u001b[A\n",
      "Training Epoch 9/25:  52%|█████▏    | 153/292 [01:40<01:33,  1.48batch/s, auc=0.8741, loss=0.9694]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 154/292 [01:40<01:28,  1.56batch/s, auc=0.8741, loss=0.9694]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 154/292 [01:40<01:28,  1.56batch/s, auc=0.8744, loss=0.6574]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 155/292 [01:40<01:24,  1.62batch/s, auc=0.8744, loss=0.6574]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 155/292 [01:41<01:24,  1.62batch/s, auc=0.8747, loss=0.7433]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 156/292 [01:41<01:21,  1.67batch/s, auc=0.8747, loss=0.7433]\u001b[A\n",
      "Training Epoch 9/25:  53%|█████▎    | 156/292 [01:41<01:21,  1.67batch/s, auc=0.8750, loss=0.7675]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 157/292 [01:41<01:19,  1.70batch/s, auc=0.8750, loss=0.7675]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 157/292 [01:43<01:19,  1.70batch/s, auc=0.8740, loss=1.2496]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 158/292 [01:43<01:38,  1.36batch/s, auc=0.8740, loss=1.2496]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 158/292 [01:43<01:38,  1.36batch/s, auc=0.8743, loss=0.5279]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 159/292 [01:43<01:30,  1.46batch/s, auc=0.8743, loss=0.5279]\u001b[A\n",
      "Training Epoch 9/25:  54%|█████▍    | 159/292 [01:44<01:30,  1.46batch/s, auc=0.8745, loss=0.6627]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▍    | 160/292 [01:44<01:25,  1.54batch/s, auc=0.8745, loss=0.6627]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▍    | 160/292 [01:44<01:25,  1.54batch/s, auc=0.8745, loss=0.9255]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 161/292 [01:44<01:21,  1.61batch/s, auc=0.8745, loss=0.9255]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 161/292 [01:45<01:21,  1.61batch/s, auc=0.8748, loss=0.4445]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 162/292 [01:45<01:18,  1.66batch/s, auc=0.8748, loss=0.4445]\u001b[A\n",
      "Training Epoch 9/25:  55%|█████▌    | 162/292 [01:46<01:18,  1.66batch/s, auc=0.8741, loss=1.0815]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 163/292 [01:46<01:36,  1.34batch/s, auc=0.8741, loss=1.0815]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 163/292 [01:46<01:36,  1.34batch/s, auc=0.8742, loss=0.6310]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 164/292 [01:46<01:28,  1.45batch/s, auc=0.8742, loss=0.6310]\u001b[A\n",
      "Training Epoch 9/25:  56%|█████▌    | 164/292 [01:47<01:28,  1.45batch/s, auc=0.8743, loss=0.6558]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 165/292 [01:47<01:22,  1.53batch/s, auc=0.8743, loss=0.6558]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 165/292 [01:48<01:22,  1.53batch/s, auc=0.8745, loss=0.6659]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 166/292 [01:48<01:18,  1.60batch/s, auc=0.8745, loss=0.6659]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 166/292 [01:48<01:18,  1.60batch/s, auc=0.8748, loss=0.5924]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 167/292 [01:48<01:15,  1.65batch/s, auc=0.8748, loss=0.5924]\u001b[A\n",
      "Training Epoch 9/25:  57%|█████▋    | 167/292 [01:49<01:15,  1.65batch/s, auc=0.8749, loss=0.5871]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 168/292 [01:49<01:13,  1.68batch/s, auc=0.8749, loss=0.5871]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 168/292 [01:49<01:13,  1.68batch/s, auc=0.8750, loss=0.6576]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 169/292 [01:49<01:11,  1.71batch/s, auc=0.8750, loss=0.6576]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 169/292 [01:50<01:11,  1.71batch/s, auc=0.8745, loss=1.0223]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 170/292 [01:50<01:29,  1.37batch/s, auc=0.8745, loss=1.0223]\u001b[A\n",
      "Training Epoch 9/25:  58%|█████▊    | 170/292 [01:51<01:29,  1.37batch/s, auc=0.8745, loss=0.6972]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▊    | 171/292 [01:51<01:22,  1.47batch/s, auc=0.8745, loss=0.6972]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▊    | 171/292 [01:52<01:22,  1.47batch/s, auc=0.8736, loss=1.1924]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 172/292 [01:52<01:35,  1.25batch/s, auc=0.8736, loss=1.1924]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 172/292 [01:53<01:35,  1.25batch/s, auc=0.8742, loss=0.4872]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 173/292 [01:53<01:26,  1.37batch/s, auc=0.8742, loss=0.4872]\u001b[A\n",
      "Training Epoch 9/25:  59%|█████▉    | 173/292 [01:53<01:26,  1.37batch/s, auc=0.8741, loss=0.8853]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 174/292 [01:53<01:20,  1.47batch/s, auc=0.8741, loss=0.8853]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 174/292 [01:54<01:20,  1.47batch/s, auc=0.8738, loss=0.8059]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 175/292 [01:54<01:15,  1.56batch/s, auc=0.8738, loss=0.8059]\u001b[A\n",
      "Training Epoch 9/25:  60%|█████▉    | 175/292 [01:54<01:15,  1.56batch/s, auc=0.8744, loss=0.4657]\u001b[A\n",
      "Training Epoch 9/25:  60%|██████    | 176/292 [01:54<01:11,  1.62batch/s, auc=0.8744, loss=0.4657]\u001b[A\n",
      "Training Epoch 9/25:  60%|██████    | 176/292 [01:55<01:11,  1.62batch/s, auc=0.8746, loss=0.7464]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 177/292 [01:55<01:09,  1.66batch/s, auc=0.8746, loss=0.7464]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 177/292 [01:56<01:09,  1.66batch/s, auc=0.8744, loss=0.9124]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 178/292 [01:56<01:24,  1.34batch/s, auc=0.8744, loss=0.9124]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████    | 178/292 [01:56<01:24,  1.34batch/s, auc=0.8748, loss=0.5693]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████▏   | 179/292 [01:56<01:17,  1.45batch/s, auc=0.8748, loss=0.5693]\u001b[A\n",
      "Training Epoch 9/25:  61%|██████▏   | 179/292 [01:57<01:17,  1.45batch/s, auc=0.8750, loss=0.7269]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 180/292 [01:57<01:12,  1.53batch/s, auc=0.8750, loss=0.7269]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 180/292 [01:58<01:12,  1.53batch/s, auc=0.8754, loss=0.5311]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 181/292 [01:58<01:09,  1.60batch/s, auc=0.8754, loss=0.5311]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 181/292 [01:59<01:09,  1.60batch/s, auc=0.8753, loss=0.9622]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 182/292 [01:59<01:23,  1.31batch/s, auc=0.8753, loss=0.9622]\u001b[A\n",
      "Training Epoch 9/25:  62%|██████▏   | 182/292 [01:59<01:23,  1.31batch/s, auc=0.8758, loss=0.4440]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 183/292 [01:59<01:16,  1.43batch/s, auc=0.8758, loss=0.4440]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 183/292 [02:00<01:16,  1.43batch/s, auc=0.8762, loss=0.7247]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 184/292 [02:00<01:11,  1.52batch/s, auc=0.8762, loss=0.7247]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 184/292 [02:00<01:11,  1.52batch/s, auc=0.8762, loss=0.9084]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 185/292 [02:00<01:07,  1.59batch/s, auc=0.8762, loss=0.9084]\u001b[A\n",
      "Training Epoch 9/25:  63%|██████▎   | 185/292 [02:01<01:07,  1.59batch/s, auc=0.8763, loss=0.6657]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▎   | 186/292 [02:01<01:04,  1.64batch/s, auc=0.8763, loss=0.6657]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▎   | 186/292 [02:01<01:04,  1.64batch/s, auc=0.8764, loss=0.5517]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 187/292 [02:01<01:02,  1.68batch/s, auc=0.8764, loss=0.5517]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 187/292 [02:02<01:02,  1.68batch/s, auc=0.8766, loss=0.6348]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 188/292 [02:02<01:00,  1.71batch/s, auc=0.8766, loss=0.6348]\u001b[A\n",
      "Training Epoch 9/25:  64%|██████▍   | 188/292 [02:03<01:00,  1.71batch/s, auc=0.8761, loss=0.9671]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▍   | 189/292 [02:03<01:15,  1.36batch/s, auc=0.8761, loss=0.9671]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▍   | 189/292 [02:04<01:15,  1.36batch/s, auc=0.8763, loss=0.7723]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 190/292 [02:04<01:09,  1.47batch/s, auc=0.8763, loss=0.7723]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 190/292 [02:04<01:09,  1.47batch/s, auc=0.8764, loss=0.6518]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 191/292 [02:04<01:05,  1.55batch/s, auc=0.8764, loss=0.6518]\u001b[A\n",
      "Training Epoch 9/25:  65%|██████▌   | 191/292 [02:05<01:05,  1.55batch/s, auc=0.8763, loss=0.7985]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 192/292 [02:05<01:02,  1.61batch/s, auc=0.8763, loss=0.7985]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 192/292 [02:06<01:02,  1.61batch/s, auc=0.8759, loss=0.9402]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 193/292 [02:06<01:15,  1.32batch/s, auc=0.8759, loss=0.9402]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▌   | 193/292 [02:06<01:15,  1.32batch/s, auc=0.8756, loss=0.8515]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▋   | 194/292 [02:06<01:08,  1.43batch/s, auc=0.8756, loss=0.8515]\u001b[A\n",
      "Training Epoch 9/25:  66%|██████▋   | 194/292 [02:07<01:08,  1.43batch/s, auc=0.8758, loss=0.6099]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 195/292 [02:07<01:03,  1.52batch/s, auc=0.8758, loss=0.6099]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 195/292 [02:07<01:03,  1.52batch/s, auc=0.8759, loss=0.5693]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 196/292 [02:07<01:00,  1.59batch/s, auc=0.8759, loss=0.5693]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 196/292 [02:08<01:00,  1.59batch/s, auc=0.8764, loss=0.5586]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 197/292 [02:08<00:57,  1.64batch/s, auc=0.8764, loss=0.5586]\u001b[A\n",
      "Training Epoch 9/25:  67%|██████▋   | 197/292 [02:09<00:57,  1.64batch/s, auc=0.8764, loss=0.5751]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 198/292 [02:09<00:55,  1.68batch/s, auc=0.8764, loss=0.5751]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 198/292 [02:09<00:55,  1.68batch/s, auc=0.8768, loss=0.5891]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 199/292 [02:09<00:54,  1.71batch/s, auc=0.8768, loss=0.5891]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 199/292 [02:10<00:54,  1.71batch/s, auc=0.8764, loss=0.9185]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 200/292 [02:10<01:07,  1.36batch/s, auc=0.8764, loss=0.9185]\u001b[A\n",
      "Training Epoch 9/25:  68%|██████▊   | 200/292 [02:11<01:07,  1.36batch/s, auc=0.8762, loss=0.8236]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 201/292 [02:11<01:16,  1.19batch/s, auc=0.8762, loss=0.8236]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 201/292 [02:12<01:16,  1.19batch/s, auc=0.8764, loss=0.6186]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 202/292 [02:12<01:07,  1.32batch/s, auc=0.8764, loss=0.6186]\u001b[A\n",
      "Training Epoch 9/25:  69%|██████▉   | 202/292 [02:12<01:07,  1.32batch/s, auc=0.8762, loss=0.8457]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 203/292 [02:12<01:02,  1.44batch/s, auc=0.8762, loss=0.8457]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 203/292 [02:13<01:02,  1.44batch/s, auc=0.8761, loss=0.6337]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 204/292 [02:13<00:57,  1.52batch/s, auc=0.8761, loss=0.6337]\u001b[A\n",
      "Training Epoch 9/25:  70%|██████▉   | 204/292 [02:14<00:57,  1.52batch/s, auc=0.8757, loss=1.0974]\u001b[A\n",
      "Training Epoch 9/25:  70%|███████   | 205/292 [02:14<01:08,  1.28batch/s, auc=0.8757, loss=1.0974]\u001b[A\n",
      "Training Epoch 9/25:  70%|███████   | 205/292 [02:15<01:08,  1.28batch/s, auc=0.8756, loss=0.7859]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 206/292 [02:15<01:15,  1.15batch/s, auc=0.8756, loss=0.7859]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 206/292 [02:16<01:15,  1.15batch/s, auc=0.8757, loss=0.6775]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 207/292 [02:16<01:06,  1.28batch/s, auc=0.8757, loss=0.6775]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 207/292 [02:16<01:06,  1.28batch/s, auc=0.8752, loss=1.1818]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 208/292 [02:16<01:00,  1.40batch/s, auc=0.8752, loss=1.1818]\u001b[A\n",
      "Training Epoch 9/25:  71%|███████   | 208/292 [02:17<01:00,  1.40batch/s, auc=0.8753, loss=0.7530]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 209/292 [02:17<00:55,  1.49batch/s, auc=0.8753, loss=0.7530]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 209/292 [02:18<00:55,  1.49batch/s, auc=0.8748, loss=0.9407]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 210/292 [02:18<01:05,  1.26batch/s, auc=0.8748, loss=0.9407]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 210/292 [02:19<01:05,  1.26batch/s, auc=0.8749, loss=0.5586]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 211/292 [02:19<00:58,  1.38batch/s, auc=0.8749, loss=0.5586]\u001b[A\n",
      "Training Epoch 9/25:  72%|███████▏  | 211/292 [02:19<00:58,  1.38batch/s, auc=0.8750, loss=0.4631]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 212/292 [02:19<00:54,  1.48batch/s, auc=0.8750, loss=0.4631]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 212/292 [02:20<00:54,  1.48batch/s, auc=0.8751, loss=0.7731]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 213/292 [02:20<00:50,  1.56batch/s, auc=0.8751, loss=0.7731]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 213/292 [02:20<00:50,  1.56batch/s, auc=0.8755, loss=0.5274]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 214/292 [02:20<00:48,  1.62batch/s, auc=0.8755, loss=0.5274]\u001b[A\n",
      "Training Epoch 9/25:  73%|███████▎  | 214/292 [02:21<00:48,  1.62batch/s, auc=0.8758, loss=0.5659]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▎  | 215/292 [02:21<00:46,  1.66batch/s, auc=0.8758, loss=0.5659]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▎  | 215/292 [02:21<00:46,  1.66batch/s, auc=0.8761, loss=0.6594]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 216/292 [02:21<00:44,  1.69batch/s, auc=0.8761, loss=0.6594]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 216/292 [02:22<00:44,  1.69batch/s, auc=0.8763, loss=0.5793]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 217/292 [02:22<00:43,  1.72batch/s, auc=0.8763, loss=0.5793]\u001b[A\n",
      "Training Epoch 9/25:  74%|███████▍  | 217/292 [02:23<00:43,  1.72batch/s, auc=0.8759, loss=0.8534]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▍  | 218/292 [02:23<00:54,  1.37batch/s, auc=0.8759, loss=0.8534]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▍  | 218/292 [02:24<00:54,  1.37batch/s, auc=0.8758, loss=0.7468]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 219/292 [02:24<01:01,  1.20batch/s, auc=0.8758, loss=0.7468]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 219/292 [02:25<01:01,  1.20batch/s, auc=0.8760, loss=0.7433]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 220/292 [02:25<00:54,  1.33batch/s, auc=0.8760, loss=0.7433]\u001b[A\n",
      "Training Epoch 9/25:  75%|███████▌  | 220/292 [02:25<00:54,  1.33batch/s, auc=0.8763, loss=0.5548]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 221/292 [02:25<00:49,  1.43batch/s, auc=0.8763, loss=0.5548]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 221/292 [02:26<00:49,  1.43batch/s, auc=0.8764, loss=0.6575]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 222/292 [02:26<00:45,  1.52batch/s, auc=0.8764, loss=0.6575]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▌  | 222/292 [02:26<00:45,  1.52batch/s, auc=0.8764, loss=0.7317]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▋  | 223/292 [02:26<00:43,  1.59batch/s, auc=0.8764, loss=0.7317]\u001b[A\n",
      "Training Epoch 9/25:  76%|███████▋  | 223/292 [02:27<00:43,  1.59batch/s, auc=0.8758, loss=1.1313]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 224/292 [02:27<00:51,  1.31batch/s, auc=0.8758, loss=1.1313]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 224/292 [02:28<00:51,  1.31batch/s, auc=0.8761, loss=0.5303]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 225/292 [02:28<00:47,  1.42batch/s, auc=0.8761, loss=0.5303]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 225/292 [02:29<00:47,  1.42batch/s, auc=0.8762, loss=0.5843]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 226/292 [02:29<00:43,  1.51batch/s, auc=0.8762, loss=0.5843]\u001b[A\n",
      "Training Epoch 9/25:  77%|███████▋  | 226/292 [02:29<00:43,  1.51batch/s, auc=0.8762, loss=0.6737]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 227/292 [02:29<00:41,  1.58batch/s, auc=0.8762, loss=0.6737]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 227/292 [02:30<00:41,  1.58batch/s, auc=0.8758, loss=1.0491]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 228/292 [02:30<00:49,  1.30batch/s, auc=0.8758, loss=1.0491]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 228/292 [02:31<00:49,  1.30batch/s, auc=0.8758, loss=0.7136]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 229/292 [02:31<00:54,  1.16batch/s, auc=0.8758, loss=0.7136]\u001b[A\n",
      "Training Epoch 9/25:  78%|███████▊  | 229/292 [02:32<00:54,  1.16batch/s, auc=0.8754, loss=1.0531]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 230/292 [02:32<00:47,  1.30batch/s, auc=0.8754, loss=1.0531]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 230/292 [02:33<00:47,  1.30batch/s, auc=0.8749, loss=1.1964]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 231/292 [02:33<00:52,  1.16batch/s, auc=0.8749, loss=1.1964]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 231/292 [02:34<00:52,  1.16batch/s, auc=0.8745, loss=1.0512]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 232/292 [02:34<00:55,  1.08batch/s, auc=0.8745, loss=1.0512]\u001b[A\n",
      "Training Epoch 9/25:  79%|███████▉  | 232/292 [02:35<00:55,  1.08batch/s, auc=0.8748, loss=0.5827]\u001b[A\n",
      "Training Epoch 9/25:  80%|███████▉  | 233/292 [02:35<00:48,  1.22batch/s, auc=0.8748, loss=0.5827]\u001b[A\n",
      "Training Epoch 9/25:  80%|███████▉  | 233/292 [02:35<00:48,  1.22batch/s, auc=0.8746, loss=0.9225]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 234/292 [02:35<00:43,  1.35batch/s, auc=0.8746, loss=0.9225]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 234/292 [02:36<00:43,  1.35batch/s, auc=0.8748, loss=0.6867]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 235/292 [02:36<00:39,  1.45batch/s, auc=0.8748, loss=0.6867]\u001b[A\n",
      "Training Epoch 9/25:  80%|████████  | 235/292 [02:36<00:39,  1.45batch/s, auc=0.8748, loss=0.8446]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 236/292 [02:36<00:36,  1.53batch/s, auc=0.8748, loss=0.8446]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 236/292 [02:37<00:36,  1.53batch/s, auc=0.8742, loss=1.0399]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 237/292 [02:37<00:42,  1.28batch/s, auc=0.8742, loss=1.0399]\u001b[A\n",
      "Training Epoch 9/25:  81%|████████  | 237/292 [02:38<00:42,  1.28batch/s, auc=0.8742, loss=0.5572]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 238/292 [02:38<00:38,  1.40batch/s, auc=0.8742, loss=0.5572]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 238/292 [02:38<00:38,  1.40batch/s, auc=0.8744, loss=0.5398]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 239/292 [02:38<00:35,  1.49batch/s, auc=0.8744, loss=0.5398]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 239/292 [02:39<00:35,  1.49batch/s, auc=0.8746, loss=0.5801]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 240/292 [02:39<00:33,  1.57batch/s, auc=0.8746, loss=0.5801]\u001b[A\n",
      "Training Epoch 9/25:  82%|████████▏ | 240/292 [02:40<00:33,  1.57batch/s, auc=0.8750, loss=0.4895]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 241/292 [02:40<00:31,  1.62batch/s, auc=0.8750, loss=0.4895]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 241/292 [02:41<00:31,  1.62batch/s, auc=0.8747, loss=0.9775]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 242/292 [02:41<00:37,  1.32batch/s, auc=0.8747, loss=0.9775]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 242/292 [02:41<00:37,  1.32batch/s, auc=0.8747, loss=0.7807]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 243/292 [02:41<00:34,  1.43batch/s, auc=0.8747, loss=0.7807]\u001b[A\n",
      "Training Epoch 9/25:  83%|████████▎ | 243/292 [02:42<00:34,  1.43batch/s, auc=0.8741, loss=1.1495]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▎ | 244/292 [02:42<00:38,  1.23batch/s, auc=0.8741, loss=1.1495]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▎ | 244/292 [02:43<00:38,  1.23batch/s, auc=0.8745, loss=0.4163]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 245/292 [02:43<00:34,  1.36batch/s, auc=0.8745, loss=0.4163]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 245/292 [02:43<00:34,  1.36batch/s, auc=0.8747, loss=0.4694]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 246/292 [02:43<00:31,  1.46batch/s, auc=0.8747, loss=0.4694]\u001b[A\n",
      "Training Epoch 9/25:  84%|████████▍ | 246/292 [02:44<00:31,  1.46batch/s, auc=0.8749, loss=0.7483]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 247/292 [02:44<00:29,  1.54batch/s, auc=0.8749, loss=0.7483]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 247/292 [02:45<00:29,  1.54batch/s, auc=0.8747, loss=0.9511]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 248/292 [02:45<00:27,  1.60batch/s, auc=0.8747, loss=0.9511]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▍ | 248/292 [02:45<00:27,  1.60batch/s, auc=0.8747, loss=0.8599]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▌ | 249/292 [02:45<00:26,  1.65batch/s, auc=0.8747, loss=0.8599]\u001b[A\n",
      "Training Epoch 9/25:  85%|████████▌ | 249/292 [02:46<00:26,  1.65batch/s, auc=0.8751, loss=0.5217]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 250/292 [02:46<00:24,  1.69batch/s, auc=0.8751, loss=0.5217]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 250/292 [02:46<00:24,  1.69batch/s, auc=0.8753, loss=0.5826]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 251/292 [02:46<00:23,  1.71batch/s, auc=0.8753, loss=0.5826]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▌ | 251/292 [02:47<00:23,  1.71batch/s, auc=0.8755, loss=0.7030]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▋ | 252/292 [02:47<00:23,  1.73batch/s, auc=0.8755, loss=0.7030]\u001b[A\n",
      "Training Epoch 9/25:  86%|████████▋ | 252/292 [02:47<00:23,  1.73batch/s, auc=0.8756, loss=0.7190]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 253/292 [02:47<00:22,  1.74batch/s, auc=0.8756, loss=0.7190]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 253/292 [02:48<00:22,  1.74batch/s, auc=0.8755, loss=0.8967]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 254/292 [02:48<00:21,  1.75batch/s, auc=0.8755, loss=0.8967]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 254/292 [02:48<00:21,  1.75batch/s, auc=0.8755, loss=0.7057]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 255/292 [02:48<00:21,  1.75batch/s, auc=0.8755, loss=0.7057]\u001b[A\n",
      "Training Epoch 9/25:  87%|████████▋ | 255/292 [02:49<00:21,  1.75batch/s, auc=0.8753, loss=0.6776]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 256/292 [02:49<00:20,  1.76batch/s, auc=0.8753, loss=0.6776]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 256/292 [02:50<00:20,  1.76batch/s, auc=0.8758, loss=0.4802]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 257/292 [02:50<00:19,  1.76batch/s, auc=0.8758, loss=0.4802]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 257/292 [02:50<00:19,  1.76batch/s, auc=0.8760, loss=0.5572]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 258/292 [02:50<00:19,  1.77batch/s, auc=0.8760, loss=0.5572]\u001b[A\n",
      "Training Epoch 9/25:  88%|████████▊ | 258/292 [02:51<00:19,  1.77batch/s, auc=0.8759, loss=0.8837]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▊ | 259/292 [02:51<00:23,  1.39batch/s, auc=0.8759, loss=0.8837]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▊ | 259/292 [02:52<00:23,  1.39batch/s, auc=0.8762, loss=0.6361]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 260/292 [02:52<00:21,  1.49batch/s, auc=0.8762, loss=0.6361]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 260/292 [02:52<00:21,  1.49batch/s, auc=0.8761, loss=0.8913]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 261/292 [02:52<00:19,  1.56batch/s, auc=0.8761, loss=0.8913]\u001b[A\n",
      "Training Epoch 9/25:  89%|████████▉ | 261/292 [02:53<00:19,  1.56batch/s, auc=0.8761, loss=0.8178]\u001b[A\n",
      "Training Epoch 9/25:  90%|████████▉ | 262/292 [02:53<00:18,  1.62batch/s, auc=0.8761, loss=0.8178]\u001b[A\n",
      "Training Epoch 9/25:  90%|████████▉ | 262/292 [02:54<00:18,  1.62batch/s, auc=0.8760, loss=0.8207]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 263/292 [02:54<00:17,  1.66batch/s, auc=0.8760, loss=0.8207]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 263/292 [02:55<00:17,  1.66batch/s, auc=0.8757, loss=0.9695]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 264/292 [02:55<00:20,  1.34batch/s, auc=0.8757, loss=0.9695]\u001b[A\n",
      "Training Epoch 9/25:  90%|█████████ | 264/292 [02:55<00:20,  1.34batch/s, auc=0.8756, loss=0.5851]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 265/292 [02:55<00:18,  1.45batch/s, auc=0.8756, loss=0.5851]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 265/292 [02:56<00:18,  1.45batch/s, auc=0.8751, loss=1.1405]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 266/292 [02:56<00:21,  1.24batch/s, auc=0.8751, loss=1.1405]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████ | 266/292 [02:57<00:21,  1.24batch/s, auc=0.8750, loss=0.6340]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████▏| 267/292 [02:57<00:18,  1.36batch/s, auc=0.8750, loss=0.6340]\u001b[A\n",
      "Training Epoch 9/25:  91%|█████████▏| 267/292 [02:57<00:18,  1.36batch/s, auc=0.8751, loss=0.5800]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 268/292 [02:57<00:16,  1.46batch/s, auc=0.8751, loss=0.5800]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 268/292 [02:58<00:16,  1.46batch/s, auc=0.8750, loss=0.6785]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 269/292 [02:58<00:14,  1.54batch/s, auc=0.8750, loss=0.6785]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 269/292 [02:59<00:14,  1.54batch/s, auc=0.8750, loss=0.7822]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 270/292 [02:59<00:13,  1.60batch/s, auc=0.8750, loss=0.7822]\u001b[A\n",
      "Training Epoch 9/25:  92%|█████████▏| 270/292 [02:59<00:13,  1.60batch/s, auc=0.8748, loss=0.7894]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 271/292 [02:59<00:12,  1.65batch/s, auc=0.8748, loss=0.7894]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 271/292 [03:00<00:12,  1.65batch/s, auc=0.8751, loss=0.6067]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 272/292 [03:00<00:11,  1.68batch/s, auc=0.8751, loss=0.6067]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 272/292 [03:00<00:11,  1.68batch/s, auc=0.8750, loss=0.7348]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 273/292 [03:00<00:11,  1.71batch/s, auc=0.8750, loss=0.7348]\u001b[A\n",
      "Training Epoch 9/25:  93%|█████████▎| 273/292 [03:01<00:11,  1.71batch/s, auc=0.8754, loss=0.4931]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 274/292 [03:01<00:10,  1.72batch/s, auc=0.8754, loss=0.4931]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 274/292 [03:01<00:10,  1.72batch/s, auc=0.8754, loss=0.6175]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 275/292 [03:01<00:09,  1.73batch/s, auc=0.8754, loss=0.6175]\u001b[A\n",
      "Training Epoch 9/25:  94%|█████████▍| 275/292 [03:02<00:09,  1.73batch/s, auc=0.8754, loss=0.5514]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 276/292 [03:02<00:09,  1.74batch/s, auc=0.8754, loss=0.5514]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 276/292 [03:02<00:09,  1.74batch/s, auc=0.8754, loss=0.7418]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 277/292 [03:02<00:08,  1.75batch/s, auc=0.8754, loss=0.7418]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▍| 277/292 [03:03<00:08,  1.75batch/s, auc=0.8755, loss=0.6610]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▌| 278/292 [03:03<00:07,  1.76batch/s, auc=0.8755, loss=0.6610]\u001b[A\n",
      "Training Epoch 9/25:  95%|█████████▌| 278/292 [03:04<00:07,  1.76batch/s, auc=0.8755, loss=0.7302]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 279/292 [03:04<00:07,  1.76batch/s, auc=0.8755, loss=0.7302]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 279/292 [03:04<00:07,  1.76batch/s, auc=0.8757, loss=0.6629]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 280/292 [03:04<00:06,  1.76batch/s, auc=0.8757, loss=0.6629]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 280/292 [03:05<00:06,  1.76batch/s, auc=0.8757, loss=0.6679]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 281/292 [03:05<00:06,  1.76batch/s, auc=0.8757, loss=0.6679]\u001b[A\n",
      "Training Epoch 9/25:  96%|█████████▌| 281/292 [03:06<00:06,  1.76batch/s, auc=0.8752, loss=1.3777]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 282/292 [03:06<00:07,  1.38batch/s, auc=0.8752, loss=1.3777]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 282/292 [03:06<00:07,  1.38batch/s, auc=0.8752, loss=0.6977]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 283/292 [03:06<00:06,  1.48batch/s, auc=0.8752, loss=0.6977]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 283/292 [03:07<00:06,  1.48batch/s, auc=0.8754, loss=0.6785]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 284/292 [03:07<00:05,  1.56batch/s, auc=0.8754, loss=0.6785]\u001b[A\n",
      "Training Epoch 9/25:  97%|█████████▋| 284/292 [03:08<00:05,  1.56batch/s, auc=0.8753, loss=0.7056]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.62batch/s, auc=0.8753, loss=0.7056]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.62batch/s, auc=0.8754, loss=0.7141]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.66batch/s, auc=0.8754, loss=0.7141]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 286/292 [03:09<00:03,  1.66batch/s, auc=0.8747, loss=1.4304]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 287/292 [03:09<00:03,  1.34batch/s, auc=0.8747, loss=1.4304]\u001b[A\n",
      "Training Epoch 9/25:  98%|█████████▊| 287/292 [03:10<00:03,  1.34batch/s, auc=0.8750, loss=0.5661]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.44batch/s, auc=0.8750, loss=0.5661]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.44batch/s, auc=0.8752, loss=0.5876]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.53batch/s, auc=0.8752, loss=0.5876]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 289/292 [03:11<00:01,  1.53batch/s, auc=0.8743, loss=1.5433]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.28batch/s, auc=0.8743, loss=1.5433]\u001b[A\n",
      "Training Epoch 9/25:  99%|█████████▉| 290/292 [03:12<00:01,  1.28batch/s, auc=0.8744, loss=0.8837]\u001b[A\n",
      "Training Epoch 9/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.39batch/s, auc=0.8744, loss=0.8837]\u001b[A\n",
      "Training Epoch 9/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.39batch/s, auc=0.8744, loss=0.7056]\u001b[A\n",
      "Training Epoch 9/25: 100%|██████████| 292/292 [03:12<00:00,  1.51batch/s, auc=0.8744, loss=0.7056]\u001b[A\n",
      "Epochs:  36%|███▌      | 9/25 [31:48<56:46, 212.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25] Train Loss: 0.7459 | Train AUROC: 0.8744 Val Loss: 0.8045 | Val AUROC: 0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 10/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8247, loss=0.7759]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 1/292 [00:01<09:29,  1.96s/batch, auc=0.8247, loss=0.7759]\u001b[A\n",
      "Training Epoch 10/25:   0%|          | 1/292 [00:03<09:29,  1.96s/batch, auc=0.7552, loss=0.7507]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 2/292 [00:03<06:56,  1.44s/batch, auc=0.7552, loss=0.7507]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 2/292 [00:03<06:56,  1.44s/batch, auc=0.8682, loss=0.5622]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 3/292 [00:03<04:58,  1.03s/batch, auc=0.8682, loss=0.5622]\u001b[A\n",
      "Training Epoch 10/25:   1%|          | 3/292 [00:04<04:58,  1.03s/batch, auc=0.8367, loss=0.8503]\u001b[A\n",
      "Training Epoch 10/25:   1%|▏         | 4/292 [00:04<05:01,  1.05s/batch, auc=0.8367, loss=0.8503]\u001b[A\n",
      "Training Epoch 10/25:   1%|▏         | 4/292 [00:05<05:01,  1.05s/batch, auc=0.8699, loss=0.6020]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 5/292 [00:05<04:09,  1.15batch/s, auc=0.8699, loss=0.6020]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 5/292 [00:05<04:09,  1.15batch/s, auc=0.8827, loss=0.6854]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 6/292 [00:05<03:38,  1.31batch/s, auc=0.8827, loss=0.6854]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 6/292 [00:06<03:38,  1.31batch/s, auc=0.8905, loss=0.5358]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 7/292 [00:06<03:17,  1.44batch/s, auc=0.8905, loss=0.5358]\u001b[A\n",
      "Training Epoch 10/25:   2%|▏         | 7/292 [00:06<03:17,  1.44batch/s, auc=0.8978, loss=0.4865]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 8/292 [00:06<03:04,  1.54batch/s, auc=0.8978, loss=0.4865]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 8/292 [00:07<03:04,  1.54batch/s, auc=0.9021, loss=0.5426]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 9/292 [00:07<02:55,  1.61batch/s, auc=0.9021, loss=0.5426]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 9/292 [00:07<02:55,  1.61batch/s, auc=0.9002, loss=0.6659]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 10/292 [00:07<02:49,  1.67batch/s, auc=0.9002, loss=0.6659]\u001b[A\n",
      "Training Epoch 10/25:   3%|▎         | 10/292 [00:08<02:49,  1.67batch/s, auc=0.8962, loss=0.6911]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 11/292 [00:08<02:44,  1.70batch/s, auc=0.8962, loss=0.6911]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 11/292 [00:09<02:44,  1.70batch/s, auc=0.8901, loss=0.7702]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 12/292 [00:09<02:41,  1.73batch/s, auc=0.8901, loss=0.7702]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 12/292 [00:09<02:41,  1.73batch/s, auc=0.8943, loss=0.6368]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 13/292 [00:09<02:39,  1.75batch/s, auc=0.8943, loss=0.6368]\u001b[A\n",
      "Training Epoch 10/25:   4%|▍         | 13/292 [00:10<02:39,  1.75batch/s, auc=0.8899, loss=0.7872]\u001b[A\n",
      "Training Epoch 10/25:   5%|▍         | 14/292 [00:10<02:37,  1.77batch/s, auc=0.8899, loss=0.7872]\u001b[A\n",
      "Training Epoch 10/25:   5%|▍         | 14/292 [00:10<02:37,  1.77batch/s, auc=0.8948, loss=0.4958]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 15/292 [00:10<02:35,  1.78batch/s, auc=0.8948, loss=0.4958]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 15/292 [00:11<02:35,  1.78batch/s, auc=0.8942, loss=0.5773]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 16/292 [00:11<02:34,  1.78batch/s, auc=0.8942, loss=0.5773]\u001b[A\n",
      "Training Epoch 10/25:   5%|▌         | 16/292 [00:11<02:34,  1.78batch/s, auc=0.8980, loss=0.5828]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 17/292 [00:11<02:33,  1.79batch/s, auc=0.8980, loss=0.5828]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 17/292 [00:12<02:33,  1.79batch/s, auc=0.8992, loss=0.5970]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 18/292 [00:12<02:32,  1.79batch/s, auc=0.8992, loss=0.5970]\u001b[A\n",
      "Training Epoch 10/25:   6%|▌         | 18/292 [00:13<02:32,  1.79batch/s, auc=0.8880, loss=1.4101]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 19/292 [00:13<03:14,  1.40batch/s, auc=0.8880, loss=1.4101]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 19/292 [00:14<03:14,  1.40batch/s, auc=0.8898, loss=0.5501]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 20/292 [00:14<03:01,  1.50batch/s, auc=0.8898, loss=0.5501]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 20/292 [00:14<03:01,  1.50batch/s, auc=0.8922, loss=0.6753]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 21/292 [00:14<02:51,  1.58batch/s, auc=0.8922, loss=0.6753]\u001b[A\n",
      "Training Epoch 10/25:   7%|▋         | 21/292 [00:15<02:51,  1.58batch/s, auc=0.8869, loss=1.1406]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 22/292 [00:15<02:44,  1.64batch/s, auc=0.8869, loss=1.1406]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 22/292 [00:15<02:44,  1.64batch/s, auc=0.8886, loss=0.6228]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 23/292 [00:15<02:39,  1.68batch/s, auc=0.8886, loss=0.6228]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 23/292 [00:16<02:39,  1.68batch/s, auc=0.8912, loss=0.5591]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 24/292 [00:16<02:35,  1.72batch/s, auc=0.8912, loss=0.5591]\u001b[A\n",
      "Training Epoch 10/25:   8%|▊         | 24/292 [00:16<02:35,  1.72batch/s, auc=0.8919, loss=0.7757]\u001b[A\n",
      "Training Epoch 10/25:   9%|▊         | 25/292 [00:16<02:33,  1.74batch/s, auc=0.8919, loss=0.7757]\u001b[A\n",
      "Training Epoch 10/25:   9%|▊         | 25/292 [00:17<02:33,  1.74batch/s, auc=0.8901, loss=0.8528]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 26/292 [00:17<02:31,  1.76batch/s, auc=0.8901, loss=0.8528]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 26/292 [00:18<02:31,  1.76batch/s, auc=0.8884, loss=0.7439]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 27/292 [00:18<03:10,  1.39batch/s, auc=0.8884, loss=0.7439]\u001b[A\n",
      "Training Epoch 10/25:   9%|▉         | 27/292 [00:19<03:10,  1.39batch/s, auc=0.8847, loss=1.0607]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 28/292 [00:19<02:56,  1.49batch/s, auc=0.8847, loss=1.0607]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 28/292 [00:19<02:56,  1.49batch/s, auc=0.8866, loss=0.5511]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 29/292 [00:19<02:47,  1.57batch/s, auc=0.8866, loss=0.5511]\u001b[A\n",
      "Training Epoch 10/25:  10%|▉         | 29/292 [00:20<02:47,  1.57batch/s, auc=0.8884, loss=0.5540]\u001b[A\n",
      "Training Epoch 10/25:  10%|█         | 30/292 [00:20<02:40,  1.63batch/s, auc=0.8884, loss=0.5540]\u001b[A\n",
      "Training Epoch 10/25:  10%|█         | 30/292 [00:20<02:40,  1.63batch/s, auc=0.8887, loss=0.6223]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 31/292 [00:20<02:35,  1.68batch/s, auc=0.8887, loss=0.6223]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 31/292 [00:21<02:35,  1.68batch/s, auc=0.8876, loss=0.5880]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 32/292 [00:21<02:31,  1.71batch/s, auc=0.8876, loss=0.5880]\u001b[A\n",
      "Training Epoch 10/25:  11%|█         | 32/292 [00:21<02:31,  1.71batch/s, auc=0.8881, loss=0.5791]\u001b[A\n",
      "Training Epoch 10/25:  11%|█▏        | 33/292 [00:21<02:28,  1.74batch/s, auc=0.8881, loss=0.5791]\u001b[A\n",
      "Training Epoch 10/25:  11%|█▏        | 33/292 [00:22<02:28,  1.74batch/s, auc=0.8906, loss=0.4231]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 34/292 [00:22<02:26,  1.76batch/s, auc=0.8906, loss=0.4231]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 34/292 [00:22<02:26,  1.76batch/s, auc=0.8900, loss=0.6327]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 35/292 [00:22<02:25,  1.77batch/s, auc=0.8900, loss=0.6327]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 35/292 [00:23<02:25,  1.77batch/s, auc=0.8853, loss=1.2397]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 36/292 [00:23<03:03,  1.39batch/s, auc=0.8853, loss=1.2397]\u001b[A\n",
      "Training Epoch 10/25:  12%|█▏        | 36/292 [00:24<03:03,  1.39batch/s, auc=0.8827, loss=1.1984]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 37/292 [00:24<02:50,  1.49batch/s, auc=0.8827, loss=1.1984]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 37/292 [00:25<02:50,  1.49batch/s, auc=0.8798, loss=0.9987]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 38/292 [00:25<03:20,  1.27batch/s, auc=0.8798, loss=0.9987]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 38/292 [00:26<03:20,  1.27batch/s, auc=0.8814, loss=0.6459]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 39/292 [00:26<03:02,  1.39batch/s, auc=0.8814, loss=0.6459]\u001b[A\n",
      "Training Epoch 10/25:  13%|█▎        | 39/292 [00:26<03:02,  1.39batch/s, auc=0.8830, loss=0.4455]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▎        | 40/292 [00:26<02:49,  1.49batch/s, auc=0.8830, loss=0.4455]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▎        | 40/292 [00:27<02:49,  1.49batch/s, auc=0.8796, loss=1.1152]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 41/292 [00:27<03:18,  1.26batch/s, auc=0.8796, loss=1.1152]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 41/292 [00:28<03:18,  1.26batch/s, auc=0.8796, loss=0.6558]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 42/292 [00:28<03:38,  1.14batch/s, auc=0.8796, loss=0.6558]\u001b[A\n",
      "Training Epoch 10/25:  14%|█▍        | 42/292 [00:29<03:38,  1.14batch/s, auc=0.8806, loss=0.5880]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▍        | 43/292 [00:29<03:14,  1.28batch/s, auc=0.8806, loss=0.5880]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▍        | 43/292 [00:29<03:14,  1.28batch/s, auc=0.8832, loss=0.5076]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 44/292 [00:29<02:56,  1.40batch/s, auc=0.8832, loss=0.5076]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 44/292 [00:30<02:56,  1.40batch/s, auc=0.8840, loss=0.5920]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 45/292 [00:30<02:44,  1.50batch/s, auc=0.8840, loss=0.5920]\u001b[A\n",
      "Training Epoch 10/25:  15%|█▌        | 45/292 [00:31<02:44,  1.50batch/s, auc=0.8845, loss=0.6506]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 46/292 [00:31<02:35,  1.58batch/s, auc=0.8845, loss=0.6506]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 46/292 [00:31<02:35,  1.58batch/s, auc=0.8816, loss=1.3161]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 47/292 [00:31<02:29,  1.64batch/s, auc=0.8816, loss=1.3161]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▌        | 47/292 [00:32<02:29,  1.64batch/s, auc=0.8818, loss=0.6376]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▋        | 48/292 [00:32<02:24,  1.68batch/s, auc=0.8818, loss=0.6376]\u001b[A\n",
      "Training Epoch 10/25:  16%|█▋        | 48/292 [00:32<02:24,  1.68batch/s, auc=0.8827, loss=0.5203]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 49/292 [00:32<02:21,  1.72batch/s, auc=0.8827, loss=0.5203]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 49/292 [00:33<02:21,  1.72batch/s, auc=0.8796, loss=1.1335]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 50/292 [00:33<02:56,  1.37batch/s, auc=0.8796, loss=1.1335]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 50/292 [00:34<02:56,  1.37batch/s, auc=0.8806, loss=0.6635]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 51/292 [00:34<02:43,  1.48batch/s, auc=0.8806, loss=0.6635]\u001b[A\n",
      "Training Epoch 10/25:  17%|█▋        | 51/292 [00:34<02:43,  1.48batch/s, auc=0.8818, loss=0.7607]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 52/292 [00:34<02:33,  1.56batch/s, auc=0.8818, loss=0.7607]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 52/292 [00:35<02:33,  1.56batch/s, auc=0.8824, loss=0.6430]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 53/292 [00:35<02:26,  1.63batch/s, auc=0.8824, loss=0.6430]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 53/292 [00:36<02:26,  1.63batch/s, auc=0.8810, loss=0.9774]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 54/292 [00:36<02:22,  1.67batch/s, auc=0.8810, loss=0.9774]\u001b[A\n",
      "Training Epoch 10/25:  18%|█▊        | 54/292 [00:36<02:22,  1.67batch/s, auc=0.8817, loss=0.5608]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 55/292 [00:36<02:18,  1.71batch/s, auc=0.8817, loss=0.5608]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 55/292 [00:37<02:18,  1.71batch/s, auc=0.8828, loss=0.5366]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 56/292 [00:37<02:16,  1.73batch/s, auc=0.8828, loss=0.5366]\u001b[A\n",
      "Training Epoch 10/25:  19%|█▉        | 56/292 [00:37<02:16,  1.73batch/s, auc=0.8841, loss=0.5205]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 57/292 [00:37<02:14,  1.75batch/s, auc=0.8841, loss=0.5205]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 57/292 [00:38<02:14,  1.75batch/s, auc=0.8847, loss=0.5547]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 58/292 [00:38<02:12,  1.76batch/s, auc=0.8847, loss=0.5547]\u001b[A\n",
      "Training Epoch 10/25:  20%|█▉        | 58/292 [00:38<02:12,  1.76batch/s, auc=0.8838, loss=0.8317]\u001b[A\n",
      "Training Epoch 10/25:  20%|██        | 59/292 [00:38<02:11,  1.77batch/s, auc=0.8838, loss=0.8317]\u001b[A\n",
      "Training Epoch 10/25:  20%|██        | 59/292 [00:39<02:11,  1.77batch/s, auc=0.8852, loss=0.5021]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 60/292 [00:39<02:10,  1.78batch/s, auc=0.8852, loss=0.5021]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 60/292 [00:39<02:10,  1.78batch/s, auc=0.8865, loss=0.5936]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 61/292 [00:39<02:09,  1.78batch/s, auc=0.8865, loss=0.5936]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 61/292 [00:40<02:09,  1.78batch/s, auc=0.8873, loss=0.5675]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 62/292 [00:40<02:08,  1.78batch/s, auc=0.8873, loss=0.5675]\u001b[A\n",
      "Training Epoch 10/25:  21%|██        | 62/292 [00:41<02:08,  1.78batch/s, auc=0.8881, loss=0.6307]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 63/292 [00:41<02:08,  1.79batch/s, auc=0.8881, loss=0.6307]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 63/292 [00:41<02:08,  1.79batch/s, auc=0.8869, loss=0.9520]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 64/292 [00:41<02:07,  1.79batch/s, auc=0.8869, loss=0.9520]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 64/292 [00:42<02:07,  1.79batch/s, auc=0.8871, loss=0.6841]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 65/292 [00:42<02:06,  1.79batch/s, auc=0.8871, loss=0.6841]\u001b[A\n",
      "Training Epoch 10/25:  22%|██▏       | 65/292 [00:42<02:06,  1.79batch/s, auc=0.8870, loss=0.7278]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 66/292 [00:42<02:06,  1.79batch/s, auc=0.8870, loss=0.7278]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 66/292 [00:43<02:06,  1.79batch/s, auc=0.8875, loss=0.5755]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 67/292 [00:43<02:05,  1.79batch/s, auc=0.8875, loss=0.5755]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 67/292 [00:43<02:05,  1.79batch/s, auc=0.8873, loss=0.6011]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 68/292 [00:43<02:05,  1.79batch/s, auc=0.8873, loss=0.6011]\u001b[A\n",
      "Training Epoch 10/25:  23%|██▎       | 68/292 [00:44<02:05,  1.79batch/s, auc=0.8871, loss=0.7672]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▎       | 69/292 [00:44<02:39,  1.40batch/s, auc=0.8871, loss=0.7672]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▎       | 69/292 [00:46<02:39,  1.40batch/s, auc=0.8860, loss=0.9337]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 70/292 [00:46<03:02,  1.22batch/s, auc=0.8860, loss=0.9337]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 70/292 [00:46<03:02,  1.22batch/s, auc=0.8860, loss=0.5834]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 71/292 [00:46<02:44,  1.35batch/s, auc=0.8860, loss=0.5834]\u001b[A\n",
      "Training Epoch 10/25:  24%|██▍       | 71/292 [00:47<02:44,  1.35batch/s, auc=0.8846, loss=1.1020]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▍       | 72/292 [00:47<03:05,  1.19batch/s, auc=0.8846, loss=1.1020]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▍       | 72/292 [00:48<03:05,  1.19batch/s, auc=0.8838, loss=0.7936]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 73/292 [00:48<02:45,  1.32batch/s, auc=0.8838, loss=0.7936]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 73/292 [00:48<02:45,  1.32batch/s, auc=0.8840, loss=0.6106]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 74/292 [00:48<02:32,  1.43batch/s, auc=0.8840, loss=0.6106]\u001b[A\n",
      "Training Epoch 10/25:  25%|██▌       | 74/292 [00:49<02:32,  1.43batch/s, auc=0.8837, loss=0.7043]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 75/292 [00:49<02:22,  1.52batch/s, auc=0.8837, loss=0.7043]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 75/292 [00:49<02:22,  1.52batch/s, auc=0.8836, loss=0.6445]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 76/292 [00:49<02:15,  1.60batch/s, auc=0.8836, loss=0.6445]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▌       | 76/292 [00:50<02:15,  1.60batch/s, auc=0.8857, loss=0.6000]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▋       | 77/292 [00:50<02:10,  1.65batch/s, auc=0.8857, loss=0.6000]\u001b[A\n",
      "Training Epoch 10/25:  26%|██▋       | 77/292 [00:50<02:10,  1.65batch/s, auc=0.8862, loss=0.7005]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 78/292 [00:50<02:06,  1.69batch/s, auc=0.8862, loss=0.7005]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 78/292 [00:51<02:06,  1.69batch/s, auc=0.8875, loss=0.5295]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 79/292 [00:51<02:04,  1.72batch/s, auc=0.8875, loss=0.5295]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 79/292 [00:52<02:04,  1.72batch/s, auc=0.8876, loss=0.6642]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 80/292 [00:52<02:02,  1.74batch/s, auc=0.8876, loss=0.6642]\u001b[A\n",
      "Training Epoch 10/25:  27%|██▋       | 80/292 [00:53<02:02,  1.74batch/s, auc=0.8869, loss=0.8229]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 81/292 [00:53<02:33,  1.38batch/s, auc=0.8869, loss=0.8229]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 81/292 [00:53<02:33,  1.38batch/s, auc=0.8869, loss=0.6881]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 82/292 [00:53<02:21,  1.48batch/s, auc=0.8869, loss=0.6881]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 82/292 [00:54<02:21,  1.48batch/s, auc=0.8874, loss=0.6724]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 83/292 [00:54<02:13,  1.56batch/s, auc=0.8874, loss=0.6724]\u001b[A\n",
      "Training Epoch 10/25:  28%|██▊       | 83/292 [00:54<02:13,  1.56batch/s, auc=0.8878, loss=0.5696]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 84/292 [00:54<02:07,  1.63batch/s, auc=0.8878, loss=0.5696]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 84/292 [00:55<02:07,  1.63batch/s, auc=0.8858, loss=1.2619]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 85/292 [00:55<02:35,  1.33batch/s, auc=0.8858, loss=1.2619]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 85/292 [00:56<02:35,  1.33batch/s, auc=0.8860, loss=0.4626]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 86/292 [00:56<02:23,  1.44batch/s, auc=0.8860, loss=0.4626]\u001b[A\n",
      "Training Epoch 10/25:  29%|██▉       | 86/292 [00:57<02:23,  1.44batch/s, auc=0.8855, loss=0.9514]\u001b[A\n",
      "Training Epoch 10/25:  30%|██▉       | 87/292 [00:57<02:14,  1.53batch/s, auc=0.8855, loss=0.9514]\u001b[A\n",
      "Training Epoch 10/25:  30%|██▉       | 87/292 [00:57<02:14,  1.53batch/s, auc=0.8844, loss=1.1948]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 88/292 [00:57<02:07,  1.60batch/s, auc=0.8844, loss=1.1948]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 88/292 [00:58<02:07,  1.60batch/s, auc=0.8822, loss=1.5110]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 89/292 [00:58<02:34,  1.32batch/s, auc=0.8822, loss=1.5110]\u001b[A\n",
      "Training Epoch 10/25:  30%|███       | 89/292 [00:59<02:34,  1.32batch/s, auc=0.8821, loss=0.7475]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 90/292 [00:59<02:21,  1.43batch/s, auc=0.8821, loss=0.7475]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 90/292 [00:59<02:21,  1.43batch/s, auc=0.8818, loss=0.7052]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 91/292 [00:59<02:12,  1.52batch/s, auc=0.8818, loss=0.7052]\u001b[A\n",
      "Training Epoch 10/25:  31%|███       | 91/292 [01:00<02:12,  1.52batch/s, auc=0.8817, loss=0.7772]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 92/292 [01:00<02:05,  1.59batch/s, auc=0.8817, loss=0.7772]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 92/292 [01:00<02:05,  1.59batch/s, auc=0.8819, loss=0.5576]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 93/292 [01:00<02:00,  1.65batch/s, auc=0.8819, loss=0.5576]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 93/292 [01:01<02:00,  1.65batch/s, auc=0.8829, loss=0.5189]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 94/292 [01:01<01:57,  1.69batch/s, auc=0.8829, loss=0.5189]\u001b[A\n",
      "Training Epoch 10/25:  32%|███▏      | 94/292 [01:02<01:57,  1.69batch/s, auc=0.8829, loss=0.5691]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 95/292 [01:02<01:54,  1.72batch/s, auc=0.8829, loss=0.5691]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 95/292 [01:02<01:54,  1.72batch/s, auc=0.8841, loss=0.6656]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 96/292 [01:02<01:52,  1.74batch/s, auc=0.8841, loss=0.6656]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 96/292 [01:03<01:52,  1.74batch/s, auc=0.8847, loss=0.6941]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 97/292 [01:03<01:51,  1.75batch/s, auc=0.8847, loss=0.6941]\u001b[A\n",
      "Training Epoch 10/25:  33%|███▎      | 97/292 [01:04<01:51,  1.75batch/s, auc=0.8829, loss=1.4009]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▎      | 98/292 [01:04<02:20,  1.38batch/s, auc=0.8829, loss=1.4009]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▎      | 98/292 [01:04<02:20,  1.38batch/s, auc=0.8835, loss=0.6266]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 99/292 [01:04<02:09,  1.49batch/s, auc=0.8835, loss=0.6266]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 99/292 [01:05<02:09,  1.49batch/s, auc=0.8824, loss=0.9690]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 100/292 [01:05<02:32,  1.26batch/s, auc=0.8824, loss=0.9690]\u001b[A\n",
      "Training Epoch 10/25:  34%|███▍      | 100/292 [01:06<02:32,  1.26batch/s, auc=0.8825, loss=0.6019]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 101/292 [01:06<02:18,  1.38batch/s, auc=0.8825, loss=0.6019]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 101/292 [01:06<02:18,  1.38batch/s, auc=0.8828, loss=0.5320]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 102/292 [01:06<02:07,  1.49batch/s, auc=0.8828, loss=0.5320]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▍      | 102/292 [01:07<02:07,  1.49batch/s, auc=0.8824, loss=0.7723]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▌      | 103/292 [01:07<02:00,  1.57batch/s, auc=0.8824, loss=0.7723]\u001b[A\n",
      "Training Epoch 10/25:  35%|███▌      | 103/292 [01:08<02:00,  1.57batch/s, auc=0.8830, loss=0.6225]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 104/292 [01:08<01:55,  1.63batch/s, auc=0.8830, loss=0.6225]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 104/292 [01:08<01:55,  1.63batch/s, auc=0.8831, loss=0.7868]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 105/292 [01:08<01:51,  1.67batch/s, auc=0.8831, loss=0.7868]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▌      | 105/292 [01:09<01:51,  1.67batch/s, auc=0.8826, loss=0.7146]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▋      | 106/292 [01:09<01:48,  1.71batch/s, auc=0.8826, loss=0.7146]\u001b[A\n",
      "Training Epoch 10/25:  36%|███▋      | 106/292 [01:10<01:48,  1.71batch/s, auc=0.8824, loss=0.8117]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 107/292 [01:10<02:15,  1.36batch/s, auc=0.8824, loss=0.8117]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 107/292 [01:10<02:15,  1.36batch/s, auc=0.8818, loss=0.7048]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 108/292 [01:10<02:05,  1.47batch/s, auc=0.8818, loss=0.7048]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 108/292 [01:11<02:05,  1.47batch/s, auc=0.8823, loss=0.5330]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 109/292 [01:11<01:57,  1.55batch/s, auc=0.8823, loss=0.5330]\u001b[A\n",
      "Training Epoch 10/25:  37%|███▋      | 109/292 [01:12<01:57,  1.55batch/s, auc=0.8801, loss=1.4957]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 110/292 [01:12<02:20,  1.29batch/s, auc=0.8801, loss=1.4957]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 110/292 [01:13<02:20,  1.29batch/s, auc=0.8795, loss=0.7505]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 111/292 [01:13<02:08,  1.41batch/s, auc=0.8795, loss=0.7505]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 111/292 [01:13<02:08,  1.41batch/s, auc=0.8801, loss=0.6135]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 112/292 [01:13<01:59,  1.51batch/s, auc=0.8801, loss=0.6135]\u001b[A\n",
      "Training Epoch 10/25:  38%|███▊      | 112/292 [01:14<01:59,  1.51batch/s, auc=0.8801, loss=0.6717]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▊      | 113/292 [01:14<02:21,  1.27batch/s, auc=0.8801, loss=0.6717]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▊      | 113/292 [01:15<02:21,  1.27batch/s, auc=0.8784, loss=1.2644]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 114/292 [01:15<02:35,  1.14batch/s, auc=0.8784, loss=1.2644]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 114/292 [01:16<02:35,  1.14batch/s, auc=0.8787, loss=0.5747]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 115/292 [01:16<02:18,  1.28batch/s, auc=0.8787, loss=0.5747]\u001b[A\n",
      "Training Epoch 10/25:  39%|███▉      | 115/292 [01:16<02:18,  1.28batch/s, auc=0.8789, loss=0.7185]\u001b[A\n",
      "Training Epoch 10/25:  40%|███▉      | 116/292 [01:16<02:05,  1.40batch/s, auc=0.8789, loss=0.7185]\u001b[A\n",
      "Training Epoch 10/25:  40%|███▉      | 116/292 [01:17<02:05,  1.40batch/s, auc=0.8775, loss=1.3505]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 117/292 [01:17<02:24,  1.21batch/s, auc=0.8775, loss=1.3505]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 117/292 [01:18<02:24,  1.21batch/s, auc=0.8778, loss=0.5655]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 118/292 [01:18<02:09,  1.34batch/s, auc=0.8778, loss=0.5655]\u001b[A\n",
      "Training Epoch 10/25:  40%|████      | 118/292 [01:19<02:09,  1.34batch/s, auc=0.8770, loss=1.0102]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 119/292 [01:19<02:25,  1.19batch/s, auc=0.8770, loss=1.0102]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 119/292 [01:20<02:25,  1.19batch/s, auc=0.8770, loss=0.6748]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 120/292 [01:20<02:37,  1.09batch/s, auc=0.8770, loss=0.6748]\u001b[A\n",
      "Training Epoch 10/25:  41%|████      | 120/292 [01:21<02:37,  1.09batch/s, auc=0.8773, loss=0.6216]\u001b[A\n",
      "Training Epoch 10/25:  41%|████▏     | 121/292 [01:21<02:17,  1.24batch/s, auc=0.8773, loss=0.6216]\u001b[A\n",
      "Training Epoch 10/25:  41%|████▏     | 121/292 [01:21<02:17,  1.24batch/s, auc=0.8774, loss=0.6228]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 122/292 [01:21<02:04,  1.36batch/s, auc=0.8774, loss=0.6228]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 122/292 [01:22<02:04,  1.36batch/s, auc=0.8761, loss=1.1488]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 123/292 [01:22<02:21,  1.20batch/s, auc=0.8761, loss=1.1488]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 123/292 [01:23<02:21,  1.20batch/s, auc=0.8754, loss=0.8968]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 124/292 [01:23<02:32,  1.10batch/s, auc=0.8754, loss=0.8968]\u001b[A\n",
      "Training Epoch 10/25:  42%|████▏     | 124/292 [01:24<02:32,  1.10batch/s, auc=0.8743, loss=1.1144]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 125/292 [01:24<02:14,  1.24batch/s, auc=0.8743, loss=1.1144]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 125/292 [01:25<02:14,  1.24batch/s, auc=0.8742, loss=0.7383]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 126/292 [01:25<02:01,  1.37batch/s, auc=0.8742, loss=0.7383]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 126/292 [01:25<02:01,  1.37batch/s, auc=0.8742, loss=0.7627]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 127/292 [01:25<01:52,  1.47batch/s, auc=0.8742, loss=0.7627]\u001b[A\n",
      "Training Epoch 10/25:  43%|████▎     | 127/292 [01:26<01:52,  1.47batch/s, auc=0.8742, loss=0.7499]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 128/292 [01:26<01:45,  1.56batch/s, auc=0.8742, loss=0.7499]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 128/292 [01:26<01:45,  1.56batch/s, auc=0.8743, loss=0.6926]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 129/292 [01:26<01:40,  1.62batch/s, auc=0.8743, loss=0.6926]\u001b[A\n",
      "Training Epoch 10/25:  44%|████▍     | 129/292 [01:27<01:40,  1.62batch/s, auc=0.8737, loss=1.0463]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 130/292 [01:27<01:37,  1.66batch/s, auc=0.8737, loss=1.0463]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 130/292 [01:27<01:37,  1.66batch/s, auc=0.8734, loss=0.8697]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 131/292 [01:27<01:34,  1.70batch/s, auc=0.8734, loss=0.8697]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▍     | 131/292 [01:28<01:34,  1.70batch/s, auc=0.8740, loss=0.4833]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▌     | 132/292 [01:28<01:32,  1.72batch/s, auc=0.8740, loss=0.4833]\u001b[A\n",
      "Training Epoch 10/25:  45%|████▌     | 132/292 [01:29<01:32,  1.72batch/s, auc=0.8733, loss=0.9785]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 133/292 [01:29<01:56,  1.37batch/s, auc=0.8733, loss=0.9785]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 133/292 [01:30<01:56,  1.37batch/s, auc=0.8724, loss=1.0701]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 134/292 [01:30<02:11,  1.20batch/s, auc=0.8724, loss=1.0701]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 134/292 [01:31<02:11,  1.20batch/s, auc=0.8728, loss=0.6121]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 135/292 [01:31<01:57,  1.33batch/s, auc=0.8728, loss=0.6121]\u001b[A\n",
      "Training Epoch 10/25:  46%|████▌     | 135/292 [01:31<01:57,  1.33batch/s, auc=0.8732, loss=0.5680]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 136/292 [01:31<01:48,  1.44batch/s, auc=0.8732, loss=0.5680]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 136/292 [01:32<01:48,  1.44batch/s, auc=0.8730, loss=0.8558]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 137/292 [01:32<01:41,  1.53batch/s, auc=0.8730, loss=0.8558]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 137/292 [01:32<01:41,  1.53batch/s, auc=0.8726, loss=0.8450]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 138/292 [01:32<01:36,  1.60batch/s, auc=0.8726, loss=0.8450]\u001b[A\n",
      "Training Epoch 10/25:  47%|████▋     | 138/292 [01:33<01:36,  1.60batch/s, auc=0.8719, loss=1.0802]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 139/292 [01:33<01:56,  1.31batch/s, auc=0.8719, loss=1.0802]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 139/292 [01:34<01:56,  1.31batch/s, auc=0.8719, loss=0.9798]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 140/292 [01:34<01:46,  1.43batch/s, auc=0.8719, loss=0.9798]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 140/292 [01:35<01:46,  1.43batch/s, auc=0.8721, loss=0.5398]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 141/292 [01:35<01:39,  1.52batch/s, auc=0.8721, loss=0.5398]\u001b[A\n",
      "Training Epoch 10/25:  48%|████▊     | 141/292 [01:35<01:39,  1.52batch/s, auc=0.8719, loss=0.9764]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▊     | 142/292 [01:35<01:34,  1.59batch/s, auc=0.8719, loss=0.9764]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▊     | 142/292 [01:36<01:34,  1.59batch/s, auc=0.8721, loss=0.7177]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 143/292 [01:36<01:30,  1.64batch/s, auc=0.8721, loss=0.7177]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 143/292 [01:37<01:30,  1.64batch/s, auc=0.8712, loss=1.1805]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 144/292 [01:37<01:50,  1.34batch/s, auc=0.8712, loss=1.1805]\u001b[A\n",
      "Training Epoch 10/25:  49%|████▉     | 144/292 [01:37<01:50,  1.34batch/s, auc=0.8712, loss=0.9204]\u001b[A\n",
      "Training Epoch 10/25:  50%|████▉     | 145/292 [01:37<01:41,  1.45batch/s, auc=0.8712, loss=0.9204]\u001b[A\n",
      "Training Epoch 10/25:  50%|████▉     | 145/292 [01:38<01:41,  1.45batch/s, auc=0.8713, loss=0.7579]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 146/292 [01:38<01:35,  1.53batch/s, auc=0.8713, loss=0.7579]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 146/292 [01:38<01:35,  1.53batch/s, auc=0.8714, loss=0.7163]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 147/292 [01:38<01:30,  1.60batch/s, auc=0.8714, loss=0.7163]\u001b[A\n",
      "Training Epoch 10/25:  50%|█████     | 147/292 [01:39<01:30,  1.60batch/s, auc=0.8718, loss=0.6366]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 148/292 [01:39<01:27,  1.65batch/s, auc=0.8718, loss=0.6366]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 148/292 [01:39<01:27,  1.65batch/s, auc=0.8721, loss=0.6458]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 149/292 [01:40<01:24,  1.69batch/s, auc=0.8721, loss=0.6458]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████     | 149/292 [01:41<01:24,  1.69batch/s, auc=0.8715, loss=1.0094]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████▏    | 150/292 [01:41<01:44,  1.36batch/s, auc=0.8715, loss=1.0094]\u001b[A\n",
      "Training Epoch 10/25:  51%|█████▏    | 150/292 [01:41<01:44,  1.36batch/s, auc=0.8720, loss=0.8121]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 151/292 [01:41<01:36,  1.46batch/s, auc=0.8720, loss=0.8121]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 151/292 [01:42<01:36,  1.46batch/s, auc=0.8724, loss=0.5811]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 152/292 [01:42<01:30,  1.55batch/s, auc=0.8724, loss=0.5811]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 152/292 [01:43<01:30,  1.55batch/s, auc=0.8719, loss=0.9910]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 153/292 [01:43<01:47,  1.29batch/s, auc=0.8719, loss=0.9910]\u001b[A\n",
      "Training Epoch 10/25:  52%|█████▏    | 153/292 [01:43<01:47,  1.29batch/s, auc=0.8723, loss=0.5782]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 154/292 [01:43<01:38,  1.41batch/s, auc=0.8723, loss=0.5782]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 154/292 [01:44<01:38,  1.41batch/s, auc=0.8726, loss=0.5467]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 155/292 [01:44<01:31,  1.50batch/s, auc=0.8726, loss=0.5467]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 155/292 [01:45<01:31,  1.50batch/s, auc=0.8721, loss=0.8112]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 156/292 [01:45<01:47,  1.27batch/s, auc=0.8721, loss=0.8112]\u001b[A\n",
      "Training Epoch 10/25:  53%|█████▎    | 156/292 [01:46<01:47,  1.27batch/s, auc=0.8720, loss=0.7407]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 157/292 [01:46<01:37,  1.39batch/s, auc=0.8720, loss=0.7407]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 157/292 [01:46<01:37,  1.39batch/s, auc=0.8720, loss=0.5833]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 158/292 [01:46<01:30,  1.49batch/s, auc=0.8720, loss=0.5833]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 158/292 [01:47<01:30,  1.49batch/s, auc=0.8720, loss=0.6456]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 159/292 [01:47<01:24,  1.56batch/s, auc=0.8720, loss=0.6456]\u001b[A\n",
      "Training Epoch 10/25:  54%|█████▍    | 159/292 [01:47<01:24,  1.56batch/s, auc=0.8712, loss=1.0143]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▍    | 160/292 [01:47<01:21,  1.62batch/s, auc=0.8712, loss=1.0143]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▍    | 160/292 [01:48<01:21,  1.62batch/s, auc=0.8714, loss=0.7060]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 161/292 [01:48<01:18,  1.67batch/s, auc=0.8714, loss=0.7060]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 161/292 [01:48<01:18,  1.67batch/s, auc=0.8716, loss=0.6577]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 162/292 [01:48<01:16,  1.70batch/s, auc=0.8716, loss=0.6577]\u001b[A\n",
      "Training Epoch 10/25:  55%|█████▌    | 162/292 [01:49<01:16,  1.70batch/s, auc=0.8719, loss=0.6341]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 163/292 [01:49<01:14,  1.73batch/s, auc=0.8719, loss=0.6341]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 163/292 [01:49<01:14,  1.73batch/s, auc=0.8725, loss=0.5586]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 164/292 [01:49<01:13,  1.74batch/s, auc=0.8725, loss=0.5586]\u001b[A\n",
      "Training Epoch 10/25:  56%|█████▌    | 164/292 [01:50<01:13,  1.74batch/s, auc=0.8728, loss=0.5378]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 165/292 [01:50<01:12,  1.76batch/s, auc=0.8728, loss=0.5378]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 165/292 [01:51<01:12,  1.76batch/s, auc=0.8727, loss=0.6540]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 166/292 [01:51<01:11,  1.76batch/s, auc=0.8727, loss=0.6540]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 166/292 [01:52<01:11,  1.76batch/s, auc=0.8726, loss=0.7914]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 167/292 [01:52<01:29,  1.39batch/s, auc=0.8726, loss=0.7914]\u001b[A\n",
      "Training Epoch 10/25:  57%|█████▋    | 167/292 [01:53<01:29,  1.39batch/s, auc=0.8722, loss=0.8430]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 168/292 [01:53<01:42,  1.21batch/s, auc=0.8722, loss=0.8430]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 168/292 [01:53<01:42,  1.21batch/s, auc=0.8728, loss=0.6795]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 169/292 [01:53<01:32,  1.34batch/s, auc=0.8728, loss=0.6795]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 169/292 [01:54<01:32,  1.34batch/s, auc=0.8719, loss=1.0947]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 170/292 [01:54<01:43,  1.18batch/s, auc=0.8719, loss=1.0947]\u001b[A\n",
      "Training Epoch 10/25:  58%|█████▊    | 170/292 [01:55<01:43,  1.18batch/s, auc=0.8721, loss=0.6406]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▊    | 171/292 [01:55<01:32,  1.31batch/s, auc=0.8721, loss=0.6406]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▊    | 171/292 [01:55<01:32,  1.31batch/s, auc=0.8724, loss=0.5656]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 172/292 [01:55<01:24,  1.42batch/s, auc=0.8724, loss=0.5656]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 172/292 [01:56<01:24,  1.42batch/s, auc=0.8726, loss=0.6299]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 173/292 [01:56<01:18,  1.52batch/s, auc=0.8726, loss=0.6299]\u001b[A\n",
      "Training Epoch 10/25:  59%|█████▉    | 173/292 [01:57<01:18,  1.52batch/s, auc=0.8729, loss=0.5302]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 174/292 [01:57<01:14,  1.59batch/s, auc=0.8729, loss=0.5302]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 174/292 [01:57<01:14,  1.59batch/s, auc=0.8726, loss=1.0261]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 175/292 [01:57<01:11,  1.64batch/s, auc=0.8726, loss=1.0261]\u001b[A\n",
      "Training Epoch 10/25:  60%|█████▉    | 175/292 [01:58<01:11,  1.64batch/s, auc=0.8727, loss=0.6808]\u001b[A\n",
      "Training Epoch 10/25:  60%|██████    | 176/292 [01:58<01:09,  1.68batch/s, auc=0.8727, loss=0.6808]\u001b[A\n",
      "Training Epoch 10/25:  60%|██████    | 176/292 [01:58<01:09,  1.68batch/s, auc=0.8731, loss=0.6024]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 177/292 [01:58<01:07,  1.71batch/s, auc=0.8731, loss=0.6024]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 177/292 [01:59<01:07,  1.71batch/s, auc=0.8734, loss=0.4690]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 178/292 [01:59<01:05,  1.73batch/s, auc=0.8734, loss=0.4690]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████    | 178/292 [02:00<01:05,  1.73batch/s, auc=0.8727, loss=1.1684]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████▏   | 179/292 [02:00<01:22,  1.37batch/s, auc=0.8727, loss=1.1684]\u001b[A\n",
      "Training Epoch 10/25:  61%|██████▏   | 179/292 [02:01<01:22,  1.37batch/s, auc=0.8724, loss=0.8348]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 180/292 [02:01<01:33,  1.20batch/s, auc=0.8724, loss=0.8348]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 180/292 [02:02<01:33,  1.20batch/s, auc=0.8724, loss=0.6173]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 181/292 [02:02<01:23,  1.33batch/s, auc=0.8724, loss=0.6173]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 181/292 [02:02<01:23,  1.33batch/s, auc=0.8726, loss=0.6992]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 182/292 [02:02<01:16,  1.44batch/s, auc=0.8726, loss=0.6992]\u001b[A\n",
      "Training Epoch 10/25:  62%|██████▏   | 182/292 [02:03<01:16,  1.44batch/s, auc=0.8728, loss=0.6095]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 183/292 [02:03<01:11,  1.52batch/s, auc=0.8728, loss=0.6095]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 183/292 [02:03<01:11,  1.52batch/s, auc=0.8730, loss=0.6022]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 184/292 [02:03<01:07,  1.59batch/s, auc=0.8730, loss=0.6022]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 184/292 [02:04<01:07,  1.59batch/s, auc=0.8729, loss=0.7333]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 185/292 [02:04<01:05,  1.64batch/s, auc=0.8729, loss=0.7333]\u001b[A\n",
      "Training Epoch 10/25:  63%|██████▎   | 185/292 [02:04<01:05,  1.64batch/s, auc=0.8726, loss=0.6498]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▎   | 186/292 [02:04<01:02,  1.68batch/s, auc=0.8726, loss=0.6498]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▎   | 186/292 [02:05<01:02,  1.68batch/s, auc=0.8727, loss=0.5905]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 187/292 [02:05<01:01,  1.71batch/s, auc=0.8727, loss=0.5905]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 187/292 [02:06<01:01,  1.71batch/s, auc=0.8729, loss=0.6852]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 188/292 [02:06<01:00,  1.73batch/s, auc=0.8729, loss=0.6852]\u001b[A\n",
      "Training Epoch 10/25:  64%|██████▍   | 188/292 [02:06<01:00,  1.73batch/s, auc=0.8729, loss=0.8288]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▍   | 189/292 [02:06<00:59,  1.74batch/s, auc=0.8729, loss=0.8288]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▍   | 189/292 [02:07<00:59,  1.74batch/s, auc=0.8731, loss=0.5736]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 190/292 [02:07<00:58,  1.75batch/s, auc=0.8731, loss=0.5736]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 190/292 [02:07<00:58,  1.75batch/s, auc=0.8731, loss=0.7654]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 191/292 [02:07<00:57,  1.76batch/s, auc=0.8731, loss=0.7654]\u001b[A\n",
      "Training Epoch 10/25:  65%|██████▌   | 191/292 [02:08<00:57,  1.76batch/s, auc=0.8732, loss=0.5804]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 192/292 [02:08<00:56,  1.77batch/s, auc=0.8732, loss=0.5804]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 192/292 [02:09<00:56,  1.77batch/s, auc=0.8727, loss=0.9951]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 193/292 [02:09<01:11,  1.39batch/s, auc=0.8727, loss=0.9951]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▌   | 193/292 [02:09<01:11,  1.39batch/s, auc=0.8726, loss=0.7572]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▋   | 194/292 [02:09<01:05,  1.49batch/s, auc=0.8726, loss=0.7572]\u001b[A\n",
      "Training Epoch 10/25:  66%|██████▋   | 194/292 [02:11<01:05,  1.49batch/s, auc=0.8721, loss=1.0907]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 195/292 [02:11<01:17,  1.26batch/s, auc=0.8721, loss=1.0907]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 195/292 [02:11<01:17,  1.26batch/s, auc=0.8722, loss=0.6131]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 196/292 [02:11<01:09,  1.38batch/s, auc=0.8722, loss=0.6131]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 196/292 [02:12<01:09,  1.38batch/s, auc=0.8719, loss=1.0899]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 197/292 [02:12<01:04,  1.48batch/s, auc=0.8719, loss=1.0899]\u001b[A\n",
      "Training Epoch 10/25:  67%|██████▋   | 197/292 [02:13<01:04,  1.48batch/s, auc=0.8711, loss=1.4031]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 198/292 [02:13<01:14,  1.25batch/s, auc=0.8711, loss=1.4031]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 198/292 [02:13<01:14,  1.25batch/s, auc=0.8713, loss=0.5579]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 199/292 [02:13<01:07,  1.38batch/s, auc=0.8713, loss=0.5579]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 199/292 [02:14<01:07,  1.38batch/s, auc=0.8714, loss=0.6903]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 200/292 [02:14<01:02,  1.48batch/s, auc=0.8714, loss=0.6903]\u001b[A\n",
      "Training Epoch 10/25:  68%|██████▊   | 200/292 [02:14<01:02,  1.48batch/s, auc=0.8714, loss=0.5288]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 201/292 [02:14<00:58,  1.56batch/s, auc=0.8714, loss=0.5288]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 201/292 [02:15<00:58,  1.56batch/s, auc=0.8709, loss=1.0955]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 202/292 [02:15<00:55,  1.62batch/s, auc=0.8709, loss=1.0955]\u001b[A\n",
      "Training Epoch 10/25:  69%|██████▉   | 202/292 [02:16<00:55,  1.62batch/s, auc=0.8712, loss=0.5389]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 203/292 [02:16<00:53,  1.66batch/s, auc=0.8712, loss=0.5389]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 203/292 [02:16<00:53,  1.66batch/s, auc=0.8712, loss=0.7767]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 204/292 [02:16<00:51,  1.69batch/s, auc=0.8712, loss=0.7767]\u001b[A\n",
      "Training Epoch 10/25:  70%|██████▉   | 204/292 [02:17<00:51,  1.69batch/s, auc=0.8708, loss=1.0995]\u001b[A\n",
      "Training Epoch 10/25:  70%|███████   | 205/292 [02:17<01:04,  1.35batch/s, auc=0.8708, loss=1.0995]\u001b[A\n",
      "Training Epoch 10/25:  70%|███████   | 205/292 [02:18<01:04,  1.35batch/s, auc=0.8710, loss=0.4996]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 206/292 [02:18<00:59,  1.46batch/s, auc=0.8710, loss=0.4996]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 206/292 [02:18<00:59,  1.46batch/s, auc=0.8713, loss=0.5833]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 207/292 [02:18<00:55,  1.54batch/s, auc=0.8713, loss=0.5833]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 207/292 [02:19<00:55,  1.54batch/s, auc=0.8714, loss=0.7357]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 208/292 [02:19<00:52,  1.60batch/s, auc=0.8714, loss=0.7357]\u001b[A\n",
      "Training Epoch 10/25:  71%|███████   | 208/292 [02:19<00:52,  1.60batch/s, auc=0.8714, loss=0.7217]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 209/292 [02:19<00:50,  1.65batch/s, auc=0.8714, loss=0.7217]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 209/292 [02:20<00:50,  1.65batch/s, auc=0.8715, loss=0.6493]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 210/292 [02:20<00:48,  1.69batch/s, auc=0.8715, loss=0.6493]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 210/292 [02:21<00:48,  1.69batch/s, auc=0.8718, loss=0.5419]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 211/292 [02:21<00:47,  1.71batch/s, auc=0.8718, loss=0.5419]\u001b[A\n",
      "Training Epoch 10/25:  72%|███████▏  | 211/292 [02:21<00:47,  1.71batch/s, auc=0.8717, loss=0.7840]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 212/292 [02:21<00:46,  1.73batch/s, auc=0.8717, loss=0.7840]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 212/292 [02:22<00:46,  1.73batch/s, auc=0.8716, loss=0.7038]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 213/292 [02:22<00:45,  1.74batch/s, auc=0.8716, loss=0.7038]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 213/292 [02:22<00:45,  1.74batch/s, auc=0.8723, loss=0.4261]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 214/292 [02:22<00:44,  1.75batch/s, auc=0.8723, loss=0.4261]\u001b[A\n",
      "Training Epoch 10/25:  73%|███████▎  | 214/292 [02:23<00:44,  1.75batch/s, auc=0.8725, loss=0.6186]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▎  | 215/292 [02:23<00:43,  1.76batch/s, auc=0.8725, loss=0.6186]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▎  | 215/292 [02:23<00:43,  1.76batch/s, auc=0.8724, loss=0.7864]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 216/292 [02:23<00:43,  1.76batch/s, auc=0.8724, loss=0.7864]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 216/292 [02:24<00:43,  1.76batch/s, auc=0.8723, loss=0.7824]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 217/292 [02:24<00:42,  1.77batch/s, auc=0.8723, loss=0.7824]\u001b[A\n",
      "Training Epoch 10/25:  74%|███████▍  | 217/292 [02:25<00:42,  1.77batch/s, auc=0.8722, loss=0.8683]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▍  | 218/292 [02:25<00:53,  1.39batch/s, auc=0.8722, loss=0.8683]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▍  | 218/292 [02:26<00:53,  1.39batch/s, auc=0.8726, loss=0.6220]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 219/292 [02:26<00:49,  1.49batch/s, auc=0.8726, loss=0.6220]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 219/292 [02:26<00:49,  1.49batch/s, auc=0.8729, loss=0.6340]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 220/292 [02:26<00:46,  1.56batch/s, auc=0.8729, loss=0.6340]\u001b[A\n",
      "Training Epoch 10/25:  75%|███████▌  | 220/292 [02:27<00:46,  1.56batch/s, auc=0.8731, loss=0.7259]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 221/292 [02:27<00:43,  1.62batch/s, auc=0.8731, loss=0.7259]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 221/292 [02:27<00:43,  1.62batch/s, auc=0.8733, loss=0.6795]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 222/292 [02:27<00:42,  1.66batch/s, auc=0.8733, loss=0.6795]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▌  | 222/292 [02:28<00:42,  1.66batch/s, auc=0.8730, loss=0.9661]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▋  | 223/292 [02:28<00:51,  1.34batch/s, auc=0.8730, loss=0.9661]\u001b[A\n",
      "Training Epoch 10/25:  76%|███████▋  | 223/292 [02:29<00:51,  1.34batch/s, auc=0.8734, loss=0.5746]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 224/292 [02:29<00:46,  1.45batch/s, auc=0.8734, loss=0.5746]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 224/292 [02:29<00:46,  1.45batch/s, auc=0.8734, loss=0.7283]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 225/292 [02:29<00:43,  1.53batch/s, auc=0.8734, loss=0.7283]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 225/292 [02:30<00:43,  1.53batch/s, auc=0.8735, loss=0.6072]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 226/292 [02:30<00:41,  1.60batch/s, auc=0.8735, loss=0.6072]\u001b[A\n",
      "Training Epoch 10/25:  77%|███████▋  | 226/292 [02:31<00:41,  1.60batch/s, auc=0.8727, loss=1.2604]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 227/292 [02:31<00:49,  1.31batch/s, auc=0.8727, loss=1.2604]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 227/292 [02:32<00:49,  1.31batch/s, auc=0.8729, loss=0.7015]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 228/292 [02:32<00:45,  1.42batch/s, auc=0.8729, loss=0.7015]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 228/292 [02:32<00:45,  1.42batch/s, auc=0.8732, loss=0.6602]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 229/292 [02:32<00:41,  1.51batch/s, auc=0.8732, loss=0.6602]\u001b[A\n",
      "Training Epoch 10/25:  78%|███████▊  | 229/292 [02:33<00:41,  1.51batch/s, auc=0.8730, loss=0.8853]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 230/292 [02:33<00:39,  1.58batch/s, auc=0.8730, loss=0.8853]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 230/292 [02:34<00:39,  1.58batch/s, auc=0.8719, loss=1.8086]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 231/292 [02:34<00:46,  1.30batch/s, auc=0.8719, loss=1.8086]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 231/292 [02:34<00:46,  1.30batch/s, auc=0.8720, loss=0.6241]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 232/292 [02:34<00:42,  1.42batch/s, auc=0.8720, loss=0.6241]\u001b[A\n",
      "Training Epoch 10/25:  79%|███████▉  | 232/292 [02:35<00:42,  1.42batch/s, auc=0.8720, loss=0.6518]\u001b[A\n",
      "Training Epoch 10/25:  80%|███████▉  | 233/292 [02:35<00:39,  1.51batch/s, auc=0.8720, loss=0.6518]\u001b[A\n",
      "Training Epoch 10/25:  80%|███████▉  | 233/292 [02:36<00:39,  1.51batch/s, auc=0.8718, loss=0.7055]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 234/292 [02:36<00:36,  1.58batch/s, auc=0.8718, loss=0.7055]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 234/292 [02:36<00:36,  1.58batch/s, auc=0.8719, loss=0.6316]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 235/292 [02:36<00:34,  1.64batch/s, auc=0.8719, loss=0.6316]\u001b[A\n",
      "Training Epoch 10/25:  80%|████████  | 235/292 [02:37<00:34,  1.64batch/s, auc=0.8721, loss=0.6917]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 236/292 [02:37<00:33,  1.67batch/s, auc=0.8721, loss=0.6917]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 236/292 [02:37<00:33,  1.67batch/s, auc=0.8718, loss=0.8673]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 237/292 [02:37<00:32,  1.70batch/s, auc=0.8718, loss=0.8673]\u001b[A\n",
      "Training Epoch 10/25:  81%|████████  | 237/292 [02:38<00:32,  1.70batch/s, auc=0.8718, loss=0.5926]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 238/292 [02:38<00:31,  1.72batch/s, auc=0.8718, loss=0.5926]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 238/292 [02:38<00:31,  1.72batch/s, auc=0.8721, loss=0.5403]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 239/292 [02:38<00:30,  1.74batch/s, auc=0.8721, loss=0.5403]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 239/292 [02:39<00:30,  1.74batch/s, auc=0.8724, loss=0.5743]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 240/292 [02:39<00:29,  1.75batch/s, auc=0.8724, loss=0.5743]\u001b[A\n",
      "Training Epoch 10/25:  82%|████████▏ | 240/292 [02:40<00:29,  1.75batch/s, auc=0.8728, loss=0.4642]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 241/292 [02:40<00:29,  1.76batch/s, auc=0.8728, loss=0.4642]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 241/292 [02:41<00:29,  1.76batch/s, auc=0.8720, loss=1.4719]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 242/292 [02:41<00:36,  1.38batch/s, auc=0.8720, loss=1.4719]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 242/292 [02:41<00:36,  1.38batch/s, auc=0.8720, loss=0.6664]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 243/292 [02:41<00:33,  1.48batch/s, auc=0.8720, loss=0.6664]\u001b[A\n",
      "Training Epoch 10/25:  83%|████████▎ | 243/292 [02:42<00:33,  1.48batch/s, auc=0.8717, loss=0.7364]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▎ | 244/292 [02:42<00:30,  1.56batch/s, auc=0.8717, loss=0.7364]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▎ | 244/292 [02:43<00:30,  1.56batch/s, auc=0.8711, loss=1.2344]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 245/292 [02:43<00:36,  1.29batch/s, auc=0.8711, loss=1.2344]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 245/292 [02:43<00:36,  1.29batch/s, auc=0.8711, loss=0.6975]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 246/292 [02:43<00:32,  1.41batch/s, auc=0.8711, loss=0.6975]\u001b[A\n",
      "Training Epoch 10/25:  84%|████████▍ | 246/292 [02:44<00:32,  1.41batch/s, auc=0.8713, loss=0.4957]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 247/292 [02:44<00:30,  1.50batch/s, auc=0.8713, loss=0.4957]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 247/292 [02:45<00:30,  1.50batch/s, auc=0.8713, loss=0.7169]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 248/292 [02:45<00:28,  1.57batch/s, auc=0.8713, loss=0.7169]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▍ | 248/292 [02:45<00:28,  1.57batch/s, auc=0.8715, loss=0.8952]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▌ | 249/292 [02:45<00:26,  1.62batch/s, auc=0.8715, loss=0.8952]\u001b[A\n",
      "Training Epoch 10/25:  85%|████████▌ | 249/292 [02:46<00:26,  1.62batch/s, auc=0.8713, loss=0.8201]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 250/292 [02:46<00:25,  1.67batch/s, auc=0.8713, loss=0.8201]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 250/292 [02:46<00:25,  1.67batch/s, auc=0.8712, loss=0.6825]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 251/292 [02:46<00:24,  1.70batch/s, auc=0.8712, loss=0.6825]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▌ | 251/292 [02:47<00:24,  1.70batch/s, auc=0.8714, loss=0.6045]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▋ | 252/292 [02:47<00:23,  1.72batch/s, auc=0.8714, loss=0.6045]\u001b[A\n",
      "Training Epoch 10/25:  86%|████████▋ | 252/292 [02:47<00:23,  1.72batch/s, auc=0.8717, loss=0.5283]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 253/292 [02:47<00:22,  1.73batch/s, auc=0.8717, loss=0.5283]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 253/292 [02:48<00:22,  1.73batch/s, auc=0.8716, loss=0.6586]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 254/292 [02:48<00:21,  1.74batch/s, auc=0.8716, loss=0.6586]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 254/292 [02:48<00:21,  1.74batch/s, auc=0.8718, loss=0.5994]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 255/292 [02:48<00:21,  1.75batch/s, auc=0.8718, loss=0.5994]\u001b[A\n",
      "Training Epoch 10/25:  87%|████████▋ | 255/292 [02:49<00:21,  1.75batch/s, auc=0.8717, loss=0.7878]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 256/292 [02:49<00:20,  1.75batch/s, auc=0.8717, loss=0.7878]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 256/292 [02:50<00:20,  1.75batch/s, auc=0.8717, loss=0.6426]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 257/292 [02:50<00:19,  1.76batch/s, auc=0.8717, loss=0.6426]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 257/292 [02:50<00:19,  1.76batch/s, auc=0.8716, loss=0.7496]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 258/292 [02:50<00:19,  1.76batch/s, auc=0.8716, loss=0.7496]\u001b[A\n",
      "Training Epoch 10/25:  88%|████████▊ | 258/292 [02:51<00:19,  1.76batch/s, auc=0.8719, loss=0.6283]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▊ | 259/292 [02:51<00:18,  1.76batch/s, auc=0.8719, loss=0.6283]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▊ | 259/292 [02:51<00:18,  1.76batch/s, auc=0.8718, loss=0.7885]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 260/292 [02:51<00:18,  1.77batch/s, auc=0.8718, loss=0.7885]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 260/292 [02:52<00:18,  1.77batch/s, auc=0.8720, loss=0.4838]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 261/292 [02:52<00:17,  1.77batch/s, auc=0.8720, loss=0.4838]\u001b[A\n",
      "Training Epoch 10/25:  89%|████████▉ | 261/292 [02:52<00:17,  1.77batch/s, auc=0.8720, loss=0.9246]\u001b[A\n",
      "Training Epoch 10/25:  90%|████████▉ | 262/292 [02:52<00:16,  1.77batch/s, auc=0.8720, loss=0.9246]\u001b[A\n",
      "Training Epoch 10/25:  90%|████████▉ | 262/292 [02:53<00:16,  1.77batch/s, auc=0.8723, loss=0.6898]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 263/292 [02:53<00:16,  1.77batch/s, auc=0.8723, loss=0.6898]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 263/292 [02:54<00:16,  1.77batch/s, auc=0.8713, loss=1.6591]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 264/292 [02:54<00:20,  1.39batch/s, auc=0.8713, loss=1.6591]\u001b[A\n",
      "Training Epoch 10/25:  90%|█████████ | 264/292 [02:55<00:20,  1.39batch/s, auc=0.8710, loss=0.9731]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 265/292 [02:55<00:22,  1.21batch/s, auc=0.8710, loss=0.9731]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 265/292 [02:56<00:22,  1.21batch/s, auc=0.8711, loss=0.7116]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 266/292 [02:56<00:19,  1.33batch/s, auc=0.8711, loss=0.7116]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████ | 266/292 [02:56<00:19,  1.33batch/s, auc=0.8713, loss=0.4852]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████▏| 267/292 [02:56<00:17,  1.44batch/s, auc=0.8713, loss=0.4852]\u001b[A\n",
      "Training Epoch 10/25:  91%|█████████▏| 267/292 [02:57<00:17,  1.44batch/s, auc=0.8715, loss=0.6775]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 268/292 [02:57<00:15,  1.52batch/s, auc=0.8715, loss=0.6775]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 268/292 [02:57<00:15,  1.52batch/s, auc=0.8716, loss=0.6774]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 269/292 [02:57<00:14,  1.59batch/s, auc=0.8716, loss=0.6774]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 269/292 [02:58<00:14,  1.59batch/s, auc=0.8718, loss=0.6121]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 270/292 [02:58<00:13,  1.64batch/s, auc=0.8718, loss=0.6121]\u001b[A\n",
      "Training Epoch 10/25:  92%|█████████▏| 270/292 [02:59<00:13,  1.64batch/s, auc=0.8721, loss=0.5538]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 271/292 [02:59<00:12,  1.67batch/s, auc=0.8721, loss=0.5538]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 271/292 [03:00<00:12,  1.67batch/s, auc=0.8716, loss=1.2415]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 272/292 [03:00<00:14,  1.35batch/s, auc=0.8716, loss=1.2415]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 272/292 [03:00<00:14,  1.35batch/s, auc=0.8717, loss=0.7553]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 273/292 [03:00<00:13,  1.45batch/s, auc=0.8717, loss=0.7553]\u001b[A\n",
      "Training Epoch 10/25:  93%|█████████▎| 273/292 [03:01<00:13,  1.45batch/s, auc=0.8718, loss=0.8144]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 274/292 [03:01<00:11,  1.53batch/s, auc=0.8718, loss=0.8144]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 274/292 [03:01<00:11,  1.53batch/s, auc=0.8718, loss=0.6058]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 275/292 [03:01<00:10,  1.60batch/s, auc=0.8718, loss=0.6058]\u001b[A\n",
      "Training Epoch 10/25:  94%|█████████▍| 275/292 [03:02<00:10,  1.60batch/s, auc=0.8713, loss=1.2875]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 276/292 [03:02<00:12,  1.31batch/s, auc=0.8713, loss=1.2875]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 276/292 [03:03<00:12,  1.31batch/s, auc=0.8714, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 277/292 [03:03<00:10,  1.42batch/s, auc=0.8714, loss=0.5814]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▍| 277/292 [03:04<00:10,  1.42batch/s, auc=0.8714, loss=0.6689]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▌| 278/292 [03:04<00:09,  1.51batch/s, auc=0.8714, loss=0.6689]\u001b[A\n",
      "Training Epoch 10/25:  95%|█████████▌| 278/292 [03:05<00:09,  1.51batch/s, auc=0.8708, loss=1.2876]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 279/292 [03:05<00:10,  1.27batch/s, auc=0.8708, loss=1.2876]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 279/292 [03:05<00:10,  1.27batch/s, auc=0.8707, loss=0.6634]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 280/292 [03:05<00:08,  1.39batch/s, auc=0.8707, loss=0.6634]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 280/292 [03:06<00:08,  1.39batch/s, auc=0.8709, loss=0.5474]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 281/292 [03:06<00:07,  1.48batch/s, auc=0.8709, loss=0.5474]\u001b[A\n",
      "Training Epoch 10/25:  96%|█████████▌| 281/292 [03:06<00:07,  1.48batch/s, auc=0.8707, loss=0.9128]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 282/292 [03:06<00:06,  1.56batch/s, auc=0.8707, loss=0.9128]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 282/292 [03:07<00:06,  1.56batch/s, auc=0.8710, loss=0.5961]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.62batch/s, auc=0.8710, loss=0.5961]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.62batch/s, auc=0.8711, loss=0.7066]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 284/292 [03:07<00:04,  1.66batch/s, auc=0.8711, loss=0.7066]\u001b[A\n",
      "Training Epoch 10/25:  97%|█████████▋| 284/292 [03:08<00:04,  1.66batch/s, auc=0.8713, loss=0.5635]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.69batch/s, auc=0.8713, loss=0.5635]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 285/292 [03:09<00:04,  1.69batch/s, auc=0.8715, loss=0.5048]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 286/292 [03:09<00:03,  1.71batch/s, auc=0.8715, loss=0.5048]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 286/292 [03:09<00:03,  1.71batch/s, auc=0.8717, loss=0.4748]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 287/292 [03:09<00:02,  1.73batch/s, auc=0.8717, loss=0.4748]\u001b[A\n",
      "Training Epoch 10/25:  98%|█████████▊| 287/292 [03:10<00:02,  1.73batch/s, auc=0.8716, loss=0.7985]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.74batch/s, auc=0.8716, loss=0.7985]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.74batch/s, auc=0.8720, loss=0.6426]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.74batch/s, auc=0.8720, loss=0.6426]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 289/292 [03:11<00:01,  1.74batch/s, auc=0.8722, loss=0.6993]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.75batch/s, auc=0.8722, loss=0.6993]\u001b[A\n",
      "Training Epoch 10/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.75batch/s, auc=0.8723, loss=0.6602]\u001b[A\n",
      "Training Epoch 10/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.75batch/s, auc=0.8723, loss=0.6602]\u001b[A\n",
      "Training Epoch 10/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.75batch/s, auc=0.8723, loss=0.8586]\u001b[A\n",
      "Training Epoch 10/25: 100%|██████████| 292/292 [03:12<00:00,  1.52batch/s, auc=0.8723, loss=0.8586]\u001b[A\n",
      "Epochs:  40%|████      | 10/25 [35:20<53:07, 212.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25] Train Loss: 0.7503 | Train AUROC: 0.8723 Val Loss: 0.9163 | Val AUROC: 0.8399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 11/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9339, loss=0.5309]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 1/292 [00:01<07:10,  1.48s/batch, auc=0.9339, loss=0.5309]\u001b[A\n",
      "Training Epoch 11/25:   0%|          | 1/292 [00:02<07:10,  1.48s/batch, auc=0.9097, loss=0.8681]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 2/292 [00:02<04:33,  1.06batch/s, auc=0.9097, loss=0.8681]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 2/292 [00:02<04:33,  1.06batch/s, auc=0.9167, loss=0.4875]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 3/292 [00:02<03:41,  1.31batch/s, auc=0.9167, loss=0.4875]\u001b[A\n",
      "Training Epoch 11/25:   1%|          | 3/292 [00:03<03:41,  1.31batch/s, auc=0.9107, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:   1%|▏         | 4/292 [00:03<03:16,  1.46batch/s, auc=0.9107, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:   1%|▏         | 4/292 [00:03<03:16,  1.46batch/s, auc=0.9203, loss=0.4542]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 5/292 [00:03<03:02,  1.57batch/s, auc=0.9203, loss=0.4542]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 5/292 [00:04<03:02,  1.57batch/s, auc=0.9252, loss=0.4826]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.9252, loss=0.4826]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.9200, loss=0.6034]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 7/292 [00:04<02:48,  1.69batch/s, auc=0.9200, loss=0.6034]\u001b[A\n",
      "Training Epoch 11/25:   2%|▏         | 7/292 [00:05<02:48,  1.69batch/s, auc=0.9167, loss=0.6704]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 8/292 [00:05<02:45,  1.71batch/s, auc=0.9167, loss=0.6704]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 8/292 [00:05<02:45,  1.71batch/s, auc=0.9124, loss=0.7916]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 9/292 [00:05<02:42,  1.74batch/s, auc=0.9124, loss=0.7916]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 9/292 [00:06<02:42,  1.74batch/s, auc=0.9068, loss=0.7735]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 10/292 [00:06<02:40,  1.76batch/s, auc=0.9068, loss=0.7735]\u001b[A\n",
      "Training Epoch 11/25:   3%|▎         | 10/292 [00:07<02:40,  1.76batch/s, auc=0.9077, loss=0.5404]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 11/292 [00:07<02:38,  1.77batch/s, auc=0.9077, loss=0.5404]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 11/292 [00:08<02:38,  1.77batch/s, auc=0.8872, loss=1.4035]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 12/292 [00:08<03:21,  1.39batch/s, auc=0.8872, loss=1.4035]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 12/292 [00:08<03:21,  1.39batch/s, auc=0.8849, loss=0.7546]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 13/292 [00:08<03:07,  1.49batch/s, auc=0.8849, loss=0.7546]\u001b[A\n",
      "Training Epoch 11/25:   4%|▍         | 13/292 [00:09<03:07,  1.49batch/s, auc=0.8891, loss=0.5897]\u001b[A\n",
      "Training Epoch 11/25:   5%|▍         | 14/292 [00:09<02:57,  1.57batch/s, auc=0.8891, loss=0.5897]\u001b[A\n",
      "Training Epoch 11/25:   5%|▍         | 14/292 [00:10<02:57,  1.57batch/s, auc=0.8811, loss=1.0118]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 15/292 [00:10<03:32,  1.30batch/s, auc=0.8811, loss=1.0118]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 15/292 [00:10<03:32,  1.30batch/s, auc=0.8829, loss=0.7215]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 16/292 [00:10<03:14,  1.42batch/s, auc=0.8829, loss=0.7215]\u001b[A\n",
      "Training Epoch 11/25:   5%|▌         | 16/292 [00:11<03:14,  1.42batch/s, auc=0.8803, loss=0.8901]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 17/292 [00:11<03:43,  1.23batch/s, auc=0.8803, loss=0.8901]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 17/292 [00:12<03:43,  1.23batch/s, auc=0.8827, loss=0.5115]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 18/292 [00:12<03:21,  1.36batch/s, auc=0.8827, loss=0.5115]\u001b[A\n",
      "Training Epoch 11/25:   6%|▌         | 18/292 [00:13<03:21,  1.36batch/s, auc=0.8844, loss=0.6185]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 19/292 [00:13<03:07,  1.46batch/s, auc=0.8844, loss=0.6185]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 19/292 [00:13<03:07,  1.46batch/s, auc=0.8866, loss=0.5215]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 20/292 [00:13<02:56,  1.55batch/s, auc=0.8866, loss=0.5215]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 20/292 [00:14<02:56,  1.55batch/s, auc=0.8890, loss=0.4428]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 21/292 [00:14<02:48,  1.61batch/s, auc=0.8890, loss=0.4428]\u001b[A\n",
      "Training Epoch 11/25:   7%|▋         | 21/292 [00:14<02:48,  1.61batch/s, auc=0.8901, loss=0.5792]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 22/292 [00:14<02:42,  1.66batch/s, auc=0.8901, loss=0.5792]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 22/292 [00:15<02:42,  1.66batch/s, auc=0.8875, loss=0.8014]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 23/292 [00:15<02:37,  1.70batch/s, auc=0.8875, loss=0.8014]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 23/292 [00:15<02:37,  1.70batch/s, auc=0.8871, loss=0.7550]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 24/292 [00:15<02:34,  1.73batch/s, auc=0.8871, loss=0.7550]\u001b[A\n",
      "Training Epoch 11/25:   8%|▊         | 24/292 [00:16<02:34,  1.73batch/s, auc=0.8898, loss=0.5306]\u001b[A\n",
      "Training Epoch 11/25:   9%|▊         | 25/292 [00:16<02:32,  1.75batch/s, auc=0.8898, loss=0.5306]\u001b[A\n",
      "Training Epoch 11/25:   9%|▊         | 25/292 [00:16<02:32,  1.75batch/s, auc=0.8881, loss=0.8903]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 26/292 [00:16<02:30,  1.76batch/s, auc=0.8881, loss=0.8903]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 26/292 [00:18<02:30,  1.76batch/s, auc=0.8837, loss=1.0568]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 27/292 [00:18<03:10,  1.39batch/s, auc=0.8837, loss=1.0568]\u001b[A\n",
      "Training Epoch 11/25:   9%|▉         | 27/292 [00:18<03:10,  1.39batch/s, auc=0.8847, loss=0.6608]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 28/292 [00:18<02:56,  1.49batch/s, auc=0.8847, loss=0.6608]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 28/292 [00:19<02:56,  1.49batch/s, auc=0.8770, loss=1.7034]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 29/292 [00:19<02:47,  1.57batch/s, auc=0.8770, loss=1.7034]\u001b[A\n",
      "Training Epoch 11/25:  10%|▉         | 29/292 [00:20<02:47,  1.57batch/s, auc=0.8737, loss=1.0219]\u001b[A\n",
      "Training Epoch 11/25:  10%|█         | 30/292 [00:20<03:20,  1.31batch/s, auc=0.8737, loss=1.0219]\u001b[A\n",
      "Training Epoch 11/25:  10%|█         | 30/292 [00:21<03:20,  1.31batch/s, auc=0.8652, loss=1.5783]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 31/292 [00:21<03:44,  1.16batch/s, auc=0.8652, loss=1.5783]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 31/292 [00:22<03:44,  1.16batch/s, auc=0.8642, loss=0.6002]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 32/292 [00:22<03:59,  1.08batch/s, auc=0.8642, loss=0.6002]\u001b[A\n",
      "Training Epoch 11/25:  11%|█         | 32/292 [00:22<03:59,  1.08batch/s, auc=0.8673, loss=0.5672]\u001b[A\n",
      "Training Epoch 11/25:  11%|█▏        | 33/292 [00:22<03:30,  1.23batch/s, auc=0.8673, loss=0.5672]\u001b[A\n",
      "Training Epoch 11/25:  11%|█▏        | 33/292 [00:23<03:30,  1.23batch/s, auc=0.8659, loss=0.9545]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 34/292 [00:23<03:09,  1.36batch/s, auc=0.8659, loss=0.9545]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 34/292 [00:24<03:09,  1.36batch/s, auc=0.8679, loss=0.6237]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 35/292 [00:24<02:55,  1.47batch/s, auc=0.8679, loss=0.6237]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 35/292 [00:24<02:55,  1.47batch/s, auc=0.8688, loss=0.6980]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 36/292 [00:24<02:45,  1.54batch/s, auc=0.8688, loss=0.6980]\u001b[A\n",
      "Training Epoch 11/25:  12%|█▏        | 36/292 [00:25<02:45,  1.54batch/s, auc=0.8676, loss=0.7078]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 37/292 [00:25<02:38,  1.61batch/s, auc=0.8676, loss=0.7078]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 37/292 [00:26<02:38,  1.61batch/s, auc=0.8643, loss=1.0284]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 38/292 [00:26<03:11,  1.32batch/s, auc=0.8643, loss=1.0284]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 38/292 [00:27<03:11,  1.32batch/s, auc=0.8636, loss=0.7061]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 39/292 [00:27<03:35,  1.18batch/s, auc=0.8636, loss=0.7061]\u001b[A\n",
      "Training Epoch 11/25:  13%|█▎        | 39/292 [00:27<03:35,  1.18batch/s, auc=0.8643, loss=0.5895]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▎        | 40/292 [00:27<03:11,  1.31batch/s, auc=0.8643, loss=0.5895]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▎        | 40/292 [00:28<03:11,  1.31batch/s, auc=0.8634, loss=0.9623]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 41/292 [00:28<03:34,  1.17batch/s, auc=0.8634, loss=0.9623]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 41/292 [00:29<03:34,  1.17batch/s, auc=0.8618, loss=0.9457]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 42/292 [00:30<03:50,  1.09batch/s, auc=0.8618, loss=0.9457]\u001b[A\n",
      "Training Epoch 11/25:  14%|█▍        | 42/292 [00:30<03:50,  1.09batch/s, auc=0.8633, loss=0.6981]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▍        | 43/292 [00:30<03:21,  1.23batch/s, auc=0.8633, loss=0.6981]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▍        | 43/292 [00:31<03:21,  1.23batch/s, auc=0.8637, loss=0.6915]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 44/292 [00:31<03:02,  1.36batch/s, auc=0.8637, loss=0.6915]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 44/292 [00:31<03:02,  1.36batch/s, auc=0.8641, loss=0.6230]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 45/292 [00:31<02:48,  1.46batch/s, auc=0.8641, loss=0.6230]\u001b[A\n",
      "Training Epoch 11/25:  15%|█▌        | 45/292 [00:32<02:48,  1.46batch/s, auc=0.8659, loss=0.5827]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 46/292 [00:32<02:38,  1.55batch/s, auc=0.8659, loss=0.5827]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 46/292 [00:32<02:38,  1.55batch/s, auc=0.8653, loss=0.7522]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 47/292 [00:32<02:31,  1.62batch/s, auc=0.8653, loss=0.7522]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▌        | 47/292 [00:33<02:31,  1.62batch/s, auc=0.8592, loss=1.5422]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▋        | 48/292 [00:33<03:04,  1.33batch/s, auc=0.8592, loss=1.5422]\u001b[A\n",
      "Training Epoch 11/25:  16%|█▋        | 48/292 [00:34<03:04,  1.33batch/s, auc=0.8606, loss=0.5215]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 49/292 [00:34<02:48,  1.44batch/s, auc=0.8606, loss=0.5215]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 49/292 [00:34<02:48,  1.44batch/s, auc=0.8603, loss=0.8394]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 50/292 [00:34<02:38,  1.53batch/s, auc=0.8603, loss=0.8394]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 50/292 [00:35<02:38,  1.53batch/s, auc=0.8620, loss=0.5795]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 51/292 [00:35<02:30,  1.60batch/s, auc=0.8620, loss=0.5795]\u001b[A\n",
      "Training Epoch 11/25:  17%|█▋        | 51/292 [00:36<02:30,  1.60batch/s, auc=0.8629, loss=0.5570]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 52/292 [00:36<02:24,  1.66batch/s, auc=0.8629, loss=0.5570]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 52/292 [00:36<02:24,  1.66batch/s, auc=0.8620, loss=1.0361]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 53/292 [00:36<02:20,  1.70batch/s, auc=0.8620, loss=1.0361]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 53/292 [00:37<02:20,  1.70batch/s, auc=0.8650, loss=0.4590]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 54/292 [00:37<02:17,  1.72batch/s, auc=0.8650, loss=0.4590]\u001b[A\n",
      "Training Epoch 11/25:  18%|█▊        | 54/292 [00:37<02:17,  1.72batch/s, auc=0.8649, loss=0.6566]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 55/292 [00:37<02:16,  1.74batch/s, auc=0.8649, loss=0.6566]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 55/292 [00:38<02:16,  1.74batch/s, auc=0.8653, loss=0.4495]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 56/292 [00:38<02:14,  1.76batch/s, auc=0.8653, loss=0.4495]\u001b[A\n",
      "Training Epoch 11/25:  19%|█▉        | 56/292 [00:39<02:14,  1.76batch/s, auc=0.8624, loss=1.2950]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 57/292 [00:39<02:49,  1.39batch/s, auc=0.8624, loss=1.2950]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 57/292 [00:40<02:49,  1.39batch/s, auc=0.8604, loss=1.0636]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 58/292 [00:40<03:13,  1.21batch/s, auc=0.8604, loss=1.0636]\u001b[A\n",
      "Training Epoch 11/25:  20%|█▉        | 58/292 [00:41<03:13,  1.21batch/s, auc=0.8606, loss=0.5544]\u001b[A\n",
      "Training Epoch 11/25:  20%|██        | 59/292 [00:41<02:53,  1.34batch/s, auc=0.8606, loss=0.5544]\u001b[A\n",
      "Training Epoch 11/25:  20%|██        | 59/292 [00:42<02:53,  1.34batch/s, auc=0.8599, loss=0.8861]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 60/292 [00:42<03:15,  1.18batch/s, auc=0.8599, loss=0.8861]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 60/292 [00:42<03:15,  1.18batch/s, auc=0.8612, loss=0.5175]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 61/292 [00:42<02:55,  1.32batch/s, auc=0.8612, loss=0.5175]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 61/292 [00:43<02:55,  1.32batch/s, auc=0.8619, loss=0.7368]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 62/292 [00:43<02:40,  1.43batch/s, auc=0.8619, loss=0.7368]\u001b[A\n",
      "Training Epoch 11/25:  21%|██        | 62/292 [00:43<02:40,  1.43batch/s, auc=0.8612, loss=0.9331]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 63/292 [00:43<02:30,  1.53batch/s, auc=0.8612, loss=0.9331]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 63/292 [00:44<02:30,  1.53batch/s, auc=0.8614, loss=0.7965]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 64/292 [00:44<02:22,  1.60batch/s, auc=0.8614, loss=0.7965]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 64/292 [00:44<02:22,  1.60batch/s, auc=0.8624, loss=0.5880]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 65/292 [00:44<02:17,  1.65batch/s, auc=0.8624, loss=0.5880]\u001b[A\n",
      "Training Epoch 11/25:  22%|██▏       | 65/292 [00:45<02:17,  1.65batch/s, auc=0.8629, loss=0.5321]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 66/292 [00:45<02:13,  1.69batch/s, auc=0.8629, loss=0.5321]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 66/292 [00:45<02:13,  1.69batch/s, auc=0.8629, loss=0.5523]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 67/292 [00:45<02:10,  1.72batch/s, auc=0.8629, loss=0.5523]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 67/292 [00:46<02:10,  1.72batch/s, auc=0.8622, loss=0.8688]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 68/292 [00:46<02:08,  1.74batch/s, auc=0.8622, loss=0.8688]\u001b[A\n",
      "Training Epoch 11/25:  23%|██▎       | 68/292 [00:47<02:08,  1.74batch/s, auc=0.8617, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▎       | 69/292 [00:47<02:41,  1.38batch/s, auc=0.8617, loss=0.7551]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▎       | 69/292 [00:48<02:41,  1.38batch/s, auc=0.8623, loss=0.5186]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 70/292 [00:48<02:29,  1.49batch/s, auc=0.8623, loss=0.5186]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 70/292 [00:48<02:29,  1.49batch/s, auc=0.8622, loss=0.7383]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 71/292 [00:48<02:21,  1.57batch/s, auc=0.8622, loss=0.7383]\u001b[A\n",
      "Training Epoch 11/25:  24%|██▍       | 71/292 [00:49<02:21,  1.57batch/s, auc=0.8627, loss=0.6245]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▍       | 72/292 [00:49<02:14,  1.63batch/s, auc=0.8627, loss=0.6245]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▍       | 72/292 [00:49<02:14,  1.63batch/s, auc=0.8645, loss=0.5681]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 73/292 [00:49<02:10,  1.68batch/s, auc=0.8645, loss=0.5681]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 73/292 [00:50<02:10,  1.68batch/s, auc=0.8663, loss=0.5713]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 74/292 [00:50<02:07,  1.71batch/s, auc=0.8663, loss=0.5713]\u001b[A\n",
      "Training Epoch 11/25:  25%|██▌       | 74/292 [00:51<02:07,  1.71batch/s, auc=0.8639, loss=1.3342]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 75/292 [00:51<02:38,  1.37batch/s, auc=0.8639, loss=1.3342]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 75/292 [00:52<02:38,  1.37batch/s, auc=0.8622, loss=1.1084]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 76/292 [00:52<03:00,  1.20batch/s, auc=0.8622, loss=1.1084]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▌       | 76/292 [00:53<03:00,  1.20batch/s, auc=0.8620, loss=0.7206]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▋       | 77/292 [00:53<02:41,  1.33batch/s, auc=0.8620, loss=0.7206]\u001b[A\n",
      "Training Epoch 11/25:  26%|██▋       | 77/292 [00:53<02:41,  1.33batch/s, auc=0.8628, loss=0.6239]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 78/292 [00:53<02:28,  1.44batch/s, auc=0.8628, loss=0.6239]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 78/292 [00:54<02:28,  1.44batch/s, auc=0.8633, loss=0.6959]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 79/292 [00:54<02:19,  1.53batch/s, auc=0.8633, loss=0.6959]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 79/292 [00:55<02:19,  1.53batch/s, auc=0.8630, loss=0.8126]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 80/292 [00:55<02:45,  1.28batch/s, auc=0.8630, loss=0.8126]\u001b[A\n",
      "Training Epoch 11/25:  27%|██▋       | 80/292 [00:55<02:45,  1.28batch/s, auc=0.8627, loss=0.5656]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 81/292 [00:55<02:30,  1.40batch/s, auc=0.8627, loss=0.5656]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 81/292 [00:56<02:30,  1.40batch/s, auc=0.8624, loss=0.7576]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 82/292 [00:56<02:19,  1.50batch/s, auc=0.8624, loss=0.7576]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 82/292 [00:56<02:19,  1.50batch/s, auc=0.8628, loss=0.6616]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 83/292 [00:56<02:12,  1.58batch/s, auc=0.8628, loss=0.6616]\u001b[A\n",
      "Training Epoch 11/25:  28%|██▊       | 83/292 [00:57<02:12,  1.58batch/s, auc=0.8634, loss=0.7037]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 84/292 [00:57<02:07,  1.64batch/s, auc=0.8634, loss=0.7037]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 84/292 [00:58<02:07,  1.64batch/s, auc=0.8637, loss=0.7711]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 85/292 [00:58<02:03,  1.68batch/s, auc=0.8637, loss=0.7711]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 85/292 [00:58<02:03,  1.68batch/s, auc=0.8646, loss=0.4485]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 86/292 [00:58<02:00,  1.71batch/s, auc=0.8646, loss=0.4485]\u001b[A\n",
      "Training Epoch 11/25:  29%|██▉       | 86/292 [00:59<02:00,  1.71batch/s, auc=0.8658, loss=0.7732]\u001b[A\n",
      "Training Epoch 11/25:  30%|██▉       | 87/292 [00:59<01:58,  1.74batch/s, auc=0.8658, loss=0.7732]\u001b[A\n",
      "Training Epoch 11/25:  30%|██▉       | 87/292 [00:59<01:58,  1.74batch/s, auc=0.8661, loss=0.5818]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 88/292 [00:59<01:56,  1.75batch/s, auc=0.8661, loss=0.5818]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 88/292 [01:00<01:56,  1.75batch/s, auc=0.8663, loss=0.6912]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 89/292 [01:00<01:55,  1.76batch/s, auc=0.8663, loss=0.6912]\u001b[A\n",
      "Training Epoch 11/25:  30%|███       | 89/292 [01:00<01:55,  1.76batch/s, auc=0.8668, loss=0.5841]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 90/292 [01:00<01:54,  1.77batch/s, auc=0.8668, loss=0.5841]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 90/292 [01:01<01:54,  1.77batch/s, auc=0.8683, loss=0.6077]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 91/292 [01:01<01:53,  1.77batch/s, auc=0.8683, loss=0.6077]\u001b[A\n",
      "Training Epoch 11/25:  31%|███       | 91/292 [01:02<01:53,  1.77batch/s, auc=0.8690, loss=0.5526]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 92/292 [01:02<01:52,  1.78batch/s, auc=0.8690, loss=0.5526]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 92/292 [01:03<01:52,  1.78batch/s, auc=0.8672, loss=1.3314]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 93/292 [01:03<02:22,  1.40batch/s, auc=0.8672, loss=1.3314]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 93/292 [01:03<02:22,  1.40batch/s, auc=0.8675, loss=0.5259]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 94/292 [01:03<02:12,  1.50batch/s, auc=0.8675, loss=0.5259]\u001b[A\n",
      "Training Epoch 11/25:  32%|███▏      | 94/292 [01:04<02:12,  1.50batch/s, auc=0.8682, loss=0.6578]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 95/292 [01:04<02:05,  1.57batch/s, auc=0.8682, loss=0.6578]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 95/292 [01:04<02:05,  1.57batch/s, auc=0.8698, loss=0.6125]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 96/292 [01:04<02:00,  1.63batch/s, auc=0.8698, loss=0.6125]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 96/292 [01:05<02:00,  1.63batch/s, auc=0.8705, loss=0.5246]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 97/292 [01:05<01:56,  1.68batch/s, auc=0.8705, loss=0.5246]\u001b[A\n",
      "Training Epoch 11/25:  33%|███▎      | 97/292 [01:05<01:56,  1.68batch/s, auc=0.8705, loss=0.7618]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▎      | 98/292 [01:05<01:53,  1.71batch/s, auc=0.8705, loss=0.7618]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▎      | 98/292 [01:06<01:53,  1.71batch/s, auc=0.8709, loss=0.7251]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 99/292 [01:06<01:51,  1.73batch/s, auc=0.8709, loss=0.7251]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 99/292 [01:06<01:51,  1.73batch/s, auc=0.8716, loss=0.5851]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 100/292 [01:06<01:49,  1.75batch/s, auc=0.8716, loss=0.5851]\u001b[A\n",
      "Training Epoch 11/25:  34%|███▍      | 100/292 [01:08<01:49,  1.75batch/s, auc=0.8702, loss=1.1948]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 101/292 [01:08<02:18,  1.38batch/s, auc=0.8702, loss=1.1948]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 101/292 [01:09<02:18,  1.38batch/s, auc=0.8691, loss=1.2268]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 102/292 [01:09<02:37,  1.21batch/s, auc=0.8691, loss=1.2268]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▍      | 102/292 [01:09<02:37,  1.21batch/s, auc=0.8693, loss=0.8229]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▌      | 103/292 [01:09<02:21,  1.34batch/s, auc=0.8693, loss=0.8229]\u001b[A\n",
      "Training Epoch 11/25:  35%|███▌      | 103/292 [01:10<02:21,  1.34batch/s, auc=0.8693, loss=0.7817]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 104/292 [01:10<02:09,  1.45batch/s, auc=0.8693, loss=0.7817]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 104/292 [01:11<02:09,  1.45batch/s, auc=0.8690, loss=1.0386]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 105/292 [01:11<02:30,  1.24batch/s, auc=0.8690, loss=1.0386]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▌      | 105/292 [01:11<02:30,  1.24batch/s, auc=0.8685, loss=0.9382]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▋      | 106/292 [01:11<02:16,  1.36batch/s, auc=0.8685, loss=0.9382]\u001b[A\n",
      "Training Epoch 11/25:  36%|███▋      | 106/292 [01:12<02:16,  1.36batch/s, auc=0.8696, loss=0.5072]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 107/292 [01:12<02:05,  1.47batch/s, auc=0.8696, loss=0.5072]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 107/292 [01:13<02:05,  1.47batch/s, auc=0.8691, loss=0.8229]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 108/292 [01:13<01:58,  1.56batch/s, auc=0.8691, loss=0.8229]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 108/292 [01:13<01:58,  1.56batch/s, auc=0.8697, loss=0.5847]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 109/292 [01:13<01:53,  1.62batch/s, auc=0.8697, loss=0.5847]\u001b[A\n",
      "Training Epoch 11/25:  37%|███▋      | 109/292 [01:14<01:53,  1.62batch/s, auc=0.8695, loss=0.6970]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 110/292 [01:14<01:49,  1.66batch/s, auc=0.8695, loss=0.6970]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 110/292 [01:15<01:49,  1.66batch/s, auc=0.8694, loss=0.6789]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 111/292 [01:15<02:14,  1.34batch/s, auc=0.8694, loss=0.6789]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 111/292 [01:15<02:14,  1.34batch/s, auc=0.8697, loss=0.5270]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 112/292 [01:15<02:03,  1.45batch/s, auc=0.8697, loss=0.5270]\u001b[A\n",
      "Training Epoch 11/25:  38%|███▊      | 112/292 [01:16<02:03,  1.45batch/s, auc=0.8700, loss=0.6776]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▊      | 113/292 [01:16<01:56,  1.54batch/s, auc=0.8700, loss=0.6776]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▊      | 113/292 [01:17<01:56,  1.54batch/s, auc=0.8692, loss=1.1086]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 114/292 [01:17<02:18,  1.29batch/s, auc=0.8692, loss=1.1086]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 114/292 [01:17<02:18,  1.29batch/s, auc=0.8697, loss=0.7456]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 115/292 [01:17<02:05,  1.41batch/s, auc=0.8697, loss=0.7456]\u001b[A\n",
      "Training Epoch 11/25:  39%|███▉      | 115/292 [01:19<02:05,  1.41batch/s, auc=0.8687, loss=1.0402]\u001b[A\n",
      "Training Epoch 11/25:  40%|███▉      | 116/292 [01:19<02:24,  1.22batch/s, auc=0.8687, loss=1.0402]\u001b[A\n",
      "Training Epoch 11/25:  40%|███▉      | 116/292 [01:19<02:24,  1.22batch/s, auc=0.8692, loss=0.6092]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 117/292 [01:19<02:09,  1.35batch/s, auc=0.8692, loss=0.6092]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 117/292 [01:20<02:09,  1.35batch/s, auc=0.8691, loss=0.8734]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 118/292 [01:20<01:59,  1.46batch/s, auc=0.8691, loss=0.8734]\u001b[A\n",
      "Training Epoch 11/25:  40%|████      | 118/292 [01:21<01:59,  1.46batch/s, auc=0.8682, loss=0.8311]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 119/292 [01:21<02:19,  1.24batch/s, auc=0.8682, loss=0.8311]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 119/292 [01:22<02:19,  1.24batch/s, auc=0.8676, loss=0.8679]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 120/292 [01:22<02:32,  1.13batch/s, auc=0.8676, loss=0.8679]\u001b[A\n",
      "Training Epoch 11/25:  41%|████      | 120/292 [01:22<02:32,  1.13batch/s, auc=0.8684, loss=0.4752]\u001b[A\n",
      "Training Epoch 11/25:  41%|████▏     | 121/292 [01:22<02:14,  1.27batch/s, auc=0.8684, loss=0.4752]\u001b[A\n",
      "Training Epoch 11/25:  41%|████▏     | 121/292 [01:23<02:14,  1.27batch/s, auc=0.8685, loss=0.6591]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 122/292 [01:23<02:02,  1.39batch/s, auc=0.8685, loss=0.6591]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 122/292 [01:23<02:02,  1.39batch/s, auc=0.8688, loss=0.6766]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 123/292 [01:23<01:53,  1.49batch/s, auc=0.8688, loss=0.6766]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 123/292 [01:24<01:53,  1.49batch/s, auc=0.8691, loss=0.6219]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 124/292 [01:24<01:47,  1.57batch/s, auc=0.8691, loss=0.6219]\u001b[A\n",
      "Training Epoch 11/25:  42%|████▏     | 124/292 [01:25<01:47,  1.57batch/s, auc=0.8687, loss=0.9638]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 125/292 [01:25<01:42,  1.63batch/s, auc=0.8687, loss=0.9638]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 125/292 [01:25<01:42,  1.63batch/s, auc=0.8693, loss=0.6511]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 126/292 [01:25<01:39,  1.67batch/s, auc=0.8693, loss=0.6511]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 126/292 [01:26<01:39,  1.67batch/s, auc=0.8703, loss=0.4447]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 127/292 [01:26<01:36,  1.71batch/s, auc=0.8703, loss=0.4447]\u001b[A\n",
      "Training Epoch 11/25:  43%|████▎     | 127/292 [01:27<01:36,  1.71batch/s, auc=0.8691, loss=1.3896]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 128/292 [01:27<02:00,  1.36batch/s, auc=0.8691, loss=1.3896]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 128/292 [01:28<02:00,  1.36batch/s, auc=0.8690, loss=0.8634]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 129/292 [01:28<02:16,  1.20batch/s, auc=0.8690, loss=0.8634]\u001b[A\n",
      "Training Epoch 11/25:  44%|████▍     | 129/292 [01:29<02:16,  1.20batch/s, auc=0.8684, loss=1.0008]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 130/292 [01:29<02:27,  1.10batch/s, auc=0.8684, loss=1.0008]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 130/292 [01:30<02:27,  1.10batch/s, auc=0.8685, loss=0.7484]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 131/292 [01:30<02:09,  1.25batch/s, auc=0.8685, loss=0.7484]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▍     | 131/292 [01:30<02:09,  1.25batch/s, auc=0.8685, loss=0.7098]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▌     | 132/292 [01:30<01:56,  1.37batch/s, auc=0.8685, loss=0.7098]\u001b[A\n",
      "Training Epoch 11/25:  45%|████▌     | 132/292 [01:31<01:56,  1.37batch/s, auc=0.8674, loss=1.2652]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 133/292 [01:31<02:12,  1.20batch/s, auc=0.8674, loss=1.2652]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 133/292 [01:32<02:12,  1.20batch/s, auc=0.8673, loss=0.8214]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 134/292 [01:32<01:58,  1.33batch/s, auc=0.8673, loss=0.8214]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 134/292 [01:33<01:58,  1.33batch/s, auc=0.8671, loss=0.7455]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 135/292 [01:33<02:13,  1.18batch/s, auc=0.8671, loss=0.7455]\u001b[A\n",
      "Training Epoch 11/25:  46%|████▌     | 135/292 [01:33<02:13,  1.18batch/s, auc=0.8678, loss=0.4849]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 136/292 [01:33<01:59,  1.31batch/s, auc=0.8678, loss=0.4849]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 136/292 [01:34<01:59,  1.31batch/s, auc=0.8678, loss=0.7476]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 137/292 [01:34<01:48,  1.42batch/s, auc=0.8678, loss=0.7476]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 137/292 [01:34<01:48,  1.42batch/s, auc=0.8673, loss=0.7954]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 138/292 [01:34<01:41,  1.52batch/s, auc=0.8673, loss=0.7954]\u001b[A\n",
      "Training Epoch 11/25:  47%|████▋     | 138/292 [01:35<01:41,  1.52batch/s, auc=0.8675, loss=0.6778]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 139/292 [01:35<01:36,  1.59batch/s, auc=0.8675, loss=0.6778]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 139/292 [01:36<01:36,  1.59batch/s, auc=0.8683, loss=0.5476]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 140/292 [01:36<01:32,  1.64batch/s, auc=0.8683, loss=0.5476]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 140/292 [01:37<01:32,  1.64batch/s, auc=0.8674, loss=1.3000]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 141/292 [01:37<01:53,  1.34batch/s, auc=0.8674, loss=1.3000]\u001b[A\n",
      "Training Epoch 11/25:  48%|████▊     | 141/292 [01:37<01:53,  1.34batch/s, auc=0.8679, loss=0.6609]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▊     | 142/292 [01:37<01:43,  1.44batch/s, auc=0.8679, loss=0.6609]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▊     | 142/292 [01:38<01:43,  1.44batch/s, auc=0.8680, loss=0.4867]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 143/292 [01:38<01:37,  1.53batch/s, auc=0.8680, loss=0.4867]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 143/292 [01:38<01:37,  1.53batch/s, auc=0.8681, loss=0.8165]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 144/292 [01:38<01:32,  1.60batch/s, auc=0.8681, loss=0.8165]\u001b[A\n",
      "Training Epoch 11/25:  49%|████▉     | 144/292 [01:39<01:32,  1.60batch/s, auc=0.8688, loss=0.4697]\u001b[A\n",
      "Training Epoch 11/25:  50%|████▉     | 145/292 [01:39<01:29,  1.65batch/s, auc=0.8688, loss=0.4697]\u001b[A\n",
      "Training Epoch 11/25:  50%|████▉     | 145/292 [01:39<01:29,  1.65batch/s, auc=0.8694, loss=0.4718]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 146/292 [01:39<01:26,  1.69batch/s, auc=0.8694, loss=0.4718]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 146/292 [01:40<01:26,  1.69batch/s, auc=0.8695, loss=0.8844]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 147/292 [01:40<01:24,  1.71batch/s, auc=0.8695, loss=0.8844]\u001b[A\n",
      "Training Epoch 11/25:  50%|█████     | 147/292 [01:41<01:24,  1.71batch/s, auc=0.8695, loss=0.6405]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 148/292 [01:41<01:23,  1.73batch/s, auc=0.8695, loss=0.6405]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 148/292 [01:41<01:23,  1.73batch/s, auc=0.8695, loss=0.9321]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 149/292 [01:41<01:21,  1.75batch/s, auc=0.8695, loss=0.9321]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████     | 149/292 [01:42<01:21,  1.75batch/s, auc=0.8696, loss=0.6817]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████▏    | 150/292 [01:42<01:20,  1.76batch/s, auc=0.8696, loss=0.6817]\u001b[A\n",
      "Training Epoch 11/25:  51%|█████▏    | 150/292 [01:42<01:20,  1.76batch/s, auc=0.8698, loss=0.7168]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 151/292 [01:42<01:19,  1.76batch/s, auc=0.8698, loss=0.7168]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 151/292 [01:43<01:19,  1.76batch/s, auc=0.8703, loss=0.5146]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 152/292 [01:43<01:19,  1.77batch/s, auc=0.8703, loss=0.5146]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 152/292 [01:43<01:19,  1.77batch/s, auc=0.8702, loss=0.8495]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 153/292 [01:43<01:18,  1.77batch/s, auc=0.8702, loss=0.8495]\u001b[A\n",
      "Training Epoch 11/25:  52%|█████▏    | 153/292 [01:44<01:18,  1.77batch/s, auc=0.8702, loss=0.6210]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 154/292 [01:44<01:17,  1.77batch/s, auc=0.8702, loss=0.6210]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 154/292 [01:45<01:17,  1.77batch/s, auc=0.8696, loss=1.0176]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 155/292 [01:45<01:38,  1.39batch/s, auc=0.8696, loss=1.0176]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 155/292 [01:46<01:38,  1.39batch/s, auc=0.8700, loss=0.4880]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 156/292 [01:46<01:31,  1.49batch/s, auc=0.8700, loss=0.4880]\u001b[A\n",
      "Training Epoch 11/25:  53%|█████▎    | 156/292 [01:47<01:31,  1.49batch/s, auc=0.8690, loss=1.1495]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 157/292 [01:47<01:46,  1.26batch/s, auc=0.8690, loss=1.1495]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 157/292 [01:48<01:46,  1.26batch/s, auc=0.8681, loss=1.2594]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 158/292 [01:48<01:57,  1.14batch/s, auc=0.8681, loss=1.2594]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 158/292 [01:48<01:57,  1.14batch/s, auc=0.8684, loss=0.4890]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 159/292 [01:48<01:44,  1.28batch/s, auc=0.8684, loss=0.4890]\u001b[A\n",
      "Training Epoch 11/25:  54%|█████▍    | 159/292 [01:49<01:44,  1.28batch/s, auc=0.8680, loss=1.0294]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▍    | 160/292 [01:49<01:55,  1.15batch/s, auc=0.8680, loss=1.0294]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▍    | 160/292 [01:50<01:55,  1.15batch/s, auc=0.8676, loss=0.9115]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 161/292 [01:50<02:02,  1.07batch/s, auc=0.8676, loss=0.9115]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 161/292 [01:51<02:02,  1.07batch/s, auc=0.8677, loss=0.8503]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 162/292 [01:51<01:46,  1.22batch/s, auc=0.8677, loss=0.8503]\u001b[A\n",
      "Training Epoch 11/25:  55%|█████▌    | 162/292 [01:52<01:46,  1.22batch/s, auc=0.8680, loss=0.5808]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 163/292 [01:52<01:35,  1.35batch/s, auc=0.8680, loss=0.5808]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 163/292 [01:52<01:35,  1.35batch/s, auc=0.8682, loss=0.6649]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 164/292 [01:52<01:28,  1.45batch/s, auc=0.8682, loss=0.6649]\u001b[A\n",
      "Training Epoch 11/25:  56%|█████▌    | 164/292 [01:53<01:28,  1.45batch/s, auc=0.8689, loss=0.6350]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 165/292 [01:53<01:22,  1.54batch/s, auc=0.8689, loss=0.6350]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 165/292 [01:54<01:22,  1.54batch/s, auc=0.8678, loss=1.3207]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 166/292 [01:54<01:38,  1.29batch/s, auc=0.8678, loss=1.3207]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 166/292 [01:54<01:38,  1.29batch/s, auc=0.8677, loss=0.7150]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 167/292 [01:54<01:29,  1.40batch/s, auc=0.8677, loss=0.7150]\u001b[A\n",
      "Training Epoch 11/25:  57%|█████▋    | 167/292 [01:55<01:29,  1.40batch/s, auc=0.8676, loss=0.8367]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 168/292 [01:55<01:22,  1.50batch/s, auc=0.8676, loss=0.8367]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 168/292 [01:56<01:22,  1.50batch/s, auc=0.8665, loss=1.2602]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 169/292 [01:56<01:37,  1.27batch/s, auc=0.8665, loss=1.2602]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 169/292 [01:57<01:37,  1.27batch/s, auc=0.8663, loss=0.7661]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 170/292 [01:57<01:27,  1.39batch/s, auc=0.8663, loss=0.7661]\u001b[A\n",
      "Training Epoch 11/25:  58%|█████▊    | 170/292 [01:57<01:27,  1.39batch/s, auc=0.8665, loss=0.6897]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▊    | 171/292 [01:57<01:21,  1.49batch/s, auc=0.8665, loss=0.6897]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▊    | 171/292 [01:58<01:21,  1.49batch/s, auc=0.8662, loss=0.9446]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 172/292 [01:58<01:16,  1.56batch/s, auc=0.8662, loss=0.9446]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 172/292 [01:58<01:16,  1.56batch/s, auc=0.8655, loss=1.0354]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 173/292 [01:58<01:13,  1.62batch/s, auc=0.8655, loss=1.0354]\u001b[A\n",
      "Training Epoch 11/25:  59%|█████▉    | 173/292 [01:59<01:13,  1.62batch/s, auc=0.8654, loss=0.6936]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 174/292 [01:59<01:10,  1.67batch/s, auc=0.8654, loss=0.6936]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 174/292 [01:59<01:10,  1.67batch/s, auc=0.8656, loss=0.6649]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 175/292 [01:59<01:08,  1.70batch/s, auc=0.8656, loss=0.6649]\u001b[A\n",
      "Training Epoch 11/25:  60%|█████▉    | 175/292 [02:00<01:08,  1.70batch/s, auc=0.8658, loss=0.5373]\u001b[A\n",
      "Training Epoch 11/25:  60%|██████    | 176/292 [02:00<01:07,  1.73batch/s, auc=0.8658, loss=0.5373]\u001b[A\n",
      "Training Epoch 11/25:  60%|██████    | 176/292 [02:00<01:07,  1.73batch/s, auc=0.8664, loss=0.5931]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 177/292 [02:00<01:05,  1.74batch/s, auc=0.8664, loss=0.5931]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 177/292 [02:01<01:05,  1.74batch/s, auc=0.8666, loss=0.7824]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 178/292 [02:01<01:04,  1.76batch/s, auc=0.8666, loss=0.7824]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████    | 178/292 [02:02<01:04,  1.76batch/s, auc=0.8665, loss=0.7464]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████▏   | 179/292 [02:02<01:04,  1.76batch/s, auc=0.8665, loss=0.7464]\u001b[A\n",
      "Training Epoch 11/25:  61%|██████▏   | 179/292 [02:02<01:04,  1.76batch/s, auc=0.8667, loss=0.6544]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 180/292 [02:02<01:03,  1.77batch/s, auc=0.8667, loss=0.6544]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 180/292 [02:03<01:03,  1.77batch/s, auc=0.8671, loss=0.5937]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 181/292 [02:03<01:02,  1.77batch/s, auc=0.8671, loss=0.5937]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 181/292 [02:03<01:02,  1.77batch/s, auc=0.8674, loss=0.7027]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 182/292 [02:03<01:01,  1.78batch/s, auc=0.8674, loss=0.7027]\u001b[A\n",
      "Training Epoch 11/25:  62%|██████▏   | 182/292 [02:04<01:01,  1.78batch/s, auc=0.8677, loss=0.6947]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 183/292 [02:04<01:01,  1.78batch/s, auc=0.8677, loss=0.6947]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 183/292 [02:04<01:01,  1.78batch/s, auc=0.8679, loss=0.5627]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 184/292 [02:04<01:00,  1.78batch/s, auc=0.8679, loss=0.5627]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 184/292 [02:05<01:00,  1.78batch/s, auc=0.8678, loss=0.6685]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 185/292 [02:05<01:00,  1.78batch/s, auc=0.8678, loss=0.6685]\u001b[A\n",
      "Training Epoch 11/25:  63%|██████▎   | 185/292 [02:06<01:00,  1.78batch/s, auc=0.8670, loss=1.1309]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▎   | 186/292 [02:06<01:15,  1.39batch/s, auc=0.8670, loss=1.1309]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▎   | 186/292 [02:07<01:15,  1.39batch/s, auc=0.8674, loss=0.6343]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 187/292 [02:07<01:10,  1.49batch/s, auc=0.8674, loss=0.6343]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 187/292 [02:07<01:10,  1.49batch/s, auc=0.8676, loss=0.6768]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 188/292 [02:07<01:06,  1.57batch/s, auc=0.8676, loss=0.6768]\u001b[A\n",
      "Training Epoch 11/25:  64%|██████▍   | 188/292 [02:08<01:06,  1.57batch/s, auc=0.8675, loss=0.8938]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▍   | 189/292 [02:08<01:03,  1.63batch/s, auc=0.8675, loss=0.8938]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▍   | 189/292 [02:09<01:03,  1.63batch/s, auc=0.8675, loss=0.9483]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 190/292 [02:09<01:16,  1.33batch/s, auc=0.8675, loss=0.9483]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 190/292 [02:09<01:16,  1.33batch/s, auc=0.8673, loss=0.8714]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 191/292 [02:09<01:10,  1.43batch/s, auc=0.8673, loss=0.8714]\u001b[A\n",
      "Training Epoch 11/25:  65%|██████▌   | 191/292 [02:10<01:10,  1.43batch/s, auc=0.8672, loss=0.7201]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 192/292 [02:10<01:05,  1.52batch/s, auc=0.8672, loss=0.7201]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 192/292 [02:10<01:05,  1.52batch/s, auc=0.8676, loss=0.7856]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 193/292 [02:10<01:02,  1.59batch/s, auc=0.8676, loss=0.7856]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▌   | 193/292 [02:11<01:02,  1.59batch/s, auc=0.8681, loss=0.5948]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▋   | 194/292 [02:11<00:59,  1.64batch/s, auc=0.8681, loss=0.5948]\u001b[A\n",
      "Training Epoch 11/25:  66%|██████▋   | 194/292 [02:12<00:59,  1.64batch/s, auc=0.8683, loss=0.6549]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 195/292 [02:12<00:57,  1.68batch/s, auc=0.8683, loss=0.6549]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 195/292 [02:12<00:57,  1.68batch/s, auc=0.8687, loss=0.5820]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 196/292 [02:12<00:56,  1.71batch/s, auc=0.8687, loss=0.5820]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 196/292 [02:13<00:56,  1.71batch/s, auc=0.8689, loss=0.6614]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 197/292 [02:13<00:54,  1.73batch/s, auc=0.8689, loss=0.6614]\u001b[A\n",
      "Training Epoch 11/25:  67%|██████▋   | 197/292 [02:13<00:54,  1.73batch/s, auc=0.8694, loss=0.5473]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 198/292 [02:13<00:53,  1.74batch/s, auc=0.8694, loss=0.5473]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 198/292 [02:14<00:53,  1.74batch/s, auc=0.8697, loss=0.5409]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 199/292 [02:14<00:53,  1.75batch/s, auc=0.8697, loss=0.5409]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 199/292 [02:14<00:53,  1.75batch/s, auc=0.8693, loss=0.8291]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 200/292 [02:14<00:52,  1.76batch/s, auc=0.8693, loss=0.8291]\u001b[A\n",
      "Training Epoch 11/25:  68%|██████▊   | 200/292 [02:15<00:52,  1.76batch/s, auc=0.8692, loss=0.8411]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 201/292 [02:15<00:51,  1.77batch/s, auc=0.8692, loss=0.8411]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 201/292 [02:16<00:51,  1.77batch/s, auc=0.8691, loss=0.8462]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 202/292 [02:16<00:50,  1.77batch/s, auc=0.8691, loss=0.8462]\u001b[A\n",
      "Training Epoch 11/25:  69%|██████▉   | 202/292 [02:17<00:50,  1.77batch/s, auc=0.8688, loss=1.0647]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 203/292 [02:17<01:04,  1.39batch/s, auc=0.8688, loss=1.0647]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 203/292 [02:17<01:04,  1.39batch/s, auc=0.8690, loss=0.7918]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 204/292 [02:17<00:59,  1.49batch/s, auc=0.8690, loss=0.7918]\u001b[A\n",
      "Training Epoch 11/25:  70%|██████▉   | 204/292 [02:18<00:59,  1.49batch/s, auc=0.8687, loss=0.7807]\u001b[A\n",
      "Training Epoch 11/25:  70%|███████   | 205/292 [02:18<01:09,  1.26batch/s, auc=0.8687, loss=0.7807]\u001b[A\n",
      "Training Epoch 11/25:  70%|███████   | 205/292 [02:19<01:09,  1.26batch/s, auc=0.8695, loss=0.4424]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 206/292 [02:19<01:02,  1.38batch/s, auc=0.8695, loss=0.4424]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 206/292 [02:19<01:02,  1.38batch/s, auc=0.8696, loss=0.6813]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 207/292 [02:19<00:57,  1.48batch/s, auc=0.8696, loss=0.6813]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 207/292 [02:20<00:57,  1.48batch/s, auc=0.8697, loss=0.7400]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 208/292 [02:20<00:53,  1.56batch/s, auc=0.8697, loss=0.7400]\u001b[A\n",
      "Training Epoch 11/25:  71%|███████   | 208/292 [02:21<00:53,  1.56batch/s, auc=0.8700, loss=0.6812]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 209/292 [02:21<00:51,  1.62batch/s, auc=0.8700, loss=0.6812]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 209/292 [02:21<00:51,  1.62batch/s, auc=0.8700, loss=0.7790]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 210/292 [02:21<00:49,  1.66batch/s, auc=0.8700, loss=0.7790]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 210/292 [02:22<00:49,  1.66batch/s, auc=0.8701, loss=0.7479]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 211/292 [02:22<00:47,  1.69batch/s, auc=0.8701, loss=0.7479]\u001b[A\n",
      "Training Epoch 11/25:  72%|███████▏  | 211/292 [02:23<00:47,  1.69batch/s, auc=0.8693, loss=1.2930]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 212/292 [02:23<00:59,  1.36batch/s, auc=0.8693, loss=1.2930]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 212/292 [02:24<00:59,  1.36batch/s, auc=0.8690, loss=0.8502]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 213/292 [02:24<01:06,  1.19batch/s, auc=0.8690, loss=0.8502]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 213/292 [02:24<01:06,  1.19batch/s, auc=0.8692, loss=0.6694]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 214/292 [02:24<00:59,  1.32batch/s, auc=0.8692, loss=0.6694]\u001b[A\n",
      "Training Epoch 11/25:  73%|███████▎  | 214/292 [02:25<00:59,  1.32batch/s, auc=0.8692, loss=0.6742]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▎  | 215/292 [02:25<00:53,  1.43batch/s, auc=0.8692, loss=0.6742]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▎  | 215/292 [02:26<00:53,  1.43batch/s, auc=0.8691, loss=0.8326]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 216/292 [02:26<00:50,  1.52batch/s, auc=0.8691, loss=0.8326]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 216/292 [02:26<00:50,  1.52batch/s, auc=0.8689, loss=0.7703]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 217/292 [02:26<00:47,  1.59batch/s, auc=0.8689, loss=0.7703]\u001b[A\n",
      "Training Epoch 11/25:  74%|███████▍  | 217/292 [02:27<00:47,  1.59batch/s, auc=0.8685, loss=1.0599]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▍  | 218/292 [02:27<00:56,  1.31batch/s, auc=0.8685, loss=1.0599]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▍  | 218/292 [02:28<00:56,  1.31batch/s, auc=0.8690, loss=0.4649]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 219/292 [02:28<00:51,  1.42batch/s, auc=0.8690, loss=0.4649]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 219/292 [02:28<00:51,  1.42batch/s, auc=0.8691, loss=0.6635]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 220/292 [02:28<00:47,  1.51batch/s, auc=0.8691, loss=0.6635]\u001b[A\n",
      "Training Epoch 11/25:  75%|███████▌  | 220/292 [02:29<00:47,  1.51batch/s, auc=0.8691, loss=0.6309]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 221/292 [02:29<00:44,  1.58batch/s, auc=0.8691, loss=0.6309]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 221/292 [02:30<00:44,  1.58batch/s, auc=0.8686, loss=1.2015]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 222/292 [02:30<00:53,  1.31batch/s, auc=0.8686, loss=1.2015]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▌  | 222/292 [02:30<00:53,  1.31batch/s, auc=0.8685, loss=0.6223]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▋  | 223/292 [02:30<00:48,  1.42batch/s, auc=0.8685, loss=0.6223]\u001b[A\n",
      "Training Epoch 11/25:  76%|███████▋  | 223/292 [02:31<00:48,  1.42batch/s, auc=0.8687, loss=0.5785]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 224/292 [02:31<00:45,  1.51batch/s, auc=0.8687, loss=0.5785]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 224/292 [02:32<00:45,  1.51batch/s, auc=0.8689, loss=0.6560]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 225/292 [02:32<00:42,  1.58batch/s, auc=0.8689, loss=0.6560]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 225/292 [02:32<00:42,  1.58batch/s, auc=0.8690, loss=0.9038]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 226/292 [02:32<00:40,  1.63batch/s, auc=0.8690, loss=0.9038]\u001b[A\n",
      "Training Epoch 11/25:  77%|███████▋  | 226/292 [02:33<00:40,  1.63batch/s, auc=0.8690, loss=0.8263]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 227/292 [02:33<00:38,  1.67batch/s, auc=0.8690, loss=0.8263]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 227/292 [02:33<00:38,  1.67batch/s, auc=0.8693, loss=0.4494]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 228/292 [02:33<00:37,  1.70batch/s, auc=0.8693, loss=0.4494]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 228/292 [02:34<00:37,  1.70batch/s, auc=0.8694, loss=0.5422]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 229/292 [02:34<00:36,  1.72batch/s, auc=0.8694, loss=0.5422]\u001b[A\n",
      "Training Epoch 11/25:  78%|███████▊  | 229/292 [02:34<00:36,  1.72batch/s, auc=0.8700, loss=0.4909]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 230/292 [02:34<00:35,  1.74batch/s, auc=0.8700, loss=0.4909]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 230/292 [02:35<00:35,  1.74batch/s, auc=0.8703, loss=0.5896]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 231/292 [02:35<00:34,  1.75batch/s, auc=0.8703, loss=0.5896]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 231/292 [02:36<00:34,  1.75batch/s, auc=0.8701, loss=0.8998]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 232/292 [02:36<00:34,  1.76batch/s, auc=0.8701, loss=0.8998]\u001b[A\n",
      "Training Epoch 11/25:  79%|███████▉  | 232/292 [02:37<00:34,  1.76batch/s, auc=0.8696, loss=1.1191]\u001b[A\n",
      "Training Epoch 11/25:  80%|███████▉  | 233/292 [02:37<00:42,  1.38batch/s, auc=0.8696, loss=1.1191]\u001b[A\n",
      "Training Epoch 11/25:  80%|███████▉  | 233/292 [02:37<00:42,  1.38batch/s, auc=0.8697, loss=0.6408]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 234/292 [02:37<00:39,  1.48batch/s, auc=0.8697, loss=0.6408]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 234/292 [02:38<00:39,  1.48batch/s, auc=0.8697, loss=0.8377]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 235/292 [02:38<00:36,  1.56batch/s, auc=0.8697, loss=0.8377]\u001b[A\n",
      "Training Epoch 11/25:  80%|████████  | 235/292 [02:39<00:36,  1.56batch/s, auc=0.8695, loss=0.8659]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 236/292 [02:39<00:43,  1.29batch/s, auc=0.8695, loss=0.8659]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 236/292 [02:40<00:43,  1.29batch/s, auc=0.8693, loss=0.8326]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 237/292 [02:40<00:47,  1.16batch/s, auc=0.8693, loss=0.8326]\u001b[A\n",
      "Training Epoch 11/25:  81%|████████  | 237/292 [02:40<00:47,  1.16batch/s, auc=0.8697, loss=0.6466]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 238/292 [02:40<00:41,  1.29batch/s, auc=0.8697, loss=0.6466]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 238/292 [02:42<00:41,  1.29batch/s, auc=0.8690, loss=1.1802]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 239/292 [02:42<00:45,  1.15batch/s, auc=0.8690, loss=1.1802]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 239/292 [02:42<00:45,  1.15batch/s, auc=0.8691, loss=0.7072]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 240/292 [02:42<00:40,  1.29batch/s, auc=0.8691, loss=0.7072]\u001b[A\n",
      "Training Epoch 11/25:  82%|████████▏ | 240/292 [02:43<00:40,  1.29batch/s, auc=0.8694, loss=0.5175]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 241/292 [02:43<00:36,  1.40batch/s, auc=0.8694, loss=0.5175]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 241/292 [02:43<00:36,  1.40batch/s, auc=0.8695, loss=0.5855]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 242/292 [02:43<00:33,  1.50batch/s, auc=0.8695, loss=0.5855]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 242/292 [02:44<00:33,  1.50batch/s, auc=0.8700, loss=0.4361]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 243/292 [02:44<00:31,  1.57batch/s, auc=0.8700, loss=0.4361]\u001b[A\n",
      "Training Epoch 11/25:  83%|████████▎ | 243/292 [02:45<00:31,  1.57batch/s, auc=0.8701, loss=0.6832]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▎ | 244/292 [02:45<00:37,  1.29batch/s, auc=0.8701, loss=0.6832]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▎ | 244/292 [02:45<00:37,  1.29batch/s, auc=0.8698, loss=1.2019]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 245/292 [02:45<00:33,  1.41batch/s, auc=0.8698, loss=1.2019]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 245/292 [02:46<00:33,  1.41batch/s, auc=0.8700, loss=0.5258]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 246/292 [02:46<00:30,  1.50batch/s, auc=0.8700, loss=0.5258]\u001b[A\n",
      "Training Epoch 11/25:  84%|████████▍ | 246/292 [02:47<00:30,  1.50batch/s, auc=0.8700, loss=0.8447]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 247/292 [02:47<00:28,  1.57batch/s, auc=0.8700, loss=0.8447]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 247/292 [02:47<00:28,  1.57batch/s, auc=0.8701, loss=0.5502]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 248/292 [02:47<00:27,  1.63batch/s, auc=0.8701, loss=0.5502]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▍ | 248/292 [02:48<00:27,  1.63batch/s, auc=0.8699, loss=0.8935]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▌ | 249/292 [02:48<00:25,  1.67batch/s, auc=0.8699, loss=0.8935]\u001b[A\n",
      "Training Epoch 11/25:  85%|████████▌ | 249/292 [02:48<00:25,  1.67batch/s, auc=0.8700, loss=0.5497]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 250/292 [02:48<00:24,  1.70batch/s, auc=0.8700, loss=0.5497]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 250/292 [02:49<00:24,  1.70batch/s, auc=0.8693, loss=1.4460]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 251/292 [02:49<00:30,  1.36batch/s, auc=0.8693, loss=1.4460]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▌ | 251/292 [02:50<00:30,  1.36batch/s, auc=0.8693, loss=0.9368]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▋ | 252/292 [02:50<00:27,  1.46batch/s, auc=0.8693, loss=0.9368]\u001b[A\n",
      "Training Epoch 11/25:  86%|████████▋ | 252/292 [02:51<00:27,  1.46batch/s, auc=0.8688, loss=1.0798]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 253/292 [02:51<00:31,  1.24batch/s, auc=0.8688, loss=1.0798]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 253/292 [02:52<00:31,  1.24batch/s, auc=0.8685, loss=0.9868]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 254/292 [02:52<00:27,  1.37batch/s, auc=0.8685, loss=0.9868]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 254/292 [02:52<00:27,  1.37batch/s, auc=0.8687, loss=0.5826]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 255/292 [02:52<00:25,  1.47batch/s, auc=0.8687, loss=0.5826]\u001b[A\n",
      "Training Epoch 11/25:  87%|████████▋ | 255/292 [02:53<00:25,  1.47batch/s, auc=0.8690, loss=0.4988]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 256/292 [02:53<00:23,  1.55batch/s, auc=0.8690, loss=0.4988]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 256/292 [02:53<00:23,  1.55batch/s, auc=0.8693, loss=0.4712]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 257/292 [02:53<00:21,  1.61batch/s, auc=0.8693, loss=0.4712]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 257/292 [02:54<00:21,  1.61batch/s, auc=0.8688, loss=1.0129]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 258/292 [02:54<00:25,  1.32batch/s, auc=0.8688, loss=1.0129]\u001b[A\n",
      "Training Epoch 11/25:  88%|████████▊ | 258/292 [02:55<00:25,  1.32batch/s, auc=0.8686, loss=0.7502]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▊ | 259/292 [02:55<00:23,  1.42batch/s, auc=0.8686, loss=0.7502]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▊ | 259/292 [02:56<00:23,  1.42batch/s, auc=0.8687, loss=0.8302]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 260/292 [02:56<00:21,  1.51batch/s, auc=0.8687, loss=0.8302]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 260/292 [02:57<00:21,  1.51batch/s, auc=0.8682, loss=1.0633]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 261/292 [02:57<00:24,  1.27batch/s, auc=0.8682, loss=1.0633]\u001b[A\n",
      "Training Epoch 11/25:  89%|████████▉ | 261/292 [02:58<00:24,  1.27batch/s, auc=0.8680, loss=0.8061]\u001b[A\n",
      "Training Epoch 11/25:  90%|████████▉ | 262/292 [02:58<00:26,  1.14batch/s, auc=0.8680, loss=0.8061]\u001b[A\n",
      "Training Epoch 11/25:  90%|████████▉ | 262/292 [02:59<00:26,  1.14batch/s, auc=0.8679, loss=0.8519]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 263/292 [02:59<00:27,  1.07batch/s, auc=0.8679, loss=0.8519]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 263/292 [02:59<00:27,  1.07batch/s, auc=0.8682, loss=0.6624]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 264/292 [02:59<00:23,  1.21batch/s, auc=0.8682, loss=0.6624]\u001b[A\n",
      "Training Epoch 11/25:  90%|█████████ | 264/292 [03:00<00:23,  1.21batch/s, auc=0.8682, loss=0.7530]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 265/292 [03:00<00:20,  1.33batch/s, auc=0.8682, loss=0.7530]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 265/292 [03:00<00:20,  1.33batch/s, auc=0.8684, loss=0.6177]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 266/292 [03:00<00:18,  1.44batch/s, auc=0.8684, loss=0.6177]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████ | 266/292 [03:01<00:18,  1.44batch/s, auc=0.8685, loss=0.5647]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████▏| 267/292 [03:01<00:16,  1.53batch/s, auc=0.8685, loss=0.5647]\u001b[A\n",
      "Training Epoch 11/25:  91%|█████████▏| 267/292 [03:02<00:16,  1.53batch/s, auc=0.8685, loss=0.7004]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 268/292 [03:02<00:18,  1.28batch/s, auc=0.8685, loss=0.7004]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 268/292 [03:03<00:18,  1.28batch/s, auc=0.8684, loss=0.7104]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 269/292 [03:03<00:16,  1.39batch/s, auc=0.8684, loss=0.7104]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 269/292 [03:03<00:16,  1.39batch/s, auc=0.8686, loss=0.5631]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 270/292 [03:03<00:14,  1.49batch/s, auc=0.8686, loss=0.5631]\u001b[A\n",
      "Training Epoch 11/25:  92%|█████████▏| 270/292 [03:04<00:14,  1.49batch/s, auc=0.8681, loss=0.9355]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 271/292 [03:04<00:16,  1.26batch/s, auc=0.8681, loss=0.9355]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 271/292 [03:05<00:16,  1.26batch/s, auc=0.8677, loss=1.0304]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 272/292 [03:05<00:17,  1.13batch/s, auc=0.8677, loss=1.0304]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 272/292 [03:06<00:17,  1.13batch/s, auc=0.8679, loss=0.6005]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 273/292 [03:06<00:14,  1.27batch/s, auc=0.8679, loss=0.6005]\u001b[A\n",
      "Training Epoch 11/25:  93%|█████████▎| 273/292 [03:07<00:14,  1.27batch/s, auc=0.8678, loss=0.8690]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 274/292 [03:07<00:12,  1.39batch/s, auc=0.8678, loss=0.8690]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 274/292 [03:08<00:12,  1.39batch/s, auc=0.8674, loss=1.1780]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 275/292 [03:08<00:14,  1.21batch/s, auc=0.8674, loss=1.1780]\u001b[A\n",
      "Training Epoch 11/25:  94%|█████████▍| 275/292 [03:08<00:14,  1.21batch/s, auc=0.8675, loss=0.5828]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 276/292 [03:08<00:12,  1.33batch/s, auc=0.8675, loss=0.5828]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 276/292 [03:09<00:12,  1.33batch/s, auc=0.8666, loss=1.6494]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 277/292 [03:09<00:12,  1.18batch/s, auc=0.8666, loss=1.6494]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▍| 277/292 [03:10<00:12,  1.18batch/s, auc=0.8669, loss=0.4720]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▌| 278/292 [03:10<00:10,  1.31batch/s, auc=0.8669, loss=0.4720]\u001b[A\n",
      "Training Epoch 11/25:  95%|█████████▌| 278/292 [03:10<00:10,  1.31batch/s, auc=0.8669, loss=0.7975]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 279/292 [03:10<00:09,  1.42batch/s, auc=0.8669, loss=0.7975]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 279/292 [03:11<00:09,  1.42batch/s, auc=0.8671, loss=0.5912]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 280/292 [03:11<00:07,  1.51batch/s, auc=0.8671, loss=0.5912]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 280/292 [03:12<00:07,  1.51batch/s, auc=0.8672, loss=0.6081]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 281/292 [03:12<00:06,  1.58batch/s, auc=0.8672, loss=0.6081]\u001b[A\n",
      "Training Epoch 11/25:  96%|█████████▌| 281/292 [03:12<00:06,  1.58batch/s, auc=0.8676, loss=0.6032]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 282/292 [03:12<00:06,  1.63batch/s, auc=0.8676, loss=0.6032]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 282/292 [03:13<00:06,  1.63batch/s, auc=0.8679, loss=0.5338]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 283/292 [03:13<00:05,  1.67batch/s, auc=0.8679, loss=0.5338]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 283/292 [03:13<00:05,  1.67batch/s, auc=0.8680, loss=0.7447]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 284/292 [03:13<00:04,  1.70batch/s, auc=0.8680, loss=0.7447]\u001b[A\n",
      "Training Epoch 11/25:  97%|█████████▋| 284/292 [03:14<00:04,  1.70batch/s, auc=0.8680, loss=0.7794]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 285/292 [03:14<00:04,  1.72batch/s, auc=0.8680, loss=0.7794]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 285/292 [03:14<00:04,  1.72batch/s, auc=0.8679, loss=0.8570]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 286/292 [03:14<00:03,  1.73batch/s, auc=0.8679, loss=0.8570]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 286/292 [03:15<00:03,  1.73batch/s, auc=0.8680, loss=0.7864]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 287/292 [03:15<00:02,  1.74batch/s, auc=0.8680, loss=0.7864]\u001b[A\n",
      "Training Epoch 11/25:  98%|█████████▊| 287/292 [03:15<00:02,  1.74batch/s, auc=0.8683, loss=0.4923]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▊| 288/292 [03:15<00:02,  1.75batch/s, auc=0.8683, loss=0.4923]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▊| 288/292 [03:16<00:02,  1.75batch/s, auc=0.8686, loss=0.4956]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 289/292 [03:16<00:01,  1.76batch/s, auc=0.8686, loss=0.4956]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 289/292 [03:17<00:01,  1.76batch/s, auc=0.8682, loss=1.3197]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 290/292 [03:17<00:01,  1.76batch/s, auc=0.8682, loss=1.3197]\u001b[A\n",
      "Training Epoch 11/25:  99%|█████████▉| 290/292 [03:17<00:01,  1.76batch/s, auc=0.8684, loss=0.7623]\u001b[A\n",
      "Training Epoch 11/25: 100%|█████████▉| 291/292 [03:17<00:00,  1.76batch/s, auc=0.8684, loss=0.7623]\u001b[A\n",
      "Training Epoch 11/25: 100%|█████████▉| 291/292 [03:18<00:00,  1.76batch/s, auc=0.8684, loss=0.7258]\u001b[A\n",
      "Training Epoch 11/25: 100%|██████████| 292/292 [03:18<00:00,  1.47batch/s, auc=0.8684, loss=0.7258]\u001b[A\n",
      "Epochs:  44%|████▍     | 11/25 [38:57<49:56, 214.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25] Train Loss: 0.7627 | Train AUROC: 0.8684 Val Loss: 0.8663 | Val AUROC: 0.8409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 12/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8305, loss=0.6763]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 1/292 [00:01<07:28,  1.54s/batch, auc=0.8305, loss=0.6763]\u001b[A\n",
      "Training Epoch 12/25:   0%|          | 1/292 [00:02<07:28,  1.54s/batch, auc=0.8967, loss=0.6491]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 2/292 [00:02<04:38,  1.04batch/s, auc=0.8967, loss=0.6491]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 2/292 [00:03<04:38,  1.04batch/s, auc=0.8818, loss=0.8860]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 3/292 [00:03<04:52,  1.01s/batch, auc=0.8818, loss=0.8860]\u001b[A\n",
      "Training Epoch 12/25:   1%|          | 3/292 [00:03<04:52,  1.01s/batch, auc=0.9043, loss=0.5150]\u001b[A\n",
      "Training Epoch 12/25:   1%|▏         | 4/292 [00:03<03:59,  1.20batch/s, auc=0.9043, loss=0.5150]\u001b[A\n",
      "Training Epoch 12/25:   1%|▏         | 4/292 [00:04<03:59,  1.20batch/s, auc=0.8964, loss=0.6729]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 5/292 [00:04<03:29,  1.37batch/s, auc=0.8964, loss=0.6729]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 5/292 [00:04<03:29,  1.37batch/s, auc=0.8965, loss=0.6377]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 6/292 [00:04<03:12,  1.49batch/s, auc=0.8965, loss=0.6377]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 6/292 [00:05<03:12,  1.49batch/s, auc=0.8872, loss=0.9285]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 7/292 [00:05<03:48,  1.25batch/s, auc=0.8872, loss=0.9285]\u001b[A\n",
      "Training Epoch 12/25:   2%|▏         | 7/292 [00:06<03:48,  1.25batch/s, auc=0.8841, loss=0.5965]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 8/292 [00:06<03:25,  1.38batch/s, auc=0.8841, loss=0.5965]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 8/292 [00:07<03:25,  1.38batch/s, auc=0.8817, loss=0.7821]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 9/292 [00:07<03:09,  1.49batch/s, auc=0.8817, loss=0.7821]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 9/292 [00:07<03:09,  1.49batch/s, auc=0.8804, loss=0.7617]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 10/292 [00:07<02:59,  1.57batch/s, auc=0.8804, loss=0.7617]\u001b[A\n",
      "Training Epoch 12/25:   3%|▎         | 10/292 [00:08<02:59,  1.57batch/s, auc=0.8679, loss=1.1291]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 11/292 [00:08<03:35,  1.30batch/s, auc=0.8679, loss=1.1291]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 11/292 [00:09<03:35,  1.30batch/s, auc=0.8673, loss=0.7508]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 12/292 [00:09<03:17,  1.42batch/s, auc=0.8673, loss=0.7508]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 12/292 [00:10<03:17,  1.42batch/s, auc=0.8656, loss=0.8718]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 13/292 [00:10<03:47,  1.22batch/s, auc=0.8656, loss=0.8718]\u001b[A\n",
      "Training Epoch 12/25:   4%|▍         | 13/292 [00:11<03:47,  1.22batch/s, auc=0.8667, loss=0.7770]\u001b[A\n",
      "Training Epoch 12/25:   5%|▍         | 14/292 [00:11<04:08,  1.12batch/s, auc=0.8667, loss=0.7770]\u001b[A\n",
      "Training Epoch 12/25:   5%|▍         | 14/292 [00:12<04:08,  1.12batch/s, auc=0.8620, loss=0.8658]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 15/292 [00:12<04:22,  1.05batch/s, auc=0.8620, loss=0.8658]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 15/292 [00:12<04:22,  1.05batch/s, auc=0.8577, loss=0.9634]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 16/292 [00:12<03:49,  1.21batch/s, auc=0.8577, loss=0.9634]\u001b[A\n",
      "Training Epoch 12/25:   5%|▌         | 16/292 [00:13<03:49,  1.21batch/s, auc=0.8577, loss=0.6036]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 17/292 [00:13<03:25,  1.34batch/s, auc=0.8577, loss=0.6036]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 17/292 [00:14<03:25,  1.34batch/s, auc=0.8650, loss=0.5657]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 18/292 [00:14<03:08,  1.45batch/s, auc=0.8650, loss=0.5657]\u001b[A\n",
      "Training Epoch 12/25:   6%|▌         | 18/292 [00:14<03:08,  1.45batch/s, auc=0.8716, loss=0.5646]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 19/292 [00:14<02:57,  1.54batch/s, auc=0.8716, loss=0.5646]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 19/292 [00:15<02:57,  1.54batch/s, auc=0.8746, loss=0.5548]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 20/292 [00:15<02:48,  1.61batch/s, auc=0.8746, loss=0.5548]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 20/292 [00:16<02:48,  1.61batch/s, auc=0.8697, loss=1.0974]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 21/292 [00:16<03:24,  1.32batch/s, auc=0.8697, loss=1.0974]\u001b[A\n",
      "Training Epoch 12/25:   7%|▋         | 21/292 [00:16<03:24,  1.32batch/s, auc=0.8724, loss=0.6161]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 22/292 [00:16<03:08,  1.44batch/s, auc=0.8724, loss=0.6161]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 22/292 [00:17<03:08,  1.44batch/s, auc=0.8684, loss=1.1116]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 23/292 [00:17<03:37,  1.24batch/s, auc=0.8684, loss=1.1116]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 23/292 [00:18<03:37,  1.24batch/s, auc=0.8657, loss=1.0241]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 24/292 [00:18<03:16,  1.36batch/s, auc=0.8657, loss=1.0241]\u001b[A\n",
      "Training Epoch 12/25:   8%|▊         | 24/292 [00:19<03:16,  1.36batch/s, auc=0.8683, loss=0.5759]\u001b[A\n",
      "Training Epoch 12/25:   9%|▊         | 25/292 [00:19<03:01,  1.47batch/s, auc=0.8683, loss=0.5759]\u001b[A\n",
      "Training Epoch 12/25:   9%|▊         | 25/292 [00:20<03:01,  1.47batch/s, auc=0.8687, loss=0.6620]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 26/292 [00:20<03:32,  1.25batch/s, auc=0.8687, loss=0.6620]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 26/292 [00:20<03:32,  1.25batch/s, auc=0.8701, loss=0.7052]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 27/292 [00:20<03:12,  1.38batch/s, auc=0.8701, loss=0.7052]\u001b[A\n",
      "Training Epoch 12/25:   9%|▉         | 27/292 [00:21<03:12,  1.38batch/s, auc=0.8703, loss=0.7508]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 28/292 [00:21<02:58,  1.48batch/s, auc=0.8703, loss=0.7508]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 28/292 [00:21<02:58,  1.48batch/s, auc=0.8745, loss=0.3943]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 29/292 [00:21<02:47,  1.57batch/s, auc=0.8745, loss=0.3943]\u001b[A\n",
      "Training Epoch 12/25:  10%|▉         | 29/292 [00:22<02:47,  1.57batch/s, auc=0.8747, loss=0.6523]\u001b[A\n",
      "Training Epoch 12/25:  10%|█         | 30/292 [00:22<02:40,  1.63batch/s, auc=0.8747, loss=0.6523]\u001b[A\n",
      "Training Epoch 12/25:  10%|█         | 30/292 [00:23<02:40,  1.63batch/s, auc=0.8668, loss=1.6343]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 31/292 [00:23<03:16,  1.33batch/s, auc=0.8668, loss=1.6343]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 31/292 [00:24<03:16,  1.33batch/s, auc=0.8629, loss=1.1915]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 32/292 [00:24<03:40,  1.18batch/s, auc=0.8629, loss=1.1915]\u001b[A\n",
      "Training Epoch 12/25:  11%|█         | 32/292 [00:25<03:40,  1.18batch/s, auc=0.8619, loss=0.8161]\u001b[A\n",
      "Training Epoch 12/25:  11%|█▏        | 33/292 [00:25<03:17,  1.31batch/s, auc=0.8619, loss=0.8161]\u001b[A\n",
      "Training Epoch 12/25:  11%|█▏        | 33/292 [00:25<03:17,  1.31batch/s, auc=0.8642, loss=0.5149]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 34/292 [00:25<03:00,  1.43batch/s, auc=0.8642, loss=0.5149]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 34/292 [00:26<03:00,  1.43batch/s, auc=0.8599, loss=1.0556]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 35/292 [00:26<03:28,  1.23batch/s, auc=0.8599, loss=1.0556]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 35/292 [00:27<03:28,  1.23batch/s, auc=0.8616, loss=0.6054]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 36/292 [00:27<03:08,  1.36batch/s, auc=0.8616, loss=0.6054]\u001b[A\n",
      "Training Epoch 12/25:  12%|█▏        | 36/292 [00:27<03:08,  1.36batch/s, auc=0.8608, loss=0.8017]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 37/292 [00:27<02:53,  1.47batch/s, auc=0.8608, loss=0.8017]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 37/292 [00:28<02:53,  1.47batch/s, auc=0.8633, loss=0.4840]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 38/292 [00:28<02:43,  1.55batch/s, auc=0.8633, loss=0.4840]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 38/292 [00:28<02:43,  1.55batch/s, auc=0.8648, loss=0.5002]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 39/292 [00:28<02:36,  1.62batch/s, auc=0.8648, loss=0.5002]\u001b[A\n",
      "Training Epoch 12/25:  13%|█▎        | 39/292 [00:29<02:36,  1.62batch/s, auc=0.8637, loss=0.9941]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▎        | 40/292 [00:29<02:30,  1.67batch/s, auc=0.8637, loss=0.9941]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▎        | 40/292 [00:29<02:30,  1.67batch/s, auc=0.8647, loss=0.7184]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 41/292 [00:29<02:26,  1.71batch/s, auc=0.8647, loss=0.7184]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 41/292 [00:31<02:26,  1.71batch/s, auc=0.8655, loss=0.7899]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 42/292 [00:31<03:02,  1.37batch/s, auc=0.8655, loss=0.7899]\u001b[A\n",
      "Training Epoch 12/25:  14%|█▍        | 42/292 [00:31<03:02,  1.37batch/s, auc=0.8646, loss=0.7489]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▍        | 43/292 [00:31<02:49,  1.47batch/s, auc=0.8646, loss=0.7489]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▍        | 43/292 [00:32<02:49,  1.47batch/s, auc=0.8659, loss=0.7278]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 44/292 [00:32<02:39,  1.56batch/s, auc=0.8659, loss=0.7278]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 44/292 [00:32<02:39,  1.56batch/s, auc=0.8640, loss=0.8949]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 45/292 [00:32<02:32,  1.62batch/s, auc=0.8640, loss=0.8949]\u001b[A\n",
      "Training Epoch 12/25:  15%|█▌        | 45/292 [00:33<02:32,  1.62batch/s, auc=0.8644, loss=0.6896]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 46/292 [00:33<02:27,  1.67batch/s, auc=0.8644, loss=0.6896]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 46/292 [00:34<02:27,  1.67batch/s, auc=0.8607, loss=1.3765]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 47/292 [00:34<03:01,  1.35batch/s, auc=0.8607, loss=1.3765]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▌        | 47/292 [00:35<03:01,  1.35batch/s, auc=0.8577, loss=1.1697]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▋        | 48/292 [00:35<03:25,  1.19batch/s, auc=0.8577, loss=1.1697]\u001b[A\n",
      "Training Epoch 12/25:  16%|█▋        | 48/292 [00:35<03:25,  1.19batch/s, auc=0.8600, loss=0.4320]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 49/292 [00:35<03:03,  1.32batch/s, auc=0.8600, loss=0.4320]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 49/292 [00:36<03:03,  1.32batch/s, auc=0.8613, loss=0.6412]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 50/292 [00:36<02:48,  1.44batch/s, auc=0.8613, loss=0.6412]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 50/292 [00:37<02:48,  1.44batch/s, auc=0.8622, loss=0.5568]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 51/292 [00:37<02:37,  1.53batch/s, auc=0.8622, loss=0.5568]\u001b[A\n",
      "Training Epoch 12/25:  17%|█▋        | 51/292 [00:37<02:37,  1.53batch/s, auc=0.8617, loss=0.7383]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 52/292 [00:37<02:30,  1.60batch/s, auc=0.8617, loss=0.7383]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 52/292 [00:38<02:30,  1.60batch/s, auc=0.8626, loss=0.6864]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 53/292 [00:38<02:24,  1.65batch/s, auc=0.8626, loss=0.6864]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 53/292 [00:38<02:24,  1.65batch/s, auc=0.8635, loss=0.5302]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 54/292 [00:38<02:20,  1.69batch/s, auc=0.8635, loss=0.5302]\u001b[A\n",
      "Training Epoch 12/25:  18%|█▊        | 54/292 [00:39<02:20,  1.69batch/s, auc=0.8631, loss=0.8573]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 55/292 [00:39<02:17,  1.72batch/s, auc=0.8631, loss=0.8573]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 55/292 [00:39<02:17,  1.72batch/s, auc=0.8639, loss=0.7580]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 56/292 [00:39<02:15,  1.75batch/s, auc=0.8639, loss=0.7580]\u001b[A\n",
      "Training Epoch 12/25:  19%|█▉        | 56/292 [00:40<02:15,  1.75batch/s, auc=0.8628, loss=0.9140]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 57/292 [00:40<02:13,  1.76batch/s, auc=0.8628, loss=0.9140]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 57/292 [00:40<02:13,  1.76batch/s, auc=0.8636, loss=0.5316]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 58/292 [00:40<02:12,  1.77batch/s, auc=0.8636, loss=0.5316]\u001b[A\n",
      "Training Epoch 12/25:  20%|█▉        | 58/292 [00:41<02:12,  1.77batch/s, auc=0.8635, loss=0.7221]\u001b[A\n",
      "Training Epoch 12/25:  20%|██        | 59/292 [00:41<02:11,  1.77batch/s, auc=0.8635, loss=0.7221]\u001b[A\n",
      "Training Epoch 12/25:  20%|██        | 59/292 [00:42<02:11,  1.77batch/s, auc=0.8637, loss=0.6346]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 60/292 [00:42<02:10,  1.78batch/s, auc=0.8637, loss=0.6346]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 60/292 [00:42<02:10,  1.78batch/s, auc=0.8655, loss=0.4582]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 61/292 [00:42<02:09,  1.78batch/s, auc=0.8655, loss=0.4582]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 61/292 [00:43<02:09,  1.78batch/s, auc=0.8671, loss=0.5507]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 62/292 [00:43<02:08,  1.79batch/s, auc=0.8671, loss=0.5507]\u001b[A\n",
      "Training Epoch 12/25:  21%|██        | 62/292 [00:43<02:08,  1.79batch/s, auc=0.8676, loss=0.7683]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 63/292 [00:43<02:08,  1.79batch/s, auc=0.8676, loss=0.7683]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 63/292 [00:44<02:08,  1.79batch/s, auc=0.8672, loss=0.7009]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 64/292 [00:44<02:42,  1.40batch/s, auc=0.8672, loss=0.7009]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 64/292 [00:45<02:42,  1.40batch/s, auc=0.8688, loss=0.6033]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 65/292 [00:45<02:31,  1.50batch/s, auc=0.8688, loss=0.6033]\u001b[A\n",
      "Training Epoch 12/25:  22%|██▏       | 65/292 [00:45<02:31,  1.50batch/s, auc=0.8696, loss=0.7479]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 66/292 [00:45<02:23,  1.58batch/s, auc=0.8696, loss=0.7479]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 66/292 [00:46<02:23,  1.58batch/s, auc=0.8700, loss=0.5902]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 67/292 [00:46<02:17,  1.64batch/s, auc=0.8700, loss=0.5902]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 67/292 [00:47<02:17,  1.64batch/s, auc=0.8693, loss=0.8670]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 68/292 [00:47<02:13,  1.68batch/s, auc=0.8693, loss=0.8670]\u001b[A\n",
      "Training Epoch 12/25:  23%|██▎       | 68/292 [00:47<02:13,  1.68batch/s, auc=0.8707, loss=0.5148]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▎       | 69/292 [00:47<02:10,  1.71batch/s, auc=0.8707, loss=0.5148]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▎       | 69/292 [00:48<02:10,  1.71batch/s, auc=0.8693, loss=0.9639]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 70/292 [00:48<02:42,  1.37batch/s, auc=0.8693, loss=0.9639]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 70/292 [00:49<02:42,  1.37batch/s, auc=0.8688, loss=0.8456]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 71/292 [00:49<02:30,  1.47batch/s, auc=0.8688, loss=0.8456]\u001b[A\n",
      "Training Epoch 12/25:  24%|██▍       | 71/292 [00:49<02:30,  1.47batch/s, auc=0.8693, loss=0.6379]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▍       | 72/292 [00:49<02:21,  1.55batch/s, auc=0.8693, loss=0.6379]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▍       | 72/292 [00:50<02:21,  1.55batch/s, auc=0.8696, loss=0.8124]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 73/292 [00:50<02:15,  1.62batch/s, auc=0.8696, loss=0.8124]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 73/292 [00:50<02:15,  1.62batch/s, auc=0.8690, loss=0.8749]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 74/292 [00:50<02:11,  1.66batch/s, auc=0.8690, loss=0.8749]\u001b[A\n",
      "Training Epoch 12/25:  25%|██▌       | 74/292 [00:51<02:11,  1.66batch/s, auc=0.8705, loss=0.5115]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 75/292 [00:51<02:07,  1.70batch/s, auc=0.8705, loss=0.5115]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 75/292 [00:52<02:07,  1.70batch/s, auc=0.8708, loss=0.5752]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 76/292 [00:52<02:05,  1.72batch/s, auc=0.8708, loss=0.5752]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▌       | 76/292 [00:52<02:05,  1.72batch/s, auc=0.8710, loss=0.5413]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▋       | 77/292 [00:52<02:03,  1.74batch/s, auc=0.8710, loss=0.5413]\u001b[A\n",
      "Training Epoch 12/25:  26%|██▋       | 77/292 [00:53<02:03,  1.74batch/s, auc=0.8699, loss=1.1010]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 78/292 [00:53<02:34,  1.38batch/s, auc=0.8699, loss=1.1010]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 78/292 [00:54<02:34,  1.38batch/s, auc=0.8711, loss=0.5468]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 79/292 [00:54<02:23,  1.48batch/s, auc=0.8711, loss=0.5468]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 79/292 [00:54<02:23,  1.48batch/s, auc=0.8713, loss=0.6428]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 80/292 [00:54<02:15,  1.56batch/s, auc=0.8713, loss=0.6428]\u001b[A\n",
      "Training Epoch 12/25:  27%|██▋       | 80/292 [00:55<02:15,  1.56batch/s, auc=0.8709, loss=0.7349]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 81/292 [00:55<02:42,  1.30batch/s, auc=0.8709, loss=0.7349]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 81/292 [00:56<02:42,  1.30batch/s, auc=0.8705, loss=0.9924]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 82/292 [00:56<02:28,  1.42batch/s, auc=0.8705, loss=0.9924]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 82/292 [00:57<02:28,  1.42batch/s, auc=0.8722, loss=0.4531]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 83/292 [00:57<02:18,  1.51batch/s, auc=0.8722, loss=0.4531]\u001b[A\n",
      "Training Epoch 12/25:  28%|██▊       | 83/292 [00:57<02:18,  1.51batch/s, auc=0.8724, loss=0.6814]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 84/292 [00:57<02:11,  1.59batch/s, auc=0.8724, loss=0.6814]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 84/292 [00:58<02:11,  1.59batch/s, auc=0.8710, loss=1.1208]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 85/292 [00:58<02:38,  1.31batch/s, auc=0.8710, loss=1.1208]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 85/292 [00:59<02:38,  1.31batch/s, auc=0.8712, loss=0.7781]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 86/292 [00:59<02:24,  1.42batch/s, auc=0.8712, loss=0.7781]\u001b[A\n",
      "Training Epoch 12/25:  29%|██▉       | 86/292 [00:59<02:24,  1.42batch/s, auc=0.8713, loss=0.7478]\u001b[A\n",
      "Training Epoch 12/25:  30%|██▉       | 87/292 [00:59<02:15,  1.52batch/s, auc=0.8713, loss=0.7478]\u001b[A\n",
      "Training Epoch 12/25:  30%|██▉       | 87/292 [01:00<02:15,  1.52batch/s, auc=0.8716, loss=0.7626]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 88/292 [01:00<02:08,  1.59batch/s, auc=0.8716, loss=0.7626]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 88/292 [01:00<02:08,  1.59batch/s, auc=0.8731, loss=0.5447]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 89/292 [01:00<02:03,  1.65batch/s, auc=0.8731, loss=0.5447]\u001b[A\n",
      "Training Epoch 12/25:  30%|███       | 89/292 [01:01<02:03,  1.65batch/s, auc=0.8741, loss=0.6266]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 90/292 [01:01<01:59,  1.69batch/s, auc=0.8741, loss=0.6266]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 90/292 [01:01<01:59,  1.69batch/s, auc=0.8751, loss=0.6731]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 91/292 [01:01<01:57,  1.72batch/s, auc=0.8751, loss=0.6731]\u001b[A\n",
      "Training Epoch 12/25:  31%|███       | 91/292 [01:02<01:57,  1.72batch/s, auc=0.8752, loss=0.7658]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 92/292 [01:02<01:55,  1.74batch/s, auc=0.8752, loss=0.7658]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 92/292 [01:03<01:55,  1.74batch/s, auc=0.8748, loss=0.8551]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 93/292 [01:03<01:53,  1.75batch/s, auc=0.8748, loss=0.8551]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 93/292 [01:03<01:53,  1.75batch/s, auc=0.8754, loss=0.7275]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 94/292 [01:03<01:52,  1.76batch/s, auc=0.8754, loss=0.7275]\u001b[A\n",
      "Training Epoch 12/25:  32%|███▏      | 94/292 [01:04<01:52,  1.76batch/s, auc=0.8756, loss=0.6890]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 95/292 [01:04<01:51,  1.77batch/s, auc=0.8756, loss=0.6890]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 95/292 [01:04<01:51,  1.77batch/s, auc=0.8757, loss=0.8513]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 96/292 [01:04<01:50,  1.78batch/s, auc=0.8757, loss=0.8513]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 96/292 [01:05<01:50,  1.78batch/s, auc=0.8730, loss=1.7744]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 97/292 [01:05<02:19,  1.40batch/s, auc=0.8730, loss=1.7744]\u001b[A\n",
      "Training Epoch 12/25:  33%|███▎      | 97/292 [01:06<02:19,  1.40batch/s, auc=0.8724, loss=0.6702]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▎      | 98/292 [01:06<02:09,  1.49batch/s, auc=0.8724, loss=0.6702]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▎      | 98/292 [01:07<02:09,  1.49batch/s, auc=0.8722, loss=0.8803]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 99/292 [01:07<02:32,  1.26batch/s, auc=0.8722, loss=0.8803]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 99/292 [01:08<02:32,  1.26batch/s, auc=0.8722, loss=0.6845]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 100/292 [01:08<02:18,  1.39batch/s, auc=0.8722, loss=0.6845]\u001b[A\n",
      "Training Epoch 12/25:  34%|███▍      | 100/292 [01:08<02:18,  1.39batch/s, auc=0.8717, loss=0.9530]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 101/292 [01:08<02:08,  1.49batch/s, auc=0.8717, loss=0.9530]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 101/292 [01:09<02:08,  1.49batch/s, auc=0.8720, loss=0.5809]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 102/292 [01:09<02:01,  1.57batch/s, auc=0.8720, loss=0.5809]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▍      | 102/292 [01:09<02:01,  1.57batch/s, auc=0.8728, loss=0.5234]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▌      | 103/292 [01:09<01:55,  1.63batch/s, auc=0.8728, loss=0.5234]\u001b[A\n",
      "Training Epoch 12/25:  35%|███▌      | 103/292 [01:10<01:55,  1.63batch/s, auc=0.8727, loss=0.8438]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 104/292 [01:10<02:21,  1.33batch/s, auc=0.8727, loss=0.8438]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 104/292 [01:11<02:21,  1.33batch/s, auc=0.8721, loss=0.8990]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 105/292 [01:11<02:38,  1.18batch/s, auc=0.8721, loss=0.8990]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▌      | 105/292 [01:12<02:38,  1.18batch/s, auc=0.8726, loss=0.5862]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▋      | 106/292 [01:12<02:21,  1.31batch/s, auc=0.8726, loss=0.5862]\u001b[A\n",
      "Training Epoch 12/25:  36%|███▋      | 106/292 [01:12<02:21,  1.31batch/s, auc=0.8725, loss=0.7105]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 107/292 [01:12<02:09,  1.43batch/s, auc=0.8725, loss=0.7105]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 107/292 [01:13<02:09,  1.43batch/s, auc=0.8728, loss=0.7313]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 108/292 [01:13<02:01,  1.52batch/s, auc=0.8728, loss=0.7313]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 108/292 [01:14<02:01,  1.52batch/s, auc=0.8736, loss=0.4572]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 109/292 [01:14<01:55,  1.59batch/s, auc=0.8736, loss=0.4572]\u001b[A\n",
      "Training Epoch 12/25:  37%|███▋      | 109/292 [01:14<01:55,  1.59batch/s, auc=0.8740, loss=0.7303]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 110/292 [01:14<01:50,  1.65batch/s, auc=0.8740, loss=0.7303]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 110/292 [01:15<01:50,  1.65batch/s, auc=0.8737, loss=0.7151]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 111/292 [01:15<01:47,  1.69batch/s, auc=0.8737, loss=0.7151]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 111/292 [01:15<01:47,  1.69batch/s, auc=0.8733, loss=0.9556]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 112/292 [01:15<01:44,  1.72batch/s, auc=0.8733, loss=0.9556]\u001b[A\n",
      "Training Epoch 12/25:  38%|███▊      | 112/292 [01:16<01:44,  1.72batch/s, auc=0.8728, loss=1.0070]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▊      | 113/292 [01:16<02:10,  1.37batch/s, auc=0.8728, loss=1.0070]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▊      | 113/292 [01:17<02:10,  1.37batch/s, auc=0.8735, loss=0.5256]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 114/292 [01:17<02:00,  1.48batch/s, auc=0.8735, loss=0.5256]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 114/292 [01:17<02:00,  1.48batch/s, auc=0.8736, loss=0.7816]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 115/292 [01:17<01:53,  1.56batch/s, auc=0.8736, loss=0.7816]\u001b[A\n",
      "Training Epoch 12/25:  39%|███▉      | 115/292 [01:18<01:53,  1.56batch/s, auc=0.8736, loss=0.6239]\u001b[A\n",
      "Training Epoch 12/25:  40%|███▉      | 116/292 [01:18<01:48,  1.62batch/s, auc=0.8736, loss=0.6239]\u001b[A\n",
      "Training Epoch 12/25:  40%|███▉      | 116/292 [01:19<01:48,  1.62batch/s, auc=0.8738, loss=0.5762]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 117/292 [01:19<01:44,  1.67batch/s, auc=0.8738, loss=0.5762]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 117/292 [01:19<01:44,  1.67batch/s, auc=0.8735, loss=0.7024]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 118/292 [01:19<01:42,  1.71batch/s, auc=0.8735, loss=0.7024]\u001b[A\n",
      "Training Epoch 12/25:  40%|████      | 118/292 [01:20<01:42,  1.71batch/s, auc=0.8737, loss=0.7890]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 119/292 [01:20<01:40,  1.73batch/s, auc=0.8737, loss=0.7890]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 119/292 [01:20<01:40,  1.73batch/s, auc=0.8736, loss=0.7000]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 120/292 [01:20<01:38,  1.75batch/s, auc=0.8736, loss=0.7000]\u001b[A\n",
      "Training Epoch 12/25:  41%|████      | 120/292 [01:21<01:38,  1.75batch/s, auc=0.8731, loss=0.9281]\u001b[A\n",
      "Training Epoch 12/25:  41%|████▏     | 121/292 [01:21<02:03,  1.38batch/s, auc=0.8731, loss=0.9281]\u001b[A\n",
      "Training Epoch 12/25:  41%|████▏     | 121/292 [01:22<02:03,  1.38batch/s, auc=0.8739, loss=0.4879]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 122/292 [01:22<01:54,  1.48batch/s, auc=0.8739, loss=0.4879]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 122/292 [01:22<01:54,  1.48batch/s, auc=0.8735, loss=0.8983]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 123/292 [01:22<01:47,  1.57batch/s, auc=0.8735, loss=0.8983]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 123/292 [01:23<01:47,  1.57batch/s, auc=0.8738, loss=0.5058]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 124/292 [01:23<01:43,  1.63batch/s, auc=0.8738, loss=0.5058]\u001b[A\n",
      "Training Epoch 12/25:  42%|████▏     | 124/292 [01:24<01:43,  1.63batch/s, auc=0.8741, loss=0.5849]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 125/292 [01:24<01:39,  1.67batch/s, auc=0.8741, loss=0.5849]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 125/292 [01:24<01:39,  1.67batch/s, auc=0.8737, loss=0.8878]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 126/292 [01:24<01:37,  1.71batch/s, auc=0.8737, loss=0.8878]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 126/292 [01:25<01:37,  1.71batch/s, auc=0.8742, loss=0.6033]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 127/292 [01:25<01:35,  1.73batch/s, auc=0.8742, loss=0.6033]\u001b[A\n",
      "Training Epoch 12/25:  43%|████▎     | 127/292 [01:26<01:35,  1.73batch/s, auc=0.8735, loss=0.9399]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 128/292 [01:26<01:59,  1.37batch/s, auc=0.8735, loss=0.9399]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 128/292 [01:26<01:59,  1.37batch/s, auc=0.8733, loss=0.7726]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 129/292 [01:26<01:50,  1.48batch/s, auc=0.8733, loss=0.7726]\u001b[A\n",
      "Training Epoch 12/25:  44%|████▍     | 129/292 [01:27<01:50,  1.48batch/s, auc=0.8735, loss=0.5405]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 130/292 [01:27<01:44,  1.56batch/s, auc=0.8735, loss=0.5405]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 130/292 [01:27<01:44,  1.56batch/s, auc=0.8742, loss=0.6882]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 131/292 [01:27<01:39,  1.62batch/s, auc=0.8742, loss=0.6882]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▍     | 131/292 [01:29<01:39,  1.62batch/s, auc=0.8736, loss=1.0086]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▌     | 132/292 [01:29<02:00,  1.32batch/s, auc=0.8736, loss=1.0086]\u001b[A\n",
      "Training Epoch 12/25:  45%|████▌     | 132/292 [01:29<02:00,  1.32batch/s, auc=0.8738, loss=0.6015]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 133/292 [01:29<01:50,  1.43batch/s, auc=0.8738, loss=0.6015]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 133/292 [01:30<01:50,  1.43batch/s, auc=0.8738, loss=0.7877]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 134/292 [01:30<01:43,  1.53batch/s, auc=0.8738, loss=0.7877]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 134/292 [01:30<01:43,  1.53batch/s, auc=0.8743, loss=0.5457]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 135/292 [01:30<01:38,  1.60batch/s, auc=0.8743, loss=0.5457]\u001b[A\n",
      "Training Epoch 12/25:  46%|████▌     | 135/292 [01:31<01:38,  1.60batch/s, auc=0.8746, loss=0.6175]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 136/292 [01:31<01:34,  1.65batch/s, auc=0.8746, loss=0.6175]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 136/292 [01:32<01:34,  1.65batch/s, auc=0.8741, loss=0.9149]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 137/292 [01:32<01:55,  1.34batch/s, auc=0.8741, loss=0.9149]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 137/292 [01:32<01:55,  1.34batch/s, auc=0.8746, loss=0.7088]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 138/292 [01:32<01:46,  1.45batch/s, auc=0.8746, loss=0.7088]\u001b[A\n",
      "Training Epoch 12/25:  47%|████▋     | 138/292 [01:33<01:46,  1.45batch/s, auc=0.8748, loss=0.7435]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 139/292 [01:33<01:39,  1.53batch/s, auc=0.8748, loss=0.7435]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 139/292 [01:34<01:39,  1.53batch/s, auc=0.8748, loss=0.6692]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 140/292 [01:34<01:34,  1.60batch/s, auc=0.8748, loss=0.6692]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 140/292 [01:34<01:34,  1.60batch/s, auc=0.8752, loss=0.7097]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 141/292 [01:34<01:31,  1.65batch/s, auc=0.8752, loss=0.7097]\u001b[A\n",
      "Training Epoch 12/25:  48%|████▊     | 141/292 [01:35<01:31,  1.65batch/s, auc=0.8756, loss=0.6324]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▊     | 142/292 [01:35<01:28,  1.69batch/s, auc=0.8756, loss=0.6324]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▊     | 142/292 [01:35<01:28,  1.69batch/s, auc=0.8760, loss=0.5788]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 143/292 [01:35<01:26,  1.72batch/s, auc=0.8760, loss=0.5788]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 143/292 [01:36<01:26,  1.72batch/s, auc=0.8754, loss=1.0335]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 144/292 [01:36<01:25,  1.74batch/s, auc=0.8754, loss=1.0335]\u001b[A\n",
      "Training Epoch 12/25:  49%|████▉     | 144/292 [01:36<01:25,  1.74batch/s, auc=0.8760, loss=0.6023]\u001b[A\n",
      "Training Epoch 12/25:  50%|████▉     | 145/292 [01:36<01:23,  1.75batch/s, auc=0.8760, loss=0.6023]\u001b[A\n",
      "Training Epoch 12/25:  50%|████▉     | 145/292 [01:37<01:23,  1.75batch/s, auc=0.8764, loss=0.7557]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 146/292 [01:37<01:22,  1.76batch/s, auc=0.8764, loss=0.7557]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 146/292 [01:37<01:22,  1.76batch/s, auc=0.8761, loss=0.7938]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 147/292 [01:37<01:22,  1.77batch/s, auc=0.8761, loss=0.7938]\u001b[A\n",
      "Training Epoch 12/25:  50%|█████     | 147/292 [01:38<01:22,  1.77batch/s, auc=0.8754, loss=0.8670]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 148/292 [01:38<01:21,  1.77batch/s, auc=0.8754, loss=0.8670]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 148/292 [01:39<01:21,  1.77batch/s, auc=0.8757, loss=0.6471]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 149/292 [01:39<01:20,  1.78batch/s, auc=0.8757, loss=0.6471]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████     | 149/292 [01:40<01:20,  1.78batch/s, auc=0.8741, loss=1.7358]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████▏    | 150/292 [01:40<01:41,  1.39batch/s, auc=0.8741, loss=1.7358]\u001b[A\n",
      "Training Epoch 12/25:  51%|█████▏    | 150/292 [01:41<01:41,  1.39batch/s, auc=0.8740, loss=0.8065]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 151/292 [01:41<01:56,  1.21batch/s, auc=0.8740, loss=0.8065]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 151/292 [01:42<01:56,  1.21batch/s, auc=0.8730, loss=1.3641]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 152/292 [01:42<02:06,  1.11batch/s, auc=0.8730, loss=1.3641]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 152/292 [01:42<02:06,  1.11batch/s, auc=0.8729, loss=0.7637]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 153/292 [01:42<01:51,  1.25batch/s, auc=0.8729, loss=0.7637]\u001b[A\n",
      "Training Epoch 12/25:  52%|█████▏    | 153/292 [01:43<01:51,  1.25batch/s, auc=0.8732, loss=0.6278]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 154/292 [01:43<01:40,  1.37batch/s, auc=0.8732, loss=0.6278]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 154/292 [01:43<01:40,  1.37batch/s, auc=0.8734, loss=0.6357]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 155/292 [01:43<01:32,  1.47batch/s, auc=0.8734, loss=0.6357]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 155/292 [01:44<01:32,  1.47batch/s, auc=0.8736, loss=0.5403]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 156/292 [01:44<01:27,  1.56batch/s, auc=0.8736, loss=0.5403]\u001b[A\n",
      "Training Epoch 12/25:  53%|█████▎    | 156/292 [01:45<01:27,  1.56batch/s, auc=0.8737, loss=0.6782]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 157/292 [01:45<01:23,  1.62batch/s, auc=0.8737, loss=0.6782]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 157/292 [01:46<01:23,  1.62batch/s, auc=0.8730, loss=1.0494]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 158/292 [01:46<01:41,  1.32batch/s, auc=0.8730, loss=1.0494]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 158/292 [01:46<01:41,  1.32batch/s, auc=0.8732, loss=0.5875]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 159/292 [01:46<01:32,  1.43batch/s, auc=0.8732, loss=0.5875]\u001b[A\n",
      "Training Epoch 12/25:  54%|█████▍    | 159/292 [01:47<01:32,  1.43batch/s, auc=0.8724, loss=1.1911]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▍    | 160/292 [01:47<01:47,  1.23batch/s, auc=0.8724, loss=1.1911]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▍    | 160/292 [01:48<01:47,  1.23batch/s, auc=0.8724, loss=0.6376]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 161/292 [01:48<01:36,  1.36batch/s, auc=0.8724, loss=0.6376]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 161/292 [01:48<01:36,  1.36batch/s, auc=0.8722, loss=0.6833]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 162/292 [01:48<01:28,  1.46batch/s, auc=0.8722, loss=0.6833]\u001b[A\n",
      "Training Epoch 12/25:  55%|█████▌    | 162/292 [01:49<01:28,  1.46batch/s, auc=0.8726, loss=0.6279]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 163/292 [01:49<01:23,  1.55batch/s, auc=0.8726, loss=0.6279]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 163/292 [01:50<01:23,  1.55batch/s, auc=0.8729, loss=0.6326]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 164/292 [01:50<01:19,  1.61batch/s, auc=0.8729, loss=0.6326]\u001b[A\n",
      "Training Epoch 12/25:  56%|█████▌    | 164/292 [01:51<01:19,  1.61batch/s, auc=0.8725, loss=0.9351]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 165/292 [01:51<01:36,  1.32batch/s, auc=0.8725, loss=0.9351]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 165/292 [01:52<01:36,  1.32batch/s, auc=0.8719, loss=1.1095]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 166/292 [01:52<01:47,  1.17batch/s, auc=0.8719, loss=1.1095]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 166/292 [01:52<01:47,  1.17batch/s, auc=0.8721, loss=0.5632]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 167/292 [01:52<01:35,  1.30batch/s, auc=0.8721, loss=0.5632]\u001b[A\n",
      "Training Epoch 12/25:  57%|█████▋    | 167/292 [01:53<01:35,  1.30batch/s, auc=0.8722, loss=0.9567]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 168/292 [01:53<01:27,  1.42batch/s, auc=0.8722, loss=0.9567]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 168/292 [01:53<01:27,  1.42batch/s, auc=0.8721, loss=0.7738]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 169/292 [01:53<01:21,  1.51batch/s, auc=0.8721, loss=0.7738]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 169/292 [01:54<01:21,  1.51batch/s, auc=0.8725, loss=0.6246]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 170/292 [01:54<01:17,  1.58batch/s, auc=0.8725, loss=0.6246]\u001b[A\n",
      "Training Epoch 12/25:  58%|█████▊    | 170/292 [01:55<01:17,  1.58batch/s, auc=0.8720, loss=1.0264]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▊    | 171/292 [01:55<01:32,  1.31batch/s, auc=0.8720, loss=1.0264]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▊    | 171/292 [01:56<01:32,  1.31batch/s, auc=0.8719, loss=0.7464]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 172/292 [01:56<01:24,  1.42batch/s, auc=0.8719, loss=0.7464]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 172/292 [01:56<01:24,  1.42batch/s, auc=0.8717, loss=0.7528]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 173/292 [01:56<01:18,  1.51batch/s, auc=0.8717, loss=0.7528]\u001b[A\n",
      "Training Epoch 12/25:  59%|█████▉    | 173/292 [01:57<01:18,  1.51batch/s, auc=0.8723, loss=0.4684]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 174/292 [01:57<01:14,  1.58batch/s, auc=0.8723, loss=0.4684]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 174/292 [01:58<01:14,  1.58batch/s, auc=0.8720, loss=0.9406]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 175/292 [01:58<01:29,  1.31batch/s, auc=0.8720, loss=0.9406]\u001b[A\n",
      "Training Epoch 12/25:  60%|█████▉    | 175/292 [01:58<01:29,  1.31batch/s, auc=0.8725, loss=0.4072]\u001b[A\n",
      "Training Epoch 12/25:  60%|██████    | 176/292 [01:58<01:21,  1.42batch/s, auc=0.8725, loss=0.4072]\u001b[A\n",
      "Training Epoch 12/25:  60%|██████    | 176/292 [01:59<01:21,  1.42batch/s, auc=0.8722, loss=0.9111]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 177/292 [01:59<01:33,  1.22batch/s, auc=0.8722, loss=0.9111]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 177/292 [02:00<01:33,  1.22batch/s, auc=0.8726, loss=0.5645]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 178/292 [02:00<01:24,  1.35batch/s, auc=0.8726, loss=0.5645]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████    | 178/292 [02:01<01:24,  1.35batch/s, auc=0.8727, loss=0.8869]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████▏   | 179/292 [02:01<01:17,  1.46batch/s, auc=0.8727, loss=0.8869]\u001b[A\n",
      "Training Epoch 12/25:  61%|██████▏   | 179/292 [02:01<01:17,  1.46batch/s, auc=0.8729, loss=0.5972]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 180/292 [02:01<01:12,  1.54batch/s, auc=0.8729, loss=0.5972]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 180/292 [02:02<01:12,  1.54batch/s, auc=0.8718, loss=1.5640]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 181/292 [02:02<01:26,  1.29batch/s, auc=0.8718, loss=1.5640]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 181/292 [02:03<01:26,  1.29batch/s, auc=0.8721, loss=0.6084]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 182/292 [02:03<01:18,  1.40batch/s, auc=0.8721, loss=0.6084]\u001b[A\n",
      "Training Epoch 12/25:  62%|██████▏   | 182/292 [02:03<01:18,  1.40batch/s, auc=0.8719, loss=0.6636]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 183/292 [02:03<01:12,  1.50batch/s, auc=0.8719, loss=0.6636]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 183/292 [02:04<01:12,  1.50batch/s, auc=0.8725, loss=0.5205]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 184/292 [02:04<01:08,  1.57batch/s, auc=0.8725, loss=0.5205]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 184/292 [02:04<01:08,  1.57batch/s, auc=0.8727, loss=0.5101]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 185/292 [02:04<01:05,  1.63batch/s, auc=0.8727, loss=0.5101]\u001b[A\n",
      "Training Epoch 12/25:  63%|██████▎   | 185/292 [02:05<01:05,  1.63batch/s, auc=0.8728, loss=0.5921]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▎   | 186/292 [02:05<01:03,  1.67batch/s, auc=0.8728, loss=0.5921]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▎   | 186/292 [02:06<01:03,  1.67batch/s, auc=0.8731, loss=0.5064]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 187/292 [02:06<01:01,  1.70batch/s, auc=0.8731, loss=0.5064]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 187/292 [02:06<01:01,  1.70batch/s, auc=0.8731, loss=0.9751]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 188/292 [02:06<01:00,  1.73batch/s, auc=0.8731, loss=0.9751]\u001b[A\n",
      "Training Epoch 12/25:  64%|██████▍   | 188/292 [02:07<01:00,  1.73batch/s, auc=0.8727, loss=0.9107]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▍   | 189/292 [02:07<01:15,  1.37batch/s, auc=0.8727, loss=0.9107]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▍   | 189/292 [02:08<01:15,  1.37batch/s, auc=0.8729, loss=0.5815]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 190/292 [02:08<01:09,  1.47batch/s, auc=0.8729, loss=0.5815]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 190/292 [02:08<01:09,  1.47batch/s, auc=0.8728, loss=0.6530]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 191/292 [02:08<01:04,  1.55batch/s, auc=0.8728, loss=0.6530]\u001b[A\n",
      "Training Epoch 12/25:  65%|██████▌   | 191/292 [02:09<01:04,  1.55batch/s, auc=0.8730, loss=0.6400]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 192/292 [02:09<01:01,  1.61batch/s, auc=0.8730, loss=0.6400]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 192/292 [02:10<01:01,  1.61batch/s, auc=0.8728, loss=0.9354]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 193/292 [02:10<01:14,  1.32batch/s, auc=0.8728, loss=0.9354]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▌   | 193/292 [02:11<01:14,  1.32batch/s, auc=0.8731, loss=0.5135]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▋   | 194/292 [02:11<01:08,  1.43batch/s, auc=0.8731, loss=0.5135]\u001b[A\n",
      "Training Epoch 12/25:  66%|██████▋   | 194/292 [02:11<01:08,  1.43batch/s, auc=0.8729, loss=0.8394]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 195/292 [02:11<01:03,  1.52batch/s, auc=0.8729, loss=0.8394]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 195/292 [02:12<01:03,  1.52batch/s, auc=0.8730, loss=0.5848]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 196/292 [02:12<01:00,  1.59batch/s, auc=0.8730, loss=0.5848]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 196/292 [02:12<01:00,  1.59batch/s, auc=0.8733, loss=0.5384]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 197/292 [02:12<00:57,  1.64batch/s, auc=0.8733, loss=0.5384]\u001b[A\n",
      "Training Epoch 12/25:  67%|██████▋   | 197/292 [02:13<00:57,  1.64batch/s, auc=0.8738, loss=0.4851]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 198/292 [02:13<00:55,  1.68batch/s, auc=0.8738, loss=0.4851]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 198/292 [02:14<00:55,  1.68batch/s, auc=0.8734, loss=0.9026]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 199/292 [02:14<01:08,  1.35batch/s, auc=0.8734, loss=0.9026]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 199/292 [02:14<01:08,  1.35batch/s, auc=0.8736, loss=0.5884]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 200/292 [02:14<01:03,  1.46batch/s, auc=0.8736, loss=0.5884]\u001b[A\n",
      "Training Epoch 12/25:  68%|██████▊   | 200/292 [02:15<01:03,  1.46batch/s, auc=0.8737, loss=0.6551]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 201/292 [02:15<00:59,  1.54batch/s, auc=0.8737, loss=0.6551]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 201/292 [02:16<00:59,  1.54batch/s, auc=0.8742, loss=0.4640]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 202/292 [02:16<00:56,  1.61batch/s, auc=0.8742, loss=0.4640]\u001b[A\n",
      "Training Epoch 12/25:  69%|██████▉   | 202/292 [02:16<00:56,  1.61batch/s, auc=0.8744, loss=0.5680]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 203/292 [02:16<00:53,  1.65batch/s, auc=0.8744, loss=0.5680]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 203/292 [02:17<00:53,  1.65batch/s, auc=0.8746, loss=0.5684]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 204/292 [02:17<00:52,  1.69batch/s, auc=0.8746, loss=0.5684]\u001b[A\n",
      "Training Epoch 12/25:  70%|██████▉   | 204/292 [02:18<00:52,  1.69batch/s, auc=0.8740, loss=1.0660]\u001b[A\n",
      "Training Epoch 12/25:  70%|███████   | 205/292 [02:18<01:04,  1.36batch/s, auc=0.8740, loss=1.0660]\u001b[A\n",
      "Training Epoch 12/25:  70%|███████   | 205/292 [02:18<01:04,  1.36batch/s, auc=0.8742, loss=0.5458]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 206/292 [02:18<00:58,  1.46batch/s, auc=0.8742, loss=0.5458]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 206/292 [02:19<00:58,  1.46batch/s, auc=0.8743, loss=0.6478]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 207/292 [02:19<00:55,  1.54batch/s, auc=0.8743, loss=0.6478]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 207/292 [02:19<00:55,  1.54batch/s, auc=0.8744, loss=0.6290]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 208/292 [02:19<00:52,  1.61batch/s, auc=0.8744, loss=0.6290]\u001b[A\n",
      "Training Epoch 12/25:  71%|███████   | 208/292 [02:21<00:52,  1.61batch/s, auc=0.8736, loss=1.2508]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 209/292 [02:21<01:03,  1.32batch/s, auc=0.8736, loss=1.2508]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 209/292 [02:21<01:03,  1.32batch/s, auc=0.8732, loss=1.0119]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 210/292 [02:21<00:57,  1.43batch/s, auc=0.8732, loss=1.0119]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 210/292 [02:22<00:57,  1.43batch/s, auc=0.8734, loss=0.5372]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 211/292 [02:22<00:53,  1.52batch/s, auc=0.8734, loss=0.5372]\u001b[A\n",
      "Training Epoch 12/25:  72%|███████▏  | 211/292 [02:22<00:53,  1.52batch/s, auc=0.8730, loss=0.9074]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 212/292 [02:22<00:50,  1.59batch/s, auc=0.8730, loss=0.9074]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 212/292 [02:23<00:50,  1.59batch/s, auc=0.8731, loss=0.5688]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 213/292 [02:23<00:48,  1.64batch/s, auc=0.8731, loss=0.5688]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 213/292 [02:23<00:48,  1.64batch/s, auc=0.8731, loss=0.4833]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 214/292 [02:23<00:46,  1.68batch/s, auc=0.8731, loss=0.4833]\u001b[A\n",
      "Training Epoch 12/25:  73%|███████▎  | 214/292 [02:24<00:46,  1.68batch/s, auc=0.8723, loss=1.3851]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▎  | 215/292 [02:24<00:57,  1.35batch/s, auc=0.8723, loss=1.3851]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▎  | 215/292 [02:25<00:57,  1.35batch/s, auc=0.8725, loss=0.5315]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 216/292 [02:25<00:52,  1.45batch/s, auc=0.8725, loss=0.5315]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 216/292 [02:26<00:52,  1.45batch/s, auc=0.8727, loss=0.5535]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 217/292 [02:26<00:48,  1.54batch/s, auc=0.8727, loss=0.5535]\u001b[A\n",
      "Training Epoch 12/25:  74%|███████▍  | 217/292 [02:26<00:48,  1.54batch/s, auc=0.8724, loss=0.9727]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▍  | 218/292 [02:26<00:46,  1.60batch/s, auc=0.8724, loss=0.9727]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▍  | 218/292 [02:27<00:46,  1.60batch/s, auc=0.8718, loss=1.2633]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 219/292 [02:27<00:55,  1.32batch/s, auc=0.8718, loss=1.2633]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 219/292 [02:28<00:55,  1.32batch/s, auc=0.8714, loss=1.0896]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 220/292 [02:28<00:50,  1.43batch/s, auc=0.8714, loss=1.0896]\u001b[A\n",
      "Training Epoch 12/25:  75%|███████▌  | 220/292 [02:29<00:50,  1.43batch/s, auc=0.8712, loss=0.9923]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 221/292 [02:29<00:57,  1.23batch/s, auc=0.8712, loss=0.9923]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 221/292 [02:30<00:57,  1.23batch/s, auc=0.8710, loss=0.7920]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 222/292 [02:30<01:02,  1.12batch/s, auc=0.8710, loss=0.7920]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▌  | 222/292 [02:30<01:02,  1.12batch/s, auc=0.8713, loss=0.4984]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▋  | 223/292 [02:30<00:54,  1.26batch/s, auc=0.8713, loss=0.4984]\u001b[A\n",
      "Training Epoch 12/25:  76%|███████▋  | 223/292 [02:32<00:54,  1.26batch/s, auc=0.8702, loss=1.5590]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 224/292 [02:32<00:59,  1.14batch/s, auc=0.8702, loss=1.5590]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 224/292 [02:33<00:59,  1.14batch/s, auc=0.8696, loss=1.2204]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 225/292 [02:33<01:02,  1.06batch/s, auc=0.8696, loss=1.2204]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 225/292 [02:33<01:02,  1.06batch/s, auc=0.8697, loss=0.7538]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 226/292 [02:33<00:54,  1.21batch/s, auc=0.8697, loss=0.7538]\u001b[A\n",
      "Training Epoch 12/25:  77%|███████▋  | 226/292 [02:34<00:54,  1.21batch/s, auc=0.8697, loss=0.6879]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 227/292 [02:34<00:48,  1.34batch/s, auc=0.8697, loss=0.6879]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 227/292 [02:34<00:48,  1.34batch/s, auc=0.8697, loss=0.7553]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 228/292 [02:34<00:44,  1.45batch/s, auc=0.8697, loss=0.7553]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 228/292 [02:35<00:44,  1.45batch/s, auc=0.8693, loss=0.9406]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 229/292 [02:35<00:41,  1.53batch/s, auc=0.8693, loss=0.9406]\u001b[A\n",
      "Training Epoch 12/25:  78%|███████▊  | 229/292 [02:35<00:41,  1.53batch/s, auc=0.8693, loss=0.7588]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 230/292 [02:35<00:38,  1.60batch/s, auc=0.8693, loss=0.7588]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 230/292 [02:36<00:38,  1.60batch/s, auc=0.8695, loss=0.6078]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 231/292 [02:36<00:36,  1.65batch/s, auc=0.8695, loss=0.6078]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 231/292 [02:37<00:36,  1.65batch/s, auc=0.8697, loss=0.6570]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 232/292 [02:37<00:35,  1.69batch/s, auc=0.8697, loss=0.6570]\u001b[A\n",
      "Training Epoch 12/25:  79%|███████▉  | 232/292 [02:37<00:35,  1.69batch/s, auc=0.8696, loss=0.7536]\u001b[A\n",
      "Training Epoch 12/25:  80%|███████▉  | 233/292 [02:37<00:34,  1.71batch/s, auc=0.8696, loss=0.7536]\u001b[A\n",
      "Training Epoch 12/25:  80%|███████▉  | 233/292 [02:38<00:34,  1.71batch/s, auc=0.8696, loss=0.7222]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 234/292 [02:38<00:33,  1.73batch/s, auc=0.8696, loss=0.7222]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 234/292 [02:39<00:33,  1.73batch/s, auc=0.8694, loss=0.9863]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 235/292 [02:39<00:41,  1.37batch/s, auc=0.8694, loss=0.9863]\u001b[A\n",
      "Training Epoch 12/25:  80%|████████  | 235/292 [02:39<00:41,  1.37batch/s, auc=0.8697, loss=0.5227]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 236/292 [02:39<00:38,  1.47batch/s, auc=0.8697, loss=0.5227]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 236/292 [02:40<00:38,  1.47batch/s, auc=0.8693, loss=0.9835]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 237/292 [02:40<00:43,  1.25batch/s, auc=0.8693, loss=0.9835]\u001b[A\n",
      "Training Epoch 12/25:  81%|████████  | 237/292 [02:41<00:43,  1.25batch/s, auc=0.8695, loss=0.5516]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 238/292 [02:41<00:39,  1.37batch/s, auc=0.8695, loss=0.5516]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 238/292 [02:42<00:39,  1.37batch/s, auc=0.8698, loss=0.6072]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 239/292 [02:42<00:35,  1.47batch/s, auc=0.8698, loss=0.6072]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 239/292 [02:42<00:35,  1.47batch/s, auc=0.8700, loss=0.4939]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 240/292 [02:42<00:33,  1.55batch/s, auc=0.8700, loss=0.4939]\u001b[A\n",
      "Training Epoch 12/25:  82%|████████▏ | 240/292 [02:43<00:33,  1.55batch/s, auc=0.8702, loss=0.5587]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 241/292 [02:43<00:31,  1.61batch/s, auc=0.8702, loss=0.5587]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 241/292 [02:43<00:31,  1.61batch/s, auc=0.8705, loss=0.5163]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 242/292 [02:43<00:30,  1.66batch/s, auc=0.8705, loss=0.5163]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 242/292 [02:44<00:30,  1.66batch/s, auc=0.8704, loss=0.7518]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 243/292 [02:44<00:28,  1.69batch/s, auc=0.8704, loss=0.7518]\u001b[A\n",
      "Training Epoch 12/25:  83%|████████▎ | 243/292 [02:45<00:28,  1.69batch/s, auc=0.8702, loss=1.0831]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▎ | 244/292 [02:45<00:35,  1.36batch/s, auc=0.8702, loss=1.0831]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▎ | 244/292 [02:45<00:35,  1.36batch/s, auc=0.8706, loss=0.5228]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 245/292 [02:45<00:32,  1.46batch/s, auc=0.8706, loss=0.5228]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 245/292 [02:46<00:32,  1.46batch/s, auc=0.8709, loss=0.4854]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 246/292 [02:46<00:29,  1.54batch/s, auc=0.8709, loss=0.4854]\u001b[A\n",
      "Training Epoch 12/25:  84%|████████▍ | 246/292 [02:47<00:29,  1.54batch/s, auc=0.8707, loss=0.8924]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 247/292 [02:47<00:28,  1.60batch/s, auc=0.8707, loss=0.8924]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 247/292 [02:47<00:28,  1.60batch/s, auc=0.8707, loss=0.6758]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 248/292 [02:47<00:26,  1.65batch/s, auc=0.8707, loss=0.6758]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▍ | 248/292 [02:48<00:26,  1.65batch/s, auc=0.8705, loss=0.7267]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▌ | 249/292 [02:48<00:25,  1.69batch/s, auc=0.8705, loss=0.7267]\u001b[A\n",
      "Training Epoch 12/25:  85%|████████▌ | 249/292 [02:48<00:25,  1.69batch/s, auc=0.8705, loss=0.6303]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 250/292 [02:48<00:24,  1.71batch/s, auc=0.8705, loss=0.6303]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 250/292 [02:49<00:24,  1.71batch/s, auc=0.8701, loss=1.1417]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 251/292 [02:49<00:30,  1.37batch/s, auc=0.8701, loss=1.1417]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▌ | 251/292 [02:50<00:30,  1.37batch/s, auc=0.8699, loss=0.6834]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▋ | 252/292 [02:50<00:27,  1.47batch/s, auc=0.8699, loss=0.6834]\u001b[A\n",
      "Training Epoch 12/25:  86%|████████▋ | 252/292 [02:50<00:27,  1.47batch/s, auc=0.8698, loss=0.8121]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 253/292 [02:50<00:25,  1.55batch/s, auc=0.8698, loss=0.8121]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 253/292 [02:51<00:25,  1.55batch/s, auc=0.8697, loss=0.8878]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 254/292 [02:51<00:23,  1.61batch/s, auc=0.8697, loss=0.8878]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 254/292 [02:52<00:23,  1.61batch/s, auc=0.8699, loss=0.5188]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 255/292 [02:52<00:22,  1.65batch/s, auc=0.8699, loss=0.5188]\u001b[A\n",
      "Training Epoch 12/25:  87%|████████▋ | 255/292 [02:52<00:22,  1.65batch/s, auc=0.8701, loss=0.6028]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 256/292 [02:52<00:21,  1.69batch/s, auc=0.8701, loss=0.6028]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 256/292 [02:53<00:21,  1.69batch/s, auc=0.8702, loss=0.8473]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 257/292 [02:53<00:20,  1.71batch/s, auc=0.8702, loss=0.8473]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 257/292 [02:53<00:20,  1.71batch/s, auc=0.8702, loss=0.6506]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 258/292 [02:53<00:19,  1.73batch/s, auc=0.8702, loss=0.6506]\u001b[A\n",
      "Training Epoch 12/25:  88%|████████▊ | 258/292 [02:54<00:19,  1.73batch/s, auc=0.8704, loss=0.5818]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▊ | 259/292 [02:54<00:18,  1.74batch/s, auc=0.8704, loss=0.5818]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▊ | 259/292 [02:54<00:18,  1.74batch/s, auc=0.8706, loss=0.6962]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 260/292 [02:54<00:18,  1.75batch/s, auc=0.8706, loss=0.6962]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 260/292 [02:55<00:18,  1.75batch/s, auc=0.8702, loss=0.9790]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 261/292 [02:55<00:17,  1.76batch/s, auc=0.8702, loss=0.9790]\u001b[A\n",
      "Training Epoch 12/25:  89%|████████▉ | 261/292 [02:56<00:17,  1.76batch/s, auc=0.8693, loss=1.4507]\u001b[A\n",
      "Training Epoch 12/25:  90%|████████▉ | 262/292 [02:56<00:21,  1.38batch/s, auc=0.8693, loss=1.4507]\u001b[A\n",
      "Training Epoch 12/25:  90%|████████▉ | 262/292 [02:57<00:21,  1.38batch/s, auc=0.8696, loss=0.5016]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 263/292 [02:57<00:19,  1.48batch/s, auc=0.8696, loss=0.5016]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 263/292 [02:58<00:19,  1.48batch/s, auc=0.8693, loss=0.9030]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 264/292 [02:58<00:22,  1.25batch/s, auc=0.8693, loss=0.9030]\u001b[A\n",
      "Training Epoch 12/25:  90%|█████████ | 264/292 [02:59<00:22,  1.25batch/s, auc=0.8690, loss=1.2578]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 265/292 [02:59<00:23,  1.13batch/s, auc=0.8690, loss=1.2578]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 265/292 [02:59<00:23,  1.13batch/s, auc=0.8691, loss=0.7470]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 266/292 [02:59<00:20,  1.27batch/s, auc=0.8691, loss=0.7470]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████ | 266/292 [03:00<00:20,  1.27batch/s, auc=0.8690, loss=0.6981]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████▏| 267/292 [03:00<00:17,  1.39batch/s, auc=0.8690, loss=0.6981]\u001b[A\n",
      "Training Epoch 12/25:  91%|█████████▏| 267/292 [03:00<00:17,  1.39batch/s, auc=0.8693, loss=0.7000]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 268/292 [03:00<00:16,  1.49batch/s, auc=0.8693, loss=0.7000]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 268/292 [03:01<00:16,  1.49batch/s, auc=0.8690, loss=1.1191]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 269/292 [03:01<00:14,  1.56batch/s, auc=0.8690, loss=1.1191]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 269/292 [03:02<00:14,  1.56batch/s, auc=0.8688, loss=0.7512]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 270/292 [03:02<00:16,  1.30batch/s, auc=0.8688, loss=0.7512]\u001b[A\n",
      "Training Epoch 12/25:  92%|█████████▏| 270/292 [03:03<00:16,  1.30batch/s, auc=0.8688, loss=0.8733]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 271/292 [03:03<00:14,  1.41batch/s, auc=0.8688, loss=0.8733]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 271/292 [03:04<00:14,  1.41batch/s, auc=0.8683, loss=1.0982]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 272/292 [03:04<00:16,  1.22batch/s, auc=0.8683, loss=1.0982]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 272/292 [03:05<00:16,  1.22batch/s, auc=0.8681, loss=0.8664]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 273/292 [03:05<00:17,  1.11batch/s, auc=0.8681, loss=0.8664]\u001b[A\n",
      "Training Epoch 12/25:  93%|█████████▎| 273/292 [03:05<00:17,  1.11batch/s, auc=0.8680, loss=0.8091]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 274/292 [03:05<00:14,  1.25batch/s, auc=0.8680, loss=0.8091]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 274/292 [03:06<00:14,  1.25batch/s, auc=0.8684, loss=0.5957]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 275/292 [03:06<00:12,  1.37batch/s, auc=0.8684, loss=0.5957]\u001b[A\n",
      "Training Epoch 12/25:  94%|█████████▍| 275/292 [03:07<00:12,  1.37batch/s, auc=0.8686, loss=0.6309]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 276/292 [03:07<00:10,  1.47batch/s, auc=0.8686, loss=0.6309]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 276/292 [03:07<00:10,  1.47batch/s, auc=0.8686, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 277/292 [03:07<00:09,  1.55batch/s, auc=0.8686, loss=0.6171]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▍| 277/292 [03:08<00:09,  1.55batch/s, auc=0.8688, loss=0.5326]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▌| 278/292 [03:08<00:08,  1.61batch/s, auc=0.8688, loss=0.5326]\u001b[A\n",
      "Training Epoch 12/25:  95%|█████████▌| 278/292 [03:08<00:08,  1.61batch/s, auc=0.8689, loss=0.5510]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 279/292 [03:08<00:07,  1.65batch/s, auc=0.8689, loss=0.5510]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 279/292 [03:09<00:07,  1.65batch/s, auc=0.8685, loss=0.9549]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 280/292 [03:09<00:08,  1.34batch/s, auc=0.8685, loss=0.9549]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 280/292 [03:10<00:08,  1.34batch/s, auc=0.8689, loss=0.6877]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 281/292 [03:10<00:07,  1.44batch/s, auc=0.8689, loss=0.6877]\u001b[A\n",
      "Training Epoch 12/25:  96%|█████████▌| 281/292 [03:10<00:07,  1.44batch/s, auc=0.8689, loss=1.0252]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 282/292 [03:10<00:06,  1.52batch/s, auc=0.8689, loss=1.0252]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 282/292 [03:11<00:06,  1.52batch/s, auc=0.8689, loss=0.6831]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 283/292 [03:11<00:05,  1.59batch/s, auc=0.8689, loss=0.6831]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 283/292 [03:12<00:05,  1.59batch/s, auc=0.8690, loss=0.6500]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 284/292 [03:12<00:04,  1.64batch/s, auc=0.8690, loss=0.6500]\u001b[A\n",
      "Training Epoch 12/25:  97%|█████████▋| 284/292 [03:12<00:04,  1.64batch/s, auc=0.8690, loss=0.6072]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 285/292 [03:12<00:04,  1.68batch/s, auc=0.8690, loss=0.6072]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 285/292 [03:13<00:04,  1.68batch/s, auc=0.8692, loss=0.7701]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 286/292 [03:13<00:03,  1.71batch/s, auc=0.8692, loss=0.7701]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 286/292 [03:14<00:03,  1.71batch/s, auc=0.8691, loss=0.8235]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.36batch/s, auc=0.8691, loss=0.8235]\u001b[A\n",
      "Training Epoch 12/25:  98%|█████████▊| 287/292 [03:15<00:03,  1.36batch/s, auc=0.8688, loss=0.9788]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▊| 288/292 [03:15<00:03,  1.19batch/s, auc=0.8688, loss=0.9788]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▊| 288/292 [03:16<00:03,  1.19batch/s, auc=0.8687, loss=0.8620]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 289/292 [03:16<00:02,  1.10batch/s, auc=0.8687, loss=0.8620]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 289/292 [03:17<00:02,  1.10batch/s, auc=0.8687, loss=0.6641]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 290/292 [03:17<00:01,  1.24batch/s, auc=0.8687, loss=0.6641]\u001b[A\n",
      "Training Epoch 12/25:  99%|█████████▉| 290/292 [03:18<00:01,  1.24batch/s, auc=0.8686, loss=0.9279]\u001b[A\n",
      "Training Epoch 12/25: 100%|█████████▉| 291/292 [03:18<00:00,  1.12batch/s, auc=0.8686, loss=0.9279]\u001b[A\n",
      "Training Epoch 12/25: 100%|█████████▉| 291/292 [03:18<00:00,  1.12batch/s, auc=0.8688, loss=0.4734]\u001b[A\n",
      "Training Epoch 12/25: 100%|██████████| 292/292 [03:18<00:00,  1.47batch/s, auc=0.8688, loss=0.4734]\u001b[A\n",
      "Epochs:  48%|████▊     | 12/25 [42:35<46:37, 215.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25] Train Loss: 0.7600 | Train AUROC: 0.8688 Val Loss: 0.8264 | Val AUROC: 0.8420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 13/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9253, loss=0.6106]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 1/292 [00:01<07:31,  1.55s/batch, auc=0.9253, loss=0.6106]\u001b[A\n",
      "Training Epoch 13/25:   0%|          | 1/292 [00:02<07:31,  1.55s/batch, auc=0.9232, loss=0.7516]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 2/292 [00:02<04:42,  1.03batch/s, auc=0.9232, loss=0.7516]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 2/292 [00:02<04:42,  1.03batch/s, auc=0.8946, loss=0.7332]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 3/292 [00:02<03:46,  1.28batch/s, auc=0.8946, loss=0.7332]\u001b[A\n",
      "Training Epoch 13/25:   1%|          | 3/292 [00:03<03:46,  1.28batch/s, auc=0.8893, loss=0.5853]\u001b[A\n",
      "Training Epoch 13/25:   1%|▏         | 4/292 [00:03<03:20,  1.44batch/s, auc=0.8893, loss=0.5853]\u001b[A\n",
      "Training Epoch 13/25:   1%|▏         | 4/292 [00:03<03:20,  1.44batch/s, auc=0.8742, loss=0.9437]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 5/292 [00:03<03:06,  1.54batch/s, auc=0.8742, loss=0.9437]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 5/292 [00:04<03:06,  1.54batch/s, auc=0.8726, loss=0.6514]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 6/292 [00:04<02:56,  1.62batch/s, auc=0.8726, loss=0.6514]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 6/292 [00:04<02:56,  1.62batch/s, auc=0.8828, loss=0.6500]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 7/292 [00:04<02:50,  1.67batch/s, auc=0.8828, loss=0.6500]\u001b[A\n",
      "Training Epoch 13/25:   2%|▏         | 7/292 [00:05<02:50,  1.67batch/s, auc=0.8898, loss=0.4744]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 8/292 [00:05<02:47,  1.70batch/s, auc=0.8898, loss=0.4744]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 8/292 [00:06<02:47,  1.70batch/s, auc=0.8933, loss=0.5224]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 9/292 [00:06<02:44,  1.72batch/s, auc=0.8933, loss=0.5224]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 9/292 [00:07<02:44,  1.72batch/s, auc=0.8834, loss=0.9681]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 10/292 [00:07<03:26,  1.36batch/s, auc=0.8834, loss=0.9681]\u001b[A\n",
      "Training Epoch 13/25:   3%|▎         | 10/292 [00:07<03:26,  1.36batch/s, auc=0.8831, loss=0.6424]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 11/292 [00:07<03:10,  1.47batch/s, auc=0.8831, loss=0.6424]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 11/292 [00:08<03:10,  1.47batch/s, auc=0.8657, loss=1.2083]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 12/292 [00:08<03:44,  1.25batch/s, auc=0.8657, loss=1.2083]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 12/292 [00:09<03:44,  1.25batch/s, auc=0.8628, loss=0.6287]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 13/292 [00:09<03:23,  1.37batch/s, auc=0.8628, loss=0.6287]\u001b[A\n",
      "Training Epoch 13/25:   4%|▍         | 13/292 [00:09<03:23,  1.37batch/s, auc=0.8629, loss=0.7252]\u001b[A\n",
      "Training Epoch 13/25:   5%|▍         | 14/292 [00:09<03:08,  1.48batch/s, auc=0.8629, loss=0.7252]\u001b[A\n",
      "Training Epoch 13/25:   5%|▍         | 14/292 [00:10<03:08,  1.48batch/s, auc=0.8612, loss=0.8840]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 15/292 [00:10<02:57,  1.56batch/s, auc=0.8612, loss=0.8840]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 15/292 [00:11<02:57,  1.56batch/s, auc=0.8617, loss=0.6493]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 16/292 [00:11<02:51,  1.61batch/s, auc=0.8617, loss=0.6493]\u001b[A\n",
      "Training Epoch 13/25:   5%|▌         | 16/292 [00:12<02:51,  1.61batch/s, auc=0.8500, loss=1.8655]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 17/292 [00:12<03:28,  1.32batch/s, auc=0.8500, loss=1.8655]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 17/292 [00:12<03:28,  1.32batch/s, auc=0.8495, loss=1.0175]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 18/292 [00:12<03:11,  1.43batch/s, auc=0.8495, loss=1.0175]\u001b[A\n",
      "Training Epoch 13/25:   6%|▌         | 18/292 [00:13<03:11,  1.43batch/s, auc=0.8524, loss=0.5487]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 19/292 [00:13<02:59,  1.52batch/s, auc=0.8524, loss=0.5487]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 19/292 [00:13<02:59,  1.52batch/s, auc=0.8572, loss=0.5805]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 20/292 [00:13<02:52,  1.58batch/s, auc=0.8572, loss=0.5805]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 20/292 [00:14<02:52,  1.58batch/s, auc=0.8508, loss=0.9467]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 21/292 [00:14<03:27,  1.31batch/s, auc=0.8508, loss=0.9467]\u001b[A\n",
      "Training Epoch 13/25:   7%|▋         | 21/292 [00:15<03:27,  1.31batch/s, auc=0.8405, loss=1.7253]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 22/292 [00:15<03:51,  1.16batch/s, auc=0.8405, loss=1.7253]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 22/292 [00:16<03:51,  1.16batch/s, auc=0.8415, loss=0.7601]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 23/292 [00:16<03:27,  1.30batch/s, auc=0.8415, loss=0.7601]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 23/292 [00:17<03:27,  1.30batch/s, auc=0.8449, loss=0.6004]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 24/292 [00:17<03:10,  1.41batch/s, auc=0.8449, loss=0.6004]\u001b[A\n",
      "Training Epoch 13/25:   8%|▊         | 24/292 [00:18<03:10,  1.41batch/s, auc=0.8426, loss=0.8969]\u001b[A\n",
      "Training Epoch 13/25:   9%|▊         | 25/292 [00:18<03:39,  1.22batch/s, auc=0.8426, loss=0.8969]\u001b[A\n",
      "Training Epoch 13/25:   9%|▊         | 25/292 [00:18<03:39,  1.22batch/s, auc=0.8407, loss=0.9081]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 26/292 [00:18<03:17,  1.35batch/s, auc=0.8407, loss=0.9081]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 26/292 [00:19<03:17,  1.35batch/s, auc=0.8411, loss=0.7499]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 27/292 [00:19<03:02,  1.46batch/s, auc=0.8411, loss=0.7499]\u001b[A\n",
      "Training Epoch 13/25:   9%|▉         | 27/292 [00:20<03:02,  1.46batch/s, auc=0.8382, loss=1.1371]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 28/292 [00:20<03:33,  1.24batch/s, auc=0.8382, loss=1.1371]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 28/292 [00:20<03:33,  1.24batch/s, auc=0.8368, loss=1.0147]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 29/292 [00:20<03:12,  1.36batch/s, auc=0.8368, loss=1.0147]\u001b[A\n",
      "Training Epoch 13/25:  10%|▉         | 29/292 [00:21<03:12,  1.36batch/s, auc=0.8399, loss=0.7401]\u001b[A\n",
      "Training Epoch 13/25:  10%|█         | 30/292 [00:21<02:58,  1.47batch/s, auc=0.8399, loss=0.7401]\u001b[A\n",
      "Training Epoch 13/25:  10%|█         | 30/292 [00:22<02:58,  1.47batch/s, auc=0.8439, loss=0.5741]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 31/292 [00:22<02:49,  1.54batch/s, auc=0.8439, loss=0.5741]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 31/292 [00:22<02:49,  1.54batch/s, auc=0.8459, loss=0.6125]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 32/292 [00:22<02:42,  1.60batch/s, auc=0.8459, loss=0.6125]\u001b[A\n",
      "Training Epoch 13/25:  11%|█         | 32/292 [00:23<02:42,  1.60batch/s, auc=0.8469, loss=0.8837]\u001b[A\n",
      "Training Epoch 13/25:  11%|█▏        | 33/292 [00:23<02:36,  1.65batch/s, auc=0.8469, loss=0.8837]\u001b[A\n",
      "Training Epoch 13/25:  11%|█▏        | 33/292 [00:23<02:36,  1.65batch/s, auc=0.8473, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 34/292 [00:23<02:32,  1.69batch/s, auc=0.8473, loss=0.6247]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 34/292 [00:24<02:32,  1.69batch/s, auc=0.8435, loss=1.0895]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 35/292 [00:24<03:09,  1.36batch/s, auc=0.8435, loss=1.0895]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 35/292 [00:25<03:09,  1.36batch/s, auc=0.8407, loss=0.9094]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 36/292 [00:25<03:34,  1.19batch/s, auc=0.8407, loss=0.9094]\u001b[A\n",
      "Training Epoch 13/25:  12%|█▏        | 36/292 [00:26<03:34,  1.19batch/s, auc=0.8423, loss=0.6912]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 37/292 [00:26<03:13,  1.32batch/s, auc=0.8423, loss=0.6912]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 37/292 [00:27<03:13,  1.32batch/s, auc=0.8410, loss=0.7520]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 38/292 [00:27<03:36,  1.17batch/s, auc=0.8410, loss=0.7520]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 38/292 [00:28<03:36,  1.17batch/s, auc=0.8392, loss=1.0143]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 39/292 [00:28<03:53,  1.09batch/s, auc=0.8392, loss=1.0143]\u001b[A\n",
      "Training Epoch 13/25:  13%|█▎        | 39/292 [00:29<03:53,  1.09batch/s, auc=0.8420, loss=0.6084]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▎        | 40/292 [00:29<03:24,  1.23batch/s, auc=0.8420, loss=0.6084]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▎        | 40/292 [00:29<03:24,  1.23batch/s, auc=0.8430, loss=0.6303]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 41/292 [00:29<03:05,  1.35batch/s, auc=0.8430, loss=0.6303]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 41/292 [00:30<03:05,  1.35batch/s, auc=0.8454, loss=0.5084]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 42/292 [00:30<02:51,  1.46batch/s, auc=0.8454, loss=0.5084]\u001b[A\n",
      "Training Epoch 13/25:  14%|█▍        | 42/292 [00:30<02:51,  1.46batch/s, auc=0.8452, loss=0.6780]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▍        | 43/292 [00:30<02:41,  1.54batch/s, auc=0.8452, loss=0.6780]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▍        | 43/292 [00:31<02:41,  1.54batch/s, auc=0.8478, loss=0.5089]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 44/292 [00:31<02:34,  1.60batch/s, auc=0.8478, loss=0.5089]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 44/292 [00:32<02:34,  1.60batch/s, auc=0.8492, loss=0.6119]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 45/292 [00:32<02:29,  1.66batch/s, auc=0.8492, loss=0.6119]\u001b[A\n",
      "Training Epoch 13/25:  15%|█▌        | 45/292 [00:32<02:29,  1.66batch/s, auc=0.8511, loss=0.6265]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 46/292 [00:32<02:25,  1.70batch/s, auc=0.8511, loss=0.6265]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 46/292 [00:33<02:25,  1.70batch/s, auc=0.8533, loss=0.5448]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 47/292 [00:33<02:22,  1.72batch/s, auc=0.8533, loss=0.5448]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▌        | 47/292 [00:34<02:22,  1.72batch/s, auc=0.8527, loss=0.8382]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▋        | 48/292 [00:34<02:58,  1.37batch/s, auc=0.8527, loss=0.8382]\u001b[A\n",
      "Training Epoch 13/25:  16%|█▋        | 48/292 [00:34<02:58,  1.37batch/s, auc=0.8544, loss=0.4516]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 49/292 [00:34<02:45,  1.47batch/s, auc=0.8544, loss=0.4516]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 49/292 [00:35<02:45,  1.47batch/s, auc=0.8516, loss=1.1380]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 50/292 [00:35<03:12,  1.25batch/s, auc=0.8516, loss=1.1380]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 50/292 [00:36<03:12,  1.25batch/s, auc=0.8523, loss=0.6317]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 51/292 [00:36<02:55,  1.37batch/s, auc=0.8523, loss=0.6317]\u001b[A\n",
      "Training Epoch 13/25:  17%|█▋        | 51/292 [00:36<02:55,  1.37batch/s, auc=0.8548, loss=0.5087]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 52/292 [00:36<02:43,  1.47batch/s, auc=0.8548, loss=0.5087]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 52/292 [00:37<02:43,  1.47batch/s, auc=0.8564, loss=0.5391]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 53/292 [00:37<02:34,  1.55batch/s, auc=0.8564, loss=0.5391]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 53/292 [00:38<02:34,  1.55batch/s, auc=0.8577, loss=0.5042]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 54/292 [00:38<02:27,  1.61batch/s, auc=0.8577, loss=0.5042]\u001b[A\n",
      "Training Epoch 13/25:  18%|█▊        | 54/292 [00:39<02:27,  1.61batch/s, auc=0.8562, loss=1.0357]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 55/292 [00:39<02:59,  1.32batch/s, auc=0.8562, loss=1.0357]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 55/292 [00:40<02:59,  1.32batch/s, auc=0.8549, loss=0.9291]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 56/292 [00:40<03:21,  1.17batch/s, auc=0.8549, loss=0.9291]\u001b[A\n",
      "Training Epoch 13/25:  19%|█▉        | 56/292 [00:40<03:21,  1.17batch/s, auc=0.8561, loss=0.6046]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 57/292 [00:40<03:00,  1.30batch/s, auc=0.8561, loss=0.6046]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 57/292 [00:41<03:00,  1.30batch/s, auc=0.8566, loss=0.6557]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 58/292 [00:41<02:45,  1.41batch/s, auc=0.8566, loss=0.6557]\u001b[A\n",
      "Training Epoch 13/25:  20%|█▉        | 58/292 [00:41<02:45,  1.41batch/s, auc=0.8588, loss=0.6482]\u001b[A\n",
      "Training Epoch 13/25:  20%|██        | 59/292 [00:41<02:35,  1.50batch/s, auc=0.8588, loss=0.6482]\u001b[A\n",
      "Training Epoch 13/25:  20%|██        | 59/292 [00:42<02:35,  1.50batch/s, auc=0.8597, loss=0.6438]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 60/292 [00:42<02:28,  1.57batch/s, auc=0.8597, loss=0.6438]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 60/292 [00:43<02:28,  1.57batch/s, auc=0.8606, loss=0.6018]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 61/292 [00:43<02:22,  1.62batch/s, auc=0.8606, loss=0.6018]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 61/292 [00:43<02:22,  1.62batch/s, auc=0.8596, loss=0.9306]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 62/292 [00:43<02:17,  1.67batch/s, auc=0.8596, loss=0.9306]\u001b[A\n",
      "Training Epoch 13/25:  21%|██        | 62/292 [00:44<02:17,  1.67batch/s, auc=0.8596, loss=0.7539]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 63/292 [00:44<02:15,  1.70batch/s, auc=0.8596, loss=0.7539]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 63/292 [00:44<02:15,  1.70batch/s, auc=0.8615, loss=0.7008]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 64/292 [00:44<02:12,  1.71batch/s, auc=0.8615, loss=0.7008]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 64/292 [00:45<02:12,  1.71batch/s, auc=0.8611, loss=1.0176]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 65/292 [00:45<02:10,  1.73batch/s, auc=0.8611, loss=1.0176]\u001b[A\n",
      "Training Epoch 13/25:  22%|██▏       | 65/292 [00:45<02:10,  1.73batch/s, auc=0.8609, loss=0.6995]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 66/292 [00:45<02:09,  1.74batch/s, auc=0.8609, loss=0.6995]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 66/292 [00:46<02:09,  1.74batch/s, auc=0.8621, loss=0.6926]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 67/292 [00:46<02:07,  1.76batch/s, auc=0.8621, loss=0.6926]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 67/292 [00:47<02:07,  1.76batch/s, auc=0.8637, loss=0.6651]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 68/292 [00:47<02:07,  1.76batch/s, auc=0.8637, loss=0.6651]\u001b[A\n",
      "Training Epoch 13/25:  23%|██▎       | 68/292 [00:47<02:07,  1.76batch/s, auc=0.8656, loss=0.4494]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▎       | 69/292 [00:47<02:06,  1.76batch/s, auc=0.8656, loss=0.4494]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▎       | 69/292 [00:48<02:06,  1.76batch/s, auc=0.8654, loss=0.9548]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 70/292 [00:48<02:05,  1.77batch/s, auc=0.8654, loss=0.9548]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 70/292 [00:48<02:05,  1.77batch/s, auc=0.8662, loss=0.6510]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 71/292 [00:48<02:05,  1.77batch/s, auc=0.8662, loss=0.6510]\u001b[A\n",
      "Training Epoch 13/25:  24%|██▍       | 71/292 [00:49<02:05,  1.77batch/s, auc=0.8667, loss=0.6612]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▍       | 72/292 [00:49<02:04,  1.77batch/s, auc=0.8667, loss=0.6612]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▍       | 72/292 [00:50<02:04,  1.77batch/s, auc=0.8657, loss=0.8614]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 73/292 [00:50<02:37,  1.39batch/s, auc=0.8657, loss=0.8614]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 73/292 [00:50<02:37,  1.39batch/s, auc=0.8654, loss=0.9818]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 74/292 [00:50<02:26,  1.49batch/s, auc=0.8654, loss=0.9818]\u001b[A\n",
      "Training Epoch 13/25:  25%|██▌       | 74/292 [00:51<02:26,  1.49batch/s, auc=0.8672, loss=0.5854]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 75/292 [00:51<02:18,  1.57batch/s, auc=0.8672, loss=0.5854]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 75/292 [00:52<02:18,  1.57batch/s, auc=0.8663, loss=1.0409]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 76/292 [00:52<02:46,  1.30batch/s, auc=0.8663, loss=1.0409]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▌       | 76/292 [00:53<02:46,  1.30batch/s, auc=0.8664, loss=0.6558]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▋       | 77/292 [00:53<02:31,  1.42batch/s, auc=0.8664, loss=0.6558]\u001b[A\n",
      "Training Epoch 13/25:  26%|██▋       | 77/292 [00:53<02:31,  1.42batch/s, auc=0.8654, loss=0.8712]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 78/292 [00:53<02:21,  1.51batch/s, auc=0.8654, loss=0.8712]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 78/292 [00:54<02:21,  1.51batch/s, auc=0.8662, loss=0.5839]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 79/292 [00:54<02:14,  1.59batch/s, auc=0.8662, loss=0.5839]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 79/292 [00:54<02:14,  1.59batch/s, auc=0.8663, loss=0.6575]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 80/292 [00:54<02:09,  1.64batch/s, auc=0.8663, loss=0.6575]\u001b[A\n",
      "Training Epoch 13/25:  27%|██▋       | 80/292 [00:55<02:09,  1.64batch/s, auc=0.8661, loss=0.8090]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 81/292 [00:55<02:05,  1.68batch/s, auc=0.8661, loss=0.8090]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 81/292 [00:55<02:05,  1.68batch/s, auc=0.8660, loss=0.7489]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 82/292 [00:55<02:02,  1.72batch/s, auc=0.8660, loss=0.7489]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 82/292 [00:56<02:02,  1.72batch/s, auc=0.8640, loss=1.1358]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 83/292 [00:56<02:32,  1.37batch/s, auc=0.8640, loss=1.1358]\u001b[A\n",
      "Training Epoch 13/25:  28%|██▊       | 83/292 [00:57<02:32,  1.37batch/s, auc=0.8628, loss=0.8655]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 84/292 [00:57<02:21,  1.47batch/s, auc=0.8628, loss=0.8655]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 84/292 [00:58<02:21,  1.47batch/s, auc=0.8620, loss=1.0115]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 85/292 [00:58<02:12,  1.56batch/s, auc=0.8620, loss=1.0115]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 85/292 [00:58<02:12,  1.56batch/s, auc=0.8602, loss=1.4583]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 86/292 [00:58<02:07,  1.62batch/s, auc=0.8602, loss=1.4583]\u001b[A\n",
      "Training Epoch 13/25:  29%|██▉       | 86/292 [00:59<02:07,  1.62batch/s, auc=0.8605, loss=0.6481]\u001b[A\n",
      "Training Epoch 13/25:  30%|██▉       | 87/292 [00:59<02:02,  1.67batch/s, auc=0.8605, loss=0.6481]\u001b[A\n",
      "Training Epoch 13/25:  30%|██▉       | 87/292 [00:59<02:02,  1.67batch/s, auc=0.8615, loss=0.5576]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 88/292 [00:59<01:59,  1.71batch/s, auc=0.8615, loss=0.5576]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 88/292 [01:00<01:59,  1.71batch/s, auc=0.8630, loss=0.5659]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 89/292 [01:00<01:57,  1.73batch/s, auc=0.8630, loss=0.5659]\u001b[A\n",
      "Training Epoch 13/25:  30%|███       | 89/292 [01:01<01:57,  1.73batch/s, auc=0.8634, loss=0.7278]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 90/292 [01:01<02:26,  1.38batch/s, auc=0.8634, loss=0.7278]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 90/292 [01:01<02:26,  1.38batch/s, auc=0.8638, loss=0.6395]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 91/292 [01:01<02:15,  1.48batch/s, auc=0.8638, loss=0.6395]\u001b[A\n",
      "Training Epoch 13/25:  31%|███       | 91/292 [01:02<02:15,  1.48batch/s, auc=0.8649, loss=0.5894]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 92/292 [01:02<02:08,  1.56batch/s, auc=0.8649, loss=0.5894]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 92/292 [01:03<02:08,  1.56batch/s, auc=0.8653, loss=0.6668]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 93/292 [01:03<02:02,  1.63batch/s, auc=0.8653, loss=0.6668]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 93/292 [01:03<02:02,  1.63batch/s, auc=0.8656, loss=0.7852]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 94/292 [01:03<01:58,  1.67batch/s, auc=0.8656, loss=0.7852]\u001b[A\n",
      "Training Epoch 13/25:  32%|███▏      | 94/292 [01:04<01:58,  1.67batch/s, auc=0.8634, loss=1.2341]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 95/292 [01:04<02:25,  1.35batch/s, auc=0.8634, loss=1.2341]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 95/292 [01:05<02:25,  1.35batch/s, auc=0.8637, loss=0.7317]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 96/292 [01:05<02:14,  1.46batch/s, auc=0.8637, loss=0.7317]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 96/292 [01:06<02:14,  1.46batch/s, auc=0.8630, loss=0.9379]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 97/292 [01:06<02:36,  1.25batch/s, auc=0.8630, loss=0.9379]\u001b[A\n",
      "Training Epoch 13/25:  33%|███▎      | 97/292 [01:06<02:36,  1.25batch/s, auc=0.8639, loss=0.5236]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▎      | 98/292 [01:06<02:21,  1.37batch/s, auc=0.8639, loss=0.5236]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▎      | 98/292 [01:07<02:21,  1.37batch/s, auc=0.8631, loss=0.8875]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 99/292 [01:07<02:40,  1.20batch/s, auc=0.8631, loss=0.8875]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 99/292 [01:08<02:40,  1.20batch/s, auc=0.8633, loss=0.5964]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 100/292 [01:08<02:24,  1.33batch/s, auc=0.8633, loss=0.5964]\u001b[A\n",
      "Training Epoch 13/25:  34%|███▍      | 100/292 [01:09<02:24,  1.33batch/s, auc=0.8634, loss=0.7879]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 101/292 [01:09<02:12,  1.44batch/s, auc=0.8634, loss=0.7879]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 101/292 [01:09<02:12,  1.44batch/s, auc=0.8635, loss=0.8817]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 102/292 [01:09<02:04,  1.53batch/s, auc=0.8635, loss=0.8817]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▍      | 102/292 [01:10<02:04,  1.53batch/s, auc=0.8641, loss=0.6283]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▌      | 103/292 [01:10<01:58,  1.60batch/s, auc=0.8641, loss=0.6283]\u001b[A\n",
      "Training Epoch 13/25:  35%|███▌      | 103/292 [01:10<01:58,  1.60batch/s, auc=0.8644, loss=0.7725]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 104/292 [01:10<01:53,  1.65batch/s, auc=0.8644, loss=0.7725]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 104/292 [01:11<01:53,  1.65batch/s, auc=0.8649, loss=0.7358]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 105/292 [01:11<01:50,  1.69batch/s, auc=0.8649, loss=0.7358]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▌      | 105/292 [01:12<01:50,  1.69batch/s, auc=0.8639, loss=1.0246]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▋      | 106/292 [01:12<02:16,  1.36batch/s, auc=0.8639, loss=1.0246]\u001b[A\n",
      "Training Epoch 13/25:  36%|███▋      | 106/292 [01:12<02:16,  1.36batch/s, auc=0.8643, loss=0.6566]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 107/292 [01:12<02:06,  1.46batch/s, auc=0.8643, loss=0.6566]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 107/292 [01:14<02:06,  1.46batch/s, auc=0.8636, loss=0.9648]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 108/292 [01:14<02:27,  1.25batch/s, auc=0.8636, loss=0.9648]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 108/292 [01:15<02:27,  1.25batch/s, auc=0.8626, loss=1.1208]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 109/292 [01:15<02:41,  1.13batch/s, auc=0.8626, loss=1.1208]\u001b[A\n",
      "Training Epoch 13/25:  37%|███▋      | 109/292 [01:15<02:41,  1.13batch/s, auc=0.8629, loss=0.8047]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 110/292 [01:15<02:22,  1.27batch/s, auc=0.8629, loss=0.8047]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 110/292 [01:16<02:22,  1.27batch/s, auc=0.8631, loss=0.7798]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 111/292 [01:16<02:09,  1.39batch/s, auc=0.8631, loss=0.7798]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 111/292 [01:16<02:09,  1.39batch/s, auc=0.8635, loss=0.6289]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 112/292 [01:16<02:00,  1.49batch/s, auc=0.8635, loss=0.6289]\u001b[A\n",
      "Training Epoch 13/25:  38%|███▊      | 112/292 [01:17<02:00,  1.49batch/s, auc=0.8635, loss=0.7864]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▊      | 113/292 [01:17<01:53,  1.57batch/s, auc=0.8635, loss=0.7864]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▊      | 113/292 [01:17<01:53,  1.57batch/s, auc=0.8635, loss=0.7046]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 114/292 [01:17<01:49,  1.63batch/s, auc=0.8635, loss=0.7046]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 114/292 [01:18<01:49,  1.63batch/s, auc=0.8645, loss=0.4935]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 115/292 [01:18<01:45,  1.67batch/s, auc=0.8645, loss=0.4935]\u001b[A\n",
      "Training Epoch 13/25:  39%|███▉      | 115/292 [01:19<01:45,  1.67batch/s, auc=0.8637, loss=1.0280]\u001b[A\n",
      "Training Epoch 13/25:  40%|███▉      | 116/292 [01:19<02:10,  1.35batch/s, auc=0.8637, loss=1.0280]\u001b[A\n",
      "Training Epoch 13/25:  40%|███▉      | 116/292 [01:20<02:10,  1.35batch/s, auc=0.8649, loss=0.4817]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 117/292 [01:20<02:00,  1.46batch/s, auc=0.8649, loss=0.4817]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 117/292 [01:21<02:00,  1.46batch/s, auc=0.8643, loss=0.9705]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 118/292 [01:21<02:19,  1.25batch/s, auc=0.8643, loss=0.9705]\u001b[A\n",
      "Training Epoch 13/25:  40%|████      | 118/292 [01:22<02:19,  1.25batch/s, auc=0.8636, loss=1.0560]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 119/292 [01:22<02:33,  1.13batch/s, auc=0.8636, loss=1.0560]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 119/292 [01:23<02:33,  1.13batch/s, auc=0.8632, loss=0.9436]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 120/292 [01:23<02:42,  1.06batch/s, auc=0.8632, loss=0.9436]\u001b[A\n",
      "Training Epoch 13/25:  41%|████      | 120/292 [01:23<02:42,  1.06batch/s, auc=0.8636, loss=0.6342]\u001b[A\n",
      "Training Epoch 13/25:  41%|████▏     | 121/292 [01:23<02:21,  1.21batch/s, auc=0.8636, loss=0.6342]\u001b[A\n",
      "Training Epoch 13/25:  41%|████▏     | 121/292 [01:24<02:21,  1.21batch/s, auc=0.8641, loss=0.4655]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 122/292 [01:24<02:06,  1.34batch/s, auc=0.8641, loss=0.4655]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 122/292 [01:25<02:06,  1.34batch/s, auc=0.8644, loss=0.5489]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 123/292 [01:25<01:56,  1.45batch/s, auc=0.8644, loss=0.5489]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 123/292 [01:25<01:56,  1.45batch/s, auc=0.8646, loss=0.6278]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 124/292 [01:25<01:49,  1.54batch/s, auc=0.8646, loss=0.6278]\u001b[A\n",
      "Training Epoch 13/25:  42%|████▏     | 124/292 [01:26<01:49,  1.54batch/s, auc=0.8653, loss=0.5294]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 125/292 [01:26<01:44,  1.60batch/s, auc=0.8653, loss=0.5294]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 125/292 [01:26<01:44,  1.60batch/s, auc=0.8659, loss=0.5206]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 126/292 [01:26<01:40,  1.66batch/s, auc=0.8659, loss=0.5206]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 126/292 [01:27<01:40,  1.66batch/s, auc=0.8669, loss=0.4038]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 127/292 [01:27<01:37,  1.69batch/s, auc=0.8669, loss=0.4038]\u001b[A\n",
      "Training Epoch 13/25:  43%|████▎     | 127/292 [01:27<01:37,  1.69batch/s, auc=0.8667, loss=0.7797]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 128/292 [01:27<01:35,  1.72batch/s, auc=0.8667, loss=0.7797]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 128/292 [01:28<01:35,  1.72batch/s, auc=0.8677, loss=0.5241]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 129/292 [01:28<01:33,  1.74batch/s, auc=0.8677, loss=0.5241]\u001b[A\n",
      "Training Epoch 13/25:  44%|████▍     | 129/292 [01:28<01:33,  1.74batch/s, auc=0.8673, loss=0.9510]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 130/292 [01:28<01:32,  1.76batch/s, auc=0.8673, loss=0.9510]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 130/292 [01:30<01:32,  1.76batch/s, auc=0.8665, loss=1.2030]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 131/292 [01:30<01:56,  1.39batch/s, auc=0.8665, loss=1.2030]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▍     | 131/292 [01:31<01:56,  1.39batch/s, auc=0.8663, loss=0.7537]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▌     | 132/292 [01:31<02:12,  1.21batch/s, auc=0.8663, loss=0.7537]\u001b[A\n",
      "Training Epoch 13/25:  45%|████▌     | 132/292 [01:31<02:12,  1.21batch/s, auc=0.8662, loss=0.7728]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 133/292 [01:31<01:58,  1.34batch/s, auc=0.8662, loss=0.7728]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 133/292 [01:32<01:58,  1.34batch/s, auc=0.8666, loss=0.5868]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 134/292 [01:32<01:49,  1.45batch/s, auc=0.8666, loss=0.5868]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 134/292 [01:32<01:49,  1.45batch/s, auc=0.8673, loss=0.6320]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 135/292 [01:32<01:42,  1.53batch/s, auc=0.8673, loss=0.6320]\u001b[A\n",
      "Training Epoch 13/25:  46%|████▌     | 135/292 [01:33<01:42,  1.53batch/s, auc=0.8669, loss=0.8467]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 136/292 [01:33<02:01,  1.28batch/s, auc=0.8669, loss=0.8467]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 136/292 [01:34<02:01,  1.28batch/s, auc=0.8671, loss=0.5864]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 137/292 [01:34<01:50,  1.40batch/s, auc=0.8671, loss=0.5864]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 137/292 [01:34<01:50,  1.40batch/s, auc=0.8674, loss=0.6691]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 138/292 [01:34<01:42,  1.50batch/s, auc=0.8674, loss=0.6691]\u001b[A\n",
      "Training Epoch 13/25:  47%|████▋     | 138/292 [01:35<01:42,  1.50batch/s, auc=0.8669, loss=1.1729]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 139/292 [01:35<01:37,  1.58batch/s, auc=0.8669, loss=1.1729]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 139/292 [01:36<01:37,  1.58batch/s, auc=0.8671, loss=0.6307]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 140/292 [01:36<01:33,  1.63batch/s, auc=0.8671, loss=0.6307]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 140/292 [01:36<01:33,  1.63batch/s, auc=0.8675, loss=0.6639]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 141/292 [01:36<01:30,  1.68batch/s, auc=0.8675, loss=0.6639]\u001b[A\n",
      "Training Epoch 13/25:  48%|████▊     | 141/292 [01:37<01:30,  1.68batch/s, auc=0.8674, loss=0.6198]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▊     | 142/292 [01:37<01:27,  1.71batch/s, auc=0.8674, loss=0.6198]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▊     | 142/292 [01:38<01:27,  1.71batch/s, auc=0.8670, loss=0.7665]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 143/292 [01:38<01:49,  1.36batch/s, auc=0.8670, loss=0.7665]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 143/292 [01:39<01:49,  1.36batch/s, auc=0.8662, loss=1.1279]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 144/292 [01:39<02:03,  1.20batch/s, auc=0.8662, loss=1.1279]\u001b[A\n",
      "Training Epoch 13/25:  49%|████▉     | 144/292 [01:39<02:03,  1.20batch/s, auc=0.8662, loss=0.8699]\u001b[A\n",
      "Training Epoch 13/25:  50%|████▉     | 145/292 [01:39<01:50,  1.33batch/s, auc=0.8662, loss=0.8699]\u001b[A\n",
      "Training Epoch 13/25:  50%|████▉     | 145/292 [01:40<01:50,  1.33batch/s, auc=0.8662, loss=0.7457]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 146/292 [01:40<01:41,  1.44batch/s, auc=0.8662, loss=0.7457]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 146/292 [01:41<01:41,  1.44batch/s, auc=0.8661, loss=0.8117]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 147/292 [01:41<01:34,  1.53batch/s, auc=0.8661, loss=0.8117]\u001b[A\n",
      "Training Epoch 13/25:  50%|█████     | 147/292 [01:41<01:34,  1.53batch/s, auc=0.8663, loss=0.6526]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 148/292 [01:41<01:30,  1.60batch/s, auc=0.8663, loss=0.6526]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 148/292 [01:42<01:30,  1.60batch/s, auc=0.8667, loss=0.4636]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 149/292 [01:42<01:26,  1.65batch/s, auc=0.8667, loss=0.4636]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████     | 149/292 [01:43<01:26,  1.65batch/s, auc=0.8664, loss=0.8990]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████▏    | 150/292 [01:43<01:46,  1.34batch/s, auc=0.8664, loss=0.8990]\u001b[A\n",
      "Training Epoch 13/25:  51%|█████▏    | 150/292 [01:43<01:46,  1.34batch/s, auc=0.8668, loss=0.7231]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 151/292 [01:43<01:37,  1.45batch/s, auc=0.8668, loss=0.7231]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 151/292 [01:44<01:37,  1.45batch/s, auc=0.8672, loss=0.7072]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 152/292 [01:44<01:31,  1.53batch/s, auc=0.8672, loss=0.7072]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 152/292 [01:45<01:31,  1.53batch/s, auc=0.8665, loss=1.0704]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 153/292 [01:45<01:48,  1.28batch/s, auc=0.8665, loss=1.0704]\u001b[A\n",
      "Training Epoch 13/25:  52%|█████▏    | 153/292 [01:46<01:48,  1.28batch/s, auc=0.8663, loss=0.8035]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 154/292 [01:46<01:59,  1.15batch/s, auc=0.8663, loss=0.8035]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 154/292 [01:47<01:59,  1.15batch/s, auc=0.8674, loss=0.4353]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 155/292 [01:47<01:46,  1.29batch/s, auc=0.8674, loss=0.4353]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 155/292 [01:47<01:46,  1.29batch/s, auc=0.8672, loss=0.8410]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 156/292 [01:47<01:36,  1.40batch/s, auc=0.8672, loss=0.8410]\u001b[A\n",
      "Training Epoch 13/25:  53%|█████▎    | 156/292 [01:48<01:36,  1.40batch/s, auc=0.8683, loss=0.3333]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 157/292 [01:48<01:29,  1.50batch/s, auc=0.8683, loss=0.3333]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 157/292 [01:48<01:29,  1.50batch/s, auc=0.8681, loss=0.6877]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 158/292 [01:48<01:25,  1.58batch/s, auc=0.8681, loss=0.6877]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 158/292 [01:49<01:25,  1.58batch/s, auc=0.8688, loss=0.4606]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 159/292 [01:49<01:21,  1.63batch/s, auc=0.8688, loss=0.4606]\u001b[A\n",
      "Training Epoch 13/25:  54%|█████▍    | 159/292 [01:50<01:21,  1.63batch/s, auc=0.8676, loss=1.4080]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▍    | 160/292 [01:50<01:39,  1.33batch/s, auc=0.8676, loss=1.4080]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▍    | 160/292 [01:50<01:39,  1.33batch/s, auc=0.8673, loss=0.6433]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 161/292 [01:50<01:31,  1.44batch/s, auc=0.8673, loss=0.6433]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 161/292 [01:51<01:31,  1.44batch/s, auc=0.8669, loss=0.7654]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 162/292 [01:51<01:25,  1.53batch/s, auc=0.8669, loss=0.7654]\u001b[A\n",
      "Training Epoch 13/25:  55%|█████▌    | 162/292 [01:52<01:25,  1.53batch/s, auc=0.8673, loss=0.7798]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 163/292 [01:52<01:20,  1.60batch/s, auc=0.8673, loss=0.7798]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 163/292 [01:52<01:20,  1.60batch/s, auc=0.8674, loss=0.7146]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 164/292 [01:52<01:17,  1.65batch/s, auc=0.8674, loss=0.7146]\u001b[A\n",
      "Training Epoch 13/25:  56%|█████▌    | 164/292 [01:53<01:17,  1.65batch/s, auc=0.8669, loss=1.0797]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 165/292 [01:53<01:34,  1.34batch/s, auc=0.8669, loss=1.0797]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 165/292 [01:54<01:34,  1.34batch/s, auc=0.8670, loss=0.6513]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 166/292 [01:54<01:27,  1.45batch/s, auc=0.8670, loss=0.6513]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 166/292 [01:55<01:27,  1.45batch/s, auc=0.8669, loss=0.7724]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 167/292 [01:55<01:40,  1.24batch/s, auc=0.8669, loss=0.7724]\u001b[A\n",
      "Training Epoch 13/25:  57%|█████▋    | 167/292 [01:55<01:40,  1.24batch/s, auc=0.8671, loss=0.6530]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 168/292 [01:55<01:30,  1.37batch/s, auc=0.8671, loss=0.6530]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 168/292 [01:56<01:30,  1.37batch/s, auc=0.8667, loss=0.9840]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 169/292 [01:56<01:23,  1.47batch/s, auc=0.8667, loss=0.9840]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 169/292 [01:57<01:23,  1.47batch/s, auc=0.8670, loss=0.6439]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 170/292 [01:57<01:18,  1.55batch/s, auc=0.8670, loss=0.6439]\u001b[A\n",
      "Training Epoch 13/25:  58%|█████▊    | 170/292 [01:58<01:18,  1.55batch/s, auc=0.8660, loss=1.4063]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▊    | 171/292 [01:58<01:33,  1.29batch/s, auc=0.8660, loss=1.4063]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▊    | 171/292 [01:59<01:33,  1.29batch/s, auc=0.8653, loss=1.1669]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 172/292 [01:59<01:43,  1.16batch/s, auc=0.8653, loss=1.1669]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 172/292 [02:00<01:43,  1.16batch/s, auc=0.8652, loss=0.8157]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 173/292 [02:00<01:50,  1.08batch/s, auc=0.8652, loss=0.8157]\u001b[A\n",
      "Training Epoch 13/25:  59%|█████▉    | 173/292 [02:00<01:50,  1.08batch/s, auc=0.8654, loss=0.7290]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 174/292 [02:00<01:36,  1.22batch/s, auc=0.8654, loss=0.7290]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 174/292 [02:01<01:36,  1.22batch/s, auc=0.8655, loss=0.5701]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 175/292 [02:01<01:26,  1.35batch/s, auc=0.8655, loss=0.5701]\u001b[A\n",
      "Training Epoch 13/25:  60%|█████▉    | 175/292 [02:01<01:26,  1.35batch/s, auc=0.8655, loss=0.7803]\u001b[A\n",
      "Training Epoch 13/25:  60%|██████    | 176/292 [02:01<01:19,  1.46batch/s, auc=0.8655, loss=0.7803]\u001b[A\n",
      "Training Epoch 13/25:  60%|██████    | 176/292 [02:02<01:19,  1.46batch/s, auc=0.8652, loss=0.8844]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 177/292 [02:02<01:14,  1.54batch/s, auc=0.8652, loss=0.8844]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 177/292 [02:03<01:14,  1.54batch/s, auc=0.8647, loss=0.9579]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 178/292 [02:03<01:28,  1.29batch/s, auc=0.8647, loss=0.9579]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████    | 178/292 [02:04<01:28,  1.29batch/s, auc=0.8650, loss=0.8703]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████▏   | 179/292 [02:04<01:20,  1.40batch/s, auc=0.8650, loss=0.8703]\u001b[A\n",
      "Training Epoch 13/25:  61%|██████▏   | 179/292 [02:04<01:20,  1.40batch/s, auc=0.8649, loss=0.6974]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 180/292 [02:04<01:14,  1.50batch/s, auc=0.8649, loss=0.6974]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 180/292 [02:05<01:14,  1.50batch/s, auc=0.8654, loss=0.5515]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 181/292 [02:05<01:10,  1.57batch/s, auc=0.8654, loss=0.5515]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 181/292 [02:05<01:10,  1.57batch/s, auc=0.8655, loss=0.6067]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 182/292 [02:05<01:07,  1.63batch/s, auc=0.8655, loss=0.6067]\u001b[A\n",
      "Training Epoch 13/25:  62%|██████▏   | 182/292 [02:06<01:07,  1.63batch/s, auc=0.8653, loss=0.8204]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 183/292 [02:06<01:05,  1.67batch/s, auc=0.8653, loss=0.8204]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 183/292 [02:06<01:05,  1.67batch/s, auc=0.8659, loss=0.5844]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 184/292 [02:06<01:03,  1.71batch/s, auc=0.8659, loss=0.5844]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 184/292 [02:07<01:03,  1.71batch/s, auc=0.8659, loss=0.6764]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 185/292 [02:07<01:01,  1.73batch/s, auc=0.8659, loss=0.6764]\u001b[A\n",
      "Training Epoch 13/25:  63%|██████▎   | 185/292 [02:08<01:01,  1.73batch/s, auc=0.8663, loss=0.6070]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▎   | 186/292 [02:08<01:00,  1.74batch/s, auc=0.8663, loss=0.6070]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▎   | 186/292 [02:08<01:00,  1.74batch/s, auc=0.8665, loss=0.6219]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 187/292 [02:08<00:59,  1.75batch/s, auc=0.8665, loss=0.6219]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 187/292 [02:09<00:59,  1.75batch/s, auc=0.8667, loss=0.6359]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 188/292 [02:09<00:58,  1.76batch/s, auc=0.8667, loss=0.6359]\u001b[A\n",
      "Training Epoch 13/25:  64%|██████▍   | 188/292 [02:09<00:58,  1.76batch/s, auc=0.8670, loss=0.8044]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▍   | 189/292 [02:09<00:58,  1.77batch/s, auc=0.8670, loss=0.8044]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▍   | 189/292 [02:10<00:58,  1.77batch/s, auc=0.8670, loss=0.7293]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 190/292 [02:10<00:57,  1.77batch/s, auc=0.8670, loss=0.7293]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 190/292 [02:10<00:57,  1.77batch/s, auc=0.8674, loss=0.5149]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 191/292 [02:10<00:56,  1.77batch/s, auc=0.8674, loss=0.5149]\u001b[A\n",
      "Training Epoch 13/25:  65%|██████▌   | 191/292 [02:11<00:56,  1.77batch/s, auc=0.8675, loss=0.8512]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 192/292 [02:11<00:56,  1.77batch/s, auc=0.8675, loss=0.8512]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 192/292 [02:11<00:56,  1.77batch/s, auc=0.8677, loss=0.6358]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 193/292 [02:11<00:55,  1.78batch/s, auc=0.8677, loss=0.6358]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▌   | 193/292 [02:12<00:55,  1.78batch/s, auc=0.8683, loss=0.7166]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▋   | 194/292 [02:12<00:55,  1.78batch/s, auc=0.8683, loss=0.7166]\u001b[A\n",
      "Training Epoch 13/25:  66%|██████▋   | 194/292 [02:13<00:55,  1.78batch/s, auc=0.8675, loss=1.3660]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 195/292 [02:13<01:09,  1.39batch/s, auc=0.8675, loss=1.3660]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 195/292 [02:14<01:09,  1.39batch/s, auc=0.8678, loss=0.6944]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 196/292 [02:14<01:04,  1.49batch/s, auc=0.8678, loss=0.6944]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 196/292 [02:14<01:04,  1.49batch/s, auc=0.8678, loss=0.6491]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 197/292 [02:14<01:00,  1.57batch/s, auc=0.8678, loss=0.6491]\u001b[A\n",
      "Training Epoch 13/25:  67%|██████▋   | 197/292 [02:15<01:00,  1.57batch/s, auc=0.8682, loss=0.5838]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 198/292 [02:15<00:57,  1.63batch/s, auc=0.8682, loss=0.5838]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 198/292 [02:15<00:57,  1.63batch/s, auc=0.8685, loss=0.6076]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 199/292 [02:15<00:55,  1.67batch/s, auc=0.8685, loss=0.6076]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 199/292 [02:16<00:55,  1.67batch/s, auc=0.8683, loss=0.6082]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 200/292 [02:16<00:54,  1.70batch/s, auc=0.8683, loss=0.6082]\u001b[A\n",
      "Training Epoch 13/25:  68%|██████▊   | 200/292 [02:16<00:54,  1.70batch/s, auc=0.8686, loss=0.6574]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 201/292 [02:16<00:52,  1.72batch/s, auc=0.8686, loss=0.6574]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 201/292 [02:17<00:52,  1.72batch/s, auc=0.8688, loss=0.6379]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 202/292 [02:17<00:51,  1.74batch/s, auc=0.8688, loss=0.6379]\u001b[A\n",
      "Training Epoch 13/25:  69%|██████▉   | 202/292 [02:18<00:51,  1.74batch/s, auc=0.8688, loss=0.7571]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 203/292 [02:18<00:50,  1.75batch/s, auc=0.8688, loss=0.7571]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 203/292 [02:18<00:50,  1.75batch/s, auc=0.8691, loss=0.6430]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 204/292 [02:18<00:50,  1.76batch/s, auc=0.8691, loss=0.6430]\u001b[A\n",
      "Training Epoch 13/25:  70%|██████▉   | 204/292 [02:19<00:50,  1.76batch/s, auc=0.8688, loss=0.8026]\u001b[A\n",
      "Training Epoch 13/25:  70%|███████   | 205/292 [02:19<01:02,  1.39batch/s, auc=0.8688, loss=0.8026]\u001b[A\n",
      "Training Epoch 13/25:  70%|███████   | 205/292 [02:20<01:02,  1.39batch/s, auc=0.8687, loss=0.7787]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 206/292 [02:20<00:57,  1.48batch/s, auc=0.8687, loss=0.7787]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 206/292 [02:20<00:57,  1.48batch/s, auc=0.8687, loss=0.7784]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 207/292 [02:20<00:54,  1.56batch/s, auc=0.8687, loss=0.7784]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 207/292 [02:21<00:54,  1.56batch/s, auc=0.8687, loss=0.5901]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 208/292 [02:21<00:51,  1.62batch/s, auc=0.8687, loss=0.5901]\u001b[A\n",
      "Training Epoch 13/25:  71%|███████   | 208/292 [02:22<00:51,  1.62batch/s, auc=0.8685, loss=0.7235]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 209/292 [02:22<00:49,  1.66batch/s, auc=0.8685, loss=0.7235]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 209/292 [02:23<00:49,  1.66batch/s, auc=0.8682, loss=0.8980]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 210/292 [02:23<01:01,  1.34batch/s, auc=0.8682, loss=0.8980]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 210/292 [02:23<01:01,  1.34batch/s, auc=0.8689, loss=0.4701]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 211/292 [02:23<00:55,  1.45batch/s, auc=0.8689, loss=0.4701]\u001b[A\n",
      "Training Epoch 13/25:  72%|███████▏  | 211/292 [02:24<00:55,  1.45batch/s, auc=0.8686, loss=0.9983]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 212/292 [02:24<01:04,  1.24batch/s, auc=0.8686, loss=0.9983]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 212/292 [02:25<01:04,  1.24batch/s, auc=0.8690, loss=0.4928]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 213/292 [02:25<00:57,  1.36batch/s, auc=0.8690, loss=0.4928]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 213/292 [02:25<00:57,  1.36batch/s, auc=0.8694, loss=0.5624]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 214/292 [02:25<00:53,  1.47batch/s, auc=0.8694, loss=0.5624]\u001b[A\n",
      "Training Epoch 13/25:  73%|███████▎  | 214/292 [02:26<00:53,  1.47batch/s, auc=0.8691, loss=0.8675]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▎  | 215/292 [02:26<00:49,  1.55batch/s, auc=0.8691, loss=0.8675]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▎  | 215/292 [02:26<00:49,  1.55batch/s, auc=0.8697, loss=0.5607]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 216/292 [02:26<00:47,  1.61batch/s, auc=0.8697, loss=0.5607]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 216/292 [02:27<00:47,  1.61batch/s, auc=0.8696, loss=0.7352]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 217/292 [02:27<00:45,  1.66batch/s, auc=0.8696, loss=0.7352]\u001b[A\n",
      "Training Epoch 13/25:  74%|███████▍  | 217/292 [02:28<00:45,  1.66batch/s, auc=0.8694, loss=0.9616]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▍  | 218/292 [02:28<00:43,  1.69batch/s, auc=0.8694, loss=0.9616]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▍  | 218/292 [02:28<00:43,  1.69batch/s, auc=0.8696, loss=0.6715]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 219/292 [02:28<00:42,  1.72batch/s, auc=0.8696, loss=0.6715]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 219/292 [02:29<00:42,  1.72batch/s, auc=0.8700, loss=0.5362]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 220/292 [02:29<00:41,  1.74batch/s, auc=0.8700, loss=0.5362]\u001b[A\n",
      "Training Epoch 13/25:  75%|███████▌  | 220/292 [02:30<00:41,  1.74batch/s, auc=0.8696, loss=1.0122]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 221/292 [02:30<00:51,  1.37batch/s, auc=0.8696, loss=1.0122]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 221/292 [02:31<00:51,  1.37batch/s, auc=0.8694, loss=0.9058]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 222/292 [02:31<00:58,  1.20batch/s, auc=0.8694, loss=0.9058]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▌  | 222/292 [02:31<00:58,  1.20batch/s, auc=0.8697, loss=0.5995]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▋  | 223/292 [02:31<00:51,  1.33batch/s, auc=0.8697, loss=0.5995]\u001b[A\n",
      "Training Epoch 13/25:  76%|███████▋  | 223/292 [02:32<00:51,  1.33batch/s, auc=0.8701, loss=0.4516]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 224/292 [02:32<00:47,  1.44batch/s, auc=0.8701, loss=0.4516]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 224/292 [02:33<00:47,  1.44batch/s, auc=0.8700, loss=0.8888]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 225/292 [02:33<00:43,  1.53batch/s, auc=0.8700, loss=0.8888]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 225/292 [02:33<00:43,  1.53batch/s, auc=0.8701, loss=0.6758]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 226/292 [02:33<00:41,  1.59batch/s, auc=0.8701, loss=0.6758]\u001b[A\n",
      "Training Epoch 13/25:  77%|███████▋  | 226/292 [02:34<00:41,  1.59batch/s, auc=0.8702, loss=0.6329]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 227/292 [02:34<00:39,  1.65batch/s, auc=0.8702, loss=0.6329]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 227/292 [02:34<00:39,  1.65batch/s, auc=0.8701, loss=0.6787]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 228/292 [02:34<00:38,  1.68batch/s, auc=0.8701, loss=0.6787]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 228/292 [02:35<00:38,  1.68batch/s, auc=0.8702, loss=0.7460]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 229/292 [02:35<00:36,  1.71batch/s, auc=0.8702, loss=0.7460]\u001b[A\n",
      "Training Epoch 13/25:  78%|███████▊  | 229/292 [02:36<00:36,  1.71batch/s, auc=0.8692, loss=1.5108]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 230/292 [02:36<00:45,  1.37batch/s, auc=0.8692, loss=1.5108]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 230/292 [02:37<00:45,  1.37batch/s, auc=0.8690, loss=0.9677]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 231/292 [02:37<00:51,  1.19batch/s, auc=0.8690, loss=0.9677]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 231/292 [02:38<00:51,  1.19batch/s, auc=0.8694, loss=0.4699]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 232/292 [02:38<00:45,  1.32batch/s, auc=0.8694, loss=0.4699]\u001b[A\n",
      "Training Epoch 13/25:  79%|███████▉  | 232/292 [02:38<00:45,  1.32batch/s, auc=0.8695, loss=0.4957]\u001b[A\n",
      "Training Epoch 13/25:  80%|███████▉  | 233/292 [02:38<00:41,  1.43batch/s, auc=0.8695, loss=0.4957]\u001b[A\n",
      "Training Epoch 13/25:  80%|███████▉  | 233/292 [02:39<00:41,  1.43batch/s, auc=0.8699, loss=0.5916]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 234/292 [02:39<00:38,  1.52batch/s, auc=0.8699, loss=0.5916]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 234/292 [02:39<00:38,  1.52batch/s, auc=0.8700, loss=0.5501]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 235/292 [02:39<00:35,  1.59batch/s, auc=0.8700, loss=0.5501]\u001b[A\n",
      "Training Epoch 13/25:  80%|████████  | 235/292 [02:40<00:35,  1.59batch/s, auc=0.8698, loss=0.8774]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 236/292 [02:40<00:42,  1.31batch/s, auc=0.8698, loss=0.8774]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 236/292 [02:41<00:42,  1.31batch/s, auc=0.8700, loss=0.6615]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 237/292 [02:41<00:38,  1.42batch/s, auc=0.8700, loss=0.6615]\u001b[A\n",
      "Training Epoch 13/25:  81%|████████  | 237/292 [02:41<00:38,  1.42batch/s, auc=0.8701, loss=0.6769]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 238/292 [02:41<00:35,  1.51batch/s, auc=0.8701, loss=0.6769]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 238/292 [02:43<00:35,  1.51batch/s, auc=0.8697, loss=0.9864]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 239/292 [02:43<00:41,  1.27batch/s, auc=0.8697, loss=0.9864]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 239/292 [02:43<00:41,  1.27batch/s, auc=0.8699, loss=0.6248]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 240/292 [02:43<00:37,  1.39batch/s, auc=0.8699, loss=0.6248]\u001b[A\n",
      "Training Epoch 13/25:  82%|████████▏ | 240/292 [02:44<00:37,  1.39batch/s, auc=0.8699, loss=0.7555]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 241/292 [02:44<00:34,  1.49batch/s, auc=0.8699, loss=0.7555]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 241/292 [02:44<00:34,  1.49batch/s, auc=0.8701, loss=0.6265]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 242/292 [02:44<00:31,  1.56batch/s, auc=0.8701, loss=0.6265]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 242/292 [02:45<00:31,  1.56batch/s, auc=0.8703, loss=0.6069]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 243/292 [02:45<00:30,  1.62batch/s, auc=0.8703, loss=0.6069]\u001b[A\n",
      "Training Epoch 13/25:  83%|████████▎ | 243/292 [02:45<00:30,  1.62batch/s, auc=0.8706, loss=0.6110]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▎ | 244/292 [02:45<00:28,  1.67batch/s, auc=0.8706, loss=0.6110]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▎ | 244/292 [02:46<00:28,  1.67batch/s, auc=0.8699, loss=1.1054]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 245/292 [02:46<00:34,  1.34batch/s, auc=0.8699, loss=1.1054]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 245/292 [02:47<00:34,  1.34batch/s, auc=0.8696, loss=1.0319]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 246/292 [02:47<00:38,  1.18batch/s, auc=0.8696, loss=1.0319]\u001b[A\n",
      "Training Epoch 13/25:  84%|████████▍ | 246/292 [02:48<00:38,  1.18batch/s, auc=0.8691, loss=0.9692]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 247/292 [02:48<00:34,  1.31batch/s, auc=0.8691, loss=0.9692]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 247/292 [02:49<00:34,  1.31batch/s, auc=0.8691, loss=0.6403]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 248/292 [02:49<00:30,  1.43batch/s, auc=0.8691, loss=0.6403]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▍ | 248/292 [02:49<00:30,  1.43batch/s, auc=0.8694, loss=0.7045]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▌ | 249/292 [02:49<00:28,  1.51batch/s, auc=0.8694, loss=0.7045]\u001b[A\n",
      "Training Epoch 13/25:  85%|████████▌ | 249/292 [02:50<00:28,  1.51batch/s, auc=0.8693, loss=0.9693]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 250/292 [02:50<00:26,  1.59batch/s, auc=0.8693, loss=0.9693]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 250/292 [02:50<00:26,  1.59batch/s, auc=0.8695, loss=0.7707]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 251/292 [02:50<00:25,  1.64batch/s, auc=0.8695, loss=0.7707]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▌ | 251/292 [02:51<00:25,  1.64batch/s, auc=0.8696, loss=0.6439]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▋ | 252/292 [02:51<00:23,  1.68batch/s, auc=0.8696, loss=0.6439]\u001b[A\n",
      "Training Epoch 13/25:  86%|████████▋ | 252/292 [02:51<00:23,  1.68batch/s, auc=0.8699, loss=0.4646]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 253/292 [02:51<00:22,  1.70batch/s, auc=0.8699, loss=0.4646]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 253/292 [02:52<00:22,  1.70batch/s, auc=0.8700, loss=0.6084]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 254/292 [02:52<00:22,  1.72batch/s, auc=0.8700, loss=0.6084]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 254/292 [02:53<00:22,  1.72batch/s, auc=0.8701, loss=0.8071]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 255/292 [02:53<00:21,  1.74batch/s, auc=0.8701, loss=0.8071]\u001b[A\n",
      "Training Epoch 13/25:  87%|████████▋ | 255/292 [02:53<00:21,  1.74batch/s, auc=0.8704, loss=0.5807]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 256/292 [02:53<00:20,  1.75batch/s, auc=0.8704, loss=0.5807]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 256/292 [02:54<00:20,  1.75batch/s, auc=0.8706, loss=0.6362]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 257/292 [02:54<00:19,  1.76batch/s, auc=0.8706, loss=0.6362]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 257/292 [02:54<00:19,  1.76batch/s, auc=0.8707, loss=0.6304]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 258/292 [02:54<00:19,  1.76batch/s, auc=0.8707, loss=0.6304]\u001b[A\n",
      "Training Epoch 13/25:  88%|████████▊ | 258/292 [02:55<00:19,  1.76batch/s, auc=0.8707, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▊ | 259/292 [02:55<00:18,  1.77batch/s, auc=0.8707, loss=0.7497]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▊ | 259/292 [02:55<00:18,  1.77batch/s, auc=0.8710, loss=0.4513]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 260/292 [02:55<00:18,  1.77batch/s, auc=0.8710, loss=0.4513]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 260/292 [02:56<00:18,  1.77batch/s, auc=0.8710, loss=0.6706]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 261/292 [02:56<00:17,  1.77batch/s, auc=0.8710, loss=0.6706]\u001b[A\n",
      "Training Epoch 13/25:  89%|████████▉ | 261/292 [02:57<00:17,  1.77batch/s, auc=0.8710, loss=0.6129]\u001b[A\n",
      "Training Epoch 13/25:  90%|████████▉ | 262/292 [02:57<00:16,  1.77batch/s, auc=0.8710, loss=0.6129]\u001b[A\n",
      "Training Epoch 13/25:  90%|████████▉ | 262/292 [02:57<00:16,  1.77batch/s, auc=0.8713, loss=0.5493]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 263/292 [02:57<00:16,  1.77batch/s, auc=0.8713, loss=0.5493]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 263/292 [02:58<00:16,  1.77batch/s, auc=0.8712, loss=0.7163]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 264/292 [02:58<00:15,  1.77batch/s, auc=0.8712, loss=0.7163]\u001b[A\n",
      "Training Epoch 13/25:  90%|█████████ | 264/292 [02:58<00:15,  1.77batch/s, auc=0.8713, loss=0.5368]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 265/292 [02:58<00:15,  1.77batch/s, auc=0.8713, loss=0.5368]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 265/292 [02:59<00:15,  1.77batch/s, auc=0.8711, loss=1.0791]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 266/292 [02:59<00:14,  1.77batch/s, auc=0.8711, loss=1.0791]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████ | 266/292 [03:00<00:14,  1.77batch/s, auc=0.8709, loss=1.0504]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████▏| 267/292 [03:00<00:17,  1.39batch/s, auc=0.8709, loss=1.0504]\u001b[A\n",
      "Training Epoch 13/25:  91%|█████████▏| 267/292 [03:00<00:17,  1.39batch/s, auc=0.8710, loss=0.5961]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 268/292 [03:00<00:16,  1.49batch/s, auc=0.8710, loss=0.5961]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 268/292 [03:01<00:16,  1.49batch/s, auc=0.8713, loss=0.4821]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 269/292 [03:01<00:14,  1.56batch/s, auc=0.8713, loss=0.4821]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 269/292 [03:02<00:14,  1.56batch/s, auc=0.8716, loss=0.4460]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 270/292 [03:02<00:13,  1.62batch/s, auc=0.8716, loss=0.4460]\u001b[A\n",
      "Training Epoch 13/25:  92%|█████████▏| 270/292 [03:02<00:13,  1.62batch/s, auc=0.8718, loss=0.5724]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 271/292 [03:02<00:12,  1.66batch/s, auc=0.8718, loss=0.5724]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 271/292 [03:03<00:12,  1.66batch/s, auc=0.8716, loss=1.0439]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 272/292 [03:03<00:14,  1.34batch/s, auc=0.8716, loss=1.0439]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 272/292 [03:04<00:14,  1.34batch/s, auc=0.8718, loss=0.6663]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 273/292 [03:04<00:13,  1.45batch/s, auc=0.8718, loss=0.6663]\u001b[A\n",
      "Training Epoch 13/25:  93%|█████████▎| 273/292 [03:04<00:13,  1.45batch/s, auc=0.8719, loss=0.6906]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 274/292 [03:04<00:11,  1.53batch/s, auc=0.8719, loss=0.6906]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 274/292 [03:05<00:11,  1.53batch/s, auc=0.8723, loss=0.6211]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 275/292 [03:05<00:10,  1.59batch/s, auc=0.8723, loss=0.6211]\u001b[A\n",
      "Training Epoch 13/25:  94%|█████████▍| 275/292 [03:05<00:10,  1.59batch/s, auc=0.8725, loss=0.5328]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 276/292 [03:05<00:09,  1.64batch/s, auc=0.8725, loss=0.5328]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 276/292 [03:06<00:09,  1.64batch/s, auc=0.8725, loss=0.6822]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 277/292 [03:06<00:08,  1.68batch/s, auc=0.8725, loss=0.6822]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▍| 277/292 [03:07<00:08,  1.68batch/s, auc=0.8726, loss=0.6724]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▌| 278/292 [03:07<00:08,  1.71batch/s, auc=0.8726, loss=0.6724]\u001b[A\n",
      "Training Epoch 13/25:  95%|█████████▌| 278/292 [03:07<00:08,  1.71batch/s, auc=0.8724, loss=0.7758]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 279/292 [03:07<00:07,  1.72batch/s, auc=0.8724, loss=0.7758]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 279/292 [03:08<00:07,  1.72batch/s, auc=0.8725, loss=0.5413]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 280/292 [03:08<00:06,  1.74batch/s, auc=0.8725, loss=0.5413]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 280/292 [03:09<00:06,  1.74batch/s, auc=0.8721, loss=1.1229]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 281/292 [03:09<00:07,  1.38batch/s, auc=0.8721, loss=1.1229]\u001b[A\n",
      "Training Epoch 13/25:  96%|█████████▌| 281/292 [03:09<00:07,  1.38batch/s, auc=0.8722, loss=0.7070]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 282/292 [03:09<00:06,  1.48batch/s, auc=0.8722, loss=0.7070]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 282/292 [03:10<00:06,  1.48batch/s, auc=0.8721, loss=0.5201]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 283/292 [03:10<00:05,  1.55batch/s, auc=0.8721, loss=0.5201]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 283/292 [03:10<00:05,  1.55batch/s, auc=0.8720, loss=0.9371]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 284/292 [03:10<00:04,  1.61batch/s, auc=0.8720, loss=0.9371]\u001b[A\n",
      "Training Epoch 13/25:  97%|█████████▋| 284/292 [03:11<00:04,  1.61batch/s, auc=0.8721, loss=0.7006]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 285/292 [03:11<00:04,  1.66batch/s, auc=0.8721, loss=0.7006]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 285/292 [03:12<00:04,  1.66batch/s, auc=0.8716, loss=1.0996]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 286/292 [03:12<00:04,  1.34batch/s, auc=0.8716, loss=1.0996]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 286/292 [03:13<00:04,  1.34batch/s, auc=0.8717, loss=0.5417]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 287/292 [03:13<00:03,  1.45batch/s, auc=0.8717, loss=0.5417]\u001b[A\n",
      "Training Epoch 13/25:  98%|█████████▊| 287/292 [03:13<00:03,  1.45batch/s, auc=0.8720, loss=0.5829]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▊| 288/292 [03:13<00:02,  1.53batch/s, auc=0.8720, loss=0.5829]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▊| 288/292 [03:14<00:02,  1.53batch/s, auc=0.8721, loss=0.5429]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 289/292 [03:14<00:01,  1.59batch/s, auc=0.8721, loss=0.5429]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 289/292 [03:14<00:01,  1.59batch/s, auc=0.8722, loss=0.6050]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 290/292 [03:14<00:01,  1.64batch/s, auc=0.8722, loss=0.6050]\u001b[A\n",
      "Training Epoch 13/25:  99%|█████████▉| 290/292 [03:15<00:01,  1.64batch/s, auc=0.8722, loss=0.6787]\u001b[A\n",
      "Training Epoch 13/25: 100%|█████████▉| 291/292 [03:15<00:00,  1.68batch/s, auc=0.8722, loss=0.6787]\u001b[A\n",
      "Training Epoch 13/25: 100%|█████████▉| 291/292 [03:15<00:00,  1.68batch/s, auc=0.8722, loss=0.5966]\u001b[A\n",
      "Training Epoch 13/25: 100%|██████████| 292/292 [03:15<00:00,  1.49batch/s, auc=0.8722, loss=0.5966]\u001b[A\n",
      "Epochs:  52%|█████▏    | 13/25 [46:11<43:02, 215.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25] Train Loss: 0.7484 | Train AUROC: 0.8722 Val Loss: 0.8738 | Val AUROC: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 14/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8780, loss=0.6251]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 1/292 [00:01<07:06,  1.47s/batch, auc=0.8780, loss=0.6251]\u001b[A\n",
      "Training Epoch 14/25:   0%|          | 1/292 [00:02<07:06,  1.47s/batch, auc=0.7972, loss=1.4682]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 2/292 [00:02<05:57,  1.23s/batch, auc=0.7972, loss=1.4682]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 2/292 [00:03<05:57,  1.23s/batch, auc=0.8227, loss=0.6154]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 3/292 [00:03<04:27,  1.08batch/s, auc=0.8227, loss=0.6154]\u001b[A\n",
      "Training Epoch 14/25:   1%|          | 3/292 [00:04<04:27,  1.08batch/s, auc=0.7740, loss=1.2250]\u001b[A\n",
      "Training Epoch 14/25:   1%|▏         | 4/292 [00:04<04:42,  1.02batch/s, auc=0.7740, loss=1.2250]\u001b[A\n",
      "Training Epoch 14/25:   1%|▏         | 4/292 [00:05<04:42,  1.02batch/s, auc=0.7836, loss=0.9097]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 5/292 [00:05<04:51,  1.01s/batch, auc=0.7836, loss=0.9097]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 5/292 [00:05<04:51,  1.01s/batch, auc=0.8068, loss=0.6873]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 6/292 [00:05<04:05,  1.16batch/s, auc=0.8068, loss=0.6873]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 6/292 [00:06<04:05,  1.16batch/s, auc=0.7902, loss=1.1580]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 7/292 [00:06<04:24,  1.08batch/s, auc=0.7902, loss=1.1580]\u001b[A\n",
      "Training Epoch 14/25:   2%|▏         | 7/292 [00:07<04:24,  1.08batch/s, auc=0.8193, loss=0.4773]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 8/292 [00:07<03:50,  1.23batch/s, auc=0.8193, loss=0.4773]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 8/292 [00:07<03:50,  1.23batch/s, auc=0.8246, loss=0.7328]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 9/292 [00:07<03:26,  1.37batch/s, auc=0.8246, loss=0.7328]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 9/292 [00:08<03:26,  1.37batch/s, auc=0.8352, loss=0.4540]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 10/292 [00:08<03:10,  1.48batch/s, auc=0.8352, loss=0.4540]\u001b[A\n",
      "Training Epoch 14/25:   3%|▎         | 10/292 [00:09<03:10,  1.48batch/s, auc=0.8306, loss=0.9136]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 11/292 [00:09<02:59,  1.57batch/s, auc=0.8306, loss=0.9136]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 11/292 [00:10<02:59,  1.57batch/s, auc=0.8281, loss=0.8945]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 12/292 [00:10<03:35,  1.30batch/s, auc=0.8281, loss=0.8945]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 12/292 [00:10<03:35,  1.30batch/s, auc=0.8308, loss=0.8129]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 13/292 [00:10<03:16,  1.42batch/s, auc=0.8308, loss=0.8129]\u001b[A\n",
      "Training Epoch 14/25:   4%|▍         | 13/292 [00:11<03:16,  1.42batch/s, auc=0.8397, loss=0.6416]\u001b[A\n",
      "Training Epoch 14/25:   5%|▍         | 14/292 [00:11<03:03,  1.52batch/s, auc=0.8397, loss=0.6416]\u001b[A\n",
      "Training Epoch 14/25:   5%|▍         | 14/292 [00:11<03:03,  1.52batch/s, auc=0.8500, loss=0.4991]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 15/292 [00:11<02:53,  1.59batch/s, auc=0.8500, loss=0.4991]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 15/292 [00:12<02:53,  1.59batch/s, auc=0.8572, loss=0.6118]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 16/292 [00:12<02:47,  1.65batch/s, auc=0.8572, loss=0.6118]\u001b[A\n",
      "Training Epoch 14/25:   5%|▌         | 16/292 [00:12<02:47,  1.65batch/s, auc=0.8593, loss=0.6046]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 17/292 [00:12<02:42,  1.69batch/s, auc=0.8593, loss=0.6046]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 17/292 [00:13<02:42,  1.69batch/s, auc=0.8619, loss=0.7150]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 18/292 [00:13<02:38,  1.72batch/s, auc=0.8619, loss=0.7150]\u001b[A\n",
      "Training Epoch 14/25:   6%|▌         | 18/292 [00:14<02:38,  1.72batch/s, auc=0.8596, loss=0.9946]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 19/292 [00:14<02:36,  1.75batch/s, auc=0.8596, loss=0.9946]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 19/292 [00:14<02:36,  1.75batch/s, auc=0.8607, loss=0.6664]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 20/292 [00:14<02:34,  1.76batch/s, auc=0.8607, loss=0.6664]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 20/292 [00:15<02:34,  1.76batch/s, auc=0.8652, loss=0.5570]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 21/292 [00:15<02:32,  1.77batch/s, auc=0.8652, loss=0.5570]\u001b[A\n",
      "Training Epoch 14/25:   7%|▋         | 21/292 [00:15<02:32,  1.77batch/s, auc=0.8634, loss=0.7640]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 22/292 [00:15<02:31,  1.78batch/s, auc=0.8634, loss=0.7640]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 22/292 [00:16<02:31,  1.78batch/s, auc=0.8628, loss=0.7096]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 23/292 [00:16<02:30,  1.79batch/s, auc=0.8628, loss=0.7096]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 23/292 [00:17<02:30,  1.79batch/s, auc=0.8588, loss=1.1643]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 24/292 [00:17<03:11,  1.40batch/s, auc=0.8588, loss=1.1643]\u001b[A\n",
      "Training Epoch 14/25:   8%|▊         | 24/292 [00:18<03:11,  1.40batch/s, auc=0.8564, loss=1.0351]\u001b[A\n",
      "Training Epoch 14/25:   9%|▊         | 25/292 [00:18<03:39,  1.22batch/s, auc=0.8564, loss=1.0351]\u001b[A\n",
      "Training Epoch 14/25:   9%|▊         | 25/292 [00:18<03:39,  1.22batch/s, auc=0.8598, loss=0.5586]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 26/292 [00:18<03:17,  1.35batch/s, auc=0.8598, loss=0.5586]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 26/292 [00:19<03:17,  1.35batch/s, auc=0.8621, loss=0.4825]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 27/292 [00:19<03:01,  1.46batch/s, auc=0.8621, loss=0.4825]\u001b[A\n",
      "Training Epoch 14/25:   9%|▉         | 27/292 [00:20<03:01,  1.46batch/s, auc=0.8646, loss=0.6039]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 28/292 [00:20<02:50,  1.55batch/s, auc=0.8646, loss=0.6039]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 28/292 [00:20<02:50,  1.55batch/s, auc=0.8647, loss=0.5309]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 29/292 [00:20<02:42,  1.61batch/s, auc=0.8647, loss=0.5309]\u001b[A\n",
      "Training Epoch 14/25:  10%|▉         | 29/292 [00:21<02:42,  1.61batch/s, auc=0.8648, loss=0.5266]\u001b[A\n",
      "Training Epoch 14/25:  10%|█         | 30/292 [00:21<02:37,  1.67batch/s, auc=0.8648, loss=0.5266]\u001b[A\n",
      "Training Epoch 14/25:  10%|█         | 30/292 [00:21<02:37,  1.67batch/s, auc=0.8637, loss=0.7290]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 31/292 [00:21<02:33,  1.70batch/s, auc=0.8637, loss=0.7290]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 31/292 [00:22<02:33,  1.70batch/s, auc=0.8639, loss=0.5477]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 32/292 [00:22<02:30,  1.73batch/s, auc=0.8639, loss=0.5477]\u001b[A\n",
      "Training Epoch 14/25:  11%|█         | 32/292 [00:22<02:30,  1.73batch/s, auc=0.8646, loss=0.7698]\u001b[A\n",
      "Training Epoch 14/25:  11%|█▏        | 33/292 [00:22<02:27,  1.75batch/s, auc=0.8646, loss=0.7698]\u001b[A\n",
      "Training Epoch 14/25:  11%|█▏        | 33/292 [00:23<02:27,  1.75batch/s, auc=0.8659, loss=0.7808]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 34/292 [00:23<02:25,  1.77batch/s, auc=0.8659, loss=0.7808]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 34/292 [00:23<02:25,  1.77batch/s, auc=0.8657, loss=0.5342]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 35/292 [00:23<02:24,  1.78batch/s, auc=0.8657, loss=0.5342]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 35/292 [00:24<02:24,  1.78batch/s, auc=0.8664, loss=0.5695]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 36/292 [00:24<02:23,  1.78batch/s, auc=0.8664, loss=0.5695]\u001b[A\n",
      "Training Epoch 14/25:  12%|█▏        | 36/292 [00:25<02:23,  1.78batch/s, auc=0.8682, loss=0.5973]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 37/292 [00:25<02:22,  1.79batch/s, auc=0.8682, loss=0.5973]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 37/292 [00:25<02:22,  1.79batch/s, auc=0.8693, loss=0.5981]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 38/292 [00:25<02:21,  1.79batch/s, auc=0.8693, loss=0.5981]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 38/292 [00:26<02:21,  1.79batch/s, auc=0.8684, loss=0.7174]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 39/292 [00:26<02:20,  1.80batch/s, auc=0.8684, loss=0.7174]\u001b[A\n",
      "Training Epoch 14/25:  13%|█▎        | 39/292 [00:26<02:20,  1.80batch/s, auc=0.8684, loss=0.5634]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▎        | 40/292 [00:26<02:20,  1.80batch/s, auc=0.8684, loss=0.5634]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▎        | 40/292 [00:27<02:20,  1.80batch/s, auc=0.8697, loss=0.6242]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 41/292 [00:27<02:19,  1.80batch/s, auc=0.8697, loss=0.6242]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 41/292 [00:27<02:19,  1.80batch/s, auc=0.8679, loss=1.2652]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 42/292 [00:27<02:18,  1.80batch/s, auc=0.8679, loss=1.2652]\u001b[A\n",
      "Training Epoch 14/25:  14%|█▍        | 42/292 [00:28<02:18,  1.80batch/s, auc=0.8693, loss=0.6495]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▍        | 43/292 [00:28<02:18,  1.80batch/s, auc=0.8693, loss=0.6495]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▍        | 43/292 [00:28<02:18,  1.80batch/s, auc=0.8679, loss=0.9824]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 44/292 [00:28<02:17,  1.80batch/s, auc=0.8679, loss=0.9824]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 44/292 [00:29<02:17,  1.80batch/s, auc=0.8663, loss=1.0750]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 45/292 [00:29<02:17,  1.80batch/s, auc=0.8663, loss=1.0750]\u001b[A\n",
      "Training Epoch 14/25:  15%|█▌        | 45/292 [00:30<02:17,  1.80batch/s, auc=0.8683, loss=0.5615]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 46/292 [00:30<02:16,  1.80batch/s, auc=0.8683, loss=0.5615]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 46/292 [00:30<02:16,  1.80batch/s, auc=0.8699, loss=0.5791]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 47/292 [00:30<02:16,  1.80batch/s, auc=0.8699, loss=0.5791]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▌        | 47/292 [00:31<02:16,  1.80batch/s, auc=0.8708, loss=0.7451]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▋        | 48/292 [00:31<02:15,  1.80batch/s, auc=0.8708, loss=0.7451]\u001b[A\n",
      "Training Epoch 14/25:  16%|█▋        | 48/292 [00:32<02:15,  1.80batch/s, auc=0.8693, loss=0.9333]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 49/292 [00:32<02:52,  1.41batch/s, auc=0.8693, loss=0.9333]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 49/292 [00:32<02:52,  1.41batch/s, auc=0.8707, loss=0.6123]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 50/292 [00:32<02:40,  1.51batch/s, auc=0.8707, loss=0.6123]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 50/292 [00:33<02:40,  1.51batch/s, auc=0.8669, loss=1.2449]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 51/292 [00:33<03:09,  1.27batch/s, auc=0.8669, loss=1.2449]\u001b[A\n",
      "Training Epoch 14/25:  17%|█▋        | 51/292 [00:34<03:09,  1.27batch/s, auc=0.8678, loss=0.7158]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 52/292 [00:34<02:52,  1.39batch/s, auc=0.8678, loss=0.7158]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 52/292 [00:34<02:52,  1.39batch/s, auc=0.8688, loss=0.7486]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 53/292 [00:34<02:40,  1.49batch/s, auc=0.8688, loss=0.7486]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 53/292 [00:36<02:40,  1.49batch/s, auc=0.8692, loss=0.7100]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 54/292 [00:36<03:08,  1.26batch/s, auc=0.8692, loss=0.7100]\u001b[A\n",
      "Training Epoch 14/25:  18%|█▊        | 54/292 [00:36<03:08,  1.26batch/s, auc=0.8697, loss=0.6687]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 55/292 [00:36<02:50,  1.39batch/s, auc=0.8697, loss=0.6687]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 55/292 [00:37<02:50,  1.39batch/s, auc=0.8720, loss=0.5176]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 56/292 [00:37<02:38,  1.49batch/s, auc=0.8720, loss=0.5176]\u001b[A\n",
      "Training Epoch 14/25:  19%|█▉        | 56/292 [00:37<02:38,  1.49batch/s, auc=0.8738, loss=0.6151]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 57/292 [00:37<02:29,  1.57batch/s, auc=0.8738, loss=0.6151]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 57/292 [00:38<02:29,  1.57batch/s, auc=0.8724, loss=0.9605]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 58/292 [00:38<02:59,  1.30batch/s, auc=0.8724, loss=0.9605]\u001b[A\n",
      "Training Epoch 14/25:  20%|█▉        | 58/292 [00:39<02:59,  1.30batch/s, auc=0.8724, loss=0.5836]\u001b[A\n",
      "Training Epoch 14/25:  20%|██        | 59/292 [00:39<02:44,  1.42batch/s, auc=0.8724, loss=0.5836]\u001b[A\n",
      "Training Epoch 14/25:  20%|██        | 59/292 [00:40<02:44,  1.42batch/s, auc=0.8694, loss=1.1804]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 60/292 [00:40<03:09,  1.23batch/s, auc=0.8694, loss=1.1804]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 60/292 [00:41<03:09,  1.23batch/s, auc=0.8668, loss=1.0624]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 61/292 [00:41<03:26,  1.12batch/s, auc=0.8668, loss=1.0624]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 61/292 [00:42<03:26,  1.12batch/s, auc=0.8665, loss=0.7968]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 62/292 [00:42<03:02,  1.26batch/s, auc=0.8665, loss=0.7968]\u001b[A\n",
      "Training Epoch 14/25:  21%|██        | 62/292 [00:43<03:02,  1.26batch/s, auc=0.8649, loss=1.0769]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 63/292 [00:43<03:20,  1.14batch/s, auc=0.8649, loss=1.0769]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 63/292 [00:43<03:20,  1.14batch/s, auc=0.8665, loss=0.6138]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 64/292 [00:43<02:57,  1.28batch/s, auc=0.8665, loss=0.6138]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 64/292 [00:44<02:57,  1.28batch/s, auc=0.8668, loss=0.5908]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 65/292 [00:44<02:41,  1.40batch/s, auc=0.8668, loss=0.5908]\u001b[A\n",
      "Training Epoch 14/25:  22%|██▏       | 65/292 [00:45<02:41,  1.40batch/s, auc=0.8652, loss=0.9964]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 66/292 [00:45<03:05,  1.22batch/s, auc=0.8652, loss=0.9964]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 66/292 [00:45<03:05,  1.22batch/s, auc=0.8666, loss=0.5383]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 67/292 [00:45<02:47,  1.35batch/s, auc=0.8666, loss=0.5383]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 67/292 [00:46<02:47,  1.35batch/s, auc=0.8667, loss=0.6879]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 68/292 [00:46<02:33,  1.46batch/s, auc=0.8667, loss=0.6879]\u001b[A\n",
      "Training Epoch 14/25:  23%|██▎       | 68/292 [00:47<02:33,  1.46batch/s, auc=0.8668, loss=0.7529]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▎       | 69/292 [00:47<02:24,  1.55batch/s, auc=0.8668, loss=0.7529]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▎       | 69/292 [00:47<02:24,  1.55batch/s, auc=0.8679, loss=0.5482]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 70/292 [00:47<02:17,  1.61batch/s, auc=0.8679, loss=0.5482]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 70/292 [00:48<02:17,  1.61batch/s, auc=0.8680, loss=0.6270]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 71/292 [00:48<02:12,  1.66batch/s, auc=0.8680, loss=0.6270]\u001b[A\n",
      "Training Epoch 14/25:  24%|██▍       | 71/292 [00:48<02:12,  1.66batch/s, auc=0.8682, loss=0.6693]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▍       | 72/292 [00:48<02:09,  1.70batch/s, auc=0.8682, loss=0.6693]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▍       | 72/292 [00:49<02:09,  1.70batch/s, auc=0.8685, loss=0.7715]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 73/292 [00:49<02:06,  1.73batch/s, auc=0.8685, loss=0.7715]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 73/292 [00:49<02:06,  1.73batch/s, auc=0.8689, loss=0.5093]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 74/292 [00:49<02:04,  1.75batch/s, auc=0.8689, loss=0.5093]\u001b[A\n",
      "Training Epoch 14/25:  25%|██▌       | 74/292 [00:50<02:04,  1.75batch/s, auc=0.8695, loss=0.6961]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 75/292 [00:50<02:03,  1.76batch/s, auc=0.8695, loss=0.6961]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 75/292 [00:51<02:03,  1.76batch/s, auc=0.8691, loss=0.8792]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 76/292 [00:51<02:35,  1.39batch/s, auc=0.8691, loss=0.8792]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▌       | 76/292 [00:52<02:35,  1.39batch/s, auc=0.8692, loss=0.7477]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▋       | 77/292 [00:52<02:57,  1.21batch/s, auc=0.8692, loss=0.7477]\u001b[A\n",
      "Training Epoch 14/25:  26%|██▋       | 77/292 [00:53<02:57,  1.21batch/s, auc=0.8707, loss=0.5255]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 78/292 [00:53<02:39,  1.34batch/s, auc=0.8707, loss=0.5255]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 78/292 [00:53<02:39,  1.34batch/s, auc=0.8702, loss=0.7774]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 79/292 [00:53<02:26,  1.45batch/s, auc=0.8702, loss=0.7774]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 79/292 [00:54<02:26,  1.45batch/s, auc=0.8705, loss=0.6783]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 80/292 [00:54<02:17,  1.54batch/s, auc=0.8705, loss=0.6783]\u001b[A\n",
      "Training Epoch 14/25:  27%|██▋       | 80/292 [00:54<02:17,  1.54batch/s, auc=0.8704, loss=0.8728]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 81/292 [00:54<02:11,  1.61batch/s, auc=0.8704, loss=0.8728]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 81/292 [00:55<02:11,  1.61batch/s, auc=0.8702, loss=0.7337]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 82/292 [00:55<02:06,  1.66batch/s, auc=0.8702, loss=0.7337]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 82/292 [00:55<02:06,  1.66batch/s, auc=0.8709, loss=0.7705]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 83/292 [00:55<02:03,  1.69batch/s, auc=0.8709, loss=0.7705]\u001b[A\n",
      "Training Epoch 14/25:  28%|██▊       | 83/292 [00:56<02:03,  1.69batch/s, auc=0.8709, loss=0.6933]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 84/292 [00:56<02:00,  1.72batch/s, auc=0.8709, loss=0.6933]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 84/292 [00:57<02:00,  1.72batch/s, auc=0.8689, loss=1.2623]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 85/292 [00:57<02:30,  1.37batch/s, auc=0.8689, loss=1.2623]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 85/292 [00:58<02:30,  1.37batch/s, auc=0.8691, loss=0.8294]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 86/292 [00:58<02:19,  1.48batch/s, auc=0.8691, loss=0.8294]\u001b[A\n",
      "Training Epoch 14/25:  29%|██▉       | 86/292 [00:58<02:19,  1.48batch/s, auc=0.8704, loss=0.5617]\u001b[A\n",
      "Training Epoch 14/25:  30%|██▉       | 87/292 [00:58<02:11,  1.56batch/s, auc=0.8704, loss=0.5617]\u001b[A\n",
      "Training Epoch 14/25:  30%|██▉       | 87/292 [00:59<02:11,  1.56batch/s, auc=0.8702, loss=0.6400]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 88/292 [00:59<02:05,  1.63batch/s, auc=0.8702, loss=0.6400]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 88/292 [00:59<02:05,  1.63batch/s, auc=0.8711, loss=0.5950]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 89/292 [00:59<02:01,  1.67batch/s, auc=0.8711, loss=0.5950]\u001b[A\n",
      "Training Epoch 14/25:  30%|███       | 89/292 [01:00<02:01,  1.67batch/s, auc=0.8715, loss=0.6714]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 90/292 [01:00<01:58,  1.71batch/s, auc=0.8715, loss=0.6714]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 90/292 [01:00<01:58,  1.71batch/s, auc=0.8728, loss=0.5915]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 91/292 [01:00<01:55,  1.73batch/s, auc=0.8728, loss=0.5915]\u001b[A\n",
      "Training Epoch 14/25:  31%|███       | 91/292 [01:01<01:55,  1.73batch/s, auc=0.8734, loss=0.4588]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 92/292 [01:01<01:54,  1.75batch/s, auc=0.8734, loss=0.4588]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 92/292 [01:01<01:54,  1.75batch/s, auc=0.8737, loss=0.5804]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 93/292 [01:01<01:52,  1.76batch/s, auc=0.8737, loss=0.5804]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 93/292 [01:02<01:52,  1.76batch/s, auc=0.8747, loss=0.5027]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 94/292 [01:02<01:51,  1.77batch/s, auc=0.8747, loss=0.5027]\u001b[A\n",
      "Training Epoch 14/25:  32%|███▏      | 94/292 [01:03<01:51,  1.77batch/s, auc=0.8749, loss=0.6214]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 95/292 [01:03<01:50,  1.78batch/s, auc=0.8749, loss=0.6214]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 95/292 [01:03<01:50,  1.78batch/s, auc=0.8751, loss=0.6386]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 96/292 [01:03<01:49,  1.79batch/s, auc=0.8751, loss=0.6386]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 96/292 [01:04<01:49,  1.79batch/s, auc=0.8762, loss=0.5225]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 97/292 [01:04<01:49,  1.79batch/s, auc=0.8762, loss=0.5225]\u001b[A\n",
      "Training Epoch 14/25:  33%|███▎      | 97/292 [01:04<01:49,  1.79batch/s, auc=0.8764, loss=0.5536]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▎      | 98/292 [01:04<01:48,  1.79batch/s, auc=0.8764, loss=0.5536]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▎      | 98/292 [01:05<01:48,  1.79batch/s, auc=0.8762, loss=0.8749]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 99/292 [01:05<01:47,  1.79batch/s, auc=0.8762, loss=0.8749]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 99/292 [01:05<01:47,  1.79batch/s, auc=0.8761, loss=0.8226]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 100/292 [01:05<01:46,  1.80batch/s, auc=0.8761, loss=0.8226]\u001b[A\n",
      "Training Epoch 14/25:  34%|███▍      | 100/292 [01:06<01:46,  1.80batch/s, auc=0.8761, loss=0.7893]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 101/292 [01:06<01:46,  1.80batch/s, auc=0.8761, loss=0.7893]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 101/292 [01:06<01:46,  1.80batch/s, auc=0.8762, loss=0.7106]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 102/292 [01:06<01:45,  1.80batch/s, auc=0.8762, loss=0.7106]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▍      | 102/292 [01:07<01:45,  1.80batch/s, auc=0.8758, loss=0.9496]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▌      | 103/292 [01:07<01:45,  1.80batch/s, auc=0.8758, loss=0.9496]\u001b[A\n",
      "Training Epoch 14/25:  35%|███▌      | 103/292 [01:08<01:45,  1.80batch/s, auc=0.8757, loss=0.7940]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 104/292 [01:08<02:13,  1.40batch/s, auc=0.8757, loss=0.7940]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 104/292 [01:09<02:13,  1.40batch/s, auc=0.8763, loss=0.6254]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 105/292 [01:09<02:04,  1.50batch/s, auc=0.8763, loss=0.6254]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▌      | 105/292 [01:09<02:04,  1.50batch/s, auc=0.8773, loss=0.5402]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▋      | 106/292 [01:09<01:58,  1.58batch/s, auc=0.8773, loss=0.5402]\u001b[A\n",
      "Training Epoch 14/25:  36%|███▋      | 106/292 [01:10<01:58,  1.58batch/s, auc=0.8771, loss=0.6026]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 107/292 [01:10<01:53,  1.63batch/s, auc=0.8771, loss=0.6026]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 107/292 [01:10<01:53,  1.63batch/s, auc=0.8777, loss=0.5350]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 108/292 [01:10<01:49,  1.68batch/s, auc=0.8777, loss=0.5350]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 108/292 [01:11<01:49,  1.68batch/s, auc=0.8775, loss=0.6498]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 109/292 [01:11<01:46,  1.71batch/s, auc=0.8775, loss=0.6498]\u001b[A\n",
      "Training Epoch 14/25:  37%|███▋      | 109/292 [01:12<01:46,  1.71batch/s, auc=0.8768, loss=1.0771]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 110/292 [01:12<02:13,  1.37batch/s, auc=0.8768, loss=1.0771]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 110/292 [01:12<02:13,  1.37batch/s, auc=0.8772, loss=0.5509]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 111/292 [01:12<02:03,  1.47batch/s, auc=0.8772, loss=0.5509]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 111/292 [01:13<02:03,  1.47batch/s, auc=0.8783, loss=0.3905]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 112/292 [01:13<01:55,  1.55batch/s, auc=0.8783, loss=0.3905]\u001b[A\n",
      "Training Epoch 14/25:  38%|███▊      | 112/292 [01:14<01:55,  1.55batch/s, auc=0.8782, loss=0.6605]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▊      | 113/292 [01:14<01:50,  1.62batch/s, auc=0.8782, loss=0.6605]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▊      | 113/292 [01:14<01:50,  1.62batch/s, auc=0.8786, loss=0.7030]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 114/292 [01:14<01:46,  1.66batch/s, auc=0.8786, loss=0.7030]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 114/292 [01:15<01:46,  1.66batch/s, auc=0.8797, loss=0.4788]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 115/292 [01:15<01:44,  1.70batch/s, auc=0.8797, loss=0.4788]\u001b[A\n",
      "Training Epoch 14/25:  39%|███▉      | 115/292 [01:16<01:44,  1.70batch/s, auc=0.8795, loss=0.8380]\u001b[A\n",
      "Training Epoch 14/25:  40%|███▉      | 116/292 [01:16<02:09,  1.36batch/s, auc=0.8795, loss=0.8380]\u001b[A\n",
      "Training Epoch 14/25:  40%|███▉      | 116/292 [01:16<02:09,  1.36batch/s, auc=0.8794, loss=0.5465]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 117/292 [01:16<01:59,  1.47batch/s, auc=0.8794, loss=0.5465]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 117/292 [01:17<01:59,  1.47batch/s, auc=0.8791, loss=0.7070]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 118/292 [01:17<01:52,  1.55batch/s, auc=0.8791, loss=0.7070]\u001b[A\n",
      "Training Epoch 14/25:  40%|████      | 118/292 [01:17<01:52,  1.55batch/s, auc=0.8789, loss=0.8862]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 119/292 [01:17<01:46,  1.62batch/s, auc=0.8789, loss=0.8862]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 119/292 [01:19<01:46,  1.62batch/s, auc=0.8779, loss=1.1605]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 120/292 [01:19<02:09,  1.32batch/s, auc=0.8779, loss=1.1605]\u001b[A\n",
      "Training Epoch 14/25:  41%|████      | 120/292 [01:20<02:09,  1.32batch/s, auc=0.8780, loss=0.7303]\u001b[A\n",
      "Training Epoch 14/25:  41%|████▏     | 121/292 [01:20<02:25,  1.18batch/s, auc=0.8780, loss=0.7303]\u001b[A\n",
      "Training Epoch 14/25:  41%|████▏     | 121/292 [01:20<02:25,  1.18batch/s, auc=0.8779, loss=0.6442]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 122/292 [01:20<02:09,  1.31batch/s, auc=0.8779, loss=0.6442]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 122/292 [01:21<02:09,  1.31batch/s, auc=0.8781, loss=0.6177]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 123/292 [01:21<01:58,  1.43batch/s, auc=0.8781, loss=0.6177]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 123/292 [01:21<01:58,  1.43batch/s, auc=0.8777, loss=1.0534]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 124/292 [01:21<01:50,  1.52batch/s, auc=0.8777, loss=1.0534]\u001b[A\n",
      "Training Epoch 14/25:  42%|████▏     | 124/292 [01:22<01:50,  1.52batch/s, auc=0.8779, loss=0.5274]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 125/292 [01:22<01:45,  1.59batch/s, auc=0.8779, loss=0.5274]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 125/292 [01:22<01:45,  1.59batch/s, auc=0.8782, loss=0.6687]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 126/292 [01:22<01:40,  1.64batch/s, auc=0.8782, loss=0.6687]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 126/292 [01:23<01:40,  1.64batch/s, auc=0.8781, loss=0.6068]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 127/292 [01:23<02:03,  1.34batch/s, auc=0.8781, loss=0.6068]\u001b[A\n",
      "Training Epoch 14/25:  43%|████▎     | 127/292 [01:24<02:03,  1.34batch/s, auc=0.8776, loss=0.7494]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 128/292 [01:24<01:53,  1.44batch/s, auc=0.8776, loss=0.7494]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 128/292 [01:25<01:53,  1.44batch/s, auc=0.8781, loss=0.6483]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 129/292 [01:25<01:46,  1.53batch/s, auc=0.8781, loss=0.6483]\u001b[A\n",
      "Training Epoch 14/25:  44%|████▍     | 129/292 [01:25<01:46,  1.53batch/s, auc=0.8778, loss=0.8178]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 130/292 [01:25<01:41,  1.60batch/s, auc=0.8778, loss=0.8178]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 130/292 [01:26<01:41,  1.60batch/s, auc=0.8780, loss=0.6395]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 131/292 [01:26<01:37,  1.65batch/s, auc=0.8780, loss=0.6395]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▍     | 131/292 [01:26<01:37,  1.65batch/s, auc=0.8785, loss=0.4823]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▌     | 132/292 [01:26<01:34,  1.69batch/s, auc=0.8785, loss=0.4823]\u001b[A\n",
      "Training Epoch 14/25:  45%|████▌     | 132/292 [01:27<01:34,  1.69batch/s, auc=0.8774, loss=1.2957]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 133/292 [01:27<01:57,  1.36batch/s, auc=0.8774, loss=1.2957]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 133/292 [01:28<01:57,  1.36batch/s, auc=0.8772, loss=0.7940]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 134/292 [01:28<01:47,  1.46batch/s, auc=0.8772, loss=0.7940]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 134/292 [01:28<01:47,  1.46batch/s, auc=0.8768, loss=0.9410]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 135/292 [01:28<01:41,  1.54batch/s, auc=0.8768, loss=0.9410]\u001b[A\n",
      "Training Epoch 14/25:  46%|████▌     | 135/292 [01:29<01:41,  1.54batch/s, auc=0.8771, loss=0.7691]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 136/292 [01:29<01:36,  1.61batch/s, auc=0.8771, loss=0.7691]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 136/292 [01:30<01:36,  1.61batch/s, auc=0.8778, loss=0.4540]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 137/292 [01:30<01:33,  1.66batch/s, auc=0.8778, loss=0.4540]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 137/292 [01:30<01:33,  1.66batch/s, auc=0.8781, loss=0.7579]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 138/292 [01:30<01:30,  1.70batch/s, auc=0.8781, loss=0.7579]\u001b[A\n",
      "Training Epoch 14/25:  47%|████▋     | 138/292 [01:31<01:30,  1.70batch/s, auc=0.8782, loss=0.6681]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 139/292 [01:31<01:28,  1.72batch/s, auc=0.8782, loss=0.6681]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 139/292 [01:31<01:28,  1.72batch/s, auc=0.8784, loss=0.6137]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 140/292 [01:31<01:27,  1.74batch/s, auc=0.8784, loss=0.6137]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 140/292 [01:32<01:27,  1.74batch/s, auc=0.8788, loss=0.6237]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 141/292 [01:32<01:26,  1.76batch/s, auc=0.8788, loss=0.6237]\u001b[A\n",
      "Training Epoch 14/25:  48%|████▊     | 141/292 [01:32<01:26,  1.76batch/s, auc=0.8790, loss=0.7586]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▊     | 142/292 [01:32<01:24,  1.76batch/s, auc=0.8790, loss=0.7586]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▊     | 142/292 [01:33<01:24,  1.76batch/s, auc=0.8791, loss=0.8278]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 143/292 [01:33<01:24,  1.77batch/s, auc=0.8791, loss=0.8278]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 143/292 [01:34<01:24,  1.77batch/s, auc=0.8787, loss=0.8102]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 144/292 [01:34<01:23,  1.78batch/s, auc=0.8787, loss=0.8102]\u001b[A\n",
      "Training Epoch 14/25:  49%|████▉     | 144/292 [01:34<01:23,  1.78batch/s, auc=0.8785, loss=0.7396]\u001b[A\n",
      "Training Epoch 14/25:  50%|████▉     | 145/292 [01:34<01:22,  1.78batch/s, auc=0.8785, loss=0.7396]\u001b[A\n",
      "Training Epoch 14/25:  50%|████▉     | 145/292 [01:35<01:22,  1.78batch/s, auc=0.8786, loss=0.9046]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 146/292 [01:35<01:21,  1.78batch/s, auc=0.8786, loss=0.9046]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 146/292 [01:35<01:21,  1.78batch/s, auc=0.8788, loss=0.7023]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 147/292 [01:35<01:21,  1.78batch/s, auc=0.8788, loss=0.7023]\u001b[A\n",
      "Training Epoch 14/25:  50%|█████     | 147/292 [01:36<01:21,  1.78batch/s, auc=0.8779, loss=1.1650]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 148/292 [01:36<01:43,  1.40batch/s, auc=0.8779, loss=1.1650]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 148/292 [01:37<01:43,  1.40batch/s, auc=0.8777, loss=0.8694]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 149/292 [01:37<01:35,  1.49batch/s, auc=0.8777, loss=0.8694]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████     | 149/292 [01:37<01:35,  1.49batch/s, auc=0.8776, loss=0.5544]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████▏    | 150/292 [01:37<01:30,  1.57batch/s, auc=0.8776, loss=0.5544]\u001b[A\n",
      "Training Epoch 14/25:  51%|█████▏    | 150/292 [01:38<01:30,  1.57batch/s, auc=0.8784, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 151/292 [01:38<01:26,  1.63batch/s, auc=0.8784, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 151/292 [01:39<01:26,  1.63batch/s, auc=0.8777, loss=0.9411]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 152/292 [01:39<01:45,  1.33batch/s, auc=0.8777, loss=0.9411]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 152/292 [01:40<01:45,  1.33batch/s, auc=0.8777, loss=0.5577]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 153/292 [01:40<01:36,  1.44batch/s, auc=0.8777, loss=0.5577]\u001b[A\n",
      "Training Epoch 14/25:  52%|█████▏    | 153/292 [01:40<01:36,  1.44batch/s, auc=0.8781, loss=0.5459]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 154/292 [01:40<01:30,  1.53batch/s, auc=0.8781, loss=0.5459]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 154/292 [01:41<01:30,  1.53batch/s, auc=0.8772, loss=0.9424]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 155/292 [01:41<01:46,  1.28batch/s, auc=0.8772, loss=0.9424]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 155/292 [01:42<01:46,  1.28batch/s, auc=0.8770, loss=0.6390]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 156/292 [01:42<01:37,  1.40batch/s, auc=0.8770, loss=0.6390]\u001b[A\n",
      "Training Epoch 14/25:  53%|█████▎    | 156/292 [01:42<01:37,  1.40batch/s, auc=0.8771, loss=0.6391]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 157/292 [01:42<01:30,  1.50batch/s, auc=0.8771, loss=0.6391]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 157/292 [01:43<01:30,  1.50batch/s, auc=0.8778, loss=0.5389]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 158/292 [01:43<01:25,  1.57batch/s, auc=0.8778, loss=0.5389]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 158/292 [01:43<01:25,  1.57batch/s, auc=0.8780, loss=0.6902]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 159/292 [01:43<01:21,  1.63batch/s, auc=0.8780, loss=0.6902]\u001b[A\n",
      "Training Epoch 14/25:  54%|█████▍    | 159/292 [01:45<01:21,  1.63batch/s, auc=0.8773, loss=1.0439]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▍    | 160/292 [01:45<01:39,  1.33batch/s, auc=0.8773, loss=1.0439]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▍    | 160/292 [01:46<01:39,  1.33batch/s, auc=0.8761, loss=1.4194]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 161/292 [01:46<01:51,  1.18batch/s, auc=0.8761, loss=1.4194]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 161/292 [01:47<01:51,  1.18batch/s, auc=0.8752, loss=1.0650]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 162/292 [01:47<01:59,  1.09batch/s, auc=0.8752, loss=1.0650]\u001b[A\n",
      "Training Epoch 14/25:  55%|█████▌    | 162/292 [01:47<01:59,  1.09batch/s, auc=0.8755, loss=0.4824]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 163/292 [01:47<01:44,  1.23batch/s, auc=0.8755, loss=0.4824]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 163/292 [01:48<01:44,  1.23batch/s, auc=0.8759, loss=0.6525]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 164/292 [01:48<01:34,  1.36batch/s, auc=0.8759, loss=0.6525]\u001b[A\n",
      "Training Epoch 14/25:  56%|█████▌    | 164/292 [01:48<01:34,  1.36batch/s, auc=0.8762, loss=0.6105]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 165/292 [01:48<01:26,  1.47batch/s, auc=0.8762, loss=0.6105]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 165/292 [01:49<01:26,  1.47batch/s, auc=0.8764, loss=0.8627]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 166/292 [01:49<01:21,  1.55batch/s, auc=0.8764, loss=0.8627]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 166/292 [01:50<01:21,  1.55batch/s, auc=0.8765, loss=0.6736]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 167/292 [01:50<01:17,  1.61batch/s, auc=0.8765, loss=0.6736]\u001b[A\n",
      "Training Epoch 14/25:  57%|█████▋    | 167/292 [01:50<01:17,  1.61batch/s, auc=0.8765, loss=0.7347]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 168/292 [01:50<01:14,  1.66batch/s, auc=0.8765, loss=0.7347]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 168/292 [01:51<01:14,  1.66batch/s, auc=0.8768, loss=0.5941]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 169/292 [01:51<01:12,  1.69batch/s, auc=0.8768, loss=0.5941]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 169/292 [01:51<01:12,  1.69batch/s, auc=0.8771, loss=0.5286]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 170/292 [01:51<01:10,  1.72batch/s, auc=0.8771, loss=0.5286]\u001b[A\n",
      "Training Epoch 14/25:  58%|█████▊    | 170/292 [01:52<01:10,  1.72batch/s, auc=0.8771, loss=0.6669]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▊    | 171/292 [01:52<01:09,  1.74batch/s, auc=0.8771, loss=0.6669]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▊    | 171/292 [01:52<01:09,  1.74batch/s, auc=0.8774, loss=0.5338]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 172/292 [01:52<01:08,  1.75batch/s, auc=0.8774, loss=0.5338]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 172/292 [01:53<01:08,  1.75batch/s, auc=0.8769, loss=1.0489]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 173/292 [01:53<01:07,  1.76batch/s, auc=0.8769, loss=1.0489]\u001b[A\n",
      "Training Epoch 14/25:  59%|█████▉    | 173/292 [01:53<01:07,  1.76batch/s, auc=0.8773, loss=0.6292]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 174/292 [01:53<01:06,  1.77batch/s, auc=0.8773, loss=0.6292]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 174/292 [01:54<01:06,  1.77batch/s, auc=0.8774, loss=0.7024]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 175/292 [01:54<01:06,  1.77batch/s, auc=0.8774, loss=0.7024]\u001b[A\n",
      "Training Epoch 14/25:  60%|█████▉    | 175/292 [01:55<01:06,  1.77batch/s, auc=0.8777, loss=0.6009]\u001b[A\n",
      "Training Epoch 14/25:  60%|██████    | 176/292 [01:55<01:05,  1.77batch/s, auc=0.8777, loss=0.6009]\u001b[A\n",
      "Training Epoch 14/25:  60%|██████    | 176/292 [01:55<01:05,  1.77batch/s, auc=0.8774, loss=0.9930]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 177/292 [01:55<01:04,  1.77batch/s, auc=0.8774, loss=0.9930]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 177/292 [01:56<01:04,  1.77batch/s, auc=0.8771, loss=0.9606]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 178/292 [01:56<01:04,  1.78batch/s, auc=0.8771, loss=0.9606]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████    | 178/292 [01:57<01:04,  1.78batch/s, auc=0.8768, loss=1.0226]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████▏   | 179/292 [01:57<01:20,  1.40batch/s, auc=0.8768, loss=1.0226]\u001b[A\n",
      "Training Epoch 14/25:  61%|██████▏   | 179/292 [01:58<01:20,  1.40batch/s, auc=0.8767, loss=0.7065]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 180/292 [01:58<01:32,  1.21batch/s, auc=0.8767, loss=0.7065]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 180/292 [01:58<01:32,  1.21batch/s, auc=0.8769, loss=0.5800]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 181/292 [01:58<01:22,  1.34batch/s, auc=0.8769, loss=0.5800]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 181/292 [01:59<01:22,  1.34batch/s, auc=0.8760, loss=1.5990]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 182/292 [01:59<01:32,  1.18batch/s, auc=0.8760, loss=1.5990]\u001b[A\n",
      "Training Epoch 14/25:  62%|██████▏   | 182/292 [02:00<01:32,  1.18batch/s, auc=0.8760, loss=0.6910]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 183/292 [02:00<01:22,  1.32batch/s, auc=0.8760, loss=0.6910]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 183/292 [02:01<01:22,  1.32batch/s, auc=0.8755, loss=0.9378]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 184/292 [02:01<01:32,  1.17batch/s, auc=0.8755, loss=0.9378]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 184/292 [02:02<01:32,  1.17batch/s, auc=0.8753, loss=0.7942]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 185/292 [02:02<01:22,  1.30batch/s, auc=0.8753, loss=0.7942]\u001b[A\n",
      "Training Epoch 14/25:  63%|██████▎   | 185/292 [02:02<01:22,  1.30batch/s, auc=0.8755, loss=0.6373]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▎   | 186/292 [02:02<01:14,  1.42batch/s, auc=0.8755, loss=0.6373]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▎   | 186/292 [02:03<01:14,  1.42batch/s, auc=0.8754, loss=0.6843]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 187/292 [02:03<01:09,  1.51batch/s, auc=0.8754, loss=0.6843]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 187/292 [02:03<01:09,  1.51batch/s, auc=0.8754, loss=0.8590]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 188/292 [02:03<01:05,  1.58batch/s, auc=0.8754, loss=0.8590]\u001b[A\n",
      "Training Epoch 14/25:  64%|██████▍   | 188/292 [02:04<01:05,  1.58batch/s, auc=0.8755, loss=0.5693]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▍   | 189/292 [02:04<01:02,  1.64batch/s, auc=0.8755, loss=0.5693]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▍   | 189/292 [02:04<01:02,  1.64batch/s, auc=0.8757, loss=0.7747]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 190/292 [02:04<01:00,  1.68batch/s, auc=0.8757, loss=0.7747]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 190/292 [02:05<01:00,  1.68batch/s, auc=0.8759, loss=0.6211]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 191/292 [02:05<00:59,  1.71batch/s, auc=0.8759, loss=0.6211]\u001b[A\n",
      "Training Epoch 14/25:  65%|██████▌   | 191/292 [02:06<00:59,  1.71batch/s, auc=0.8761, loss=0.6855]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 192/292 [02:06<00:57,  1.73batch/s, auc=0.8761, loss=0.6855]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 192/292 [02:07<00:57,  1.73batch/s, auc=0.8754, loss=0.9966]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 193/292 [02:07<01:12,  1.37batch/s, auc=0.8754, loss=0.9966]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▌   | 193/292 [02:07<01:12,  1.37batch/s, auc=0.8756, loss=0.5508]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▋   | 194/292 [02:07<01:06,  1.47batch/s, auc=0.8756, loss=0.5508]\u001b[A\n",
      "Training Epoch 14/25:  66%|██████▋   | 194/292 [02:08<01:06,  1.47batch/s, auc=0.8753, loss=0.8875]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 195/292 [02:08<01:02,  1.55batch/s, auc=0.8753, loss=0.8875]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 195/292 [02:08<01:02,  1.55batch/s, auc=0.8751, loss=0.7532]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 196/292 [02:08<00:59,  1.62batch/s, auc=0.8751, loss=0.7532]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 196/292 [02:09<00:59,  1.62batch/s, auc=0.8748, loss=1.0086]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 197/292 [02:09<01:11,  1.32batch/s, auc=0.8748, loss=1.0086]\u001b[A\n",
      "Training Epoch 14/25:  67%|██████▋   | 197/292 [02:11<01:11,  1.32batch/s, auc=0.8743, loss=0.9257]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 198/292 [02:11<01:20,  1.17batch/s, auc=0.8743, loss=0.9257]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 198/292 [02:11<01:20,  1.17batch/s, auc=0.8745, loss=0.5500]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 199/292 [02:11<01:11,  1.31batch/s, auc=0.8745, loss=0.5500]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 199/292 [02:12<01:11,  1.31batch/s, auc=0.8747, loss=0.7176]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 200/292 [02:12<01:04,  1.42batch/s, auc=0.8747, loss=0.7176]\u001b[A\n",
      "Training Epoch 14/25:  68%|██████▊   | 200/292 [02:12<01:04,  1.42batch/s, auc=0.8747, loss=0.7088]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 201/292 [02:12<01:00,  1.51batch/s, auc=0.8747, loss=0.7088]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 201/292 [02:13<01:00,  1.51batch/s, auc=0.8750, loss=0.5678]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 202/292 [02:13<00:56,  1.58batch/s, auc=0.8750, loss=0.5678]\u001b[A\n",
      "Training Epoch 14/25:  69%|██████▉   | 202/292 [02:13<00:56,  1.58batch/s, auc=0.8754, loss=0.4693]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 203/292 [02:13<00:54,  1.64batch/s, auc=0.8754, loss=0.4693]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 203/292 [02:14<00:54,  1.64batch/s, auc=0.8755, loss=0.6713]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 204/292 [02:14<00:52,  1.68batch/s, auc=0.8755, loss=0.6713]\u001b[A\n",
      "Training Epoch 14/25:  70%|██████▉   | 204/292 [02:14<00:52,  1.68batch/s, auc=0.8755, loss=0.6206]\u001b[A\n",
      "Training Epoch 14/25:  70%|███████   | 205/292 [02:14<00:50,  1.71batch/s, auc=0.8755, loss=0.6206]\u001b[A\n",
      "Training Epoch 14/25:  70%|███████   | 205/292 [02:15<00:50,  1.71batch/s, auc=0.8754, loss=0.6258]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 206/292 [02:15<00:49,  1.73batch/s, auc=0.8754, loss=0.6258]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 206/292 [02:16<00:49,  1.73batch/s, auc=0.8752, loss=0.8902]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 207/292 [02:16<00:48,  1.75batch/s, auc=0.8752, loss=0.8902]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 207/292 [02:16<00:48,  1.75batch/s, auc=0.8757, loss=0.5824]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 208/292 [02:16<00:47,  1.75batch/s, auc=0.8757, loss=0.5824]\u001b[A\n",
      "Training Epoch 14/25:  71%|███████   | 208/292 [02:17<00:47,  1.75batch/s, auc=0.8760, loss=0.5383]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 209/292 [02:17<00:47,  1.76batch/s, auc=0.8760, loss=0.5383]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 209/292 [02:18<00:47,  1.76batch/s, auc=0.8759, loss=0.8246]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 210/292 [02:18<00:59,  1.39batch/s, auc=0.8759, loss=0.8246]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 210/292 [02:18<00:59,  1.39batch/s, auc=0.8762, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 211/292 [02:18<00:54,  1.49batch/s, auc=0.8762, loss=0.5095]\u001b[A\n",
      "Training Epoch 14/25:  72%|███████▏  | 211/292 [02:19<00:54,  1.49batch/s, auc=0.8764, loss=0.4298]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 212/292 [02:19<00:51,  1.57batch/s, auc=0.8764, loss=0.4298]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 212/292 [02:19<00:51,  1.57batch/s, auc=0.8761, loss=1.0968]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 213/292 [02:19<00:48,  1.62batch/s, auc=0.8761, loss=1.0968]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 213/292 [02:21<00:48,  1.62batch/s, auc=0.8751, loss=1.4212]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 214/292 [02:21<00:58,  1.33batch/s, auc=0.8751, loss=1.4212]\u001b[A\n",
      "Training Epoch 14/25:  73%|███████▎  | 214/292 [02:21<00:58,  1.33batch/s, auc=0.8754, loss=0.5442]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▎  | 215/292 [02:21<00:53,  1.44batch/s, auc=0.8754, loss=0.5442]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▎  | 215/292 [02:22<00:53,  1.44batch/s, auc=0.8754, loss=0.9355]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 216/292 [02:22<00:49,  1.52batch/s, auc=0.8754, loss=0.9355]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 216/292 [02:22<00:49,  1.52batch/s, auc=0.8750, loss=0.8600]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 217/292 [02:22<00:47,  1.59batch/s, auc=0.8750, loss=0.8600]\u001b[A\n",
      "Training Epoch 14/25:  74%|███████▍  | 217/292 [02:23<00:47,  1.59batch/s, auc=0.8747, loss=0.9157]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▍  | 218/292 [02:23<00:44,  1.65batch/s, auc=0.8747, loss=0.9157]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▍  | 218/292 [02:23<00:44,  1.65batch/s, auc=0.8749, loss=0.4975]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 219/292 [02:23<00:43,  1.69batch/s, auc=0.8749, loss=0.4975]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 219/292 [02:24<00:43,  1.69batch/s, auc=0.8738, loss=1.5243]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 220/292 [02:24<00:53,  1.35batch/s, auc=0.8738, loss=1.5243]\u001b[A\n",
      "Training Epoch 14/25:  75%|███████▌  | 220/292 [02:25<00:53,  1.35batch/s, auc=0.8737, loss=0.7839]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 221/292 [02:25<00:48,  1.45batch/s, auc=0.8737, loss=0.7839]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 221/292 [02:26<00:48,  1.45batch/s, auc=0.8737, loss=0.8773]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 222/292 [02:26<00:45,  1.54batch/s, auc=0.8737, loss=0.8773]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▌  | 222/292 [02:26<00:45,  1.54batch/s, auc=0.8737, loss=0.6708]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▋  | 223/292 [02:26<00:43,  1.60batch/s, auc=0.8737, loss=0.6708]\u001b[A\n",
      "Training Epoch 14/25:  76%|███████▋  | 223/292 [02:27<00:43,  1.60batch/s, auc=0.8738, loss=0.6654]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 224/292 [02:27<00:41,  1.65batch/s, auc=0.8738, loss=0.6654]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 224/292 [02:28<00:41,  1.65batch/s, auc=0.8736, loss=0.8950]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 225/292 [02:28<00:50,  1.34batch/s, auc=0.8736, loss=0.8950]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 225/292 [02:28<00:50,  1.34batch/s, auc=0.8738, loss=0.6835]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 226/292 [02:28<00:45,  1.45batch/s, auc=0.8738, loss=0.6835]\u001b[A\n",
      "Training Epoch 14/25:  77%|███████▋  | 226/292 [02:29<00:45,  1.45batch/s, auc=0.8741, loss=0.6692]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 227/292 [02:29<00:42,  1.53batch/s, auc=0.8741, loss=0.6692]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 227/292 [02:29<00:42,  1.53batch/s, auc=0.8743, loss=0.5317]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 228/292 [02:29<00:40,  1.60batch/s, auc=0.8743, loss=0.5317]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 228/292 [02:30<00:40,  1.60batch/s, auc=0.8744, loss=0.6926]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 229/292 [02:30<00:38,  1.65batch/s, auc=0.8744, loss=0.6926]\u001b[A\n",
      "Training Epoch 14/25:  78%|███████▊  | 229/292 [02:31<00:38,  1.65batch/s, auc=0.8745, loss=0.5043]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 230/292 [02:31<00:36,  1.68batch/s, auc=0.8745, loss=0.5043]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 230/292 [02:31<00:36,  1.68batch/s, auc=0.8749, loss=0.4837]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 231/292 [02:31<00:35,  1.71batch/s, auc=0.8749, loss=0.4837]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 231/292 [02:32<00:35,  1.71batch/s, auc=0.8753, loss=0.6058]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 232/292 [02:32<00:34,  1.73batch/s, auc=0.8753, loss=0.6058]\u001b[A\n",
      "Training Epoch 14/25:  79%|███████▉  | 232/292 [02:32<00:34,  1.73batch/s, auc=0.8755, loss=0.5713]\u001b[A\n",
      "Training Epoch 14/25:  80%|███████▉  | 233/292 [02:32<00:33,  1.75batch/s, auc=0.8755, loss=0.5713]\u001b[A\n",
      "Training Epoch 14/25:  80%|███████▉  | 233/292 [02:33<00:33,  1.75batch/s, auc=0.8756, loss=0.7104]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 234/292 [02:33<00:33,  1.75batch/s, auc=0.8756, loss=0.7104]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 234/292 [02:33<00:33,  1.75batch/s, auc=0.8757, loss=0.5924]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 235/292 [02:33<00:32,  1.76batch/s, auc=0.8757, loss=0.5924]\u001b[A\n",
      "Training Epoch 14/25:  80%|████████  | 235/292 [02:34<00:32,  1.76batch/s, auc=0.8756, loss=0.6399]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 236/292 [02:34<00:31,  1.77batch/s, auc=0.8756, loss=0.6399]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 236/292 [02:35<00:31,  1.77batch/s, auc=0.8757, loss=0.6718]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 237/292 [02:35<00:31,  1.77batch/s, auc=0.8757, loss=0.6718]\u001b[A\n",
      "Training Epoch 14/25:  81%|████████  | 237/292 [02:35<00:31,  1.77batch/s, auc=0.8759, loss=0.6671]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 238/292 [02:35<00:30,  1.77batch/s, auc=0.8759, loss=0.6671]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 238/292 [02:36<00:30,  1.77batch/s, auc=0.8759, loss=0.7344]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 239/292 [02:36<00:29,  1.77batch/s, auc=0.8759, loss=0.7344]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 239/292 [02:37<00:29,  1.77batch/s, auc=0.8754, loss=1.1366]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 240/292 [02:37<00:37,  1.39batch/s, auc=0.8754, loss=1.1366]\u001b[A\n",
      "Training Epoch 14/25:  82%|████████▏ | 240/292 [02:37<00:37,  1.39batch/s, auc=0.8758, loss=0.4561]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 241/292 [02:37<00:34,  1.49batch/s, auc=0.8758, loss=0.4561]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 241/292 [02:38<00:34,  1.49batch/s, auc=0.8760, loss=0.6878]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 242/292 [02:38<00:32,  1.56batch/s, auc=0.8760, loss=0.6878]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 242/292 [02:38<00:32,  1.56batch/s, auc=0.8763, loss=0.5630]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 243/292 [02:38<00:30,  1.62batch/s, auc=0.8763, loss=0.5630]\u001b[A\n",
      "Training Epoch 14/25:  83%|████████▎ | 243/292 [02:39<00:30,  1.62batch/s, auc=0.8761, loss=0.9845]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▎ | 244/292 [02:39<00:28,  1.66batch/s, auc=0.8761, loss=0.9845]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▎ | 244/292 [02:40<00:28,  1.66batch/s, auc=0.8755, loss=1.1304]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 245/292 [02:40<00:34,  1.34batch/s, auc=0.8755, loss=1.1304]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 245/292 [02:41<00:34,  1.34batch/s, auc=0.8756, loss=0.6500]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 246/292 [02:41<00:31,  1.45batch/s, auc=0.8756, loss=0.6500]\u001b[A\n",
      "Training Epoch 14/25:  84%|████████▍ | 246/292 [02:41<00:31,  1.45batch/s, auc=0.8756, loss=0.7607]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 247/292 [02:41<00:29,  1.53batch/s, auc=0.8756, loss=0.7607]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 247/292 [02:42<00:29,  1.53batch/s, auc=0.8757, loss=0.7033]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 248/292 [02:42<00:27,  1.60batch/s, auc=0.8757, loss=0.7033]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▍ | 248/292 [02:43<00:27,  1.60batch/s, auc=0.8751, loss=1.0896]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▌ | 249/292 [02:43<00:32,  1.31batch/s, auc=0.8751, loss=1.0896]\u001b[A\n",
      "Training Epoch 14/25:  85%|████████▌ | 249/292 [02:44<00:32,  1.31batch/s, auc=0.8748, loss=0.7877]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 250/292 [02:44<00:36,  1.17batch/s, auc=0.8748, loss=0.7877]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 250/292 [02:45<00:36,  1.17batch/s, auc=0.8745, loss=0.9992]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 251/292 [02:45<00:37,  1.08batch/s, auc=0.8745, loss=0.9992]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▌ | 251/292 [02:46<00:37,  1.08batch/s, auc=0.8744, loss=0.8384]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▋ | 252/292 [02:46<00:38,  1.03batch/s, auc=0.8744, loss=0.8384]\u001b[A\n",
      "Training Epoch 14/25:  86%|████████▋ | 252/292 [02:47<00:38,  1.03batch/s, auc=0.8746, loss=0.6407]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 253/292 [02:47<00:33,  1.18batch/s, auc=0.8746, loss=0.6407]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 253/292 [02:47<00:33,  1.18batch/s, auc=0.8746, loss=0.6325]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 254/292 [02:47<00:29,  1.31batch/s, auc=0.8746, loss=0.6325]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 254/292 [02:48<00:29,  1.31batch/s, auc=0.8750, loss=0.5552]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 255/292 [02:48<00:26,  1.42batch/s, auc=0.8750, loss=0.5552]\u001b[A\n",
      "Training Epoch 14/25:  87%|████████▋ | 255/292 [02:48<00:26,  1.42batch/s, auc=0.8749, loss=0.6651]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 256/292 [02:48<00:23,  1.51batch/s, auc=0.8749, loss=0.6651]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 256/292 [02:49<00:23,  1.51batch/s, auc=0.8744, loss=1.0388]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 257/292 [02:49<00:27,  1.27batch/s, auc=0.8744, loss=1.0388]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 257/292 [02:50<00:27,  1.27batch/s, auc=0.8745, loss=0.6200]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 258/292 [02:50<00:24,  1.39batch/s, auc=0.8745, loss=0.6200]\u001b[A\n",
      "Training Epoch 14/25:  88%|████████▊ | 258/292 [02:51<00:24,  1.39batch/s, auc=0.8748, loss=0.5775]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▊ | 259/292 [02:51<00:22,  1.48batch/s, auc=0.8748, loss=0.5775]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▊ | 259/292 [02:51<00:22,  1.48batch/s, auc=0.8748, loss=0.7044]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 260/292 [02:51<00:20,  1.56batch/s, auc=0.8748, loss=0.7044]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 260/292 [02:52<00:20,  1.56batch/s, auc=0.8751, loss=0.5248]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 261/292 [02:52<00:19,  1.62batch/s, auc=0.8751, loss=0.5248]\u001b[A\n",
      "Training Epoch 14/25:  89%|████████▉ | 261/292 [02:53<00:19,  1.62batch/s, auc=0.8749, loss=0.8027]\u001b[A\n",
      "Training Epoch 14/25:  90%|████████▉ | 262/292 [02:53<00:22,  1.32batch/s, auc=0.8749, loss=0.8027]\u001b[A\n",
      "Training Epoch 14/25:  90%|████████▉ | 262/292 [02:53<00:22,  1.32batch/s, auc=0.8751, loss=0.7103]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 263/292 [02:53<00:20,  1.43batch/s, auc=0.8751, loss=0.7103]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 263/292 [02:54<00:20,  1.43batch/s, auc=0.8752, loss=0.6749]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 264/292 [02:54<00:18,  1.52batch/s, auc=0.8752, loss=0.6749]\u001b[A\n",
      "Training Epoch 14/25:  90%|█████████ | 264/292 [02:54<00:18,  1.52batch/s, auc=0.8754, loss=0.6717]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 265/292 [02:54<00:17,  1.59batch/s, auc=0.8754, loss=0.6717]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 265/292 [02:56<00:17,  1.59batch/s, auc=0.8752, loss=0.9521]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 266/292 [02:56<00:19,  1.31batch/s, auc=0.8752, loss=0.9521]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████ | 266/292 [02:56<00:19,  1.31batch/s, auc=0.8754, loss=0.6155]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████▏| 267/292 [02:56<00:17,  1.42batch/s, auc=0.8754, loss=0.6155]\u001b[A\n",
      "Training Epoch 14/25:  91%|█████████▏| 267/292 [02:57<00:17,  1.42batch/s, auc=0.8753, loss=0.8672]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 268/292 [02:57<00:15,  1.51batch/s, auc=0.8753, loss=0.8672]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 268/292 [02:57<00:15,  1.51batch/s, auc=0.8752, loss=0.7652]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 269/292 [02:57<00:14,  1.58batch/s, auc=0.8752, loss=0.7652]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 269/292 [02:58<00:14,  1.58batch/s, auc=0.8752, loss=0.8016]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 270/292 [02:58<00:13,  1.63batch/s, auc=0.8752, loss=0.8016]\u001b[A\n",
      "Training Epoch 14/25:  92%|█████████▏| 270/292 [02:58<00:13,  1.63batch/s, auc=0.8758, loss=0.4728]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 271/292 [02:58<00:12,  1.67batch/s, auc=0.8758, loss=0.4728]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 271/292 [02:59<00:12,  1.67batch/s, auc=0.8754, loss=0.9374]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 272/292 [02:59<00:14,  1.35batch/s, auc=0.8754, loss=0.9374]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 272/292 [03:00<00:14,  1.35batch/s, auc=0.8756, loss=0.5375]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 273/292 [03:00<00:13,  1.45batch/s, auc=0.8756, loss=0.5375]\u001b[A\n",
      "Training Epoch 14/25:  93%|█████████▎| 273/292 [03:01<00:13,  1.45batch/s, auc=0.8757, loss=0.8303]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 274/292 [03:01<00:11,  1.54batch/s, auc=0.8757, loss=0.8303]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 274/292 [03:02<00:11,  1.54batch/s, auc=0.8755, loss=0.6763]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 275/292 [03:02<00:13,  1.28batch/s, auc=0.8755, loss=0.6763]\u001b[A\n",
      "Training Epoch 14/25:  94%|█████████▍| 275/292 [03:03<00:13,  1.28batch/s, auc=0.8750, loss=1.3793]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 276/292 [03:03<00:13,  1.15batch/s, auc=0.8750, loss=1.3793]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 276/292 [03:04<00:13,  1.15batch/s, auc=0.8748, loss=0.9126]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 277/292 [03:04<00:14,  1.07batch/s, auc=0.8748, loss=0.9126]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▍| 277/292 [03:05<00:14,  1.07batch/s, auc=0.8750, loss=0.6402]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▌| 278/292 [03:05<00:13,  1.02batch/s, auc=0.8750, loss=0.6402]\u001b[A\n",
      "Training Epoch 14/25:  95%|█████████▌| 278/292 [03:05<00:13,  1.02batch/s, auc=0.8752, loss=0.5357]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 279/292 [03:05<00:11,  1.17batch/s, auc=0.8752, loss=0.5357]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 279/292 [03:06<00:11,  1.17batch/s, auc=0.8755, loss=0.6689]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 280/292 [03:06<00:09,  1.30batch/s, auc=0.8755, loss=0.6689]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 280/292 [03:07<00:09,  1.30batch/s, auc=0.8750, loss=0.9850]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 281/292 [03:07<00:09,  1.16batch/s, auc=0.8750, loss=0.9850]\u001b[A\n",
      "Training Epoch 14/25:  96%|█████████▌| 281/292 [03:08<00:09,  1.16batch/s, auc=0.8751, loss=0.5665]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 282/292 [03:08<00:07,  1.29batch/s, auc=0.8751, loss=0.5665]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 282/292 [03:08<00:07,  1.29batch/s, auc=0.8753, loss=0.6150]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 283/292 [03:08<00:06,  1.41batch/s, auc=0.8753, loss=0.6150]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 283/292 [03:09<00:06,  1.41batch/s, auc=0.8756, loss=0.5673]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 284/292 [03:09<00:05,  1.50batch/s, auc=0.8756, loss=0.5673]\u001b[A\n",
      "Training Epoch 14/25:  97%|█████████▋| 284/292 [03:10<00:05,  1.50batch/s, auc=0.8754, loss=1.0060]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 285/292 [03:10<00:05,  1.26batch/s, auc=0.8754, loss=1.0060]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 285/292 [03:11<00:05,  1.26batch/s, auc=0.8751, loss=0.9084]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 286/292 [03:11<00:05,  1.14batch/s, auc=0.8751, loss=0.9084]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 286/292 [03:11<00:05,  1.14batch/s, auc=0.8752, loss=0.6308]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 287/292 [03:11<00:03,  1.28batch/s, auc=0.8752, loss=0.6308]\u001b[A\n",
      "Training Epoch 14/25:  98%|█████████▊| 287/292 [03:12<00:03,  1.28batch/s, auc=0.8753, loss=0.7156]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▊| 288/292 [03:12<00:02,  1.39batch/s, auc=0.8753, loss=0.7156]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▊| 288/292 [03:13<00:02,  1.39batch/s, auc=0.8755, loss=0.4943]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 289/292 [03:13<00:02,  1.49batch/s, auc=0.8755, loss=0.4943]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 289/292 [03:14<00:02,  1.49batch/s, auc=0.8753, loss=0.8466]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 290/292 [03:14<00:01,  1.26batch/s, auc=0.8753, loss=0.8466]\u001b[A\n",
      "Training Epoch 14/25:  99%|█████████▉| 290/292 [03:15<00:01,  1.26batch/s, auc=0.8751, loss=0.8675]\u001b[A\n",
      "Training Epoch 14/25: 100%|█████████▉| 291/292 [03:15<00:00,  1.14batch/s, auc=0.8751, loss=0.8675]\u001b[A\n",
      "Training Epoch 14/25: 100%|█████████▉| 291/292 [03:15<00:00,  1.14batch/s, auc=0.8752, loss=0.5362]\u001b[A\n",
      "Training Epoch 14/25: 100%|██████████| 292/292 [03:15<00:00,  1.49batch/s, auc=0.8752, loss=0.5362]\u001b[A\n",
      "Epochs:  56%|█████▌    | 14/25 [49:46<39:26, 215.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25] Train Loss: 0.7403 | Train AUROC: 0.8752 Val Loss: 0.7846 | Val AUROC: 0.8651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 15/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9424, loss=0.5013]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 1/292 [00:01<07:16,  1.50s/batch, auc=0.9424, loss=0.5013]\u001b[A\n",
      "Training Epoch 15/25:   0%|          | 1/292 [00:02<07:16,  1.50s/batch, auc=0.8077, loss=1.2990]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 2/292 [00:02<06:01,  1.25s/batch, auc=0.8077, loss=1.2990]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 2/292 [00:03<06:01,  1.25s/batch, auc=0.8203, loss=0.7508]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 3/292 [00:03<05:37,  1.17s/batch, auc=0.8203, loss=0.7508]\u001b[A\n",
      "Training Epoch 15/25:   1%|          | 3/292 [00:04<05:37,  1.17s/batch, auc=0.7870, loss=1.1652]\u001b[A\n",
      "Training Epoch 15/25:   1%|▏         | 4/292 [00:04<05:25,  1.13s/batch, auc=0.7870, loss=1.1652]\u001b[A\n",
      "Training Epoch 15/25:   1%|▏         | 4/292 [00:05<05:25,  1.13s/batch, auc=0.7655, loss=1.4737]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 5/292 [00:05<05:18,  1.11s/batch, auc=0.7655, loss=1.4737]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 5/292 [00:06<05:18,  1.11s/batch, auc=0.7653, loss=1.0309]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 6/292 [00:06<05:13,  1.10s/batch, auc=0.7653, loss=1.0309]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 6/292 [00:07<05:13,  1.10s/batch, auc=0.7983, loss=0.6834]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 7/292 [00:07<04:21,  1.09batch/s, auc=0.7983, loss=0.6834]\u001b[A\n",
      "Training Epoch 15/25:   2%|▏         | 7/292 [00:07<04:21,  1.09batch/s, auc=0.8171, loss=0.5661]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 8/292 [00:07<03:47,  1.25batch/s, auc=0.8171, loss=0.5661]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 8/292 [00:08<03:47,  1.25batch/s, auc=0.8210, loss=0.5379]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 9/292 [00:08<03:25,  1.38batch/s, auc=0.8210, loss=0.5379]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 9/292 [00:09<03:25,  1.38batch/s, auc=0.8229, loss=0.7565]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 10/292 [00:09<03:09,  1.49batch/s, auc=0.8229, loss=0.7565]\u001b[A\n",
      "Training Epoch 15/25:   3%|▎         | 10/292 [00:09<03:09,  1.49batch/s, auc=0.8311, loss=0.5564]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 11/292 [00:09<02:59,  1.56batch/s, auc=0.8311, loss=0.5564]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 11/292 [00:10<02:59,  1.56batch/s, auc=0.8391, loss=0.6672]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 12/292 [00:10<02:52,  1.62batch/s, auc=0.8391, loss=0.6672]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 12/292 [00:10<02:52,  1.62batch/s, auc=0.8457, loss=0.7188]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 13/292 [00:10<02:46,  1.67batch/s, auc=0.8457, loss=0.7188]\u001b[A\n",
      "Training Epoch 15/25:   4%|▍         | 13/292 [00:11<02:46,  1.67batch/s, auc=0.8457, loss=0.9754]\u001b[A\n",
      "Training Epoch 15/25:   5%|▍         | 14/292 [00:11<02:42,  1.71batch/s, auc=0.8457, loss=0.9754]\u001b[A\n",
      "Training Epoch 15/25:   5%|▍         | 14/292 [00:12<02:42,  1.71batch/s, auc=0.8351, loss=1.3766]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 15/292 [00:12<03:22,  1.37batch/s, auc=0.8351, loss=1.3766]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 15/292 [00:12<03:22,  1.37batch/s, auc=0.8411, loss=0.5345]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 16/292 [00:12<03:06,  1.48batch/s, auc=0.8411, loss=0.5345]\u001b[A\n",
      "Training Epoch 15/25:   5%|▌         | 16/292 [00:14<03:06,  1.48batch/s, auc=0.8367, loss=1.0779]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 17/292 [00:14<03:38,  1.26batch/s, auc=0.8367, loss=1.0779]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 17/292 [00:14<03:38,  1.26batch/s, auc=0.8402, loss=0.7606]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 18/292 [00:14<03:18,  1.38batch/s, auc=0.8402, loss=0.7606]\u001b[A\n",
      "Training Epoch 15/25:   6%|▌         | 18/292 [00:15<03:18,  1.38batch/s, auc=0.8511, loss=0.4953]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 19/292 [00:15<03:03,  1.48batch/s, auc=0.8511, loss=0.4953]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 19/292 [00:16<03:03,  1.48batch/s, auc=0.8484, loss=1.1656]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 20/292 [00:16<03:35,  1.26batch/s, auc=0.8484, loss=1.1656]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 20/292 [00:16<03:35,  1.26batch/s, auc=0.8497, loss=0.5236]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 21/292 [00:16<03:15,  1.39batch/s, auc=0.8497, loss=0.5236]\u001b[A\n",
      "Training Epoch 15/25:   7%|▋         | 21/292 [00:17<03:15,  1.39batch/s, auc=0.8431, loss=1.4032]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 22/292 [00:17<03:43,  1.21batch/s, auc=0.8431, loss=1.4032]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 22/292 [00:18<03:43,  1.21batch/s, auc=0.8467, loss=0.7484]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 23/292 [00:18<03:20,  1.34batch/s, auc=0.8467, loss=0.7484]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 23/292 [00:18<03:20,  1.34batch/s, auc=0.8509, loss=0.7024]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 24/292 [00:18<03:04,  1.46batch/s, auc=0.8509, loss=0.7024]\u001b[A\n",
      "Training Epoch 15/25:   8%|▊         | 24/292 [00:19<03:04,  1.46batch/s, auc=0.8563, loss=0.7441]\u001b[A\n",
      "Training Epoch 15/25:   9%|▊         | 25/292 [00:19<02:52,  1.55batch/s, auc=0.8563, loss=0.7441]\u001b[A\n",
      "Training Epoch 15/25:   9%|▊         | 25/292 [00:20<02:52,  1.55batch/s, auc=0.8577, loss=0.8451]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 26/292 [00:20<02:44,  1.61batch/s, auc=0.8577, loss=0.8451]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 26/292 [00:20<02:44,  1.61batch/s, auc=0.8611, loss=0.5637]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 27/292 [00:20<02:39,  1.67batch/s, auc=0.8611, loss=0.5637]\u001b[A\n",
      "Training Epoch 15/25:   9%|▉         | 27/292 [00:21<02:39,  1.67batch/s, auc=0.8542, loss=1.3382]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 28/292 [00:21<03:15,  1.35batch/s, auc=0.8542, loss=1.3382]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 28/292 [00:22<03:15,  1.35batch/s, auc=0.8561, loss=0.5329]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 29/292 [00:22<03:00,  1.46batch/s, auc=0.8561, loss=0.5329]\u001b[A\n",
      "Training Epoch 15/25:  10%|▉         | 29/292 [00:22<03:00,  1.46batch/s, auc=0.8584, loss=0.5830]\u001b[A\n",
      "Training Epoch 15/25:  10%|█         | 30/292 [00:22<02:49,  1.55batch/s, auc=0.8584, loss=0.5830]\u001b[A\n",
      "Training Epoch 15/25:  10%|█         | 30/292 [00:23<02:49,  1.55batch/s, auc=0.8613, loss=0.7185]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 31/292 [00:23<02:41,  1.62batch/s, auc=0.8613, loss=0.7185]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 31/292 [00:23<02:41,  1.62batch/s, auc=0.8607, loss=0.7366]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 32/292 [00:23<02:35,  1.67batch/s, auc=0.8607, loss=0.7366]\u001b[A\n",
      "Training Epoch 15/25:  11%|█         | 32/292 [00:24<02:35,  1.67batch/s, auc=0.8574, loss=1.1374]\u001b[A\n",
      "Training Epoch 15/25:  11%|█▏        | 33/292 [00:24<03:11,  1.35batch/s, auc=0.8574, loss=1.1374]\u001b[A\n",
      "Training Epoch 15/25:  11%|█▏        | 33/292 [00:25<03:11,  1.35batch/s, auc=0.8571, loss=0.7155]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 34/292 [00:25<02:56,  1.46batch/s, auc=0.8571, loss=0.7155]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 34/292 [00:26<02:56,  1.46batch/s, auc=0.8589, loss=0.5888]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 35/292 [00:26<02:46,  1.55batch/s, auc=0.8589, loss=0.5888]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 35/292 [00:26<02:46,  1.55batch/s, auc=0.8613, loss=0.5046]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 36/292 [00:26<02:38,  1.61batch/s, auc=0.8613, loss=0.5046]\u001b[A\n",
      "Training Epoch 15/25:  12%|█▏        | 36/292 [00:27<02:38,  1.61batch/s, auc=0.8581, loss=1.1555]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 37/292 [00:27<03:12,  1.32batch/s, auc=0.8581, loss=1.1555]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 37/292 [00:28<03:12,  1.32batch/s, auc=0.8601, loss=0.7373]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 38/292 [00:28<02:56,  1.44batch/s, auc=0.8601, loss=0.7373]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 38/292 [00:28<02:56,  1.44batch/s, auc=0.8602, loss=0.5852]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 39/292 [00:28<02:45,  1.53batch/s, auc=0.8602, loss=0.5852]\u001b[A\n",
      "Training Epoch 15/25:  13%|█▎        | 39/292 [00:29<02:45,  1.53batch/s, auc=0.8606, loss=0.7708]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▎        | 40/292 [00:29<02:37,  1.60batch/s, auc=0.8606, loss=0.7708]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▎        | 40/292 [00:29<02:37,  1.60batch/s, auc=0.8608, loss=0.6992]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 41/292 [00:29<02:31,  1.65batch/s, auc=0.8608, loss=0.6992]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 41/292 [00:30<02:31,  1.65batch/s, auc=0.8611, loss=0.7078]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 42/292 [00:30<02:27,  1.70batch/s, auc=0.8611, loss=0.7078]\u001b[A\n",
      "Training Epoch 15/25:  14%|█▍        | 42/292 [00:31<02:27,  1.70batch/s, auc=0.8621, loss=0.6194]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▍        | 43/292 [00:31<02:24,  1.72batch/s, auc=0.8621, loss=0.6194]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▍        | 43/292 [00:31<02:24,  1.72batch/s, auc=0.8632, loss=0.5501]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 44/292 [00:31<02:21,  1.75batch/s, auc=0.8632, loss=0.5501]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 44/292 [00:32<02:21,  1.75batch/s, auc=0.8642, loss=0.5233]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 45/292 [00:32<02:20,  1.76batch/s, auc=0.8642, loss=0.5233]\u001b[A\n",
      "Training Epoch 15/25:  15%|█▌        | 45/292 [00:32<02:20,  1.76batch/s, auc=0.8660, loss=0.6264]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 46/292 [00:32<02:18,  1.77batch/s, auc=0.8660, loss=0.6264]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 46/292 [00:33<02:18,  1.77batch/s, auc=0.8663, loss=0.5708]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 47/292 [00:33<02:17,  1.78batch/s, auc=0.8663, loss=0.5708]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▌        | 47/292 [00:33<02:17,  1.78batch/s, auc=0.8660, loss=0.7592]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▋        | 48/292 [00:33<02:16,  1.79batch/s, auc=0.8660, loss=0.7592]\u001b[A\n",
      "Training Epoch 15/25:  16%|█▋        | 48/292 [00:34<02:16,  1.79batch/s, auc=0.8650, loss=0.6432]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 49/292 [00:34<02:15,  1.79batch/s, auc=0.8650, loss=0.6432]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 49/292 [00:34<02:15,  1.79batch/s, auc=0.8662, loss=0.6250]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 50/292 [00:34<02:15,  1.79batch/s, auc=0.8662, loss=0.6250]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 50/292 [00:35<02:15,  1.79batch/s, auc=0.8671, loss=0.4953]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 51/292 [00:35<02:14,  1.79batch/s, auc=0.8671, loss=0.4953]\u001b[A\n",
      "Training Epoch 15/25:  17%|█▋        | 51/292 [00:36<02:14,  1.79batch/s, auc=0.8657, loss=1.1084]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 52/292 [00:36<02:50,  1.40batch/s, auc=0.8657, loss=1.1084]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 52/292 [00:37<02:50,  1.40batch/s, auc=0.8642, loss=0.8696]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 53/292 [00:37<03:15,  1.22batch/s, auc=0.8642, loss=0.8696]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 53/292 [00:38<03:15,  1.22batch/s, auc=0.8663, loss=0.6188]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 54/292 [00:38<02:56,  1.35batch/s, auc=0.8663, loss=0.6188]\u001b[A\n",
      "Training Epoch 15/25:  18%|█▊        | 54/292 [00:38<02:56,  1.35batch/s, auc=0.8685, loss=0.4002]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 55/292 [00:38<02:42,  1.46batch/s, auc=0.8685, loss=0.4002]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 55/292 [00:39<02:42,  1.46batch/s, auc=0.8682, loss=0.8010]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 56/292 [00:39<02:32,  1.55batch/s, auc=0.8682, loss=0.8010]\u001b[A\n",
      "Training Epoch 15/25:  19%|█▉        | 56/292 [00:40<02:32,  1.55batch/s, auc=0.8680, loss=0.8280]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 57/292 [00:40<03:01,  1.29batch/s, auc=0.8680, loss=0.8280]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 57/292 [00:40<03:01,  1.29batch/s, auc=0.8693, loss=0.5675]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 58/292 [00:40<02:45,  1.41batch/s, auc=0.8693, loss=0.5675]\u001b[A\n",
      "Training Epoch 15/25:  20%|█▉        | 58/292 [00:41<02:45,  1.41batch/s, auc=0.8681, loss=1.1062]\u001b[A\n",
      "Training Epoch 15/25:  20%|██        | 59/292 [00:41<02:34,  1.51batch/s, auc=0.8681, loss=1.1062]\u001b[A\n",
      "Training Epoch 15/25:  20%|██        | 59/292 [00:42<02:34,  1.51batch/s, auc=0.8695, loss=0.5915]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 60/292 [00:42<02:26,  1.58batch/s, auc=0.8695, loss=0.5915]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 60/292 [00:42<02:26,  1.58batch/s, auc=0.8705, loss=0.4958]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 61/292 [00:42<02:20,  1.64batch/s, auc=0.8705, loss=0.4958]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 61/292 [00:43<02:20,  1.64batch/s, auc=0.8711, loss=0.6394]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 62/292 [00:43<02:16,  1.69batch/s, auc=0.8711, loss=0.6394]\u001b[A\n",
      "Training Epoch 15/25:  21%|██        | 62/292 [00:43<02:16,  1.69batch/s, auc=0.8727, loss=0.4886]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 63/292 [00:43<02:13,  1.72batch/s, auc=0.8727, loss=0.4886]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 63/292 [00:44<02:13,  1.72batch/s, auc=0.8720, loss=0.9287]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 64/292 [00:44<02:10,  1.74batch/s, auc=0.8720, loss=0.9287]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 64/292 [00:45<02:10,  1.74batch/s, auc=0.8709, loss=0.9574]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 65/292 [00:45<02:44,  1.38batch/s, auc=0.8709, loss=0.9574]\u001b[A\n",
      "Training Epoch 15/25:  22%|██▏       | 65/292 [00:45<02:44,  1.38batch/s, auc=0.8703, loss=0.8374]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 66/292 [00:45<02:32,  1.48batch/s, auc=0.8703, loss=0.8374]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 66/292 [00:46<02:32,  1.48batch/s, auc=0.8695, loss=0.8888]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 67/292 [00:46<02:58,  1.26batch/s, auc=0.8695, loss=0.8888]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 67/292 [00:47<02:58,  1.26batch/s, auc=0.8704, loss=0.5371]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 68/292 [00:47<02:41,  1.38batch/s, auc=0.8704, loss=0.5371]\u001b[A\n",
      "Training Epoch 15/25:  23%|██▎       | 68/292 [00:48<02:41,  1.38batch/s, auc=0.8700, loss=0.6631]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▎       | 69/292 [00:48<02:30,  1.49batch/s, auc=0.8700, loss=0.6631]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▎       | 69/292 [00:48<02:30,  1.49batch/s, auc=0.8720, loss=0.3133]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 70/292 [00:48<02:21,  1.57batch/s, auc=0.8720, loss=0.3133]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 70/292 [00:49<02:21,  1.57batch/s, auc=0.8727, loss=0.6855]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 71/292 [00:49<02:15,  1.63batch/s, auc=0.8727, loss=0.6855]\u001b[A\n",
      "Training Epoch 15/25:  24%|██▍       | 71/292 [00:50<02:15,  1.63batch/s, auc=0.8699, loss=1.4818]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▍       | 72/292 [00:50<02:45,  1.33batch/s, auc=0.8699, loss=1.4818]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▍       | 72/292 [00:50<02:45,  1.33batch/s, auc=0.8706, loss=0.5036]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 73/292 [00:50<02:31,  1.44batch/s, auc=0.8706, loss=0.5036]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 73/292 [00:51<02:31,  1.44batch/s, auc=0.8715, loss=0.5181]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 74/292 [00:51<02:22,  1.53batch/s, auc=0.8715, loss=0.5181]\u001b[A\n",
      "Training Epoch 15/25:  25%|██▌       | 74/292 [00:51<02:22,  1.53batch/s, auc=0.8718, loss=0.7355]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 75/292 [00:51<02:15,  1.60batch/s, auc=0.8718, loss=0.7355]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 75/292 [00:52<02:15,  1.60batch/s, auc=0.8721, loss=0.7090]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 76/292 [00:52<02:10,  1.66batch/s, auc=0.8721, loss=0.7090]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▌       | 76/292 [00:53<02:10,  1.66batch/s, auc=0.8718, loss=0.7747]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▋       | 77/292 [00:53<02:06,  1.70batch/s, auc=0.8718, loss=0.7747]\u001b[A\n",
      "Training Epoch 15/25:  26%|██▋       | 77/292 [00:54<02:06,  1.70batch/s, auc=0.8709, loss=0.9433]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 78/292 [00:54<02:37,  1.36batch/s, auc=0.8709, loss=0.9433]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 78/292 [00:54<02:37,  1.36batch/s, auc=0.8708, loss=0.5763]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 79/292 [00:54<02:25,  1.47batch/s, auc=0.8708, loss=0.5763]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 79/292 [00:55<02:25,  1.47batch/s, auc=0.8696, loss=1.1290]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 80/292 [00:55<02:49,  1.25batch/s, auc=0.8696, loss=1.1290]\u001b[A\n",
      "Training Epoch 15/25:  27%|██▋       | 80/292 [00:56<02:49,  1.25batch/s, auc=0.8693, loss=0.7538]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 81/292 [00:56<03:05,  1.14batch/s, auc=0.8693, loss=0.7538]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 81/292 [00:57<03:05,  1.14batch/s, auc=0.8677, loss=1.1488]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 82/292 [00:57<03:17,  1.07batch/s, auc=0.8677, loss=1.1488]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 82/292 [00:58<03:17,  1.07batch/s, auc=0.8683, loss=0.8619]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 83/292 [00:58<02:52,  1.21batch/s, auc=0.8683, loss=0.8619]\u001b[A\n",
      "Training Epoch 15/25:  28%|██▊       | 83/292 [00:59<02:52,  1.21batch/s, auc=0.8700, loss=0.4108]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 84/292 [00:59<02:34,  1.34batch/s, auc=0.8700, loss=0.4108]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 84/292 [00:59<02:34,  1.34batch/s, auc=0.8706, loss=0.5803]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 85/292 [00:59<02:22,  1.45batch/s, auc=0.8706, loss=0.5803]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 85/292 [01:00<02:22,  1.45batch/s, auc=0.8707, loss=0.7242]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 86/292 [01:00<02:13,  1.54batch/s, auc=0.8707, loss=0.7242]\u001b[A\n",
      "Training Epoch 15/25:  29%|██▉       | 86/292 [01:00<02:13,  1.54batch/s, auc=0.8711, loss=0.6523]\u001b[A\n",
      "Training Epoch 15/25:  30%|██▉       | 87/292 [01:00<02:07,  1.61batch/s, auc=0.8711, loss=0.6523]\u001b[A\n",
      "Training Epoch 15/25:  30%|██▉       | 87/292 [01:01<02:07,  1.61batch/s, auc=0.8707, loss=0.7464]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 88/292 [01:01<02:34,  1.32batch/s, auc=0.8707, loss=0.7464]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 88/292 [01:02<02:34,  1.32batch/s, auc=0.8703, loss=1.0316]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 89/292 [01:02<02:21,  1.43batch/s, auc=0.8703, loss=1.0316]\u001b[A\n",
      "Training Epoch 15/25:  30%|███       | 89/292 [01:02<02:21,  1.43batch/s, auc=0.8706, loss=0.6799]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 90/292 [01:02<02:12,  1.53batch/s, auc=0.8706, loss=0.6799]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 90/292 [01:03<02:12,  1.53batch/s, auc=0.8695, loss=1.0154]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 91/292 [01:03<02:37,  1.28batch/s, auc=0.8695, loss=1.0154]\u001b[A\n",
      "Training Epoch 15/25:  31%|███       | 91/292 [01:04<02:37,  1.28batch/s, auc=0.8695, loss=0.9674]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 92/292 [01:04<02:22,  1.40batch/s, auc=0.8695, loss=0.9674]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 92/292 [01:05<02:22,  1.40batch/s, auc=0.8687, loss=0.9749]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 93/292 [01:05<02:43,  1.22batch/s, auc=0.8687, loss=0.9749]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 93/292 [01:06<02:43,  1.22batch/s, auc=0.8690, loss=0.6323]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 94/292 [01:06<02:26,  1.35batch/s, auc=0.8690, loss=0.6323]\u001b[A\n",
      "Training Epoch 15/25:  32%|███▏      | 94/292 [01:06<02:26,  1.35batch/s, auc=0.8702, loss=0.4391]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 95/292 [01:06<02:15,  1.46batch/s, auc=0.8702, loss=0.4391]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 95/292 [01:07<02:15,  1.46batch/s, auc=0.8699, loss=0.8172]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 96/292 [01:07<02:37,  1.25batch/s, auc=0.8699, loss=0.8172]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 96/292 [01:08<02:37,  1.25batch/s, auc=0.8700, loss=0.7833]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 97/292 [01:08<02:52,  1.13batch/s, auc=0.8700, loss=0.7833]\u001b[A\n",
      "Training Epoch 15/25:  33%|███▎      | 97/292 [01:09<02:52,  1.13batch/s, auc=0.8706, loss=0.6379]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▎      | 98/292 [01:09<02:32,  1.27batch/s, auc=0.8706, loss=0.6379]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▎      | 98/292 [01:09<02:32,  1.27batch/s, auc=0.8709, loss=0.6530]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 99/292 [01:09<02:18,  1.39batch/s, auc=0.8709, loss=0.6530]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 99/292 [01:10<02:18,  1.39batch/s, auc=0.8720, loss=0.4088]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 100/292 [01:10<02:08,  1.49batch/s, auc=0.8720, loss=0.4088]\u001b[A\n",
      "Training Epoch 15/25:  34%|███▍      | 100/292 [01:11<02:08,  1.49batch/s, auc=0.8711, loss=1.0515]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 101/292 [01:11<02:31,  1.26batch/s, auc=0.8711, loss=1.0515]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 101/292 [01:12<02:31,  1.26batch/s, auc=0.8694, loss=1.1956]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 102/292 [01:12<02:46,  1.14batch/s, auc=0.8694, loss=1.1956]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▍      | 102/292 [01:13<02:46,  1.14batch/s, auc=0.8689, loss=1.1167]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▌      | 103/292 [01:13<02:27,  1.28batch/s, auc=0.8689, loss=1.1167]\u001b[A\n",
      "Training Epoch 15/25:  35%|███▌      | 103/292 [01:14<02:27,  1.28batch/s, auc=0.8680, loss=1.0056]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 104/292 [01:14<02:43,  1.15batch/s, auc=0.8680, loss=1.0056]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 104/292 [01:14<02:43,  1.15batch/s, auc=0.8676, loss=0.7859]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 105/292 [01:14<02:25,  1.29batch/s, auc=0.8676, loss=0.7859]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▌      | 105/292 [01:15<02:25,  1.29batch/s, auc=0.8659, loss=1.3321]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▋      | 106/292 [01:15<02:40,  1.16batch/s, auc=0.8659, loss=1.3321]\u001b[A\n",
      "Training Epoch 15/25:  36%|███▋      | 106/292 [01:17<02:40,  1.16batch/s, auc=0.8655, loss=0.8521]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 107/292 [01:17<02:51,  1.08batch/s, auc=0.8655, loss=0.8521]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 107/292 [01:17<02:51,  1.08batch/s, auc=0.8661, loss=0.7095]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 108/292 [01:17<02:30,  1.22batch/s, auc=0.8661, loss=0.7095]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 108/292 [01:18<02:30,  1.22batch/s, auc=0.8660, loss=0.7170]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 109/292 [01:18<02:15,  1.35batch/s, auc=0.8660, loss=0.7170]\u001b[A\n",
      "Training Epoch 15/25:  37%|███▋      | 109/292 [01:19<02:15,  1.35batch/s, auc=0.8648, loss=1.2411]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 110/292 [01:19<02:32,  1.19batch/s, auc=0.8648, loss=1.2411]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 110/292 [01:19<02:32,  1.19batch/s, auc=0.8651, loss=0.6329]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 111/292 [01:19<02:16,  1.32batch/s, auc=0.8651, loss=0.6329]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 111/292 [01:20<02:16,  1.32batch/s, auc=0.8661, loss=0.5076]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 112/292 [01:20<02:05,  1.44batch/s, auc=0.8661, loss=0.5076]\u001b[A\n",
      "Training Epoch 15/25:  38%|███▊      | 112/292 [01:20<02:05,  1.44batch/s, auc=0.8667, loss=0.6053]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▊      | 113/292 [01:20<01:57,  1.53batch/s, auc=0.8667, loss=0.6053]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▊      | 113/292 [01:21<01:57,  1.53batch/s, auc=0.8670, loss=0.6251]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 114/292 [01:21<01:51,  1.60batch/s, auc=0.8670, loss=0.6251]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 114/292 [01:22<01:51,  1.60batch/s, auc=0.8663, loss=1.1323]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 115/292 [01:22<02:14,  1.32batch/s, auc=0.8663, loss=1.1323]\u001b[A\n",
      "Training Epoch 15/25:  39%|███▉      | 115/292 [01:23<02:14,  1.32batch/s, auc=0.8667, loss=0.5633]\u001b[A\n",
      "Training Epoch 15/25:  40%|███▉      | 116/292 [01:23<02:03,  1.43batch/s, auc=0.8667, loss=0.5633]\u001b[A\n",
      "Training Epoch 15/25:  40%|███▉      | 116/292 [01:24<02:03,  1.43batch/s, auc=0.8662, loss=0.9205]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 117/292 [01:24<02:22,  1.23batch/s, auc=0.8662, loss=0.9205]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 117/292 [01:24<02:22,  1.23batch/s, auc=0.8660, loss=0.8778]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 118/292 [01:24<02:07,  1.36batch/s, auc=0.8660, loss=0.8778]\u001b[A\n",
      "Training Epoch 15/25:  40%|████      | 118/292 [01:25<02:07,  1.36batch/s, auc=0.8665, loss=0.5242]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 119/292 [01:25<01:58,  1.47batch/s, auc=0.8665, loss=0.5242]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 119/292 [01:25<01:58,  1.47batch/s, auc=0.8663, loss=0.7736]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 120/292 [01:25<01:50,  1.55batch/s, auc=0.8663, loss=0.7736]\u001b[A\n",
      "Training Epoch 15/25:  41%|████      | 120/292 [01:26<01:50,  1.55batch/s, auc=0.8662, loss=0.7114]\u001b[A\n",
      "Training Epoch 15/25:  41%|████▏     | 121/292 [01:26<01:45,  1.61batch/s, auc=0.8662, loss=0.7114]\u001b[A\n",
      "Training Epoch 15/25:  41%|████▏     | 121/292 [01:26<01:45,  1.61batch/s, auc=0.8661, loss=0.7368]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 122/292 [01:26<01:42,  1.66batch/s, auc=0.8661, loss=0.7368]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 122/292 [01:27<01:42,  1.66batch/s, auc=0.8657, loss=0.7422]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 123/292 [01:27<02:05,  1.35batch/s, auc=0.8657, loss=0.7422]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 123/292 [01:28<02:05,  1.35batch/s, auc=0.8661, loss=0.6128]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 124/292 [01:28<01:55,  1.46batch/s, auc=0.8661, loss=0.6128]\u001b[A\n",
      "Training Epoch 15/25:  42%|████▏     | 124/292 [01:29<01:55,  1.46batch/s, auc=0.8649, loss=1.4385]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 125/292 [01:29<02:14,  1.24batch/s, auc=0.8649, loss=1.4385]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 125/292 [01:30<02:14,  1.24batch/s, auc=0.8654, loss=0.6185]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 126/292 [01:30<02:01,  1.37batch/s, auc=0.8654, loss=0.6185]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 126/292 [01:31<02:01,  1.37batch/s, auc=0.8648, loss=1.0282]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 127/292 [01:31<02:17,  1.20batch/s, auc=0.8648, loss=1.0282]\u001b[A\n",
      "Training Epoch 15/25:  43%|████▎     | 127/292 [01:32<02:17,  1.20batch/s, auc=0.8642, loss=1.0791]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 128/292 [01:32<02:28,  1.10batch/s, auc=0.8642, loss=1.0791]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 128/292 [01:32<02:28,  1.10batch/s, auc=0.8647, loss=0.6739]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 129/292 [01:32<02:10,  1.25batch/s, auc=0.8647, loss=0.6739]\u001b[A\n",
      "Training Epoch 15/25:  44%|████▍     | 129/292 [01:33<02:10,  1.25batch/s, auc=0.8654, loss=0.5509]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 130/292 [01:33<01:58,  1.37batch/s, auc=0.8654, loss=0.5509]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 130/292 [01:34<01:58,  1.37batch/s, auc=0.8660, loss=0.5820]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 131/292 [01:34<01:49,  1.47batch/s, auc=0.8660, loss=0.5820]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▍     | 131/292 [01:34<01:49,  1.47batch/s, auc=0.8660, loss=0.7857]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▌     | 132/292 [01:34<01:42,  1.56batch/s, auc=0.8660, loss=0.7857]\u001b[A\n",
      "Training Epoch 15/25:  45%|████▌     | 132/292 [01:35<01:42,  1.56batch/s, auc=0.8660, loss=0.8066]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 133/292 [01:35<01:38,  1.62batch/s, auc=0.8660, loss=0.8066]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 133/292 [01:36<01:38,  1.62batch/s, auc=0.8653, loss=1.0989]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 134/292 [01:36<01:59,  1.33batch/s, auc=0.8653, loss=1.0989]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 134/292 [01:37<01:59,  1.33batch/s, auc=0.8644, loss=0.9368]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 135/292 [01:37<02:13,  1.18batch/s, auc=0.8644, loss=0.9368]\u001b[A\n",
      "Training Epoch 15/25:  46%|████▌     | 135/292 [01:37<02:13,  1.18batch/s, auc=0.8644, loss=0.7513]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 136/292 [01:37<01:59,  1.31batch/s, auc=0.8644, loss=0.7513]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 136/292 [01:38<01:59,  1.31batch/s, auc=0.8650, loss=0.6515]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 137/292 [01:38<01:48,  1.43batch/s, auc=0.8650, loss=0.6515]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 137/292 [01:38<01:48,  1.43batch/s, auc=0.8653, loss=0.6921]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 138/292 [01:38<01:41,  1.52batch/s, auc=0.8653, loss=0.6921]\u001b[A\n",
      "Training Epoch 15/25:  47%|████▋     | 138/292 [01:39<01:41,  1.52batch/s, auc=0.8662, loss=0.3598]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 139/292 [01:39<01:36,  1.59batch/s, auc=0.8662, loss=0.3598]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 139/292 [01:40<01:36,  1.59batch/s, auc=0.8662, loss=0.7520]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 140/292 [01:40<01:32,  1.65batch/s, auc=0.8662, loss=0.7520]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 140/292 [01:41<01:32,  1.65batch/s, auc=0.8660, loss=0.7645]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 141/292 [01:41<01:52,  1.34batch/s, auc=0.8660, loss=0.7645]\u001b[A\n",
      "Training Epoch 15/25:  48%|████▊     | 141/292 [01:41<01:52,  1.34batch/s, auc=0.8661, loss=0.6965]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▊     | 142/292 [01:41<01:43,  1.45batch/s, auc=0.8661, loss=0.6965]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▊     | 142/292 [01:42<01:43,  1.45batch/s, auc=0.8652, loss=1.1892]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 143/292 [01:42<02:00,  1.24batch/s, auc=0.8652, loss=1.1892]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 143/292 [01:43<02:00,  1.24batch/s, auc=0.8655, loss=0.6970]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 144/292 [01:43<01:48,  1.36batch/s, auc=0.8655, loss=0.6970]\u001b[A\n",
      "Training Epoch 15/25:  49%|████▉     | 144/292 [01:43<01:48,  1.36batch/s, auc=0.8655, loss=0.8883]\u001b[A\n",
      "Training Epoch 15/25:  50%|████▉     | 145/292 [01:43<01:40,  1.47batch/s, auc=0.8655, loss=0.8883]\u001b[A\n",
      "Training Epoch 15/25:  50%|████▉     | 145/292 [01:44<01:40,  1.47batch/s, auc=0.8657, loss=0.8859]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 146/292 [01:44<01:34,  1.55batch/s, auc=0.8657, loss=0.8859]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 146/292 [01:45<01:34,  1.55batch/s, auc=0.8664, loss=0.5295]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 147/292 [01:45<01:29,  1.61batch/s, auc=0.8664, loss=0.5295]\u001b[A\n",
      "Training Epoch 15/25:  50%|█████     | 147/292 [01:45<01:29,  1.61batch/s, auc=0.8668, loss=0.5692]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 148/292 [01:45<01:26,  1.66batch/s, auc=0.8668, loss=0.5692]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 148/292 [01:46<01:26,  1.66batch/s, auc=0.8667, loss=0.7037]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 149/292 [01:46<01:46,  1.34batch/s, auc=0.8667, loss=0.7037]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████     | 149/292 [01:47<01:46,  1.34batch/s, auc=0.8669, loss=0.6948]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████▏    | 150/292 [01:47<01:37,  1.45batch/s, auc=0.8669, loss=0.6948]\u001b[A\n",
      "Training Epoch 15/25:  51%|█████▏    | 150/292 [01:47<01:37,  1.45batch/s, auc=0.8671, loss=0.7043]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 151/292 [01:47<01:31,  1.54batch/s, auc=0.8671, loss=0.7043]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 151/292 [01:48<01:31,  1.54batch/s, auc=0.8671, loss=0.6681]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 152/292 [01:48<01:27,  1.60batch/s, auc=0.8671, loss=0.6681]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 152/292 [01:48<01:27,  1.60batch/s, auc=0.8670, loss=0.5987]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 153/292 [01:48<01:24,  1.65batch/s, auc=0.8670, loss=0.5987]\u001b[A\n",
      "Training Epoch 15/25:  52%|█████▏    | 153/292 [01:49<01:24,  1.65batch/s, auc=0.8666, loss=0.9469]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 154/292 [01:49<01:42,  1.34batch/s, auc=0.8666, loss=0.9469]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 154/292 [01:50<01:42,  1.34batch/s, auc=0.8666, loss=0.7513]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 155/292 [01:50<01:34,  1.45batch/s, auc=0.8666, loss=0.7513]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 155/292 [01:51<01:34,  1.45batch/s, auc=0.8671, loss=0.4956]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 156/292 [01:51<01:28,  1.54batch/s, auc=0.8671, loss=0.4956]\u001b[A\n",
      "Training Epoch 15/25:  53%|█████▎    | 156/292 [01:51<01:28,  1.54batch/s, auc=0.8672, loss=0.7102]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 157/292 [01:51<01:24,  1.61batch/s, auc=0.8672, loss=0.7102]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 157/292 [01:52<01:24,  1.61batch/s, auc=0.8667, loss=0.8798]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 158/292 [01:52<01:41,  1.32batch/s, auc=0.8667, loss=0.8798]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 158/292 [01:53<01:41,  1.32batch/s, auc=0.8667, loss=0.9535]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 159/292 [01:53<01:32,  1.43batch/s, auc=0.8667, loss=0.9535]\u001b[A\n",
      "Training Epoch 15/25:  54%|█████▍    | 159/292 [01:53<01:32,  1.43batch/s, auc=0.8671, loss=0.6548]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▍    | 160/292 [01:53<01:26,  1.52batch/s, auc=0.8671, loss=0.6548]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▍    | 160/292 [01:54<01:26,  1.52batch/s, auc=0.8680, loss=0.4640]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 161/292 [01:54<01:22,  1.59batch/s, auc=0.8680, loss=0.4640]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 161/292 [01:54<01:22,  1.59batch/s, auc=0.8685, loss=0.5515]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 162/292 [01:54<01:18,  1.65batch/s, auc=0.8685, loss=0.5515]\u001b[A\n",
      "Training Epoch 15/25:  55%|█████▌    | 162/292 [01:55<01:18,  1.65batch/s, auc=0.8686, loss=0.5449]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 163/292 [01:55<01:16,  1.69batch/s, auc=0.8686, loss=0.5449]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 163/292 [01:56<01:16,  1.69batch/s, auc=0.8687, loss=0.6634]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 164/292 [01:56<01:14,  1.71batch/s, auc=0.8687, loss=0.6634]\u001b[A\n",
      "Training Epoch 15/25:  56%|█████▌    | 164/292 [01:56<01:14,  1.71batch/s, auc=0.8689, loss=0.5674]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 165/292 [01:56<01:13,  1.73batch/s, auc=0.8689, loss=0.5674]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 165/292 [01:57<01:13,  1.73batch/s, auc=0.8687, loss=0.9723]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 166/292 [01:57<01:12,  1.75batch/s, auc=0.8687, loss=0.9723]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 166/292 [01:57<01:12,  1.75batch/s, auc=0.8688, loss=0.6573]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 167/292 [01:57<01:11,  1.76batch/s, auc=0.8688, loss=0.6573]\u001b[A\n",
      "Training Epoch 15/25:  57%|█████▋    | 167/292 [01:58<01:11,  1.76batch/s, auc=0.8687, loss=0.5970]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 168/292 [01:58<01:10,  1.77batch/s, auc=0.8687, loss=0.5970]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 168/292 [01:58<01:10,  1.77batch/s, auc=0.8688, loss=0.6507]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 169/292 [01:58<01:09,  1.77batch/s, auc=0.8688, loss=0.6507]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 169/292 [01:59<01:09,  1.77batch/s, auc=0.8692, loss=0.5083]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 170/292 [01:59<01:08,  1.78batch/s, auc=0.8692, loss=0.5083]\u001b[A\n",
      "Training Epoch 15/25:  58%|█████▊    | 170/292 [02:00<01:08,  1.78batch/s, auc=0.8696, loss=0.5913]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▊    | 171/292 [02:00<01:07,  1.78batch/s, auc=0.8696, loss=0.5913]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▊    | 171/292 [02:00<01:07,  1.78batch/s, auc=0.8699, loss=0.5663]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 172/292 [02:00<01:07,  1.78batch/s, auc=0.8699, loss=0.5663]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 172/292 [02:01<01:07,  1.78batch/s, auc=0.8703, loss=0.6041]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 173/292 [02:01<01:06,  1.78batch/s, auc=0.8703, loss=0.6041]\u001b[A\n",
      "Training Epoch 15/25:  59%|█████▉    | 173/292 [02:02<01:06,  1.78batch/s, auc=0.8697, loss=1.0702]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 174/292 [02:02<01:24,  1.40batch/s, auc=0.8697, loss=1.0702]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 174/292 [02:02<01:24,  1.40batch/s, auc=0.8698, loss=0.6611]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 175/292 [02:02<01:18,  1.49batch/s, auc=0.8698, loss=0.6611]\u001b[A\n",
      "Training Epoch 15/25:  60%|█████▉    | 175/292 [02:03<01:18,  1.49batch/s, auc=0.8693, loss=1.4440]\u001b[A\n",
      "Training Epoch 15/25:  60%|██████    | 176/292 [02:03<01:31,  1.26batch/s, auc=0.8693, loss=1.4440]\u001b[A\n",
      "Training Epoch 15/25:  60%|██████    | 176/292 [02:04<01:31,  1.26batch/s, auc=0.8697, loss=0.5059]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 177/292 [02:04<01:23,  1.38batch/s, auc=0.8697, loss=0.5059]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 177/292 [02:04<01:23,  1.38batch/s, auc=0.8695, loss=0.7502]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 178/292 [02:04<01:16,  1.49batch/s, auc=0.8695, loss=0.7502]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████    | 178/292 [02:06<01:16,  1.49batch/s, auc=0.8690, loss=0.9022]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████▏   | 179/292 [02:06<01:29,  1.26batch/s, auc=0.8690, loss=0.9022]\u001b[A\n",
      "Training Epoch 15/25:  61%|██████▏   | 179/292 [02:07<01:29,  1.26batch/s, auc=0.8683, loss=1.1170]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 180/292 [02:07<01:38,  1.14batch/s, auc=0.8683, loss=1.1170]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 180/292 [02:07<01:38,  1.14batch/s, auc=0.8686, loss=0.5034]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 181/292 [02:07<01:26,  1.28batch/s, auc=0.8686, loss=0.5034]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 181/292 [02:08<01:26,  1.28batch/s, auc=0.8685, loss=0.9454]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 182/292 [02:08<01:18,  1.40batch/s, auc=0.8685, loss=0.9454]\u001b[A\n",
      "Training Epoch 15/25:  62%|██████▏   | 182/292 [02:08<01:18,  1.40batch/s, auc=0.8682, loss=0.7974]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 183/292 [02:08<01:12,  1.49batch/s, auc=0.8682, loss=0.7974]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 183/292 [02:09<01:12,  1.49batch/s, auc=0.8683, loss=0.5944]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 184/292 [02:09<01:08,  1.57batch/s, auc=0.8683, loss=0.5944]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 184/292 [02:09<01:08,  1.57batch/s, auc=0.8680, loss=0.9427]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 185/292 [02:09<01:05,  1.63batch/s, auc=0.8680, loss=0.9427]\u001b[A\n",
      "Training Epoch 15/25:  63%|██████▎   | 185/292 [02:10<01:05,  1.63batch/s, auc=0.8684, loss=0.5020]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▎   | 186/292 [02:10<01:03,  1.67batch/s, auc=0.8684, loss=0.5020]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▎   | 186/292 [02:11<01:03,  1.67batch/s, auc=0.8689, loss=0.5092]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 187/292 [02:11<01:01,  1.70batch/s, auc=0.8689, loss=0.5092]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 187/292 [02:12<01:01,  1.70batch/s, auc=0.8685, loss=1.0409]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 188/292 [02:12<01:16,  1.36batch/s, auc=0.8685, loss=1.0409]\u001b[A\n",
      "Training Epoch 15/25:  64%|██████▍   | 188/292 [02:13<01:16,  1.36batch/s, auc=0.8684, loss=0.7746]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▍   | 189/292 [02:13<01:26,  1.19batch/s, auc=0.8684, loss=0.7746]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▍   | 189/292 [02:13<01:26,  1.19batch/s, auc=0.8686, loss=0.7100]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 190/292 [02:13<01:16,  1.32batch/s, auc=0.8686, loss=0.7100]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 190/292 [02:14<01:16,  1.32batch/s, auc=0.8690, loss=0.5355]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 191/292 [02:14<01:10,  1.44batch/s, auc=0.8690, loss=0.5355]\u001b[A\n",
      "Training Epoch 15/25:  65%|██████▌   | 191/292 [02:14<01:10,  1.44batch/s, auc=0.8692, loss=0.7872]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 192/292 [02:14<01:05,  1.52batch/s, auc=0.8692, loss=0.7872]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 192/292 [02:15<01:05,  1.52batch/s, auc=0.8695, loss=0.6039]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 193/292 [02:15<01:02,  1.59batch/s, auc=0.8695, loss=0.6039]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▌   | 193/292 [02:16<01:02,  1.59batch/s, auc=0.8693, loss=0.8882]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▋   | 194/292 [02:16<00:59,  1.65batch/s, auc=0.8693, loss=0.8882]\u001b[A\n",
      "Training Epoch 15/25:  66%|██████▋   | 194/292 [02:17<00:59,  1.65batch/s, auc=0.8688, loss=1.0425]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 195/292 [02:17<01:12,  1.33batch/s, auc=0.8688, loss=1.0425]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 195/292 [02:17<01:12,  1.33batch/s, auc=0.8693, loss=0.6950]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 196/292 [02:17<01:06,  1.44batch/s, auc=0.8693, loss=0.6950]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 196/292 [02:18<01:06,  1.44batch/s, auc=0.8694, loss=0.7308]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 197/292 [02:18<01:02,  1.53batch/s, auc=0.8694, loss=0.7308]\u001b[A\n",
      "Training Epoch 15/25:  67%|██████▋   | 197/292 [02:18<01:02,  1.53batch/s, auc=0.8696, loss=0.5430]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 198/292 [02:18<00:58,  1.60batch/s, auc=0.8696, loss=0.5430]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 198/292 [02:19<00:58,  1.60batch/s, auc=0.8697, loss=0.5405]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 199/292 [02:19<00:56,  1.65batch/s, auc=0.8697, loss=0.5405]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 199/292 [02:19<00:56,  1.65batch/s, auc=0.8699, loss=0.5799]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 200/292 [02:19<00:54,  1.69batch/s, auc=0.8699, loss=0.5799]\u001b[A\n",
      "Training Epoch 15/25:  68%|██████▊   | 200/292 [02:20<00:54,  1.69batch/s, auc=0.8700, loss=0.7361]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 201/292 [02:20<00:53,  1.71batch/s, auc=0.8700, loss=0.7361]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 201/292 [02:21<00:53,  1.71batch/s, auc=0.8703, loss=0.6249]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 202/292 [02:21<00:51,  1.73batch/s, auc=0.8703, loss=0.6249]\u001b[A\n",
      "Training Epoch 15/25:  69%|██████▉   | 202/292 [02:21<00:51,  1.73batch/s, auc=0.8703, loss=0.7669]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 203/292 [02:21<00:50,  1.75batch/s, auc=0.8703, loss=0.7669]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 203/292 [02:22<00:50,  1.75batch/s, auc=0.8704, loss=0.6556]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 204/292 [02:22<00:50,  1.76batch/s, auc=0.8704, loss=0.6556]\u001b[A\n",
      "Training Epoch 15/25:  70%|██████▉   | 204/292 [02:22<00:50,  1.76batch/s, auc=0.8708, loss=0.5404]\u001b[A\n",
      "Training Epoch 15/25:  70%|███████   | 205/292 [02:22<00:49,  1.76batch/s, auc=0.8708, loss=0.5404]\u001b[A\n",
      "Training Epoch 15/25:  70%|███████   | 205/292 [02:23<00:49,  1.76batch/s, auc=0.8709, loss=0.7150]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 206/292 [02:23<00:48,  1.77batch/s, auc=0.8709, loss=0.7150]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 206/292 [02:24<00:48,  1.77batch/s, auc=0.8701, loss=1.1031]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 207/292 [02:24<01:01,  1.39batch/s, auc=0.8701, loss=1.1031]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 207/292 [02:24<01:01,  1.39batch/s, auc=0.8703, loss=0.6117]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 208/292 [02:24<00:56,  1.49batch/s, auc=0.8703, loss=0.6117]\u001b[A\n",
      "Training Epoch 15/25:  71%|███████   | 208/292 [02:25<00:56,  1.49batch/s, auc=0.8710, loss=0.4915]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 209/292 [02:25<00:53,  1.56batch/s, auc=0.8710, loss=0.4915]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 209/292 [02:26<00:53,  1.56batch/s, auc=0.8706, loss=0.7635]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 210/292 [02:26<01:03,  1.30batch/s, auc=0.8706, loss=0.7635]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 210/292 [02:27<01:03,  1.30batch/s, auc=0.8701, loss=1.2230]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 211/292 [02:27<00:57,  1.41batch/s, auc=0.8701, loss=1.2230]\u001b[A\n",
      "Training Epoch 15/25:  72%|███████▏  | 211/292 [02:28<00:57,  1.41batch/s, auc=0.8695, loss=1.1751]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 212/292 [02:28<01:05,  1.22batch/s, auc=0.8695, loss=1.1751]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 212/292 [02:29<01:05,  1.22batch/s, auc=0.8693, loss=0.9241]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 213/292 [02:29<01:10,  1.11batch/s, auc=0.8693, loss=0.9241]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 213/292 [02:30<01:10,  1.11batch/s, auc=0.8686, loss=1.2829]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 214/292 [02:30<01:14,  1.05batch/s, auc=0.8686, loss=1.2829]\u001b[A\n",
      "Training Epoch 15/25:  73%|███████▎  | 214/292 [02:30<01:14,  1.05batch/s, auc=0.8685, loss=0.7688]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▎  | 215/292 [02:30<01:04,  1.20batch/s, auc=0.8685, loss=0.7688]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▎  | 215/292 [02:31<01:04,  1.20batch/s, auc=0.8687, loss=0.6288]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 216/292 [02:31<00:57,  1.33batch/s, auc=0.8687, loss=0.6288]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 216/292 [02:32<00:57,  1.33batch/s, auc=0.8687, loss=0.6696]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 217/292 [02:32<00:52,  1.44batch/s, auc=0.8687, loss=0.6696]\u001b[A\n",
      "Training Epoch 15/25:  74%|███████▍  | 217/292 [02:33<00:52,  1.44batch/s, auc=0.8682, loss=0.9259]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▍  | 218/292 [02:33<00:59,  1.23batch/s, auc=0.8682, loss=0.9259]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▍  | 218/292 [02:33<00:59,  1.23batch/s, auc=0.8686, loss=0.5872]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 219/292 [02:33<00:53,  1.36batch/s, auc=0.8686, loss=0.5872]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 219/292 [02:34<00:53,  1.36batch/s, auc=0.8683, loss=0.8929]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 220/292 [02:34<00:49,  1.46batch/s, auc=0.8683, loss=0.8929]\u001b[A\n",
      "Training Epoch 15/25:  75%|███████▌  | 220/292 [02:35<00:49,  1.46batch/s, auc=0.8682, loss=0.8807]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 221/292 [02:35<00:57,  1.24batch/s, auc=0.8682, loss=0.8807]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 221/292 [02:35<00:57,  1.24batch/s, auc=0.8685, loss=0.5678]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 222/292 [02:35<00:51,  1.37batch/s, auc=0.8685, loss=0.5678]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▌  | 222/292 [02:36<00:51,  1.37batch/s, auc=0.8688, loss=0.6761]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▋  | 223/292 [02:36<00:46,  1.47batch/s, auc=0.8688, loss=0.6761]\u001b[A\n",
      "Training Epoch 15/25:  76%|███████▋  | 223/292 [02:37<00:46,  1.47batch/s, auc=0.8691, loss=0.7498]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 224/292 [02:37<00:43,  1.55batch/s, auc=0.8691, loss=0.7498]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 224/292 [02:37<00:43,  1.55batch/s, auc=0.8693, loss=0.7115]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 225/292 [02:37<00:41,  1.61batch/s, auc=0.8693, loss=0.7115]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 225/292 [02:38<00:41,  1.61batch/s, auc=0.8691, loss=0.7924]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 226/292 [02:38<00:50,  1.32batch/s, auc=0.8691, loss=0.7924]\u001b[A\n",
      "Training Epoch 15/25:  77%|███████▋  | 226/292 [02:39<00:50,  1.32batch/s, auc=0.8692, loss=0.5918]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 227/292 [02:39<00:45,  1.43batch/s, auc=0.8692, loss=0.5918]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 227/292 [02:39<00:45,  1.43batch/s, auc=0.8693, loss=0.8772]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 228/292 [02:39<00:42,  1.52batch/s, auc=0.8693, loss=0.8772]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 228/292 [02:40<00:42,  1.52batch/s, auc=0.8696, loss=0.5512]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 229/292 [02:40<00:39,  1.59batch/s, auc=0.8696, loss=0.5512]\u001b[A\n",
      "Training Epoch 15/25:  78%|███████▊  | 229/292 [02:40<00:39,  1.59batch/s, auc=0.8700, loss=0.5844]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 230/292 [02:40<00:37,  1.64batch/s, auc=0.8700, loss=0.5844]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 230/292 [02:41<00:37,  1.64batch/s, auc=0.8702, loss=0.5610]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 231/292 [02:41<00:36,  1.68batch/s, auc=0.8702, loss=0.5610]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 231/292 [02:42<00:36,  1.68batch/s, auc=0.8706, loss=0.6159]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 232/292 [02:42<00:35,  1.71batch/s, auc=0.8706, loss=0.6159]\u001b[A\n",
      "Training Epoch 15/25:  79%|███████▉  | 232/292 [02:42<00:35,  1.71batch/s, auc=0.8709, loss=0.6523]\u001b[A\n",
      "Training Epoch 15/25:  80%|███████▉  | 233/292 [02:42<00:34,  1.73batch/s, auc=0.8709, loss=0.6523]\u001b[A\n",
      "Training Epoch 15/25:  80%|███████▉  | 233/292 [02:43<00:34,  1.73batch/s, auc=0.8705, loss=0.7569]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 234/292 [02:43<00:33,  1.74batch/s, auc=0.8705, loss=0.7569]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 234/292 [02:43<00:33,  1.74batch/s, auc=0.8709, loss=0.4309]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 235/292 [02:43<00:32,  1.75batch/s, auc=0.8709, loss=0.4309]\u001b[A\n",
      "Training Epoch 15/25:  80%|████████  | 235/292 [02:44<00:32,  1.75batch/s, auc=0.8709, loss=0.6976]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 236/292 [02:44<00:31,  1.76batch/s, auc=0.8709, loss=0.6976]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 236/292 [02:44<00:31,  1.76batch/s, auc=0.8712, loss=0.5002]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 237/292 [02:44<00:31,  1.76batch/s, auc=0.8712, loss=0.5002]\u001b[A\n",
      "Training Epoch 15/25:  81%|████████  | 237/292 [02:45<00:31,  1.76batch/s, auc=0.8710, loss=1.0032]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 238/292 [02:45<00:38,  1.39batch/s, auc=0.8710, loss=1.0032]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 238/292 [02:46<00:38,  1.39batch/s, auc=0.8710, loss=0.6315]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 239/292 [02:46<00:35,  1.49batch/s, auc=0.8710, loss=0.6315]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 239/292 [02:47<00:35,  1.49batch/s, auc=0.8712, loss=0.6541]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 240/292 [02:47<00:33,  1.56batch/s, auc=0.8712, loss=0.6541]\u001b[A\n",
      "Training Epoch 15/25:  82%|████████▏ | 240/292 [02:48<00:33,  1.56batch/s, auc=0.8707, loss=0.9741]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 241/292 [02:48<00:39,  1.30batch/s, auc=0.8707, loss=0.9741]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 241/292 [02:48<00:39,  1.30batch/s, auc=0.8706, loss=0.7195]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 242/292 [02:48<00:35,  1.41batch/s, auc=0.8706, loss=0.7195]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 242/292 [02:49<00:35,  1.41batch/s, auc=0.8703, loss=0.8611]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 243/292 [02:49<00:32,  1.51batch/s, auc=0.8703, loss=0.8611]\u001b[A\n",
      "Training Epoch 15/25:  83%|████████▎ | 243/292 [02:50<00:32,  1.51batch/s, auc=0.8700, loss=1.0622]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▎ | 244/292 [02:50<00:37,  1.27batch/s, auc=0.8700, loss=1.0622]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▎ | 244/292 [02:50<00:37,  1.27batch/s, auc=0.8702, loss=0.5479]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 245/292 [02:50<00:33,  1.39batch/s, auc=0.8702, loss=0.5479]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 245/292 [02:51<00:33,  1.39batch/s, auc=0.8702, loss=0.7452]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 246/292 [02:51<00:30,  1.48batch/s, auc=0.8702, loss=0.7452]\u001b[A\n",
      "Training Epoch 15/25:  84%|████████▍ | 246/292 [02:52<00:30,  1.48batch/s, auc=0.8704, loss=0.5720]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 247/292 [02:52<00:28,  1.56batch/s, auc=0.8704, loss=0.5720]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 247/292 [02:52<00:28,  1.56batch/s, auc=0.8703, loss=0.7603]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 248/292 [02:52<00:27,  1.62batch/s, auc=0.8703, loss=0.7603]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▍ | 248/292 [02:53<00:27,  1.62batch/s, auc=0.8705, loss=0.5894]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▌ | 249/292 [02:53<00:25,  1.66batch/s, auc=0.8705, loss=0.5894]\u001b[A\n",
      "Training Epoch 15/25:  85%|████████▌ | 249/292 [02:53<00:25,  1.66batch/s, auc=0.8706, loss=0.5321]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 250/292 [02:53<00:24,  1.70batch/s, auc=0.8706, loss=0.5321]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 250/292 [02:54<00:24,  1.70batch/s, auc=0.8709, loss=0.4276]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 251/292 [02:54<00:23,  1.72batch/s, auc=0.8709, loss=0.4276]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▌ | 251/292 [02:54<00:23,  1.72batch/s, auc=0.8713, loss=0.5270]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▋ | 252/292 [02:54<00:23,  1.74batch/s, auc=0.8713, loss=0.5270]\u001b[A\n",
      "Training Epoch 15/25:  86%|████████▋ | 252/292 [02:55<00:23,  1.74batch/s, auc=0.8714, loss=0.6246]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 253/292 [02:55<00:22,  1.75batch/s, auc=0.8714, loss=0.6246]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 253/292 [02:55<00:22,  1.75batch/s, auc=0.8716, loss=0.5662]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 254/292 [02:55<00:21,  1.76batch/s, auc=0.8716, loss=0.5662]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 254/292 [02:56<00:21,  1.76batch/s, auc=0.8717, loss=0.6382]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 255/292 [02:56<00:20,  1.76batch/s, auc=0.8717, loss=0.6382]\u001b[A\n",
      "Training Epoch 15/25:  87%|████████▋ | 255/292 [02:57<00:20,  1.76batch/s, auc=0.8716, loss=0.8624]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 256/292 [02:57<00:20,  1.77batch/s, auc=0.8716, loss=0.8624]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 256/292 [02:57<00:20,  1.77batch/s, auc=0.8716, loss=0.7286]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 257/292 [02:57<00:19,  1.77batch/s, auc=0.8716, loss=0.7286]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 257/292 [02:58<00:19,  1.77batch/s, auc=0.8712, loss=1.2207]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 258/292 [02:58<00:19,  1.77batch/s, auc=0.8712, loss=1.2207]\u001b[A\n",
      "Training Epoch 15/25:  88%|████████▊ | 258/292 [02:59<00:19,  1.77batch/s, auc=0.8710, loss=0.7437]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▊ | 259/292 [02:59<00:23,  1.39batch/s, auc=0.8710, loss=0.7437]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▊ | 259/292 [02:59<00:23,  1.39batch/s, auc=0.8709, loss=0.9261]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 260/292 [02:59<00:21,  1.49batch/s, auc=0.8709, loss=0.9261]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 260/292 [03:00<00:21,  1.49batch/s, auc=0.8709, loss=0.7641]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 261/292 [03:00<00:19,  1.56batch/s, auc=0.8709, loss=0.7641]\u001b[A\n",
      "Training Epoch 15/25:  89%|████████▉ | 261/292 [03:00<00:19,  1.56batch/s, auc=0.8709, loss=0.7242]\u001b[A\n",
      "Training Epoch 15/25:  90%|████████▉ | 262/292 [03:00<00:18,  1.62batch/s, auc=0.8709, loss=0.7242]\u001b[A\n",
      "Training Epoch 15/25:  90%|████████▉ | 262/292 [03:01<00:18,  1.62batch/s, auc=0.8707, loss=0.7990]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 263/292 [03:01<00:17,  1.66batch/s, auc=0.8707, loss=0.7990]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 263/292 [03:02<00:17,  1.66batch/s, auc=0.8707, loss=0.6025]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 264/292 [03:02<00:16,  1.70batch/s, auc=0.8707, loss=0.6025]\u001b[A\n",
      "Training Epoch 15/25:  90%|█████████ | 264/292 [03:02<00:16,  1.70batch/s, auc=0.8707, loss=0.7799]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 265/292 [03:02<00:15,  1.72batch/s, auc=0.8707, loss=0.7799]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 265/292 [03:03<00:15,  1.72batch/s, auc=0.8702, loss=1.0866]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 266/292 [03:03<00:19,  1.37batch/s, auc=0.8702, loss=1.0866]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████ | 266/292 [03:04<00:19,  1.37batch/s, auc=0.8701, loss=0.7427]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████▏| 267/292 [03:04<00:20,  1.20batch/s, auc=0.8701, loss=0.7427]\u001b[A\n",
      "Training Epoch 15/25:  91%|█████████▏| 267/292 [03:05<00:20,  1.20batch/s, auc=0.8698, loss=1.0265]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 268/292 [03:05<00:18,  1.33batch/s, auc=0.8698, loss=1.0265]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 268/292 [03:05<00:18,  1.33batch/s, auc=0.8698, loss=0.7063]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 269/292 [03:05<00:16,  1.43batch/s, auc=0.8698, loss=0.7063]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 269/292 [03:06<00:16,  1.43batch/s, auc=0.8701, loss=0.5671]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 270/292 [03:06<00:14,  1.52batch/s, auc=0.8701, loss=0.5671]\u001b[A\n",
      "Training Epoch 15/25:  92%|█████████▏| 270/292 [03:07<00:14,  1.52batch/s, auc=0.8699, loss=1.0100]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 271/292 [03:07<00:13,  1.59batch/s, auc=0.8699, loss=1.0100]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 271/292 [03:07<00:13,  1.59batch/s, auc=0.8701, loss=0.4946]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 272/292 [03:07<00:12,  1.64batch/s, auc=0.8701, loss=0.4946]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 272/292 [03:08<00:12,  1.64batch/s, auc=0.8699, loss=0.8460]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 273/292 [03:08<00:14,  1.33batch/s, auc=0.8699, loss=0.8460]\u001b[A\n",
      "Training Epoch 15/25:  93%|█████████▎| 273/292 [03:09<00:14,  1.33batch/s, auc=0.8701, loss=0.5985]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 274/292 [03:09<00:12,  1.44batch/s, auc=0.8701, loss=0.5985]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 274/292 [03:10<00:12,  1.44batch/s, auc=0.8698, loss=1.1401]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 275/292 [03:10<00:13,  1.23batch/s, auc=0.8698, loss=1.1401]\u001b[A\n",
      "Training Epoch 15/25:  94%|█████████▍| 275/292 [03:10<00:13,  1.23batch/s, auc=0.8699, loss=0.5401]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 276/292 [03:10<00:11,  1.36batch/s, auc=0.8699, loss=0.5401]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 276/292 [03:11<00:11,  1.36batch/s, auc=0.8702, loss=0.4332]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 277/292 [03:11<00:10,  1.46batch/s, auc=0.8702, loss=0.4332]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▍| 277/292 [03:12<00:10,  1.46batch/s, auc=0.8699, loss=1.0894]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▌| 278/292 [03:12<00:09,  1.54batch/s, auc=0.8699, loss=1.0894]\u001b[A\n",
      "Training Epoch 15/25:  95%|█████████▌| 278/292 [03:13<00:09,  1.54batch/s, auc=0.8696, loss=1.0328]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 279/292 [03:13<00:10,  1.29batch/s, auc=0.8696, loss=1.0328]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 279/292 [03:13<00:10,  1.29batch/s, auc=0.8698, loss=0.6490]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 280/292 [03:13<00:08,  1.40batch/s, auc=0.8698, loss=0.6490]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 280/292 [03:14<00:08,  1.40batch/s, auc=0.8695, loss=1.0105]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 281/292 [03:14<00:07,  1.49batch/s, auc=0.8695, loss=1.0105]\u001b[A\n",
      "Training Epoch 15/25:  96%|█████████▌| 281/292 [03:14<00:07,  1.49batch/s, auc=0.8696, loss=0.5877]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 282/292 [03:14<00:06,  1.57batch/s, auc=0.8696, loss=0.5877]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 282/292 [03:15<00:06,  1.57batch/s, auc=0.8699, loss=0.5903]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 283/292 [03:15<00:05,  1.63batch/s, auc=0.8699, loss=0.5903]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 283/292 [03:15<00:05,  1.63batch/s, auc=0.8700, loss=0.7840]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 284/292 [03:15<00:04,  1.67batch/s, auc=0.8700, loss=0.7840]\u001b[A\n",
      "Training Epoch 15/25:  97%|█████████▋| 284/292 [03:16<00:04,  1.67batch/s, auc=0.8700, loss=0.7124]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 285/292 [03:16<00:04,  1.70batch/s, auc=0.8700, loss=0.7124]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 285/292 [03:17<00:04,  1.70batch/s, auc=0.8699, loss=0.9067]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 286/292 [03:17<00:03,  1.72batch/s, auc=0.8699, loss=0.9067]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 286/292 [03:17<00:03,  1.72batch/s, auc=0.8700, loss=0.6864]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 287/292 [03:17<00:02,  1.73batch/s, auc=0.8700, loss=0.6864]\u001b[A\n",
      "Training Epoch 15/25:  98%|█████████▊| 287/292 [03:18<00:02,  1.73batch/s, auc=0.8702, loss=0.5685]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▊| 288/292 [03:18<00:02,  1.75batch/s, auc=0.8702, loss=0.5685]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▊| 288/292 [03:18<00:02,  1.75batch/s, auc=0.8704, loss=0.4832]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 289/292 [03:18<00:01,  1.75batch/s, auc=0.8704, loss=0.4832]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 289/292 [03:19<00:01,  1.75batch/s, auc=0.8706, loss=0.5006]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 290/292 [03:19<00:01,  1.76batch/s, auc=0.8706, loss=0.5006]\u001b[A\n",
      "Training Epoch 15/25:  99%|█████████▉| 290/292 [03:19<00:01,  1.76batch/s, auc=0.8707, loss=0.6757]\u001b[A\n",
      "Training Epoch 15/25: 100%|█████████▉| 291/292 [03:19<00:00,  1.76batch/s, auc=0.8707, loss=0.6757]\u001b[A\n",
      "Training Epoch 15/25: 100%|█████████▉| 291/292 [03:20<00:00,  1.76batch/s, auc=0.8708, loss=0.5918]\u001b[A\n",
      "Training Epoch 15/25: 100%|██████████| 292/292 [03:20<00:00,  1.46batch/s, auc=0.8708, loss=0.5918]\u001b[A\n",
      "Epochs:  60%|██████    | 15/25 [53:25<36:05, 216.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25] Train Loss: 0.7552 | Train AUROC: 0.8708 Val Loss: 0.8006 | Val AUROC: 0.8517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 16/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 0/292 [00:02<?, ?batch/s, auc=0.6591, loss=1.0362]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 1/292 [00:02<09:51,  2.03s/batch, auc=0.6591, loss=1.0362]\u001b[A\n",
      "Training Epoch 16/25:   0%|          | 1/292 [00:02<09:51,  2.03s/batch, auc=0.8013, loss=0.7547]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 2/292 [00:02<05:37,  1.16s/batch, auc=0.8013, loss=0.7547]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 2/292 [00:03<05:37,  1.16s/batch, auc=0.8502, loss=0.7169]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 3/292 [00:03<04:15,  1.13batch/s, auc=0.8502, loss=0.7169]\u001b[A\n",
      "Training Epoch 16/25:   1%|          | 3/292 [00:03<04:15,  1.13batch/s, auc=0.8815, loss=0.5444]\u001b[A\n",
      "Training Epoch 16/25:   1%|▏         | 4/292 [00:03<03:37,  1.32batch/s, auc=0.8815, loss=0.5444]\u001b[A\n",
      "Training Epoch 16/25:   1%|▏         | 4/292 [00:04<03:37,  1.32batch/s, auc=0.8881, loss=0.5882]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 5/292 [00:04<03:15,  1.46batch/s, auc=0.8881, loss=0.5882]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 5/292 [00:05<03:15,  1.46batch/s, auc=0.8748, loss=1.0308]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 6/292 [00:05<03:53,  1.23batch/s, auc=0.8748, loss=1.0308]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 6/292 [00:06<03:53,  1.23batch/s, auc=0.8615, loss=0.8806]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 7/292 [00:06<04:15,  1.11batch/s, auc=0.8615, loss=0.8806]\u001b[A\n",
      "Training Epoch 16/25:   2%|▏         | 7/292 [00:07<04:15,  1.11batch/s, auc=0.8487, loss=0.9507]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 8/292 [00:07<04:30,  1.05batch/s, auc=0.8487, loss=0.9507]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 8/292 [00:08<04:30,  1.05batch/s, auc=0.8529, loss=0.5750]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 9/292 [00:08<03:54,  1.21batch/s, auc=0.8529, loss=0.5750]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 9/292 [00:09<03:54,  1.21batch/s, auc=0.8306, loss=1.4760]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 10/292 [00:09<04:14,  1.11batch/s, auc=0.8306, loss=1.4760]\u001b[A\n",
      "Training Epoch 16/25:   3%|▎         | 10/292 [00:09<04:14,  1.11batch/s, auc=0.8327, loss=0.7192]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 11/292 [00:09<03:43,  1.26batch/s, auc=0.8327, loss=0.7192]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 11/292 [00:10<03:43,  1.26batch/s, auc=0.8456, loss=0.6277]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 12/292 [00:10<03:22,  1.38batch/s, auc=0.8456, loss=0.6277]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 12/292 [00:10<03:22,  1.38batch/s, auc=0.8489, loss=0.6283]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 13/292 [00:10<03:07,  1.49batch/s, auc=0.8489, loss=0.6283]\u001b[A\n",
      "Training Epoch 16/25:   4%|▍         | 13/292 [00:11<03:07,  1.49batch/s, auc=0.8483, loss=0.6875]\u001b[A\n",
      "Training Epoch 16/25:   5%|▍         | 14/292 [00:11<02:56,  1.57batch/s, auc=0.8483, loss=0.6875]\u001b[A\n",
      "Training Epoch 16/25:   5%|▍         | 14/292 [00:11<02:56,  1.57batch/s, auc=0.8558, loss=0.5416]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 15/292 [00:11<02:49,  1.64batch/s, auc=0.8558, loss=0.5416]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 15/292 [00:12<02:49,  1.64batch/s, auc=0.8629, loss=0.5343]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 16/292 [00:12<02:44,  1.68batch/s, auc=0.8629, loss=0.5343]\u001b[A\n",
      "Training Epoch 16/25:   5%|▌         | 16/292 [00:12<02:44,  1.68batch/s, auc=0.8668, loss=0.6796]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 17/292 [00:12<02:40,  1.72batch/s, auc=0.8668, loss=0.6796]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 17/292 [00:13<02:40,  1.72batch/s, auc=0.8666, loss=0.6118]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 18/292 [00:13<02:37,  1.74batch/s, auc=0.8666, loss=0.6118]\u001b[A\n",
      "Training Epoch 16/25:   6%|▌         | 18/292 [00:14<02:37,  1.74batch/s, auc=0.8675, loss=0.9461]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 19/292 [00:14<02:35,  1.76batch/s, auc=0.8675, loss=0.9461]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 19/292 [00:14<02:35,  1.76batch/s, auc=0.8719, loss=0.6418]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 20/292 [00:14<02:33,  1.77batch/s, auc=0.8719, loss=0.6418]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 20/292 [00:15<02:33,  1.77batch/s, auc=0.8756, loss=0.6096]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 21/292 [00:15<02:32,  1.78batch/s, auc=0.8756, loss=0.6096]\u001b[A\n",
      "Training Epoch 16/25:   7%|▋         | 21/292 [00:15<02:32,  1.78batch/s, auc=0.8788, loss=0.6130]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 22/292 [00:15<02:31,  1.79batch/s, auc=0.8788, loss=0.6130]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 22/292 [00:16<02:31,  1.79batch/s, auc=0.8773, loss=0.9192]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 23/292 [00:16<03:12,  1.40batch/s, auc=0.8773, loss=0.9192]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 23/292 [00:17<03:12,  1.40batch/s, auc=0.8831, loss=0.5209]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 24/292 [00:17<02:58,  1.50batch/s, auc=0.8831, loss=0.5209]\u001b[A\n",
      "Training Epoch 16/25:   8%|▊         | 24/292 [00:17<02:58,  1.50batch/s, auc=0.8831, loss=0.6456]\u001b[A\n",
      "Training Epoch 16/25:   9%|▊         | 25/292 [00:17<02:48,  1.58batch/s, auc=0.8831, loss=0.6456]\u001b[A\n",
      "Training Epoch 16/25:   9%|▊         | 25/292 [00:18<02:48,  1.58batch/s, auc=0.8841, loss=0.6634]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 26/292 [00:18<02:42,  1.64batch/s, auc=0.8841, loss=0.6634]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 26/292 [00:19<02:42,  1.64batch/s, auc=0.8871, loss=0.5234]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 27/292 [00:19<02:37,  1.69batch/s, auc=0.8871, loss=0.5234]\u001b[A\n",
      "Training Epoch 16/25:   9%|▉         | 27/292 [00:19<02:37,  1.69batch/s, auc=0.8891, loss=0.4958]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 28/292 [00:19<02:33,  1.72batch/s, auc=0.8891, loss=0.4958]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 28/292 [00:20<02:33,  1.72batch/s, auc=0.8893, loss=0.5941]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 29/292 [00:20<02:30,  1.74batch/s, auc=0.8893, loss=0.5941]\u001b[A\n",
      "Training Epoch 16/25:  10%|▉         | 29/292 [00:20<02:30,  1.74batch/s, auc=0.8879, loss=0.6671]\u001b[A\n",
      "Training Epoch 16/25:  10%|█         | 30/292 [00:20<02:28,  1.76batch/s, auc=0.8879, loss=0.6671]\u001b[A\n",
      "Training Epoch 16/25:  10%|█         | 30/292 [00:21<02:28,  1.76batch/s, auc=0.8875, loss=0.7931]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 31/292 [00:21<03:07,  1.39batch/s, auc=0.8875, loss=0.7931]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 31/292 [00:22<03:07,  1.39batch/s, auc=0.8883, loss=0.5118]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 32/292 [00:22<02:54,  1.49batch/s, auc=0.8883, loss=0.5118]\u001b[A\n",
      "Training Epoch 16/25:  11%|█         | 32/292 [00:22<02:54,  1.49batch/s, auc=0.8871, loss=0.7311]\u001b[A\n",
      "Training Epoch 16/25:  11%|█▏        | 33/292 [00:22<02:44,  1.57batch/s, auc=0.8871, loss=0.7311]\u001b[A\n",
      "Training Epoch 16/25:  11%|█▏        | 33/292 [00:23<02:44,  1.57batch/s, auc=0.8914, loss=0.4315]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 34/292 [00:23<02:37,  1.64batch/s, auc=0.8914, loss=0.4315]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 34/292 [00:23<02:37,  1.64batch/s, auc=0.8909, loss=0.6465]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 35/292 [00:23<02:32,  1.68batch/s, auc=0.8909, loss=0.6465]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 35/292 [00:25<02:32,  1.68batch/s, auc=0.8899, loss=0.7933]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 36/292 [00:25<03:08,  1.36batch/s, auc=0.8899, loss=0.7933]\u001b[A\n",
      "Training Epoch 16/25:  12%|█▏        | 36/292 [00:25<03:08,  1.36batch/s, auc=0.8894, loss=0.7032]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 37/292 [00:25<02:54,  1.46batch/s, auc=0.8894, loss=0.7032]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 37/292 [00:26<02:54,  1.46batch/s, auc=0.8877, loss=1.0343]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 38/292 [00:26<02:43,  1.55batch/s, auc=0.8877, loss=1.0343]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 38/292 [00:26<02:43,  1.55batch/s, auc=0.8888, loss=0.7375]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 39/292 [00:26<02:36,  1.62batch/s, auc=0.8888, loss=0.7375]\u001b[A\n",
      "Training Epoch 16/25:  13%|█▎        | 39/292 [00:27<02:36,  1.62batch/s, auc=0.8895, loss=0.7402]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▎        | 40/292 [00:27<02:31,  1.67batch/s, auc=0.8895, loss=0.7402]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▎        | 40/292 [00:27<02:31,  1.67batch/s, auc=0.8886, loss=0.7697]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 41/292 [00:27<02:27,  1.70batch/s, auc=0.8886, loss=0.7697]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 41/292 [00:28<02:27,  1.70batch/s, auc=0.8894, loss=0.5726]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 42/292 [00:28<02:24,  1.73batch/s, auc=0.8894, loss=0.5726]\u001b[A\n",
      "Training Epoch 16/25:  14%|█▍        | 42/292 [00:29<02:24,  1.73batch/s, auc=0.8865, loss=1.1311]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▍        | 43/292 [00:29<03:00,  1.38batch/s, auc=0.8865, loss=1.1311]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▍        | 43/292 [00:30<03:00,  1.38batch/s, auc=0.8871, loss=0.7871]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 44/292 [00:30<02:47,  1.48batch/s, auc=0.8871, loss=0.7871]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 44/292 [00:30<02:47,  1.48batch/s, auc=0.8880, loss=0.4915]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 45/292 [00:30<02:37,  1.56batch/s, auc=0.8880, loss=0.4915]\u001b[A\n",
      "Training Epoch 16/25:  15%|█▌        | 45/292 [00:31<02:37,  1.56batch/s, auc=0.8890, loss=0.5782]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 46/292 [00:31<02:31,  1.63batch/s, auc=0.8890, loss=0.5782]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 46/292 [00:32<02:31,  1.63batch/s, auc=0.8863, loss=1.1052]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 47/292 [00:32<03:04,  1.33batch/s, auc=0.8863, loss=1.1052]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▌        | 47/292 [00:33<03:04,  1.33batch/s, auc=0.8857, loss=0.8661]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▋        | 48/292 [00:33<03:26,  1.18batch/s, auc=0.8857, loss=0.8661]\u001b[A\n",
      "Training Epoch 16/25:  16%|█▋        | 48/292 [00:33<03:26,  1.18batch/s, auc=0.8846, loss=0.8550]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 49/292 [00:33<03:04,  1.32batch/s, auc=0.8846, loss=0.8550]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 49/292 [00:34<03:04,  1.32batch/s, auc=0.8850, loss=0.7305]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 50/292 [00:34<03:26,  1.17batch/s, auc=0.8850, loss=0.7305]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 50/292 [00:35<03:26,  1.17batch/s, auc=0.8872, loss=0.4130]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 51/292 [00:35<03:04,  1.31batch/s, auc=0.8872, loss=0.4130]\u001b[A\n",
      "Training Epoch 16/25:  17%|█▋        | 51/292 [00:36<03:04,  1.31batch/s, auc=0.8883, loss=0.4536]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 52/292 [00:36<02:48,  1.42batch/s, auc=0.8883, loss=0.4536]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 52/292 [00:36<02:48,  1.42batch/s, auc=0.8902, loss=0.4258]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 53/292 [00:36<02:37,  1.52batch/s, auc=0.8902, loss=0.4258]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 53/292 [00:37<02:37,  1.52batch/s, auc=0.8893, loss=0.9212]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 54/292 [00:37<02:29,  1.59batch/s, auc=0.8893, loss=0.9212]\u001b[A\n",
      "Training Epoch 16/25:  18%|█▊        | 54/292 [00:37<02:29,  1.59batch/s, auc=0.8865, loss=1.1269]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 55/292 [00:37<02:23,  1.65batch/s, auc=0.8865, loss=1.1269]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 55/292 [00:38<02:23,  1.65batch/s, auc=0.8883, loss=0.3793]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 56/292 [00:38<02:19,  1.69batch/s, auc=0.8883, loss=0.3793]\u001b[A\n",
      "Training Epoch 16/25:  19%|█▉        | 56/292 [00:38<02:19,  1.69batch/s, auc=0.8874, loss=0.7758]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 57/292 [00:38<02:16,  1.72batch/s, auc=0.8874, loss=0.7758]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 57/292 [00:39<02:16,  1.72batch/s, auc=0.8880, loss=0.9645]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 58/292 [00:39<02:14,  1.74batch/s, auc=0.8880, loss=0.9645]\u001b[A\n",
      "Training Epoch 16/25:  20%|█▉        | 58/292 [00:39<02:14,  1.74batch/s, auc=0.8876, loss=0.7303]\u001b[A\n",
      "Training Epoch 16/25:  20%|██        | 59/292 [00:39<02:12,  1.76batch/s, auc=0.8876, loss=0.7303]\u001b[A\n",
      "Training Epoch 16/25:  20%|██        | 59/292 [00:40<02:12,  1.76batch/s, auc=0.8876, loss=0.7242]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 60/292 [00:40<02:11,  1.77batch/s, auc=0.8876, loss=0.7242]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 60/292 [00:41<02:11,  1.77batch/s, auc=0.8875, loss=0.8510]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 61/292 [00:41<02:09,  1.78batch/s, auc=0.8875, loss=0.8510]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 61/292 [00:41<02:09,  1.78batch/s, auc=0.8867, loss=0.8862]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 62/292 [00:41<02:08,  1.78batch/s, auc=0.8867, loss=0.8862]\u001b[A\n",
      "Training Epoch 16/25:  21%|██        | 62/292 [00:42<02:08,  1.78batch/s, auc=0.8864, loss=0.8291]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 63/292 [00:42<02:07,  1.79batch/s, auc=0.8864, loss=0.8291]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 63/292 [00:42<02:07,  1.79batch/s, auc=0.8869, loss=0.5715]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 64/292 [00:42<02:07,  1.79batch/s, auc=0.8869, loss=0.5715]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 64/292 [00:43<02:07,  1.79batch/s, auc=0.8870, loss=0.8493]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 65/292 [00:43<02:06,  1.79batch/s, auc=0.8870, loss=0.8493]\u001b[A\n",
      "Training Epoch 16/25:  22%|██▏       | 65/292 [00:43<02:06,  1.79batch/s, auc=0.8873, loss=0.5930]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 66/292 [00:43<02:05,  1.79batch/s, auc=0.8873, loss=0.5930]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 66/292 [00:44<02:05,  1.79batch/s, auc=0.8873, loss=0.7046]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 67/292 [00:44<02:05,  1.79batch/s, auc=0.8873, loss=0.7046]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 67/292 [00:45<02:05,  1.79batch/s, auc=0.8865, loss=0.8069]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 68/292 [00:45<02:39,  1.41batch/s, auc=0.8865, loss=0.8069]\u001b[A\n",
      "Training Epoch 16/25:  23%|██▎       | 68/292 [00:46<02:39,  1.41batch/s, auc=0.8855, loss=1.0201]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▎       | 69/292 [00:46<03:02,  1.22batch/s, auc=0.8855, loss=1.0201]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▎       | 69/292 [00:47<03:02,  1.22batch/s, auc=0.8853, loss=0.6334]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 70/292 [00:47<02:44,  1.35batch/s, auc=0.8853, loss=0.6334]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 70/292 [00:47<02:44,  1.35batch/s, auc=0.8855, loss=0.6522]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 71/292 [00:47<02:31,  1.46batch/s, auc=0.8855, loss=0.6522]\u001b[A\n",
      "Training Epoch 16/25:  24%|██▍       | 71/292 [00:48<02:31,  1.46batch/s, auc=0.8855, loss=0.5951]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▍       | 72/292 [00:48<02:22,  1.54batch/s, auc=0.8855, loss=0.5951]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▍       | 72/292 [00:48<02:22,  1.54batch/s, auc=0.8860, loss=0.6435]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 73/292 [00:48<02:15,  1.61batch/s, auc=0.8860, loss=0.6435]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 73/292 [00:49<02:15,  1.61batch/s, auc=0.8859, loss=0.7873]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 74/292 [00:49<02:11,  1.66batch/s, auc=0.8859, loss=0.7873]\u001b[A\n",
      "Training Epoch 16/25:  25%|██▌       | 74/292 [00:49<02:11,  1.66batch/s, auc=0.8859, loss=0.6172]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 75/292 [00:49<02:07,  1.70batch/s, auc=0.8859, loss=0.6172]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 75/292 [00:50<02:07,  1.70batch/s, auc=0.8862, loss=0.5778]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 76/292 [00:50<02:05,  1.73batch/s, auc=0.8862, loss=0.5778]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▌       | 76/292 [00:51<02:05,  1.73batch/s, auc=0.8850, loss=1.0354]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▋       | 77/292 [00:51<02:36,  1.38batch/s, auc=0.8850, loss=1.0354]\u001b[A\n",
      "Training Epoch 16/25:  26%|██▋       | 77/292 [00:52<02:36,  1.38batch/s, auc=0.8836, loss=1.0458]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 78/292 [00:52<02:57,  1.20batch/s, auc=0.8836, loss=1.0458]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 78/292 [00:53<02:57,  1.20batch/s, auc=0.8834, loss=0.5660]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 79/292 [00:53<02:39,  1.34batch/s, auc=0.8834, loss=0.5660]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 79/292 [00:53<02:39,  1.34batch/s, auc=0.8822, loss=0.9904]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 80/292 [00:53<02:26,  1.45batch/s, auc=0.8822, loss=0.9904]\u001b[A\n",
      "Training Epoch 16/25:  27%|██▋       | 80/292 [00:54<02:26,  1.45batch/s, auc=0.8827, loss=0.5940]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 81/292 [00:54<02:17,  1.54batch/s, auc=0.8827, loss=0.5940]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 81/292 [00:54<02:17,  1.54batch/s, auc=0.8821, loss=0.7466]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 82/292 [00:54<02:10,  1.61batch/s, auc=0.8821, loss=0.7466]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 82/292 [00:55<02:10,  1.61batch/s, auc=0.8825, loss=0.5638]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 83/292 [00:55<02:05,  1.66batch/s, auc=0.8825, loss=0.5638]\u001b[A\n",
      "Training Epoch 16/25:  28%|██▊       | 83/292 [00:55<02:05,  1.66batch/s, auc=0.8831, loss=0.5473]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 84/292 [00:55<02:02,  1.70batch/s, auc=0.8831, loss=0.5473]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 84/292 [00:56<02:02,  1.70batch/s, auc=0.8836, loss=0.5761]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 85/292 [00:56<01:59,  1.73batch/s, auc=0.8836, loss=0.5761]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 85/292 [00:57<01:59,  1.73batch/s, auc=0.8834, loss=0.7207]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 86/292 [00:57<01:58,  1.75batch/s, auc=0.8834, loss=0.7207]\u001b[A\n",
      "Training Epoch 16/25:  29%|██▉       | 86/292 [00:58<01:58,  1.75batch/s, auc=0.8815, loss=1.6638]\u001b[A\n",
      "Training Epoch 16/25:  30%|██▉       | 87/292 [00:58<02:28,  1.38batch/s, auc=0.8815, loss=1.6638]\u001b[A\n",
      "Training Epoch 16/25:  30%|██▉       | 87/292 [00:58<02:28,  1.38batch/s, auc=0.8818, loss=0.6489]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 88/292 [00:58<02:17,  1.48batch/s, auc=0.8818, loss=0.6489]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 88/292 [00:59<02:17,  1.48batch/s, auc=0.8818, loss=0.6451]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 89/292 [00:59<02:09,  1.57batch/s, auc=0.8818, loss=0.6451]\u001b[A\n",
      "Training Epoch 16/25:  30%|███       | 89/292 [01:00<02:09,  1.57batch/s, auc=0.8809, loss=1.1914]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 90/292 [01:00<02:35,  1.30batch/s, auc=0.8809, loss=1.1914]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 90/292 [01:00<02:35,  1.30batch/s, auc=0.8810, loss=0.5509]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 91/292 [01:00<02:21,  1.42batch/s, auc=0.8810, loss=0.5509]\u001b[A\n",
      "Training Epoch 16/25:  31%|███       | 91/292 [01:01<02:21,  1.42batch/s, auc=0.8817, loss=0.5253]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 92/292 [01:01<02:12,  1.51batch/s, auc=0.8817, loss=0.5253]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 92/292 [01:02<02:12,  1.51batch/s, auc=0.8811, loss=0.8258]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 93/292 [01:02<02:36,  1.28batch/s, auc=0.8811, loss=0.8258]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 93/292 [01:03<02:36,  1.28batch/s, auc=0.8813, loss=0.6948]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 94/292 [01:03<02:52,  1.15batch/s, auc=0.8813, loss=0.6948]\u001b[A\n",
      "Training Epoch 16/25:  32%|███▏      | 94/292 [01:04<02:52,  1.15batch/s, auc=0.8820, loss=0.5595]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 95/292 [01:04<02:33,  1.29batch/s, auc=0.8820, loss=0.5595]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 95/292 [01:05<02:33,  1.29batch/s, auc=0.8809, loss=0.9856]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 96/292 [01:05<02:49,  1.15batch/s, auc=0.8809, loss=0.9856]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 96/292 [01:06<02:49,  1.15batch/s, auc=0.8799, loss=1.0008]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 97/292 [01:06<03:01,  1.08batch/s, auc=0.8799, loss=1.0008]\u001b[A\n",
      "Training Epoch 16/25:  33%|███▎      | 97/292 [01:06<03:01,  1.08batch/s, auc=0.8806, loss=0.4898]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▎      | 98/292 [01:06<02:38,  1.22batch/s, auc=0.8806, loss=0.4898]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▎      | 98/292 [01:07<02:38,  1.22batch/s, auc=0.8797, loss=0.9517]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 99/292 [01:07<02:52,  1.12batch/s, auc=0.8797, loss=0.9517]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 99/292 [01:08<02:52,  1.12batch/s, auc=0.8797, loss=0.8039]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 100/292 [01:08<02:32,  1.26batch/s, auc=0.8797, loss=0.8039]\u001b[A\n",
      "Training Epoch 16/25:  34%|███▍      | 100/292 [01:09<02:32,  1.26batch/s, auc=0.8797, loss=0.5414]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 101/292 [01:09<02:18,  1.38batch/s, auc=0.8797, loss=0.5414]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 101/292 [01:09<02:18,  1.38batch/s, auc=0.8799, loss=0.6478]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 102/292 [01:09<02:07,  1.49batch/s, auc=0.8799, loss=0.6478]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▍      | 102/292 [01:10<02:07,  1.49batch/s, auc=0.8803, loss=0.6454]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▌      | 103/292 [01:10<02:00,  1.57batch/s, auc=0.8803, loss=0.6454]\u001b[A\n",
      "Training Epoch 16/25:  35%|███▌      | 103/292 [01:10<02:00,  1.57batch/s, auc=0.8810, loss=0.6159]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 104/292 [01:10<01:55,  1.63batch/s, auc=0.8810, loss=0.6159]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 104/292 [01:11<01:55,  1.63batch/s, auc=0.8806, loss=0.9637]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 105/292 [01:11<01:51,  1.68batch/s, auc=0.8806, loss=0.9637]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▌      | 105/292 [01:11<01:51,  1.68batch/s, auc=0.8802, loss=0.8053]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▋      | 106/292 [01:11<01:48,  1.71batch/s, auc=0.8802, loss=0.8053]\u001b[A\n",
      "Training Epoch 16/25:  36%|███▋      | 106/292 [01:12<01:48,  1.71batch/s, auc=0.8803, loss=0.6414]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 107/292 [01:12<01:46,  1.74batch/s, auc=0.8803, loss=0.6414]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 107/292 [01:12<01:46,  1.74batch/s, auc=0.8805, loss=0.6883]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 108/292 [01:12<01:44,  1.75batch/s, auc=0.8805, loss=0.6883]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 108/292 [01:13<01:44,  1.75batch/s, auc=0.8793, loss=1.1060]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 109/292 [01:13<02:12,  1.38batch/s, auc=0.8793, loss=1.1060]\u001b[A\n",
      "Training Epoch 16/25:  37%|███▋      | 109/292 [01:14<02:12,  1.38batch/s, auc=0.8800, loss=0.4189]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 110/292 [01:14<02:02,  1.48batch/s, auc=0.8800, loss=0.4189]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 110/292 [01:15<02:02,  1.48batch/s, auc=0.8783, loss=1.4706]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 111/292 [01:15<02:23,  1.26batch/s, auc=0.8783, loss=1.4706]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 111/292 [01:16<02:23,  1.26batch/s, auc=0.8780, loss=0.7020]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 112/292 [01:16<02:10,  1.38batch/s, auc=0.8780, loss=0.7020]\u001b[A\n",
      "Training Epoch 16/25:  38%|███▊      | 112/292 [01:17<02:10,  1.38batch/s, auc=0.8771, loss=0.9665]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▊      | 113/292 [01:17<02:28,  1.21batch/s, auc=0.8771, loss=0.9665]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▊      | 113/292 [01:17<02:28,  1.21batch/s, auc=0.8768, loss=0.7370]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 114/292 [01:17<02:13,  1.34batch/s, auc=0.8768, loss=0.7370]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 114/292 [01:18<02:13,  1.34batch/s, auc=0.8768, loss=0.7195]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 115/292 [01:18<02:02,  1.45batch/s, auc=0.8768, loss=0.7195]\u001b[A\n",
      "Training Epoch 16/25:  39%|███▉      | 115/292 [01:18<02:02,  1.45batch/s, auc=0.8770, loss=0.5948]\u001b[A\n",
      "Training Epoch 16/25:  40%|███▉      | 116/292 [01:18<01:54,  1.54batch/s, auc=0.8770, loss=0.5948]\u001b[A\n",
      "Training Epoch 16/25:  40%|███▉      | 116/292 [01:19<01:54,  1.54batch/s, auc=0.8780, loss=0.4190]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 117/292 [01:19<01:49,  1.60batch/s, auc=0.8780, loss=0.4190]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 117/292 [01:20<01:49,  1.60batch/s, auc=0.8767, loss=1.3256]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 118/292 [01:20<02:11,  1.32batch/s, auc=0.8767, loss=1.3256]\u001b[A\n",
      "Training Epoch 16/25:  40%|████      | 118/292 [01:21<02:11,  1.32batch/s, auc=0.8758, loss=0.8824]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 119/292 [01:21<02:27,  1.17batch/s, auc=0.8758, loss=0.8824]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 119/292 [01:22<02:27,  1.17batch/s, auc=0.8762, loss=0.6661]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 120/292 [01:22<02:11,  1.31batch/s, auc=0.8762, loss=0.6661]\u001b[A\n",
      "Training Epoch 16/25:  41%|████      | 120/292 [01:22<02:11,  1.31batch/s, auc=0.8767, loss=0.5564]\u001b[A\n",
      "Training Epoch 16/25:  41%|████▏     | 121/292 [01:22<02:00,  1.42batch/s, auc=0.8767, loss=0.5564]\u001b[A\n",
      "Training Epoch 16/25:  41%|████▏     | 121/292 [01:23<02:00,  1.42batch/s, auc=0.8765, loss=0.7632]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 122/292 [01:23<01:52,  1.52batch/s, auc=0.8765, loss=0.7632]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 122/292 [01:23<01:52,  1.52batch/s, auc=0.8768, loss=0.6566]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 123/292 [01:23<01:46,  1.59batch/s, auc=0.8768, loss=0.6566]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 123/292 [01:24<01:46,  1.59batch/s, auc=0.8777, loss=0.6508]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 124/292 [01:24<01:42,  1.64batch/s, auc=0.8777, loss=0.6508]\u001b[A\n",
      "Training Epoch 16/25:  42%|████▏     | 124/292 [01:25<01:42,  1.64batch/s, auc=0.8770, loss=0.9297]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 125/292 [01:25<02:04,  1.34batch/s, auc=0.8770, loss=0.9297]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 125/292 [01:26<02:04,  1.34batch/s, auc=0.8778, loss=0.4120]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 126/292 [01:26<01:54,  1.45batch/s, auc=0.8778, loss=0.4120]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 126/292 [01:26<01:54,  1.45batch/s, auc=0.8779, loss=0.6887]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 127/292 [01:26<01:47,  1.53batch/s, auc=0.8779, loss=0.6887]\u001b[A\n",
      "Training Epoch 16/25:  43%|████▎     | 127/292 [01:27<01:47,  1.53batch/s, auc=0.8782, loss=0.6468]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 128/292 [01:27<01:42,  1.60batch/s, auc=0.8782, loss=0.6468]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 128/292 [01:27<01:42,  1.60batch/s, auc=0.8785, loss=0.7665]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 129/292 [01:27<01:38,  1.65batch/s, auc=0.8785, loss=0.7665]\u001b[A\n",
      "Training Epoch 16/25:  44%|████▍     | 129/292 [01:28<01:38,  1.65batch/s, auc=0.8773, loss=1.4332]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 130/292 [01:28<02:00,  1.34batch/s, auc=0.8773, loss=1.4332]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 130/292 [01:29<02:00,  1.34batch/s, auc=0.8776, loss=0.7662]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 131/292 [01:29<01:51,  1.45batch/s, auc=0.8776, loss=0.7662]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▍     | 131/292 [01:29<01:51,  1.45batch/s, auc=0.8772, loss=0.7276]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▌     | 132/292 [01:29<01:44,  1.54batch/s, auc=0.8772, loss=0.7276]\u001b[A\n",
      "Training Epoch 16/25:  45%|████▌     | 132/292 [01:31<01:44,  1.54batch/s, auc=0.8768, loss=0.8666]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 133/292 [01:31<02:03,  1.28batch/s, auc=0.8768, loss=0.8666]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 133/292 [01:31<02:03,  1.28batch/s, auc=0.8767, loss=0.6877]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 134/292 [01:31<01:52,  1.40batch/s, auc=0.8767, loss=0.6877]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 134/292 [01:32<01:52,  1.40batch/s, auc=0.8767, loss=0.6773]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 135/292 [01:32<01:44,  1.50batch/s, auc=0.8767, loss=0.6773]\u001b[A\n",
      "Training Epoch 16/25:  46%|████▌     | 135/292 [01:32<01:44,  1.50batch/s, auc=0.8776, loss=0.4298]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 136/292 [01:32<01:38,  1.58batch/s, auc=0.8776, loss=0.4298]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 136/292 [01:33<01:38,  1.58batch/s, auc=0.8775, loss=0.6906]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 137/292 [01:33<01:34,  1.63batch/s, auc=0.8775, loss=0.6906]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 137/292 [01:33<01:34,  1.63batch/s, auc=0.8777, loss=0.7353]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 138/292 [01:33<01:31,  1.68batch/s, auc=0.8777, loss=0.7353]\u001b[A\n",
      "Training Epoch 16/25:  47%|████▋     | 138/292 [01:34<01:31,  1.68batch/s, auc=0.8777, loss=0.6600]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 139/292 [01:34<01:29,  1.71batch/s, auc=0.8777, loss=0.6600]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 139/292 [01:35<01:29,  1.71batch/s, auc=0.8770, loss=1.0702]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 140/292 [01:35<01:51,  1.37batch/s, auc=0.8770, loss=1.0702]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 140/292 [01:35<01:51,  1.37batch/s, auc=0.8773, loss=0.5192]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 141/292 [01:35<01:42,  1.47batch/s, auc=0.8773, loss=0.5192]\u001b[A\n",
      "Training Epoch 16/25:  48%|████▊     | 141/292 [01:36<01:42,  1.47batch/s, auc=0.8772, loss=0.6805]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▊     | 142/292 [01:36<01:36,  1.55batch/s, auc=0.8772, loss=0.6805]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▊     | 142/292 [01:37<01:36,  1.55batch/s, auc=0.8775, loss=0.5118]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 143/292 [01:37<01:32,  1.62batch/s, auc=0.8775, loss=0.5118]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 143/292 [01:37<01:32,  1.62batch/s, auc=0.8782, loss=0.4834]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 144/292 [01:37<01:28,  1.66batch/s, auc=0.8782, loss=0.4834]\u001b[A\n",
      "Training Epoch 16/25:  49%|████▉     | 144/292 [01:38<01:28,  1.66batch/s, auc=0.8790, loss=0.4968]\u001b[A\n",
      "Training Epoch 16/25:  50%|████▉     | 145/292 [01:38<01:26,  1.70batch/s, auc=0.8790, loss=0.4968]\u001b[A\n",
      "Training Epoch 16/25:  50%|████▉     | 145/292 [01:38<01:26,  1.70batch/s, auc=0.8790, loss=0.6870]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 146/292 [01:38<01:24,  1.72batch/s, auc=0.8790, loss=0.6870]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 146/292 [01:39<01:24,  1.72batch/s, auc=0.8784, loss=0.9987]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 147/292 [01:39<01:45,  1.37batch/s, auc=0.8784, loss=0.9987]\u001b[A\n",
      "Training Epoch 16/25:  50%|█████     | 147/292 [01:40<01:45,  1.37batch/s, auc=0.8775, loss=1.2553]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 148/292 [01:40<02:00,  1.20batch/s, auc=0.8775, loss=1.2553]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 148/292 [01:41<02:00,  1.20batch/s, auc=0.8776, loss=0.6705]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 149/292 [01:41<01:47,  1.33batch/s, auc=0.8776, loss=0.6705]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████     | 149/292 [01:42<01:47,  1.33batch/s, auc=0.8780, loss=0.5145]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████▏    | 150/292 [01:42<01:38,  1.44batch/s, auc=0.8780, loss=0.5145]\u001b[A\n",
      "Training Epoch 16/25:  51%|█████▏    | 150/292 [01:42<01:38,  1.44batch/s, auc=0.8789, loss=0.4828]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 151/292 [01:42<01:32,  1.53batch/s, auc=0.8789, loss=0.4828]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 151/292 [01:43<01:32,  1.53batch/s, auc=0.8789, loss=0.8219]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 152/292 [01:43<01:49,  1.28batch/s, auc=0.8789, loss=0.8219]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 152/292 [01:44<01:49,  1.28batch/s, auc=0.8792, loss=0.6395]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 153/292 [01:44<01:39,  1.40batch/s, auc=0.8792, loss=0.6395]\u001b[A\n",
      "Training Epoch 16/25:  52%|█████▏    | 153/292 [01:45<01:39,  1.40batch/s, auc=0.8778, loss=1.3399]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 154/292 [01:45<01:53,  1.21batch/s, auc=0.8778, loss=1.3399]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 154/292 [01:45<01:53,  1.21batch/s, auc=0.8784, loss=0.5265]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 155/292 [01:45<01:41,  1.34batch/s, auc=0.8784, loss=0.5265]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 155/292 [01:46<01:41,  1.34batch/s, auc=0.8782, loss=0.7512]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 156/292 [01:46<01:33,  1.45batch/s, auc=0.8782, loss=0.7512]\u001b[A\n",
      "Training Epoch 16/25:  53%|█████▎    | 156/292 [01:47<01:33,  1.45batch/s, auc=0.8782, loss=0.7115]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 157/292 [01:47<01:27,  1.54batch/s, auc=0.8782, loss=0.7115]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 157/292 [01:47<01:27,  1.54batch/s, auc=0.8780, loss=0.8307]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 158/292 [01:47<01:23,  1.60batch/s, auc=0.8780, loss=0.8307]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 158/292 [01:48<01:23,  1.60batch/s, auc=0.8778, loss=0.6820]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 159/292 [01:48<01:20,  1.65batch/s, auc=0.8778, loss=0.6820]\u001b[A\n",
      "Training Epoch 16/25:  54%|█████▍    | 159/292 [01:48<01:20,  1.65batch/s, auc=0.8779, loss=0.7075]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▍    | 160/292 [01:48<01:18,  1.69batch/s, auc=0.8779, loss=0.7075]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▍    | 160/292 [01:49<01:18,  1.69batch/s, auc=0.8783, loss=0.4944]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 161/292 [01:49<01:16,  1.72batch/s, auc=0.8783, loss=0.4944]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 161/292 [01:49<01:16,  1.72batch/s, auc=0.8788, loss=0.5051]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 162/292 [01:49<01:14,  1.74batch/s, auc=0.8788, loss=0.5051]\u001b[A\n",
      "Training Epoch 16/25:  55%|█████▌    | 162/292 [01:50<01:14,  1.74batch/s, auc=0.8782, loss=1.1658]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 163/292 [01:50<01:33,  1.38batch/s, auc=0.8782, loss=1.1658]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 163/292 [01:51<01:33,  1.38batch/s, auc=0.8782, loss=0.7322]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 164/292 [01:51<01:26,  1.48batch/s, auc=0.8782, loss=0.7322]\u001b[A\n",
      "Training Epoch 16/25:  56%|█████▌    | 164/292 [01:52<01:26,  1.48batch/s, auc=0.8786, loss=0.5131]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 165/292 [01:52<01:21,  1.56batch/s, auc=0.8786, loss=0.5131]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 165/292 [01:53<01:21,  1.56batch/s, auc=0.8781, loss=0.9609]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 166/292 [01:53<01:37,  1.30batch/s, auc=0.8781, loss=0.9609]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 166/292 [01:54<01:37,  1.30batch/s, auc=0.8771, loss=1.2305]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 167/292 [01:54<01:47,  1.16batch/s, auc=0.8771, loss=1.2305]\u001b[A\n",
      "Training Epoch 16/25:  57%|█████▋    | 167/292 [01:54<01:47,  1.16batch/s, auc=0.8771, loss=0.9113]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 168/292 [01:54<01:35,  1.29batch/s, auc=0.8771, loss=0.9113]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 168/292 [01:55<01:35,  1.29batch/s, auc=0.8767, loss=0.8322]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 169/292 [01:55<01:46,  1.16batch/s, auc=0.8767, loss=0.8322]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 169/292 [01:56<01:46,  1.16batch/s, auc=0.8770, loss=0.5562]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 170/292 [01:56<01:34,  1.29batch/s, auc=0.8770, loss=0.5562]\u001b[A\n",
      "Training Epoch 16/25:  58%|█████▊    | 170/292 [01:56<01:34,  1.29batch/s, auc=0.8774, loss=0.5297]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▊    | 171/292 [01:56<01:25,  1.41batch/s, auc=0.8774, loss=0.5297]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▊    | 171/292 [01:57<01:25,  1.41batch/s, auc=0.8773, loss=0.5795]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 172/292 [01:57<01:19,  1.51batch/s, auc=0.8773, loss=0.5795]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 172/292 [01:58<01:19,  1.51batch/s, auc=0.8771, loss=0.8851]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 173/292 [01:58<01:33,  1.27batch/s, auc=0.8771, loss=0.8851]\u001b[A\n",
      "Training Epoch 16/25:  59%|█████▉    | 173/292 [01:59<01:33,  1.27batch/s, auc=0.8770, loss=0.7795]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 174/292 [01:59<01:24,  1.39batch/s, auc=0.8770, loss=0.7795]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 174/292 [01:59<01:24,  1.39batch/s, auc=0.8766, loss=0.8695]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 175/292 [01:59<01:18,  1.49batch/s, auc=0.8766, loss=0.8695]\u001b[A\n",
      "Training Epoch 16/25:  60%|█████▉    | 175/292 [02:00<01:18,  1.49batch/s, auc=0.8769, loss=0.5352]\u001b[A\n",
      "Training Epoch 16/25:  60%|██████    | 176/292 [02:00<01:14,  1.57batch/s, auc=0.8769, loss=0.5352]\u001b[A\n",
      "Training Epoch 16/25:  60%|██████    | 176/292 [02:00<01:14,  1.57batch/s, auc=0.8767, loss=0.8065]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 177/292 [02:00<01:10,  1.62batch/s, auc=0.8767, loss=0.8065]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 177/292 [02:01<01:10,  1.62batch/s, auc=0.8773, loss=0.4752]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 178/292 [02:01<01:08,  1.67batch/s, auc=0.8773, loss=0.4752]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████    | 178/292 [02:01<01:08,  1.67batch/s, auc=0.8771, loss=0.8872]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████▏   | 179/292 [02:01<01:06,  1.70batch/s, auc=0.8771, loss=0.8872]\u001b[A\n",
      "Training Epoch 16/25:  61%|██████▏   | 179/292 [02:02<01:06,  1.70batch/s, auc=0.8775, loss=0.5745]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 180/292 [02:02<01:04,  1.73batch/s, auc=0.8775, loss=0.5745]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 180/292 [02:03<01:04,  1.73batch/s, auc=0.8773, loss=0.7286]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 181/292 [02:03<01:03,  1.74batch/s, auc=0.8773, loss=0.7286]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 181/292 [02:03<01:03,  1.74batch/s, auc=0.8772, loss=0.6420]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 182/292 [02:03<01:02,  1.76batch/s, auc=0.8772, loss=0.6420]\u001b[A\n",
      "Training Epoch 16/25:  62%|██████▏   | 182/292 [02:04<01:02,  1.76batch/s, auc=0.8770, loss=0.9007]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 183/292 [02:04<01:01,  1.76batch/s, auc=0.8770, loss=0.9007]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 183/292 [02:04<01:01,  1.76batch/s, auc=0.8762, loss=1.2853]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 184/292 [02:04<01:00,  1.77batch/s, auc=0.8762, loss=1.2853]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 184/292 [02:05<01:00,  1.77batch/s, auc=0.8765, loss=0.5753]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 185/292 [02:05<01:00,  1.77batch/s, auc=0.8765, loss=0.5753]\u001b[A\n",
      "Training Epoch 16/25:  63%|██████▎   | 185/292 [02:05<01:00,  1.77batch/s, auc=0.8772, loss=0.4346]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▎   | 186/292 [02:05<00:59,  1.78batch/s, auc=0.8772, loss=0.4346]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▎   | 186/292 [02:06<00:59,  1.78batch/s, auc=0.8769, loss=0.8508]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 187/292 [02:06<00:59,  1.78batch/s, auc=0.8769, loss=0.8508]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 187/292 [02:06<00:59,  1.78batch/s, auc=0.8772, loss=0.4488]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 188/292 [02:06<00:58,  1.78batch/s, auc=0.8772, loss=0.4488]\u001b[A\n",
      "Training Epoch 16/25:  64%|██████▍   | 188/292 [02:07<00:58,  1.78batch/s, auc=0.8775, loss=0.4786]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▍   | 189/292 [02:07<00:57,  1.78batch/s, auc=0.8775, loss=0.4786]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▍   | 189/292 [02:08<00:57,  1.78batch/s, auc=0.8770, loss=1.0964]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 190/292 [02:08<01:13,  1.40batch/s, auc=0.8770, loss=1.0964]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 190/292 [02:09<01:13,  1.40batch/s, auc=0.8774, loss=0.5412]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 191/292 [02:09<01:07,  1.49batch/s, auc=0.8774, loss=0.5412]\u001b[A\n",
      "Training Epoch 16/25:  65%|██████▌   | 191/292 [02:09<01:07,  1.49batch/s, auc=0.8775, loss=0.7379]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 192/292 [02:09<01:03,  1.57batch/s, auc=0.8775, loss=0.7379]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 192/292 [02:10<01:03,  1.57batch/s, auc=0.8775, loss=0.7402]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 193/292 [02:10<01:00,  1.63batch/s, auc=0.8775, loss=0.7402]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▌   | 193/292 [02:10<01:00,  1.63batch/s, auc=0.8778, loss=0.5878]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▋   | 194/292 [02:10<00:58,  1.67batch/s, auc=0.8778, loss=0.5878]\u001b[A\n",
      "Training Epoch 16/25:  66%|██████▋   | 194/292 [02:11<00:58,  1.67batch/s, auc=0.8774, loss=0.9795]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 195/292 [02:11<01:12,  1.35batch/s, auc=0.8774, loss=0.9795]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 195/292 [02:12<01:12,  1.35batch/s, auc=0.8776, loss=0.7373]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 196/292 [02:12<01:06,  1.45batch/s, auc=0.8776, loss=0.7373]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 196/292 [02:13<01:06,  1.45batch/s, auc=0.8778, loss=0.6790]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 197/292 [02:13<01:01,  1.54batch/s, auc=0.8778, loss=0.6790]\u001b[A\n",
      "Training Epoch 16/25:  67%|██████▋   | 197/292 [02:13<01:01,  1.54batch/s, auc=0.8776, loss=0.7314]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 198/292 [02:13<00:58,  1.60batch/s, auc=0.8776, loss=0.7314]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 198/292 [02:14<00:58,  1.60batch/s, auc=0.8781, loss=0.5790]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 199/292 [02:14<00:56,  1.65batch/s, auc=0.8781, loss=0.5790]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 199/292 [02:14<00:56,  1.65batch/s, auc=0.8780, loss=0.7570]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 200/292 [02:14<00:54,  1.69batch/s, auc=0.8780, loss=0.7570]\u001b[A\n",
      "Training Epoch 16/25:  68%|██████▊   | 200/292 [02:15<00:54,  1.69batch/s, auc=0.8777, loss=0.8571]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 201/292 [02:15<01:07,  1.36batch/s, auc=0.8777, loss=0.8571]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 201/292 [02:16<01:07,  1.36batch/s, auc=0.8776, loss=0.7343]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 202/292 [02:16<01:15,  1.19batch/s, auc=0.8776, loss=0.7343]\u001b[A\n",
      "Training Epoch 16/25:  69%|██████▉   | 202/292 [02:17<01:15,  1.19batch/s, auc=0.8776, loss=0.8255]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 203/292 [02:17<01:07,  1.32batch/s, auc=0.8776, loss=0.8255]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 203/292 [02:18<01:07,  1.32batch/s, auc=0.8771, loss=0.8441]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 204/292 [02:18<01:01,  1.43batch/s, auc=0.8771, loss=0.8441]\u001b[A\n",
      "Training Epoch 16/25:  70%|██████▉   | 204/292 [02:18<01:01,  1.43batch/s, auc=0.8774, loss=0.6192]\u001b[A\n",
      "Training Epoch 16/25:  70%|███████   | 205/292 [02:18<00:57,  1.52batch/s, auc=0.8774, loss=0.6192]\u001b[A\n",
      "Training Epoch 16/25:  70%|███████   | 205/292 [02:19<00:57,  1.52batch/s, auc=0.8772, loss=0.7858]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 206/292 [02:19<00:54,  1.59batch/s, auc=0.8772, loss=0.7858]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 206/292 [02:20<00:54,  1.59batch/s, auc=0.8766, loss=1.2594]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 207/292 [02:20<01:04,  1.31batch/s, auc=0.8766, loss=1.2594]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 207/292 [02:20<01:04,  1.31batch/s, auc=0.8766, loss=0.6046]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 208/292 [02:20<00:58,  1.42batch/s, auc=0.8766, loss=0.6046]\u001b[A\n",
      "Training Epoch 16/25:  71%|███████   | 208/292 [02:21<00:58,  1.42batch/s, auc=0.8763, loss=0.8595]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 209/292 [02:21<00:54,  1.51batch/s, auc=0.8763, loss=0.8595]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 209/292 [02:21<00:54,  1.51batch/s, auc=0.8764, loss=0.7062]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 210/292 [02:21<00:51,  1.58batch/s, auc=0.8764, loss=0.7062]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 210/292 [02:22<00:51,  1.58batch/s, auc=0.8758, loss=1.1672]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 211/292 [02:22<01:02,  1.31batch/s, auc=0.8758, loss=1.1672]\u001b[A\n",
      "Training Epoch 16/25:  72%|███████▏  | 211/292 [02:23<01:02,  1.31batch/s, auc=0.8760, loss=0.6077]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 212/292 [02:23<00:56,  1.42batch/s, auc=0.8760, loss=0.6077]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 212/292 [02:24<00:56,  1.42batch/s, auc=0.8762, loss=0.5062]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 213/292 [02:24<00:52,  1.51batch/s, auc=0.8762, loss=0.5062]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 213/292 [02:24<00:52,  1.51batch/s, auc=0.8765, loss=0.5504]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 214/292 [02:24<00:49,  1.58batch/s, auc=0.8765, loss=0.5504]\u001b[A\n",
      "Training Epoch 16/25:  73%|███████▎  | 214/292 [02:25<00:49,  1.58batch/s, auc=0.8765, loss=0.4739]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▎  | 215/292 [02:25<00:47,  1.64batch/s, auc=0.8765, loss=0.4739]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▎  | 215/292 [02:25<00:47,  1.64batch/s, auc=0.8761, loss=0.9039]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 216/292 [02:25<00:45,  1.68batch/s, auc=0.8761, loss=0.9039]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 216/292 [02:26<00:45,  1.68batch/s, auc=0.8759, loss=0.7585]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 217/292 [02:26<00:43,  1.70batch/s, auc=0.8759, loss=0.7585]\u001b[A\n",
      "Training Epoch 16/25:  74%|███████▍  | 217/292 [02:26<00:43,  1.70batch/s, auc=0.8760, loss=0.5691]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▍  | 218/292 [02:26<00:42,  1.73batch/s, auc=0.8760, loss=0.5691]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▍  | 218/292 [02:27<00:42,  1.73batch/s, auc=0.8763, loss=0.4627]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 219/292 [02:27<00:41,  1.74batch/s, auc=0.8763, loss=0.4627]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 219/292 [02:28<00:41,  1.74batch/s, auc=0.8763, loss=0.7814]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 220/292 [02:28<00:41,  1.75batch/s, auc=0.8763, loss=0.7814]\u001b[A\n",
      "Training Epoch 16/25:  75%|███████▌  | 220/292 [02:29<00:41,  1.75batch/s, auc=0.8754, loss=1.5271]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 221/292 [02:29<00:51,  1.38batch/s, auc=0.8754, loss=1.5271]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 221/292 [02:29<00:51,  1.38batch/s, auc=0.8753, loss=0.6019]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 222/292 [02:29<00:47,  1.48batch/s, auc=0.8753, loss=0.6019]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▌  | 222/292 [02:30<00:47,  1.48batch/s, auc=0.8749, loss=0.9185]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▋  | 223/292 [02:30<00:54,  1.26batch/s, auc=0.8749, loss=0.9185]\u001b[A\n",
      "Training Epoch 16/25:  76%|███████▋  | 223/292 [02:31<00:54,  1.26batch/s, auc=0.8752, loss=0.5075]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 224/292 [02:31<00:49,  1.38batch/s, auc=0.8752, loss=0.5075]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 224/292 [02:31<00:49,  1.38batch/s, auc=0.8755, loss=0.8416]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 225/292 [02:31<00:45,  1.48batch/s, auc=0.8755, loss=0.8416]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 225/292 [02:32<00:45,  1.48batch/s, auc=0.8754, loss=0.8308]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 226/292 [02:32<00:42,  1.55batch/s, auc=0.8754, loss=0.8308]\u001b[A\n",
      "Training Epoch 16/25:  77%|███████▋  | 226/292 [02:33<00:42,  1.55batch/s, auc=0.8757, loss=0.6026]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 227/292 [02:33<00:40,  1.62batch/s, auc=0.8757, loss=0.6026]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 227/292 [02:33<00:40,  1.62batch/s, auc=0.8757, loss=0.8541]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 228/292 [02:33<00:38,  1.66batch/s, auc=0.8757, loss=0.8541]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 228/292 [02:34<00:38,  1.66batch/s, auc=0.8755, loss=0.7575]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 229/292 [02:34<00:37,  1.70batch/s, auc=0.8755, loss=0.7575]\u001b[A\n",
      "Training Epoch 16/25:  78%|███████▊  | 229/292 [02:34<00:37,  1.70batch/s, auc=0.8759, loss=0.4643]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 230/292 [02:34<00:36,  1.72batch/s, auc=0.8759, loss=0.4643]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 230/292 [02:35<00:36,  1.72batch/s, auc=0.8757, loss=0.6369]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 231/292 [02:35<00:35,  1.74batch/s, auc=0.8757, loss=0.6369]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 231/292 [02:35<00:35,  1.74batch/s, auc=0.8757, loss=0.7307]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 232/292 [02:35<00:34,  1.75batch/s, auc=0.8757, loss=0.7307]\u001b[A\n",
      "Training Epoch 16/25:  79%|███████▉  | 232/292 [02:36<00:34,  1.75batch/s, auc=0.8755, loss=0.6500]\u001b[A\n",
      "Training Epoch 16/25:  80%|███████▉  | 233/292 [02:36<00:33,  1.76batch/s, auc=0.8755, loss=0.6500]\u001b[A\n",
      "Training Epoch 16/25:  80%|███████▉  | 233/292 [02:36<00:33,  1.76batch/s, auc=0.8757, loss=0.6231]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 234/292 [02:36<00:32,  1.76batch/s, auc=0.8757, loss=0.6231]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 234/292 [02:38<00:32,  1.76batch/s, auc=0.8753, loss=0.9948]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 235/292 [02:38<00:41,  1.39batch/s, auc=0.8753, loss=0.9948]\u001b[A\n",
      "Training Epoch 16/25:  80%|████████  | 235/292 [02:38<00:41,  1.39batch/s, auc=0.8753, loss=0.7105]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 236/292 [02:38<00:37,  1.48batch/s, auc=0.8753, loss=0.7105]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 236/292 [02:39<00:37,  1.48batch/s, auc=0.8754, loss=0.5706]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 237/292 [02:39<00:35,  1.56batch/s, auc=0.8754, loss=0.5706]\u001b[A\n",
      "Training Epoch 16/25:  81%|████████  | 237/292 [02:39<00:35,  1.56batch/s, auc=0.8757, loss=0.4121]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 238/292 [02:39<00:33,  1.62batch/s, auc=0.8757, loss=0.4121]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 238/292 [02:40<00:33,  1.62batch/s, auc=0.8757, loss=0.6926]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 239/292 [02:40<00:31,  1.66batch/s, auc=0.8757, loss=0.6926]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 239/292 [02:40<00:31,  1.66batch/s, auc=0.8759, loss=0.6297]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 240/292 [02:40<00:30,  1.69batch/s, auc=0.8759, loss=0.6297]\u001b[A\n",
      "Training Epoch 16/25:  82%|████████▏ | 240/292 [02:41<00:30,  1.69batch/s, auc=0.8760, loss=0.6823]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 241/292 [02:41<00:29,  1.71batch/s, auc=0.8760, loss=0.6823]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 241/292 [02:41<00:29,  1.71batch/s, auc=0.8760, loss=0.7241]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 242/292 [02:41<00:28,  1.73batch/s, auc=0.8760, loss=0.7241]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 242/292 [02:42<00:28,  1.73batch/s, auc=0.8762, loss=0.5913]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 243/292 [02:42<00:28,  1.75batch/s, auc=0.8762, loss=0.5913]\u001b[A\n",
      "Training Epoch 16/25:  83%|████████▎ | 243/292 [02:43<00:28,  1.75batch/s, auc=0.8764, loss=0.5729]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▎ | 244/292 [02:43<00:27,  1.75batch/s, auc=0.8764, loss=0.5729]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▎ | 244/292 [02:44<00:27,  1.75batch/s, auc=0.8759, loss=1.0392]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 245/292 [02:44<00:33,  1.38batch/s, auc=0.8759, loss=1.0392]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 245/292 [02:45<00:33,  1.38batch/s, auc=0.8757, loss=0.8215]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 246/292 [02:45<00:38,  1.20batch/s, auc=0.8757, loss=0.8215]\u001b[A\n",
      "Training Epoch 16/25:  84%|████████▍ | 246/292 [02:45<00:38,  1.20batch/s, auc=0.8758, loss=0.6616]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 247/292 [02:45<00:33,  1.33batch/s, auc=0.8758, loss=0.6616]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 247/292 [02:46<00:33,  1.33batch/s, auc=0.8758, loss=0.7647]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 248/292 [02:46<00:30,  1.44batch/s, auc=0.8758, loss=0.7647]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▍ | 248/292 [02:46<00:30,  1.44batch/s, auc=0.8757, loss=0.8373]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▌ | 249/292 [02:46<00:28,  1.53batch/s, auc=0.8757, loss=0.8373]\u001b[A\n",
      "Training Epoch 16/25:  85%|████████▌ | 249/292 [02:47<00:28,  1.53batch/s, auc=0.8759, loss=0.5603]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 250/292 [02:47<00:26,  1.59batch/s, auc=0.8759, loss=0.5603]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 250/292 [02:48<00:26,  1.59batch/s, auc=0.8758, loss=0.8282]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 251/292 [02:48<00:24,  1.64batch/s, auc=0.8758, loss=0.8282]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▌ | 251/292 [02:49<00:24,  1.64batch/s, auc=0.8756, loss=0.8286]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▋ | 252/292 [02:49<00:30,  1.33batch/s, auc=0.8756, loss=0.8286]\u001b[A\n",
      "Training Epoch 16/25:  86%|████████▋ | 252/292 [02:50<00:30,  1.33batch/s, auc=0.8755, loss=0.8073]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 253/292 [02:50<00:33,  1.18batch/s, auc=0.8755, loss=0.8073]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 253/292 [02:50<00:33,  1.18batch/s, auc=0.8754, loss=0.7866]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 254/292 [02:50<00:29,  1.31batch/s, auc=0.8754, loss=0.7866]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 254/292 [02:51<00:29,  1.31batch/s, auc=0.8747, loss=1.3447]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 255/292 [02:51<00:31,  1.17batch/s, auc=0.8747, loss=1.3447]\u001b[A\n",
      "Training Epoch 16/25:  87%|████████▋ | 255/292 [02:52<00:31,  1.17batch/s, auc=0.8749, loss=0.5626]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 256/292 [02:52<00:27,  1.30batch/s, auc=0.8749, loss=0.5626]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 256/292 [02:53<00:27,  1.30batch/s, auc=0.8751, loss=0.7056]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 257/292 [02:53<00:24,  1.41batch/s, auc=0.8751, loss=0.7056]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 257/292 [02:53<00:24,  1.41batch/s, auc=0.8750, loss=0.8234]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 258/292 [02:53<00:22,  1.50batch/s, auc=0.8750, loss=0.8234]\u001b[A\n",
      "Training Epoch 16/25:  88%|████████▊ | 258/292 [02:54<00:22,  1.50batch/s, auc=0.8755, loss=0.4937]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▊ | 259/292 [02:54<00:20,  1.58batch/s, auc=0.8755, loss=0.4937]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▊ | 259/292 [02:55<00:20,  1.58batch/s, auc=0.8754, loss=0.8269]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 260/292 [02:55<00:24,  1.30batch/s, auc=0.8754, loss=0.8269]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 260/292 [02:55<00:24,  1.30batch/s, auc=0.8753, loss=0.6266]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 261/292 [02:55<00:21,  1.41batch/s, auc=0.8753, loss=0.6266]\u001b[A\n",
      "Training Epoch 16/25:  89%|████████▉ | 261/292 [02:56<00:21,  1.41batch/s, auc=0.8758, loss=0.4875]\u001b[A\n",
      "Training Epoch 16/25:  90%|████████▉ | 262/292 [02:56<00:19,  1.50batch/s, auc=0.8758, loss=0.4875]\u001b[A\n",
      "Training Epoch 16/25:  90%|████████▉ | 262/292 [02:56<00:19,  1.50batch/s, auc=0.8759, loss=0.5538]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 263/292 [02:56<00:18,  1.58batch/s, auc=0.8759, loss=0.5538]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 263/292 [02:57<00:18,  1.58batch/s, auc=0.8763, loss=0.4768]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 264/292 [02:57<00:17,  1.63batch/s, auc=0.8763, loss=0.4768]\u001b[A\n",
      "Training Epoch 16/25:  90%|█████████ | 264/292 [02:58<00:17,  1.63batch/s, auc=0.8765, loss=0.8750]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 265/292 [02:58<00:16,  1.67batch/s, auc=0.8765, loss=0.8750]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 265/292 [02:58<00:16,  1.67batch/s, auc=0.8767, loss=0.4917]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 266/292 [02:58<00:15,  1.70batch/s, auc=0.8767, loss=0.4917]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████ | 266/292 [02:59<00:15,  1.70batch/s, auc=0.8768, loss=0.5736]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████▏| 267/292 [02:59<00:14,  1.72batch/s, auc=0.8768, loss=0.5736]\u001b[A\n",
      "Training Epoch 16/25:  91%|█████████▏| 267/292 [03:00<00:14,  1.72batch/s, auc=0.8762, loss=1.1988]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 268/292 [03:00<00:17,  1.37batch/s, auc=0.8762, loss=1.1988]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 268/292 [03:00<00:17,  1.37batch/s, auc=0.8761, loss=0.6929]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 269/292 [03:00<00:15,  1.47batch/s, auc=0.8761, loss=0.6929]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 269/292 [03:01<00:15,  1.47batch/s, auc=0.8759, loss=0.9418]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 270/292 [03:01<00:17,  1.25batch/s, auc=0.8759, loss=0.9418]\u001b[A\n",
      "Training Epoch 16/25:  92%|█████████▏| 270/292 [03:02<00:17,  1.25batch/s, auc=0.8760, loss=0.5315]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 271/292 [03:02<00:15,  1.37batch/s, auc=0.8760, loss=0.5315]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 271/292 [03:03<00:15,  1.37batch/s, auc=0.8756, loss=1.0517]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 272/292 [03:03<00:16,  1.20batch/s, auc=0.8756, loss=1.0517]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 272/292 [03:04<00:16,  1.20batch/s, auc=0.8760, loss=0.5204]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 273/292 [03:04<00:14,  1.32batch/s, auc=0.8760, loss=0.5204]\u001b[A\n",
      "Training Epoch 16/25:  93%|█████████▎| 273/292 [03:04<00:14,  1.32batch/s, auc=0.8760, loss=0.6004]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 274/292 [03:04<00:12,  1.43batch/s, auc=0.8760, loss=0.6004]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 274/292 [03:05<00:12,  1.43batch/s, auc=0.8764, loss=0.4798]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 275/292 [03:05<00:11,  1.52batch/s, auc=0.8764, loss=0.4798]\u001b[A\n",
      "Training Epoch 16/25:  94%|█████████▍| 275/292 [03:06<00:11,  1.52batch/s, auc=0.8761, loss=0.9702]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 276/292 [03:06<00:12,  1.28batch/s, auc=0.8761, loss=0.9702]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 276/292 [03:06<00:12,  1.28batch/s, auc=0.8763, loss=0.6337]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 277/292 [03:06<00:10,  1.39batch/s, auc=0.8763, loss=0.6337]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▍| 277/292 [03:07<00:10,  1.39batch/s, auc=0.8762, loss=0.6246]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▌| 278/292 [03:07<00:09,  1.49batch/s, auc=0.8762, loss=0.6246]\u001b[A\n",
      "Training Epoch 16/25:  95%|█████████▌| 278/292 [03:08<00:09,  1.49batch/s, auc=0.8763, loss=0.5056]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 279/292 [03:08<00:08,  1.56batch/s, auc=0.8763, loss=0.5056]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 279/292 [03:09<00:08,  1.56batch/s, auc=0.8762, loss=0.8171]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 280/292 [03:09<00:09,  1.29batch/s, auc=0.8762, loss=0.8171]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 280/292 [03:09<00:09,  1.29batch/s, auc=0.8764, loss=0.6165]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 281/292 [03:09<00:07,  1.41batch/s, auc=0.8764, loss=0.6165]\u001b[A\n",
      "Training Epoch 16/25:  96%|█████████▌| 281/292 [03:10<00:07,  1.41batch/s, auc=0.8763, loss=0.7549]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 282/292 [03:10<00:06,  1.50batch/s, auc=0.8763, loss=0.7549]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 282/292 [03:11<00:06,  1.50batch/s, auc=0.8761, loss=0.8830]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 283/292 [03:11<00:07,  1.26batch/s, auc=0.8761, loss=0.8830]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 283/292 [03:11<00:07,  1.26batch/s, auc=0.8761, loss=0.6715]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 284/292 [03:11<00:05,  1.38batch/s, auc=0.8761, loss=0.6715]\u001b[A\n",
      "Training Epoch 16/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.38batch/s, auc=0.8759, loss=0.8513]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 285/292 [03:12<00:04,  1.48batch/s, auc=0.8759, loss=0.8513]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 285/292 [03:13<00:04,  1.48batch/s, auc=0.8760, loss=0.5632]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 286/292 [03:13<00:03,  1.56batch/s, auc=0.8760, loss=0.5632]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 286/292 [03:14<00:03,  1.56batch/s, auc=0.8755, loss=1.2376]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.29batch/s, auc=0.8755, loss=1.2376]\u001b[A\n",
      "Training Epoch 16/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.29batch/s, auc=0.8758, loss=0.6303]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▊| 288/292 [03:14<00:02,  1.41batch/s, auc=0.8758, loss=0.6303]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▊| 288/292 [03:15<00:02,  1.41batch/s, auc=0.8759, loss=0.6326]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 289/292 [03:15<00:01,  1.50batch/s, auc=0.8759, loss=0.6326]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 289/292 [03:16<00:01,  1.50batch/s, auc=0.8753, loss=1.3341]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 290/292 [03:16<00:01,  1.27batch/s, auc=0.8753, loss=1.3341]\u001b[A\n",
      "Training Epoch 16/25:  99%|█████████▉| 290/292 [03:16<00:01,  1.27batch/s, auc=0.8755, loss=0.6644]\u001b[A\n",
      "Training Epoch 16/25: 100%|█████████▉| 291/292 [03:16<00:00,  1.38batch/s, auc=0.8755, loss=0.6644]\u001b[A\n",
      "Training Epoch 16/25: 100%|█████████▉| 291/292 [03:17<00:00,  1.38batch/s, auc=0.8754, loss=0.9187]\u001b[A\n",
      "Training Epoch 16/25: 100%|██████████| 292/292 [03:17<00:00,  1.48batch/s, auc=0.8754, loss=0.9187]\u001b[A\n",
      "Epochs:  64%|██████▍   | 16/25 [57:02<32:29, 216.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25] Train Loss: 0.7398 | Train AUROC: 0.8754 Val Loss: 0.7911 | Val AUROC: 0.8518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 17/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8042, loss=0.6345]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 1/292 [00:01<07:10,  1.48s/batch, auc=0.8042, loss=0.6345]\u001b[A\n",
      "Training Epoch 17/25:   0%|          | 1/292 [00:02<07:10,  1.48s/batch, auc=0.8829, loss=0.5656]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 2/292 [00:02<04:31,  1.07batch/s, auc=0.8829, loss=0.5656]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 2/292 [00:02<04:31,  1.07batch/s, auc=0.8786, loss=0.6471]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 3/292 [00:02<03:39,  1.32batch/s, auc=0.8786, loss=0.6471]\u001b[A\n",
      "Training Epoch 17/25:   1%|          | 3/292 [00:03<03:39,  1.32batch/s, auc=0.8843, loss=0.5769]\u001b[A\n",
      "Training Epoch 17/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.8843, loss=0.5769]\u001b[A\n",
      "Training Epoch 17/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.8640, loss=0.9069]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 5/292 [00:03<03:01,  1.58batch/s, auc=0.8640, loss=0.9069]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 5/292 [00:04<03:01,  1.58batch/s, auc=0.8679, loss=0.6202]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.8679, loss=0.6202]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.8705, loss=0.6265]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 7/292 [00:04<02:48,  1.70batch/s, auc=0.8705, loss=0.6265]\u001b[A\n",
      "Training Epoch 17/25:   2%|▏         | 7/292 [00:05<02:48,  1.70batch/s, auc=0.8824, loss=0.5862]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 8/292 [00:05<02:44,  1.73batch/s, auc=0.8824, loss=0.5862]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 8/292 [00:06<02:44,  1.73batch/s, auc=0.8758, loss=0.7818]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 9/292 [00:06<03:27,  1.36batch/s, auc=0.8758, loss=0.7818]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 9/292 [00:06<03:27,  1.36batch/s, auc=0.8781, loss=0.7818]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 10/292 [00:06<03:11,  1.47batch/s, auc=0.8781, loss=0.7818]\u001b[A\n",
      "Training Epoch 17/25:   3%|▎         | 10/292 [00:08<03:11,  1.47batch/s, auc=0.8599, loss=1.2239]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 11/292 [00:08<03:44,  1.25batch/s, auc=0.8599, loss=1.2239]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 11/292 [00:09<03:44,  1.25batch/s, auc=0.8576, loss=0.7357]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 12/292 [00:09<04:07,  1.13batch/s, auc=0.8576, loss=0.7357]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 12/292 [00:09<04:07,  1.13batch/s, auc=0.8635, loss=0.6625]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 13/292 [00:09<03:38,  1.28batch/s, auc=0.8635, loss=0.6625]\u001b[A\n",
      "Training Epoch 17/25:   4%|▍         | 13/292 [00:10<03:38,  1.28batch/s, auc=0.8692, loss=0.4246]\u001b[A\n",
      "Training Epoch 17/25:   5%|▍         | 14/292 [00:10<03:18,  1.40batch/s, auc=0.8692, loss=0.4246]\u001b[A\n",
      "Training Epoch 17/25:   5%|▍         | 14/292 [00:10<03:18,  1.40batch/s, auc=0.8670, loss=0.6889]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 15/292 [00:10<03:04,  1.50batch/s, auc=0.8670, loss=0.6889]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 15/292 [00:11<03:04,  1.50batch/s, auc=0.8752, loss=0.5161]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 16/292 [00:11<02:54,  1.58batch/s, auc=0.8752, loss=0.5161]\u001b[A\n",
      "Training Epoch 17/25:   5%|▌         | 16/292 [00:11<02:54,  1.58batch/s, auc=0.8754, loss=0.7313]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 17/292 [00:11<02:47,  1.64batch/s, auc=0.8754, loss=0.7313]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 17/292 [00:12<02:47,  1.64batch/s, auc=0.8740, loss=0.6054]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 18/292 [00:12<03:24,  1.34batch/s, auc=0.8740, loss=0.6054]\u001b[A\n",
      "Training Epoch 17/25:   6%|▌         | 18/292 [00:14<03:24,  1.34batch/s, auc=0.8719, loss=0.7800]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 19/292 [00:14<03:50,  1.18batch/s, auc=0.8719, loss=0.7800]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 19/292 [00:14<03:50,  1.18batch/s, auc=0.8744, loss=0.6946]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 20/292 [00:14<03:25,  1.32batch/s, auc=0.8744, loss=0.6946]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 20/292 [00:15<03:25,  1.32batch/s, auc=0.8758, loss=0.5373]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 21/292 [00:15<03:08,  1.43batch/s, auc=0.8758, loss=0.5373]\u001b[A\n",
      "Training Epoch 17/25:   7%|▋         | 21/292 [00:15<03:08,  1.43batch/s, auc=0.8810, loss=0.5537]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 22/292 [00:15<02:56,  1.53batch/s, auc=0.8810, loss=0.5537]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 22/292 [00:16<02:56,  1.53batch/s, auc=0.8792, loss=0.8760]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 23/292 [00:16<02:48,  1.60batch/s, auc=0.8792, loss=0.8760]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 23/292 [00:17<02:48,  1.60batch/s, auc=0.8762, loss=0.9469]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 24/292 [00:17<03:23,  1.32batch/s, auc=0.8762, loss=0.9469]\u001b[A\n",
      "Training Epoch 17/25:   8%|▊         | 24/292 [00:18<03:23,  1.32batch/s, auc=0.8735, loss=0.8098]\u001b[A\n",
      "Training Epoch 17/25:   9%|▊         | 25/292 [00:18<03:47,  1.17batch/s, auc=0.8735, loss=0.8098]\u001b[A\n",
      "Training Epoch 17/25:   9%|▊         | 25/292 [00:18<03:47,  1.17batch/s, auc=0.8755, loss=0.4879]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 26/292 [00:18<03:23,  1.31batch/s, auc=0.8755, loss=0.4879]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 26/292 [00:19<03:23,  1.31batch/s, auc=0.8807, loss=0.5888]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 27/292 [00:19<03:05,  1.43batch/s, auc=0.8807, loss=0.5888]\u001b[A\n",
      "Training Epoch 17/25:   9%|▉         | 27/292 [00:20<03:05,  1.43batch/s, auc=0.8825, loss=0.4578]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 28/292 [00:20<02:53,  1.52batch/s, auc=0.8825, loss=0.4578]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 28/292 [00:20<02:53,  1.52batch/s, auc=0.8862, loss=0.5069]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 29/292 [00:20<02:44,  1.60batch/s, auc=0.8862, loss=0.5069]\u001b[A\n",
      "Training Epoch 17/25:  10%|▉         | 29/292 [00:21<02:44,  1.60batch/s, auc=0.8887, loss=0.3914]\u001b[A\n",
      "Training Epoch 17/25:  10%|█         | 30/292 [00:21<02:38,  1.65batch/s, auc=0.8887, loss=0.3914]\u001b[A\n",
      "Training Epoch 17/25:  10%|█         | 30/292 [00:22<02:38,  1.65batch/s, auc=0.8842, loss=1.0077]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 31/292 [00:22<03:14,  1.34batch/s, auc=0.8842, loss=1.0077]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 31/292 [00:22<03:14,  1.34batch/s, auc=0.8859, loss=0.6744]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 32/292 [00:22<02:58,  1.45batch/s, auc=0.8859, loss=0.6744]\u001b[A\n",
      "Training Epoch 17/25:  11%|█         | 32/292 [00:23<02:58,  1.45batch/s, auc=0.8837, loss=0.8692]\u001b[A\n",
      "Training Epoch 17/25:  11%|█▏        | 33/292 [00:23<02:47,  1.54batch/s, auc=0.8837, loss=0.8692]\u001b[A\n",
      "Training Epoch 17/25:  11%|█▏        | 33/292 [00:23<02:47,  1.54batch/s, auc=0.8855, loss=0.6040]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 34/292 [00:23<02:39,  1.61batch/s, auc=0.8855, loss=0.6040]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 34/292 [00:24<02:39,  1.61batch/s, auc=0.8835, loss=1.4601]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 35/292 [00:24<02:34,  1.67batch/s, auc=0.8835, loss=1.4601]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 35/292 [00:25<02:34,  1.67batch/s, auc=0.8838, loss=0.6197]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 36/292 [00:25<02:30,  1.70batch/s, auc=0.8838, loss=0.6197]\u001b[A\n",
      "Training Epoch 17/25:  12%|█▏        | 36/292 [00:25<02:30,  1.70batch/s, auc=0.8818, loss=0.9592]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 37/292 [00:25<02:27,  1.73batch/s, auc=0.8818, loss=0.9592]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 37/292 [00:26<02:27,  1.73batch/s, auc=0.8802, loss=0.8741]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 38/292 [00:26<02:24,  1.75batch/s, auc=0.8802, loss=0.8741]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 38/292 [00:26<02:24,  1.75batch/s, auc=0.8795, loss=0.6251]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 39/292 [00:26<02:23,  1.77batch/s, auc=0.8795, loss=0.6251]\u001b[A\n",
      "Training Epoch 17/25:  13%|█▎        | 39/292 [00:27<02:23,  1.77batch/s, auc=0.8725, loss=1.7431]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▎        | 40/292 [00:27<03:00,  1.39batch/s, auc=0.8725, loss=1.7431]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▎        | 40/292 [00:28<03:00,  1.39batch/s, auc=0.8733, loss=0.5271]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 41/292 [00:28<02:48,  1.49batch/s, auc=0.8733, loss=0.5271]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 41/292 [00:28<02:48,  1.49batch/s, auc=0.8748, loss=0.6654]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 42/292 [00:28<02:39,  1.57batch/s, auc=0.8748, loss=0.6654]\u001b[A\n",
      "Training Epoch 17/25:  14%|█▍        | 42/292 [00:29<02:39,  1.57batch/s, auc=0.8765, loss=0.5779]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▍        | 43/292 [00:29<02:32,  1.63batch/s, auc=0.8765, loss=0.5779]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▍        | 43/292 [00:29<02:32,  1.63batch/s, auc=0.8765, loss=0.7309]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 44/292 [00:29<02:27,  1.68batch/s, auc=0.8765, loss=0.7309]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 44/292 [00:30<02:27,  1.68batch/s, auc=0.8779, loss=0.6503]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 45/292 [00:30<02:23,  1.72batch/s, auc=0.8779, loss=0.6503]\u001b[A\n",
      "Training Epoch 17/25:  15%|█▌        | 45/292 [00:31<02:23,  1.72batch/s, auc=0.8778, loss=0.7578]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 46/292 [00:31<02:21,  1.74batch/s, auc=0.8778, loss=0.7578]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 46/292 [00:31<02:21,  1.74batch/s, auc=0.8784, loss=0.5875]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 47/292 [00:31<02:19,  1.75batch/s, auc=0.8784, loss=0.5875]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▌        | 47/292 [00:32<02:19,  1.75batch/s, auc=0.8801, loss=0.6393]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▋        | 48/292 [00:32<02:18,  1.77batch/s, auc=0.8801, loss=0.6393]\u001b[A\n",
      "Training Epoch 17/25:  16%|█▋        | 48/292 [00:32<02:18,  1.77batch/s, auc=0.8810, loss=0.7978]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 49/292 [00:32<02:16,  1.78batch/s, auc=0.8810, loss=0.7978]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 49/292 [00:33<02:16,  1.78batch/s, auc=0.8824, loss=0.7053]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 50/292 [00:33<02:15,  1.78batch/s, auc=0.8824, loss=0.7053]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 50/292 [00:34<02:15,  1.78batch/s, auc=0.8818, loss=0.8004]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 51/292 [00:34<02:52,  1.40batch/s, auc=0.8818, loss=0.8004]\u001b[A\n",
      "Training Epoch 17/25:  17%|█▋        | 51/292 [00:34<02:52,  1.40batch/s, auc=0.8825, loss=0.6595]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 52/292 [00:34<02:39,  1.50batch/s, auc=0.8825, loss=0.6595]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 52/292 [00:35<02:39,  1.50batch/s, auc=0.8836, loss=0.4967]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 53/292 [00:35<02:31,  1.58batch/s, auc=0.8836, loss=0.4967]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 53/292 [00:36<02:31,  1.58batch/s, auc=0.8844, loss=0.6374]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 54/292 [00:36<02:25,  1.64batch/s, auc=0.8844, loss=0.6374]\u001b[A\n",
      "Training Epoch 17/25:  18%|█▊        | 54/292 [00:36<02:25,  1.64batch/s, auc=0.8844, loss=0.6818]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 55/292 [00:36<02:20,  1.69batch/s, auc=0.8844, loss=0.6818]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 55/292 [00:37<02:20,  1.69batch/s, auc=0.8852, loss=0.4533]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 56/292 [00:37<02:17,  1.72batch/s, auc=0.8852, loss=0.4533]\u001b[A\n",
      "Training Epoch 17/25:  19%|█▉        | 56/292 [00:37<02:17,  1.72batch/s, auc=0.8866, loss=0.4473]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 57/292 [00:37<02:14,  1.74batch/s, auc=0.8866, loss=0.4473]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 57/292 [00:38<02:14,  1.74batch/s, auc=0.8875, loss=0.5557]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 58/292 [00:38<02:12,  1.76batch/s, auc=0.8875, loss=0.5557]\u001b[A\n",
      "Training Epoch 17/25:  20%|█▉        | 58/292 [00:38<02:12,  1.76batch/s, auc=0.8861, loss=0.7090]\u001b[A\n",
      "Training Epoch 17/25:  20%|██        | 59/292 [00:38<02:11,  1.77batch/s, auc=0.8861, loss=0.7090]\u001b[A\n",
      "Training Epoch 17/25:  20%|██        | 59/292 [00:39<02:11,  1.77batch/s, auc=0.8871, loss=0.5303]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 60/292 [00:39<02:10,  1.78batch/s, auc=0.8871, loss=0.5303]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 60/292 [00:40<02:10,  1.78batch/s, auc=0.8855, loss=0.9407]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 61/292 [00:40<02:45,  1.40batch/s, auc=0.8855, loss=0.9407]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 61/292 [00:41<02:45,  1.40batch/s, auc=0.8854, loss=0.6541]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 62/292 [00:41<02:33,  1.50batch/s, auc=0.8854, loss=0.6541]\u001b[A\n",
      "Training Epoch 17/25:  21%|██        | 62/292 [00:41<02:33,  1.50batch/s, auc=0.8862, loss=0.6305]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 63/292 [00:41<02:25,  1.58batch/s, auc=0.8862, loss=0.6305]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 63/292 [00:42<02:25,  1.58batch/s, auc=0.8848, loss=1.0308]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 64/292 [00:42<02:54,  1.31batch/s, auc=0.8848, loss=1.0308]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 64/292 [00:43<02:54,  1.31batch/s, auc=0.8854, loss=0.5152]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 65/292 [00:43<02:39,  1.42batch/s, auc=0.8854, loss=0.5152]\u001b[A\n",
      "Training Epoch 17/25:  22%|██▏       | 65/292 [00:43<02:39,  1.42batch/s, auc=0.8853, loss=0.6392]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 66/292 [00:43<02:28,  1.52batch/s, auc=0.8853, loss=0.6392]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 66/292 [00:44<02:28,  1.52batch/s, auc=0.8840, loss=1.0197]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 67/292 [00:44<02:21,  1.59batch/s, auc=0.8840, loss=1.0197]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 67/292 [00:45<02:21,  1.59batch/s, auc=0.8819, loss=1.2645]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 68/292 [00:45<02:50,  1.31batch/s, auc=0.8819, loss=1.2645]\u001b[A\n",
      "Training Epoch 17/25:  23%|██▎       | 68/292 [00:46<02:50,  1.31batch/s, auc=0.8807, loss=0.9691]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▎       | 69/292 [00:46<03:10,  1.17batch/s, auc=0.8807, loss=0.9691]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▎       | 69/292 [00:47<03:10,  1.17batch/s, auc=0.8781, loss=1.3588]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 70/292 [00:47<03:24,  1.09batch/s, auc=0.8781, loss=1.3588]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 70/292 [00:48<03:24,  1.09batch/s, auc=0.8761, loss=1.1988]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 71/292 [00:48<03:33,  1.04batch/s, auc=0.8761, loss=1.1988]\u001b[A\n",
      "Training Epoch 17/25:  24%|██▍       | 71/292 [00:49<03:33,  1.04batch/s, auc=0.8747, loss=1.2116]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▍       | 72/292 [00:49<03:39,  1.00batch/s, auc=0.8747, loss=1.2116]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▍       | 72/292 [00:50<03:39,  1.00batch/s, auc=0.8756, loss=0.6041]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 73/292 [00:50<03:09,  1.15batch/s, auc=0.8756, loss=0.6041]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 73/292 [00:50<03:09,  1.15batch/s, auc=0.8750, loss=0.6519]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 74/292 [00:50<02:48,  1.29batch/s, auc=0.8750, loss=0.6519]\u001b[A\n",
      "Training Epoch 17/25:  25%|██▌       | 74/292 [00:51<02:48,  1.29batch/s, auc=0.8755, loss=0.6682]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 75/292 [00:51<02:33,  1.41batch/s, auc=0.8755, loss=0.6682]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 75/292 [00:51<02:33,  1.41batch/s, auc=0.8766, loss=0.4681]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 76/292 [00:51<02:23,  1.51batch/s, auc=0.8766, loss=0.4681]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▌       | 76/292 [00:52<02:23,  1.51batch/s, auc=0.8771, loss=0.5119]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▋       | 77/292 [00:52<02:15,  1.58batch/s, auc=0.8771, loss=0.5119]\u001b[A\n",
      "Training Epoch 17/25:  26%|██▋       | 77/292 [00:53<02:15,  1.58batch/s, auc=0.8779, loss=0.6136]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 78/292 [00:53<02:10,  1.64batch/s, auc=0.8779, loss=0.6136]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 78/292 [00:53<02:10,  1.64batch/s, auc=0.8784, loss=0.8919]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 79/292 [00:53<02:06,  1.69batch/s, auc=0.8784, loss=0.8919]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 79/292 [00:54<02:06,  1.69batch/s, auc=0.8773, loss=1.0998]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 80/292 [00:54<02:36,  1.36batch/s, auc=0.8773, loss=1.0998]\u001b[A\n",
      "Training Epoch 17/25:  27%|██▋       | 80/292 [00:55<02:36,  1.36batch/s, auc=0.8771, loss=0.7942]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 81/292 [00:55<02:24,  1.46batch/s, auc=0.8771, loss=0.7942]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 81/292 [00:55<02:24,  1.46batch/s, auc=0.8768, loss=0.8540]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 82/292 [00:55<02:15,  1.55batch/s, auc=0.8768, loss=0.8540]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 82/292 [00:56<02:15,  1.55batch/s, auc=0.8763, loss=0.9613]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 83/292 [00:56<02:09,  1.61batch/s, auc=0.8763, loss=0.9613]\u001b[A\n",
      "Training Epoch 17/25:  28%|██▊       | 83/292 [00:56<02:09,  1.61batch/s, auc=0.8761, loss=0.5962]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 84/292 [00:56<02:04,  1.66batch/s, auc=0.8761, loss=0.5962]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 84/292 [00:57<02:04,  1.66batch/s, auc=0.8756, loss=0.8085]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 85/292 [00:57<02:01,  1.70batch/s, auc=0.8756, loss=0.8085]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 85/292 [00:58<02:01,  1.70batch/s, auc=0.8759, loss=0.8229]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 86/292 [00:58<01:59,  1.73batch/s, auc=0.8759, loss=0.8229]\u001b[A\n",
      "Training Epoch 17/25:  29%|██▉       | 86/292 [00:58<01:59,  1.73batch/s, auc=0.8762, loss=0.6016]\u001b[A\n",
      "Training Epoch 17/25:  30%|██▉       | 87/292 [00:58<01:57,  1.75batch/s, auc=0.8762, loss=0.6016]\u001b[A\n",
      "Training Epoch 17/25:  30%|██▉       | 87/292 [00:59<01:57,  1.75batch/s, auc=0.8766, loss=0.6946]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 88/292 [00:59<01:55,  1.76batch/s, auc=0.8766, loss=0.6946]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 88/292 [01:00<01:55,  1.76batch/s, auc=0.8760, loss=0.8638]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 89/292 [01:00<02:26,  1.39batch/s, auc=0.8760, loss=0.8638]\u001b[A\n",
      "Training Epoch 17/25:  30%|███       | 89/292 [01:00<02:26,  1.39batch/s, auc=0.8759, loss=0.8377]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 90/292 [01:00<02:15,  1.49batch/s, auc=0.8759, loss=0.8377]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 90/292 [01:01<02:15,  1.49batch/s, auc=0.8757, loss=0.6089]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 91/292 [01:01<02:08,  1.57batch/s, auc=0.8757, loss=0.6089]\u001b[A\n",
      "Training Epoch 17/25:  31%|███       | 91/292 [01:01<02:08,  1.57batch/s, auc=0.8754, loss=0.8820]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 92/292 [01:01<02:02,  1.63batch/s, auc=0.8754, loss=0.8820]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 92/292 [01:02<02:02,  1.63batch/s, auc=0.8750, loss=0.8704]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 93/292 [01:02<01:58,  1.68batch/s, auc=0.8750, loss=0.8704]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 93/292 [01:03<01:58,  1.68batch/s, auc=0.8737, loss=1.0417]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 94/292 [01:03<02:26,  1.35batch/s, auc=0.8737, loss=1.0417]\u001b[A\n",
      "Training Epoch 17/25:  32%|███▏      | 94/292 [01:04<02:26,  1.35batch/s, auc=0.8734, loss=0.7500]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 95/292 [01:04<02:14,  1.46batch/s, auc=0.8734, loss=0.7500]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 95/292 [01:05<02:14,  1.46batch/s, auc=0.8724, loss=0.8802]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 96/292 [01:05<02:36,  1.25batch/s, auc=0.8724, loss=0.8802]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 96/292 [01:06<02:36,  1.25batch/s, auc=0.8714, loss=0.9094]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 97/292 [01:06<02:52,  1.13batch/s, auc=0.8714, loss=0.9094]\u001b[A\n",
      "Training Epoch 17/25:  33%|███▎      | 97/292 [01:06<02:52,  1.13batch/s, auc=0.8715, loss=0.6075]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▎      | 98/292 [01:06<02:32,  1.27batch/s, auc=0.8715, loss=0.6075]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▎      | 98/292 [01:07<02:32,  1.27batch/s, auc=0.8707, loss=0.9459]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 99/292 [01:07<02:48,  1.15batch/s, auc=0.8707, loss=0.9459]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 99/292 [01:08<02:48,  1.15batch/s, auc=0.8698, loss=0.9018]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 100/292 [01:08<02:59,  1.07batch/s, auc=0.8698, loss=0.9018]\u001b[A\n",
      "Training Epoch 17/25:  34%|███▍      | 100/292 [01:09<02:59,  1.07batch/s, auc=0.8706, loss=0.6784]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 101/292 [01:09<02:36,  1.22batch/s, auc=0.8706, loss=0.6784]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 101/292 [01:10<02:36,  1.22batch/s, auc=0.8706, loss=0.7565]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 102/292 [01:10<02:20,  1.35batch/s, auc=0.8706, loss=0.7565]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▍      | 102/292 [01:10<02:20,  1.35batch/s, auc=0.8705, loss=0.8289]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▌      | 103/292 [01:10<02:09,  1.46batch/s, auc=0.8705, loss=0.8289]\u001b[A\n",
      "Training Epoch 17/25:  35%|███▌      | 103/292 [01:11<02:09,  1.46batch/s, auc=0.8700, loss=1.0322]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 104/292 [01:11<02:30,  1.25batch/s, auc=0.8700, loss=1.0322]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 104/292 [01:12<02:30,  1.25batch/s, auc=0.8686, loss=1.2142]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 105/292 [01:12<02:45,  1.13batch/s, auc=0.8686, loss=1.2142]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▌      | 105/292 [01:13<02:45,  1.13batch/s, auc=0.8698, loss=0.4622]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▋      | 106/292 [01:13<02:26,  1.27batch/s, auc=0.8698, loss=0.4622]\u001b[A\n",
      "Training Epoch 17/25:  36%|███▋      | 106/292 [01:14<02:26,  1.27batch/s, auc=0.8680, loss=1.4130]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 107/292 [01:14<02:41,  1.15batch/s, auc=0.8680, loss=1.4130]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 107/292 [01:14<02:41,  1.15batch/s, auc=0.8689, loss=0.5629]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 108/292 [01:14<02:23,  1.28batch/s, auc=0.8689, loss=0.5629]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 108/292 [01:15<02:23,  1.28batch/s, auc=0.8698, loss=0.6069]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 109/292 [01:15<02:10,  1.40batch/s, auc=0.8698, loss=0.6069]\u001b[A\n",
      "Training Epoch 17/25:  37%|███▋      | 109/292 [01:16<02:10,  1.40batch/s, auc=0.8702, loss=0.6092]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 110/292 [01:16<02:01,  1.50batch/s, auc=0.8702, loss=0.6092]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 110/292 [01:16<02:01,  1.50batch/s, auc=0.8707, loss=0.6401]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 111/292 [01:16<01:54,  1.58batch/s, auc=0.8707, loss=0.6401]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 111/292 [01:17<01:54,  1.58batch/s, auc=0.8705, loss=0.7672]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 112/292 [01:17<01:49,  1.64batch/s, auc=0.8705, loss=0.7672]\u001b[A\n",
      "Training Epoch 17/25:  38%|███▊      | 112/292 [01:18<01:49,  1.64batch/s, auc=0.8698, loss=1.1263]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▊      | 113/292 [01:18<02:14,  1.33batch/s, auc=0.8698, loss=1.1263]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▊      | 113/292 [01:18<02:14,  1.33batch/s, auc=0.8704, loss=0.5682]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 114/292 [01:18<02:03,  1.44batch/s, auc=0.8704, loss=0.5682]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 114/292 [01:19<02:03,  1.44batch/s, auc=0.8707, loss=0.6221]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 115/292 [01:19<01:55,  1.53batch/s, auc=0.8707, loss=0.6221]\u001b[A\n",
      "Training Epoch 17/25:  39%|███▉      | 115/292 [01:19<01:55,  1.53batch/s, auc=0.8719, loss=0.5187]\u001b[A\n",
      "Training Epoch 17/25:  40%|███▉      | 116/292 [01:19<01:49,  1.60batch/s, auc=0.8719, loss=0.5187]\u001b[A\n",
      "Training Epoch 17/25:  40%|███▉      | 116/292 [01:20<01:49,  1.60batch/s, auc=0.8718, loss=0.8015]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 117/292 [01:20<01:45,  1.66batch/s, auc=0.8718, loss=0.8015]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 117/292 [01:21<01:45,  1.66batch/s, auc=0.8715, loss=0.8011]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 118/292 [01:21<02:09,  1.34batch/s, auc=0.8715, loss=0.8011]\u001b[A\n",
      "Training Epoch 17/25:  40%|████      | 118/292 [01:22<02:09,  1.34batch/s, auc=0.8722, loss=0.4626]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 119/292 [01:22<01:59,  1.45batch/s, auc=0.8722, loss=0.4626]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 119/292 [01:22<01:59,  1.45batch/s, auc=0.8723, loss=0.5450]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 120/292 [01:22<01:51,  1.54batch/s, auc=0.8723, loss=0.5450]\u001b[A\n",
      "Training Epoch 17/25:  41%|████      | 120/292 [01:23<01:51,  1.54batch/s, auc=0.8727, loss=0.6498]\u001b[A\n",
      "Training Epoch 17/25:  41%|████▏     | 121/292 [01:23<01:46,  1.60batch/s, auc=0.8727, loss=0.6498]\u001b[A\n",
      "Training Epoch 17/25:  41%|████▏     | 121/292 [01:23<01:46,  1.60batch/s, auc=0.8737, loss=0.4519]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 122/292 [01:23<01:42,  1.65batch/s, auc=0.8737, loss=0.4519]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 122/292 [01:24<01:42,  1.65batch/s, auc=0.8728, loss=1.0401]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 123/292 [01:24<02:06,  1.34batch/s, auc=0.8728, loss=1.0401]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 123/292 [01:25<02:06,  1.34batch/s, auc=0.8725, loss=1.0847]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 124/292 [01:25<01:55,  1.45batch/s, auc=0.8725, loss=1.0847]\u001b[A\n",
      "Training Epoch 17/25:  42%|████▏     | 124/292 [01:25<01:55,  1.45batch/s, auc=0.8724, loss=0.5873]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 125/292 [01:25<01:48,  1.54batch/s, auc=0.8724, loss=0.5873]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 125/292 [01:26<01:48,  1.54batch/s, auc=0.8725, loss=0.6793]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 126/292 [01:26<01:43,  1.61batch/s, auc=0.8725, loss=0.6793]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 126/292 [01:27<01:43,  1.61batch/s, auc=0.8728, loss=0.6208]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 127/292 [01:27<01:39,  1.66batch/s, auc=0.8728, loss=0.6208]\u001b[A\n",
      "Training Epoch 17/25:  43%|████▎     | 127/292 [01:27<01:39,  1.66batch/s, auc=0.8734, loss=0.5065]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 128/292 [01:27<01:36,  1.70batch/s, auc=0.8734, loss=0.5065]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 128/292 [01:28<01:36,  1.70batch/s, auc=0.8739, loss=0.6015]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 129/292 [01:28<01:34,  1.72batch/s, auc=0.8739, loss=0.6015]\u001b[A\n",
      "Training Epoch 17/25:  44%|████▍     | 129/292 [01:28<01:34,  1.72batch/s, auc=0.8746, loss=0.5296]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 130/292 [01:28<01:33,  1.74batch/s, auc=0.8746, loss=0.5296]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 130/292 [01:29<01:33,  1.74batch/s, auc=0.8748, loss=0.5833]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 131/292 [01:29<01:31,  1.76batch/s, auc=0.8748, loss=0.5833]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▍     | 131/292 [01:29<01:31,  1.76batch/s, auc=0.8746, loss=0.6948]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▌     | 132/292 [01:29<01:30,  1.76batch/s, auc=0.8746, loss=0.6948]\u001b[A\n",
      "Training Epoch 17/25:  45%|████▌     | 132/292 [01:30<01:30,  1.76batch/s, auc=0.8745, loss=0.6836]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 133/292 [01:30<01:29,  1.77batch/s, auc=0.8745, loss=0.6836]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 133/292 [01:31<01:29,  1.77batch/s, auc=0.8738, loss=0.9105]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 134/292 [01:31<01:53,  1.39batch/s, auc=0.8738, loss=0.9105]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 134/292 [01:32<01:53,  1.39batch/s, auc=0.8738, loss=0.7547]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 135/292 [01:32<02:09,  1.21batch/s, auc=0.8738, loss=0.7547]\u001b[A\n",
      "Training Epoch 17/25:  46%|████▌     | 135/292 [01:33<02:09,  1.21batch/s, auc=0.8736, loss=0.8816]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 136/292 [01:33<02:20,  1.11batch/s, auc=0.8736, loss=0.8816]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 136/292 [01:34<02:20,  1.11batch/s, auc=0.8739, loss=0.4921]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 137/292 [01:34<02:03,  1.25batch/s, auc=0.8739, loss=0.4921]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 137/292 [01:35<02:03,  1.25batch/s, auc=0.8731, loss=0.9303]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 138/292 [01:35<02:15,  1.13batch/s, auc=0.8731, loss=0.9303]\u001b[A\n",
      "Training Epoch 17/25:  47%|████▋     | 138/292 [01:35<02:15,  1.13batch/s, auc=0.8733, loss=0.4635]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 139/292 [01:35<01:59,  1.28batch/s, auc=0.8733, loss=0.4635]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 139/292 [01:36<01:59,  1.28batch/s, auc=0.8733, loss=0.6766]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 140/292 [01:36<01:48,  1.39batch/s, auc=0.8733, loss=0.6766]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 140/292 [01:36<01:48,  1.39batch/s, auc=0.8733, loss=0.5807]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 141/292 [01:36<01:41,  1.49batch/s, auc=0.8733, loss=0.5807]\u001b[A\n",
      "Training Epoch 17/25:  48%|████▊     | 141/292 [01:37<01:41,  1.49batch/s, auc=0.8735, loss=0.7397]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▊     | 142/292 [01:37<01:35,  1.57batch/s, auc=0.8735, loss=0.7397]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▊     | 142/292 [01:38<01:35,  1.57batch/s, auc=0.8742, loss=0.4514]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 143/292 [01:38<01:31,  1.63batch/s, auc=0.8742, loss=0.4514]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 143/292 [01:38<01:31,  1.63batch/s, auc=0.8742, loss=0.7440]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 144/292 [01:38<01:28,  1.67batch/s, auc=0.8742, loss=0.7440]\u001b[A\n",
      "Training Epoch 17/25:  49%|████▉     | 144/292 [01:39<01:28,  1.67batch/s, auc=0.8747, loss=0.4005]\u001b[A\n",
      "Training Epoch 17/25:  50%|████▉     | 145/292 [01:39<01:26,  1.71batch/s, auc=0.8747, loss=0.4005]\u001b[A\n",
      "Training Epoch 17/25:  50%|████▉     | 145/292 [01:40<01:26,  1.71batch/s, auc=0.8740, loss=0.9818]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 146/292 [01:40<01:46,  1.36batch/s, auc=0.8740, loss=0.9818]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 146/292 [01:40<01:46,  1.36batch/s, auc=0.8749, loss=0.4984]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 147/292 [01:40<01:38,  1.47batch/s, auc=0.8749, loss=0.4984]\u001b[A\n",
      "Training Epoch 17/25:  50%|█████     | 147/292 [01:41<01:38,  1.47batch/s, auc=0.8752, loss=0.6437]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 148/292 [01:41<01:32,  1.55batch/s, auc=0.8752, loss=0.6437]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 148/292 [01:41<01:32,  1.55batch/s, auc=0.8756, loss=0.6341]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 149/292 [01:41<01:28,  1.61batch/s, auc=0.8756, loss=0.6341]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████     | 149/292 [01:42<01:28,  1.61batch/s, auc=0.8758, loss=0.4308]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████▏    | 150/292 [01:42<01:25,  1.66batch/s, auc=0.8758, loss=0.4308]\u001b[A\n",
      "Training Epoch 17/25:  51%|█████▏    | 150/292 [01:43<01:25,  1.66batch/s, auc=0.8764, loss=0.5086]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 151/292 [01:43<01:23,  1.70batch/s, auc=0.8764, loss=0.5086]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 151/292 [01:43<01:23,  1.70batch/s, auc=0.8766, loss=0.5065]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 152/292 [01:43<01:21,  1.73batch/s, auc=0.8766, loss=0.5065]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 152/292 [01:44<01:21,  1.73batch/s, auc=0.8765, loss=0.7664]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 153/292 [01:44<01:19,  1.74batch/s, auc=0.8765, loss=0.7664]\u001b[A\n",
      "Training Epoch 17/25:  52%|█████▏    | 153/292 [01:44<01:19,  1.74batch/s, auc=0.8766, loss=0.6152]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 154/292 [01:44<01:18,  1.76batch/s, auc=0.8766, loss=0.6152]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 154/292 [01:45<01:18,  1.76batch/s, auc=0.8760, loss=1.0554]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 155/292 [01:45<01:17,  1.76batch/s, auc=0.8760, loss=1.0554]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 155/292 [01:45<01:17,  1.76batch/s, auc=0.8745, loss=1.6163]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 156/292 [01:45<01:16,  1.77batch/s, auc=0.8745, loss=1.6163]\u001b[A\n",
      "Training Epoch 17/25:  53%|█████▎    | 156/292 [01:46<01:16,  1.77batch/s, auc=0.8748, loss=0.6217]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 157/292 [01:46<01:16,  1.77batch/s, auc=0.8748, loss=0.6217]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 157/292 [01:47<01:16,  1.77batch/s, auc=0.8735, loss=1.5671]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 158/292 [01:47<01:36,  1.39batch/s, auc=0.8735, loss=1.5671]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 158/292 [01:48<01:36,  1.39batch/s, auc=0.8739, loss=0.5511]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 159/292 [01:48<01:29,  1.49batch/s, auc=0.8739, loss=0.5511]\u001b[A\n",
      "Training Epoch 17/25:  54%|█████▍    | 159/292 [01:48<01:29,  1.49batch/s, auc=0.8745, loss=0.5788]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▍    | 160/292 [01:48<01:24,  1.57batch/s, auc=0.8745, loss=0.5788]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▍    | 160/292 [01:49<01:24,  1.57batch/s, auc=0.8750, loss=0.4676]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 161/292 [01:49<01:20,  1.63batch/s, auc=0.8750, loss=0.4676]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 161/292 [01:49<01:20,  1.63batch/s, auc=0.8747, loss=0.8170]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 162/292 [01:49<01:17,  1.67batch/s, auc=0.8747, loss=0.8170]\u001b[A\n",
      "Training Epoch 17/25:  55%|█████▌    | 162/292 [01:50<01:17,  1.67batch/s, auc=0.8752, loss=0.5823]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 163/292 [01:50<01:15,  1.71batch/s, auc=0.8752, loss=0.5823]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 163/292 [01:50<01:15,  1.71batch/s, auc=0.8758, loss=0.5663]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 164/292 [01:50<01:13,  1.73batch/s, auc=0.8758, loss=0.5663]\u001b[A\n",
      "Training Epoch 17/25:  56%|█████▌    | 164/292 [01:51<01:13,  1.73batch/s, auc=0.8758, loss=0.9264]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 165/292 [01:51<01:12,  1.74batch/s, auc=0.8758, loss=0.9264]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 165/292 [01:52<01:12,  1.74batch/s, auc=0.8753, loss=0.9118]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 166/292 [01:52<01:11,  1.76batch/s, auc=0.8753, loss=0.9118]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 166/292 [01:52<01:11,  1.76batch/s, auc=0.8753, loss=0.6569]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 167/292 [01:52<01:10,  1.77batch/s, auc=0.8753, loss=0.6569]\u001b[A\n",
      "Training Epoch 17/25:  57%|█████▋    | 167/292 [01:53<01:10,  1.77batch/s, auc=0.8757, loss=0.4188]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 168/292 [01:53<01:09,  1.77batch/s, auc=0.8757, loss=0.4188]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 168/292 [01:54<01:09,  1.77batch/s, auc=0.8751, loss=0.9554]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 169/292 [01:54<01:28,  1.39batch/s, auc=0.8751, loss=0.9554]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 169/292 [01:55<01:28,  1.39batch/s, auc=0.8746, loss=0.9815]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 170/292 [01:55<01:40,  1.21batch/s, auc=0.8746, loss=0.9815]\u001b[A\n",
      "Training Epoch 17/25:  58%|█████▊    | 170/292 [01:55<01:40,  1.21batch/s, auc=0.8751, loss=0.6269]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▊    | 171/292 [01:55<01:30,  1.34batch/s, auc=0.8751, loss=0.6269]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▊    | 171/292 [01:56<01:30,  1.34batch/s, auc=0.8750, loss=0.9362]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 172/292 [01:56<01:22,  1.45batch/s, auc=0.8750, loss=0.9362]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 172/292 [01:56<01:22,  1.45batch/s, auc=0.8749, loss=0.9737]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 173/292 [01:56<01:17,  1.54batch/s, auc=0.8749, loss=0.9737]\u001b[A\n",
      "Training Epoch 17/25:  59%|█████▉    | 173/292 [01:58<01:17,  1.54batch/s, auc=0.8740, loss=1.1713]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 174/292 [01:58<01:31,  1.28batch/s, auc=0.8740, loss=1.1713]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 174/292 [01:58<01:31,  1.28batch/s, auc=0.8744, loss=0.5563]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 175/292 [01:58<01:23,  1.40batch/s, auc=0.8744, loss=0.5563]\u001b[A\n",
      "Training Epoch 17/25:  60%|█████▉    | 175/292 [01:59<01:23,  1.40batch/s, auc=0.8745, loss=0.5690]\u001b[A\n",
      "Training Epoch 17/25:  60%|██████    | 176/292 [01:59<01:17,  1.50batch/s, auc=0.8745, loss=0.5690]\u001b[A\n",
      "Training Epoch 17/25:  60%|██████    | 176/292 [01:59<01:17,  1.50batch/s, auc=0.8745, loss=0.5891]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 177/292 [01:59<01:13,  1.58batch/s, auc=0.8745, loss=0.5891]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 177/292 [02:00<01:13,  1.58batch/s, auc=0.8740, loss=0.8992]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 178/292 [02:00<01:27,  1.30batch/s, auc=0.8740, loss=0.8992]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████    | 178/292 [02:01<01:27,  1.30batch/s, auc=0.8740, loss=0.7671]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████▏   | 179/292 [02:01<01:19,  1.42batch/s, auc=0.8740, loss=0.7671]\u001b[A\n",
      "Training Epoch 17/25:  61%|██████▏   | 179/292 [02:01<01:19,  1.42batch/s, auc=0.8745, loss=0.5880]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 180/292 [02:01<01:14,  1.51batch/s, auc=0.8745, loss=0.5880]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 180/292 [02:02<01:14,  1.51batch/s, auc=0.8749, loss=0.7354]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 181/292 [02:02<01:10,  1.59batch/s, auc=0.8749, loss=0.7354]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 181/292 [02:03<01:10,  1.59batch/s, auc=0.8751, loss=0.5757]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 182/292 [02:03<01:07,  1.64batch/s, auc=0.8751, loss=0.5757]\u001b[A\n",
      "Training Epoch 17/25:  62%|██████▏   | 182/292 [02:04<01:07,  1.64batch/s, auc=0.8751, loss=0.7726]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 183/292 [02:04<01:21,  1.33batch/s, auc=0.8751, loss=0.7726]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 183/292 [02:04<01:21,  1.33batch/s, auc=0.8753, loss=0.6251]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 184/292 [02:04<01:14,  1.44batch/s, auc=0.8753, loss=0.6251]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 184/292 [02:05<01:14,  1.44batch/s, auc=0.8756, loss=0.6592]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 185/292 [02:05<01:09,  1.53batch/s, auc=0.8756, loss=0.6592]\u001b[A\n",
      "Training Epoch 17/25:  63%|██████▎   | 185/292 [02:05<01:09,  1.53batch/s, auc=0.8755, loss=0.7976]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▎   | 186/292 [02:05<01:06,  1.60batch/s, auc=0.8755, loss=0.7976]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▎   | 186/292 [02:06<01:06,  1.60batch/s, auc=0.8750, loss=0.8809]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 187/292 [02:06<01:19,  1.31batch/s, auc=0.8750, loss=0.8809]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 187/292 [02:07<01:19,  1.31batch/s, auc=0.8747, loss=0.8067]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 188/292 [02:07<01:12,  1.43batch/s, auc=0.8747, loss=0.8067]\u001b[A\n",
      "Training Epoch 17/25:  64%|██████▍   | 188/292 [02:07<01:12,  1.43batch/s, auc=0.8752, loss=0.5371]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▍   | 189/292 [02:07<01:07,  1.52batch/s, auc=0.8752, loss=0.5371]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▍   | 189/292 [02:08<01:07,  1.52batch/s, auc=0.8755, loss=0.4706]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 190/292 [02:08<01:04,  1.59batch/s, auc=0.8755, loss=0.4706]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 190/292 [02:09<01:04,  1.59batch/s, auc=0.8757, loss=0.6954]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 191/292 [02:09<01:01,  1.64batch/s, auc=0.8757, loss=0.6954]\u001b[A\n",
      "Training Epoch 17/25:  65%|██████▌   | 191/292 [02:10<01:01,  1.64batch/s, auc=0.8749, loss=1.2184]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 192/292 [02:10<01:14,  1.33batch/s, auc=0.8749, loss=1.2184]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 192/292 [02:10<01:14,  1.33batch/s, auc=0.8752, loss=0.4791]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 193/292 [02:10<01:08,  1.44batch/s, auc=0.8752, loss=0.4791]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▌   | 193/292 [02:11<01:08,  1.44batch/s, auc=0.8746, loss=1.0011]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▋   | 194/292 [02:11<01:04,  1.53batch/s, auc=0.8746, loss=1.0011]\u001b[A\n",
      "Training Epoch 17/25:  66%|██████▋   | 194/292 [02:11<01:04,  1.53batch/s, auc=0.8750, loss=0.5359]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 195/292 [02:11<01:00,  1.60batch/s, auc=0.8750, loss=0.5359]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 195/292 [02:12<01:00,  1.60batch/s, auc=0.8751, loss=0.7849]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 196/292 [02:12<00:58,  1.65batch/s, auc=0.8751, loss=0.7849]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 196/292 [02:13<00:58,  1.65batch/s, auc=0.8746, loss=1.2253]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 197/292 [02:13<01:11,  1.34batch/s, auc=0.8746, loss=1.2253]\u001b[A\n",
      "Training Epoch 17/25:  67%|██████▋   | 197/292 [02:14<01:11,  1.34batch/s, auc=0.8745, loss=0.7238]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 198/292 [02:14<01:05,  1.45batch/s, auc=0.8745, loss=0.7238]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 198/292 [02:15<01:05,  1.45batch/s, auc=0.8740, loss=1.0785]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 199/292 [02:15<01:15,  1.24batch/s, auc=0.8740, loss=1.0785]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 199/292 [02:15<01:15,  1.24batch/s, auc=0.8739, loss=0.5219]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 200/292 [02:15<01:07,  1.36batch/s, auc=0.8739, loss=0.5219]\u001b[A\n",
      "Training Epoch 17/25:  68%|██████▊   | 200/292 [02:16<01:07,  1.36batch/s, auc=0.8739, loss=0.7283]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 201/292 [02:16<01:02,  1.47batch/s, auc=0.8739, loss=0.7283]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 201/292 [02:16<01:02,  1.47batch/s, auc=0.8740, loss=0.7163]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 202/292 [02:16<00:58,  1.54batch/s, auc=0.8740, loss=0.7163]\u001b[A\n",
      "Training Epoch 17/25:  69%|██████▉   | 202/292 [02:17<00:58,  1.54batch/s, auc=0.8743, loss=0.5841]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 203/292 [02:17<00:55,  1.61batch/s, auc=0.8743, loss=0.5841]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 203/292 [02:17<00:55,  1.61batch/s, auc=0.8743, loss=0.7725]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 204/292 [02:17<00:53,  1.66batch/s, auc=0.8743, loss=0.7725]\u001b[A\n",
      "Training Epoch 17/25:  70%|██████▉   | 204/292 [02:18<00:53,  1.66batch/s, auc=0.8746, loss=0.5063]\u001b[A\n",
      "Training Epoch 17/25:  70%|███████   | 205/292 [02:18<00:51,  1.69batch/s, auc=0.8746, loss=0.5063]\u001b[A\n",
      "Training Epoch 17/25:  70%|███████   | 205/292 [02:19<00:51,  1.69batch/s, auc=0.8750, loss=0.4392]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 206/292 [02:19<00:50,  1.72batch/s, auc=0.8750, loss=0.4392]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 206/292 [02:20<00:50,  1.72batch/s, auc=0.8749, loss=0.7550]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 207/292 [02:20<01:02,  1.37batch/s, auc=0.8749, loss=0.7550]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 207/292 [02:20<01:02,  1.37batch/s, auc=0.8751, loss=0.7921]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 208/292 [02:20<00:57,  1.47batch/s, auc=0.8751, loss=0.7921]\u001b[A\n",
      "Training Epoch 17/25:  71%|███████   | 208/292 [02:21<00:57,  1.47batch/s, auc=0.8755, loss=0.5115]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 209/292 [02:21<00:53,  1.55batch/s, auc=0.8755, loss=0.5115]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 209/292 [02:21<00:53,  1.55batch/s, auc=0.8757, loss=0.6435]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 210/292 [02:21<00:50,  1.61batch/s, auc=0.8757, loss=0.6435]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 210/292 [02:22<00:50,  1.61batch/s, auc=0.8760, loss=0.6659]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 211/292 [02:22<00:48,  1.66batch/s, auc=0.8760, loss=0.6659]\u001b[A\n",
      "Training Epoch 17/25:  72%|███████▏  | 211/292 [02:22<00:48,  1.66batch/s, auc=0.8759, loss=0.7198]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 212/292 [02:22<00:47,  1.70batch/s, auc=0.8759, loss=0.7198]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 212/292 [02:24<00:47,  1.70batch/s, auc=0.8751, loss=1.3272]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 213/292 [02:24<00:58,  1.36batch/s, auc=0.8751, loss=1.3272]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 213/292 [02:24<00:58,  1.36batch/s, auc=0.8751, loss=0.8639]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 214/292 [02:24<00:53,  1.46batch/s, auc=0.8751, loss=0.8639]\u001b[A\n",
      "Training Epoch 17/25:  73%|███████▎  | 214/292 [02:25<00:53,  1.46batch/s, auc=0.8749, loss=0.5972]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▎  | 215/292 [02:25<00:49,  1.55batch/s, auc=0.8749, loss=0.5972]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▎  | 215/292 [02:25<00:49,  1.55batch/s, auc=0.8753, loss=0.6454]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 216/292 [02:25<00:47,  1.61batch/s, auc=0.8753, loss=0.6454]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 216/292 [02:26<00:47,  1.61batch/s, auc=0.8753, loss=0.6744]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 217/292 [02:26<00:45,  1.66batch/s, auc=0.8753, loss=0.6744]\u001b[A\n",
      "Training Epoch 17/25:  74%|███████▍  | 217/292 [02:26<00:45,  1.66batch/s, auc=0.8758, loss=0.5274]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▍  | 218/292 [02:26<00:43,  1.69batch/s, auc=0.8758, loss=0.5274]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▍  | 218/292 [02:27<00:43,  1.69batch/s, auc=0.8759, loss=0.6623]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 219/292 [02:27<00:42,  1.72batch/s, auc=0.8759, loss=0.6623]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 219/292 [02:27<00:42,  1.72batch/s, auc=0.8763, loss=0.5213]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 220/292 [02:27<00:41,  1.73batch/s, auc=0.8763, loss=0.5213]\u001b[A\n",
      "Training Epoch 17/25:  75%|███████▌  | 220/292 [02:29<00:41,  1.73batch/s, auc=0.8755, loss=1.2493]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 221/292 [02:29<00:51,  1.38batch/s, auc=0.8755, loss=1.2493]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 221/292 [02:29<00:51,  1.38batch/s, auc=0.8756, loss=0.8355]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 222/292 [02:29<00:47,  1.48batch/s, auc=0.8756, loss=0.8355]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▌  | 222/292 [02:30<00:47,  1.48batch/s, auc=0.8761, loss=0.4534]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▋  | 223/292 [02:30<00:44,  1.55batch/s, auc=0.8761, loss=0.4534]\u001b[A\n",
      "Training Epoch 17/25:  76%|███████▋  | 223/292 [02:30<00:44,  1.55batch/s, auc=0.8756, loss=1.0061]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 224/292 [02:30<00:42,  1.62batch/s, auc=0.8756, loss=1.0061]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 224/292 [02:31<00:42,  1.62batch/s, auc=0.8757, loss=0.7487]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 225/292 [02:31<00:50,  1.32batch/s, auc=0.8757, loss=0.7487]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 225/292 [02:32<00:50,  1.32batch/s, auc=0.8755, loss=0.6002]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 226/292 [02:32<00:46,  1.43batch/s, auc=0.8755, loss=0.6002]\u001b[A\n",
      "Training Epoch 17/25:  77%|███████▋  | 226/292 [02:32<00:46,  1.43batch/s, auc=0.8760, loss=0.6054]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 227/292 [02:32<00:42,  1.52batch/s, auc=0.8760, loss=0.6054]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 227/292 [02:33<00:42,  1.52batch/s, auc=0.8761, loss=0.5685]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 228/292 [02:33<00:40,  1.59batch/s, auc=0.8761, loss=0.5685]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 228/292 [02:34<00:40,  1.59batch/s, auc=0.8759, loss=1.1442]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 229/292 [02:34<00:38,  1.64batch/s, auc=0.8759, loss=1.1442]\u001b[A\n",
      "Training Epoch 17/25:  78%|███████▊  | 229/292 [02:34<00:38,  1.64batch/s, auc=0.8757, loss=0.8855]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 230/292 [02:34<00:36,  1.68batch/s, auc=0.8757, loss=0.8855]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 230/292 [02:35<00:36,  1.68batch/s, auc=0.8759, loss=0.7978]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 231/292 [02:35<00:35,  1.71batch/s, auc=0.8759, loss=0.7978]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 231/292 [02:35<00:35,  1.71batch/s, auc=0.8760, loss=0.6725]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 232/292 [02:35<00:34,  1.73batch/s, auc=0.8760, loss=0.6725]\u001b[A\n",
      "Training Epoch 17/25:  79%|███████▉  | 232/292 [02:36<00:34,  1.73batch/s, auc=0.8754, loss=1.1993]\u001b[A\n",
      "Training Epoch 17/25:  80%|███████▉  | 233/292 [02:36<00:42,  1.37batch/s, auc=0.8754, loss=1.1993]\u001b[A\n",
      "Training Epoch 17/25:  80%|███████▉  | 233/292 [02:37<00:42,  1.37batch/s, auc=0.8754, loss=0.5731]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 234/292 [02:37<00:39,  1.47batch/s, auc=0.8754, loss=0.5731]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 234/292 [02:37<00:39,  1.47batch/s, auc=0.8756, loss=0.5108]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 235/292 [02:37<00:36,  1.55batch/s, auc=0.8756, loss=0.5108]\u001b[A\n",
      "Training Epoch 17/25:  80%|████████  | 235/292 [02:38<00:36,  1.55batch/s, auc=0.8759, loss=0.5608]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 236/292 [02:38<00:34,  1.61batch/s, auc=0.8759, loss=0.5608]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 236/292 [02:39<00:34,  1.61batch/s, auc=0.8761, loss=0.5630]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 237/292 [02:39<00:33,  1.66batch/s, auc=0.8761, loss=0.5630]\u001b[A\n",
      "Training Epoch 17/25:  81%|████████  | 237/292 [02:39<00:33,  1.66batch/s, auc=0.8765, loss=0.6947]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 238/292 [02:39<00:31,  1.69batch/s, auc=0.8765, loss=0.6947]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 238/292 [02:40<00:31,  1.69batch/s, auc=0.8766, loss=0.6336]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 239/292 [02:40<00:30,  1.72batch/s, auc=0.8766, loss=0.6336]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 239/292 [02:40<00:30,  1.72batch/s, auc=0.8763, loss=0.8054]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 240/292 [02:40<00:29,  1.73batch/s, auc=0.8763, loss=0.8054]\u001b[A\n",
      "Training Epoch 17/25:  82%|████████▏ | 240/292 [02:41<00:29,  1.73batch/s, auc=0.8760, loss=0.9136]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 241/292 [02:41<00:37,  1.37batch/s, auc=0.8760, loss=0.9136]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 241/292 [02:42<00:37,  1.37batch/s, auc=0.8756, loss=1.1282]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 242/292 [02:42<00:41,  1.20batch/s, auc=0.8756, loss=1.1282]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 242/292 [02:43<00:41,  1.20batch/s, auc=0.8756, loss=0.5485]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 243/292 [02:43<00:36,  1.33batch/s, auc=0.8756, loss=0.5485]\u001b[A\n",
      "Training Epoch 17/25:  83%|████████▎ | 243/292 [02:44<00:36,  1.33batch/s, auc=0.8761, loss=0.5060]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▎ | 244/292 [02:44<00:33,  1.44batch/s, auc=0.8761, loss=0.5060]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▎ | 244/292 [02:44<00:33,  1.44batch/s, auc=0.8764, loss=0.7523]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 245/292 [02:44<00:30,  1.53batch/s, auc=0.8764, loss=0.7523]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 245/292 [02:45<00:30,  1.53batch/s, auc=0.8766, loss=0.5796]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 246/292 [02:45<00:28,  1.59batch/s, auc=0.8766, loss=0.5796]\u001b[A\n",
      "Training Epoch 17/25:  84%|████████▍ | 246/292 [02:45<00:28,  1.59batch/s, auc=0.8761, loss=1.1439]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 247/292 [02:45<00:27,  1.64batch/s, auc=0.8761, loss=1.1439]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 247/292 [02:46<00:27,  1.64batch/s, auc=0.8759, loss=0.6831]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 248/292 [02:46<00:26,  1.68batch/s, auc=0.8759, loss=0.6831]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▍ | 248/292 [02:47<00:26,  1.68batch/s, auc=0.8756, loss=0.9376]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▌ | 249/292 [02:47<00:31,  1.35batch/s, auc=0.8756, loss=0.9376]\u001b[A\n",
      "Training Epoch 17/25:  85%|████████▌ | 249/292 [02:47<00:31,  1.35batch/s, auc=0.8757, loss=0.6860]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 250/292 [02:47<00:28,  1.46batch/s, auc=0.8757, loss=0.6860]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 250/292 [02:48<00:28,  1.46batch/s, auc=0.8754, loss=1.0479]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 251/292 [02:48<00:26,  1.54batch/s, auc=0.8754, loss=1.0479]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▌ | 251/292 [02:49<00:26,  1.54batch/s, auc=0.8758, loss=0.5293]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▋ | 252/292 [02:49<00:24,  1.61batch/s, auc=0.8758, loss=0.5293]\u001b[A\n",
      "Training Epoch 17/25:  86%|████████▋ | 252/292 [02:49<00:24,  1.61batch/s, auc=0.8761, loss=0.8187]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 253/292 [02:49<00:23,  1.65batch/s, auc=0.8761, loss=0.8187]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 253/292 [02:50<00:23,  1.65batch/s, auc=0.8762, loss=0.5551]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 254/292 [02:50<00:22,  1.69batch/s, auc=0.8762, loss=0.5551]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 254/292 [02:50<00:22,  1.69batch/s, auc=0.8763, loss=0.4813]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 255/292 [02:50<00:21,  1.71batch/s, auc=0.8763, loss=0.4813]\u001b[A\n",
      "Training Epoch 17/25:  87%|████████▋ | 255/292 [02:51<00:21,  1.71batch/s, auc=0.8766, loss=0.4279]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 256/292 [02:51<00:20,  1.73batch/s, auc=0.8766, loss=0.4279]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 256/292 [02:51<00:20,  1.73batch/s, auc=0.8769, loss=0.6327]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 257/292 [02:51<00:20,  1.74batch/s, auc=0.8769, loss=0.6327]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 257/292 [02:52<00:20,  1.74batch/s, auc=0.8771, loss=0.6456]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 258/292 [02:52<00:19,  1.75batch/s, auc=0.8771, loss=0.6456]\u001b[A\n",
      "Training Epoch 17/25:  88%|████████▊ | 258/292 [02:53<00:19,  1.75batch/s, auc=0.8772, loss=0.7002]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▊ | 259/292 [02:53<00:18,  1.76batch/s, auc=0.8772, loss=0.7002]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▊ | 259/292 [02:53<00:18,  1.76batch/s, auc=0.8770, loss=0.8391]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 260/292 [02:53<00:18,  1.76batch/s, auc=0.8770, loss=0.8391]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 260/292 [02:54<00:18,  1.76batch/s, auc=0.8764, loss=1.2282]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 261/292 [02:54<00:22,  1.39batch/s, auc=0.8764, loss=1.2282]\u001b[A\n",
      "Training Epoch 17/25:  89%|████████▉ | 261/292 [02:55<00:22,  1.39batch/s, auc=0.8762, loss=0.8454]\u001b[A\n",
      "Training Epoch 17/25:  90%|████████▉ | 262/292 [02:55<00:20,  1.48batch/s, auc=0.8762, loss=0.8454]\u001b[A\n",
      "Training Epoch 17/25:  90%|████████▉ | 262/292 [02:55<00:20,  1.48batch/s, auc=0.8764, loss=0.5115]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 263/292 [02:55<00:18,  1.56batch/s, auc=0.8764, loss=0.5115]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 263/292 [02:56<00:18,  1.56batch/s, auc=0.8764, loss=0.7004]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 264/292 [02:56<00:17,  1.62batch/s, auc=0.8764, loss=0.7004]\u001b[A\n",
      "Training Epoch 17/25:  90%|█████████ | 264/292 [02:56<00:17,  1.62batch/s, auc=0.8767, loss=0.5791]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 265/292 [02:56<00:16,  1.66batch/s, auc=0.8767, loss=0.5791]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 265/292 [02:57<00:16,  1.66batch/s, auc=0.8764, loss=1.0216]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 266/292 [02:57<00:15,  1.69batch/s, auc=0.8764, loss=1.0216]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████ | 266/292 [02:58<00:15,  1.69batch/s, auc=0.8764, loss=0.5602]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████▏| 267/292 [02:58<00:14,  1.72batch/s, auc=0.8764, loss=0.5602]\u001b[A\n",
      "Training Epoch 17/25:  91%|█████████▏| 267/292 [02:58<00:14,  1.72batch/s, auc=0.8766, loss=0.5324]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 268/292 [02:58<00:13,  1.73batch/s, auc=0.8766, loss=0.5324]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 268/292 [02:59<00:13,  1.73batch/s, auc=0.8769, loss=0.5747]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 269/292 [02:59<00:13,  1.75batch/s, auc=0.8769, loss=0.5747]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 269/292 [02:59<00:13,  1.75batch/s, auc=0.8772, loss=0.6248]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 270/292 [02:59<00:12,  1.75batch/s, auc=0.8772, loss=0.6248]\u001b[A\n",
      "Training Epoch 17/25:  92%|█████████▏| 270/292 [03:00<00:12,  1.75batch/s, auc=0.8773, loss=0.6982]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 271/292 [03:00<00:11,  1.76batch/s, auc=0.8773, loss=0.6982]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 271/292 [03:00<00:11,  1.76batch/s, auc=0.8774, loss=0.6715]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 272/292 [03:00<00:11,  1.76batch/s, auc=0.8774, loss=0.6715]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 272/292 [03:01<00:11,  1.76batch/s, auc=0.8774, loss=0.6730]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 273/292 [03:01<00:10,  1.77batch/s, auc=0.8774, loss=0.6730]\u001b[A\n",
      "Training Epoch 17/25:  93%|█████████▎| 273/292 [03:01<00:10,  1.77batch/s, auc=0.8773, loss=0.6985]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 274/292 [03:02<00:10,  1.77batch/s, auc=0.8773, loss=0.6985]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 274/292 [03:02<00:10,  1.77batch/s, auc=0.8775, loss=0.5395]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 275/292 [03:02<00:09,  1.77batch/s, auc=0.8775, loss=0.5395]\u001b[A\n",
      "Training Epoch 17/25:  94%|█████████▍| 275/292 [03:03<00:09,  1.77batch/s, auc=0.8774, loss=0.8905]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 276/292 [03:03<00:09,  1.77batch/s, auc=0.8774, loss=0.8905]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 276/292 [03:03<00:09,  1.77batch/s, auc=0.8777, loss=0.5253]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 277/292 [03:03<00:08,  1.77batch/s, auc=0.8777, loss=0.5253]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▍| 277/292 [03:04<00:08,  1.77batch/s, auc=0.8778, loss=0.5808]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▌| 278/292 [03:04<00:07,  1.77batch/s, auc=0.8778, loss=0.5808]\u001b[A\n",
      "Training Epoch 17/25:  95%|█████████▌| 278/292 [03:04<00:07,  1.77batch/s, auc=0.8779, loss=0.5856]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 279/292 [03:04<00:07,  1.77batch/s, auc=0.8779, loss=0.5856]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 279/292 [03:05<00:07,  1.77batch/s, auc=0.8775, loss=1.0312]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 280/292 [03:05<00:08,  1.39batch/s, auc=0.8775, loss=1.0312]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 280/292 [03:06<00:08,  1.39batch/s, auc=0.8776, loss=0.5874]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 281/292 [03:06<00:07,  1.49batch/s, auc=0.8776, loss=0.5874]\u001b[A\n",
      "Training Epoch 17/25:  96%|█████████▌| 281/292 [03:07<00:07,  1.49batch/s, auc=0.8776, loss=0.6141]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 282/292 [03:07<00:06,  1.56batch/s, auc=0.8776, loss=0.6141]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 282/292 [03:07<00:06,  1.56batch/s, auc=0.8775, loss=0.8114]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 283/292 [03:07<00:05,  1.62batch/s, auc=0.8775, loss=0.8114]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 283/292 [03:08<00:05,  1.62batch/s, auc=0.8773, loss=0.6353]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 284/292 [03:08<00:04,  1.66batch/s, auc=0.8773, loss=0.6353]\u001b[A\n",
      "Training Epoch 17/25:  97%|█████████▋| 284/292 [03:08<00:04,  1.66batch/s, auc=0.8771, loss=0.8528]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.69batch/s, auc=0.8771, loss=0.8528]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 285/292 [03:09<00:04,  1.69batch/s, auc=0.8769, loss=0.9272]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 286/292 [03:09<00:03,  1.72batch/s, auc=0.8769, loss=0.9272]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 286/292 [03:10<00:03,  1.72batch/s, auc=0.8764, loss=1.0177]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 287/292 [03:10<00:03,  1.37batch/s, auc=0.8764, loss=1.0177]\u001b[A\n",
      "Training Epoch 17/25:  98%|█████████▊| 287/292 [03:11<00:03,  1.37batch/s, auc=0.8763, loss=0.9876]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▊| 288/292 [03:11<00:03,  1.20batch/s, auc=0.8763, loss=0.9876]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▊| 288/292 [03:12<00:03,  1.20batch/s, auc=0.8762, loss=0.6722]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 289/292 [03:12<00:02,  1.33batch/s, auc=0.8762, loss=0.6722]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 289/292 [03:12<00:02,  1.33batch/s, auc=0.8764, loss=0.5715]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 290/292 [03:12<00:01,  1.43batch/s, auc=0.8764, loss=0.5715]\u001b[A\n",
      "Training Epoch 17/25:  99%|█████████▉| 290/292 [03:13<00:01,  1.43batch/s, auc=0.8766, loss=0.6169]\u001b[A\n",
      "Training Epoch 17/25: 100%|█████████▉| 291/292 [03:13<00:00,  1.52batch/s, auc=0.8766, loss=0.6169]\u001b[A\n",
      "Training Epoch 17/25: 100%|█████████▉| 291/292 [03:13<00:00,  1.52batch/s, auc=0.8766, loss=0.7296]\u001b[A\n",
      "Training Epoch 17/25: 100%|██████████| 292/292 [03:13<00:00,  1.51batch/s, auc=0.8766, loss=0.7296]\u001b[A\n",
      "Epochs:  68%|██████▊   | 17/25 [1:00:35<28:44, 215.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25] Train Loss: 0.7363 | Train AUROC: 0.8766 Val Loss: 0.8617 | Val AUROC: 0.8472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 18/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9495, loss=0.5734]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 1/292 [00:01<07:25,  1.53s/batch, auc=0.9495, loss=0.5734]\u001b[A\n",
      "Training Epoch 18/25:   0%|          | 1/292 [00:02<07:25,  1.53s/batch, auc=0.9520, loss=0.5813]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 2/292 [00:02<04:37,  1.05batch/s, auc=0.9520, loss=0.5813]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 2/292 [00:02<04:37,  1.05batch/s, auc=0.9381, loss=0.6654]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 3/292 [00:02<03:43,  1.29batch/s, auc=0.9381, loss=0.6654]\u001b[A\n",
      "Training Epoch 18/25:   1%|          | 3/292 [00:03<03:43,  1.29batch/s, auc=0.9325, loss=0.5413]\u001b[A\n",
      "Training Epoch 18/25:   1%|▏         | 4/292 [00:03<03:18,  1.45batch/s, auc=0.9325, loss=0.5413]\u001b[A\n",
      "Training Epoch 18/25:   1%|▏         | 4/292 [00:04<03:18,  1.45batch/s, auc=0.9236, loss=0.7511]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 5/292 [00:04<03:57,  1.21batch/s, auc=0.9236, loss=0.7511]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 5/292 [00:04<03:57,  1.21batch/s, auc=0.9067, loss=0.8197]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 6/292 [00:04<03:29,  1.36batch/s, auc=0.9067, loss=0.8197]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 6/292 [00:05<03:29,  1.36batch/s, auc=0.8937, loss=0.8271]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 7/292 [00:05<03:12,  1.48batch/s, auc=0.8937, loss=0.8271]\u001b[A\n",
      "Training Epoch 18/25:   2%|▏         | 7/292 [00:05<03:12,  1.48batch/s, auc=0.8971, loss=0.6804]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 8/292 [00:05<03:00,  1.57batch/s, auc=0.8971, loss=0.6804]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 8/292 [00:07<03:00,  1.57batch/s, auc=0.8825, loss=1.0914]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 9/292 [00:07<03:38,  1.30batch/s, auc=0.8825, loss=1.0914]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 9/292 [00:08<03:38,  1.30batch/s, auc=0.8681, loss=1.2402]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 10/292 [00:08<04:03,  1.16batch/s, auc=0.8681, loss=1.2402]\u001b[A\n",
      "Training Epoch 18/25:   3%|▎         | 10/292 [00:08<04:03,  1.16batch/s, auc=0.8736, loss=0.5839]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 11/292 [00:08<03:36,  1.30batch/s, auc=0.8736, loss=0.5839]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 11/292 [00:09<03:36,  1.30batch/s, auc=0.8789, loss=0.5081]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 12/292 [00:09<03:16,  1.42batch/s, auc=0.8789, loss=0.5081]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 12/292 [00:10<03:16,  1.42batch/s, auc=0.8769, loss=0.8108]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 13/292 [00:10<03:47,  1.23batch/s, auc=0.8769, loss=0.8108]\u001b[A\n",
      "Training Epoch 18/25:   4%|▍         | 13/292 [00:10<03:47,  1.23batch/s, auc=0.8784, loss=0.7726]\u001b[A\n",
      "Training Epoch 18/25:   5%|▍         | 14/292 [00:10<03:24,  1.36batch/s, auc=0.8784, loss=0.7726]\u001b[A\n",
      "Training Epoch 18/25:   5%|▍         | 14/292 [00:11<03:24,  1.36batch/s, auc=0.8773, loss=0.8856]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 15/292 [00:11<03:08,  1.47batch/s, auc=0.8773, loss=0.8856]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 15/292 [00:11<03:08,  1.47batch/s, auc=0.8805, loss=0.4681]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 16/292 [00:11<02:57,  1.56batch/s, auc=0.8805, loss=0.4681]\u001b[A\n",
      "Training Epoch 18/25:   5%|▌         | 16/292 [00:12<02:57,  1.56batch/s, auc=0.8759, loss=0.8300]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 17/292 [00:12<03:31,  1.30batch/s, auc=0.8759, loss=0.8300]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 17/292 [00:14<03:31,  1.30batch/s, auc=0.8709, loss=1.0400]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 18/292 [00:14<03:55,  1.16batch/s, auc=0.8709, loss=1.0400]\u001b[A\n",
      "Training Epoch 18/25:   6%|▌         | 18/292 [00:14<03:55,  1.16batch/s, auc=0.8717, loss=0.6328]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 19/292 [00:14<03:29,  1.30batch/s, auc=0.8717, loss=0.6328]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 19/292 [00:15<03:29,  1.30batch/s, auc=0.8751, loss=0.5009]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 20/292 [00:15<03:11,  1.42batch/s, auc=0.8751, loss=0.5009]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 20/292 [00:16<03:11,  1.42batch/s, auc=0.8668, loss=1.1815]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 21/292 [00:16<03:40,  1.23batch/s, auc=0.8668, loss=1.1815]\u001b[A\n",
      "Training Epoch 18/25:   7%|▋         | 21/292 [00:16<03:40,  1.23batch/s, auc=0.8692, loss=0.5858]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 22/292 [00:16<03:18,  1.36batch/s, auc=0.8692, loss=0.5858]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 22/292 [00:17<03:18,  1.36batch/s, auc=0.8699, loss=0.6158]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 23/292 [00:17<03:03,  1.47batch/s, auc=0.8699, loss=0.6158]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 23/292 [00:17<03:03,  1.47batch/s, auc=0.8729, loss=0.6926]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 24/292 [00:17<02:52,  1.55batch/s, auc=0.8729, loss=0.6926]\u001b[A\n",
      "Training Epoch 18/25:   8%|▊         | 24/292 [00:18<02:52,  1.55batch/s, auc=0.8766, loss=0.4970]\u001b[A\n",
      "Training Epoch 18/25:   9%|▊         | 25/292 [00:18<02:44,  1.62batch/s, auc=0.8766, loss=0.4970]\u001b[A\n",
      "Training Epoch 18/25:   9%|▊         | 25/292 [00:18<02:44,  1.62batch/s, auc=0.8766, loss=0.7644]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 26/292 [00:18<02:39,  1.67batch/s, auc=0.8766, loss=0.7644]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 26/292 [00:20<02:39,  1.67batch/s, auc=0.8776, loss=0.7131]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 27/292 [00:20<03:15,  1.35batch/s, auc=0.8776, loss=0.7131]\u001b[A\n",
      "Training Epoch 18/25:   9%|▉         | 27/292 [00:20<03:15,  1.35batch/s, auc=0.8757, loss=0.7326]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 28/292 [00:20<03:00,  1.46batch/s, auc=0.8757, loss=0.7326]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 28/292 [00:21<03:00,  1.46batch/s, auc=0.8751, loss=0.6749]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 29/292 [00:21<02:49,  1.55batch/s, auc=0.8751, loss=0.6749]\u001b[A\n",
      "Training Epoch 18/25:  10%|▉         | 29/292 [00:21<02:49,  1.55batch/s, auc=0.8777, loss=0.5414]\u001b[A\n",
      "Training Epoch 18/25:  10%|█         | 30/292 [00:21<02:41,  1.62batch/s, auc=0.8777, loss=0.5414]\u001b[A\n",
      "Training Epoch 18/25:  10%|█         | 30/292 [00:22<02:41,  1.62batch/s, auc=0.8776, loss=0.6045]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 31/292 [00:22<02:36,  1.67batch/s, auc=0.8776, loss=0.6045]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 31/292 [00:22<02:36,  1.67batch/s, auc=0.8795, loss=0.5026]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 32/292 [00:22<02:32,  1.71batch/s, auc=0.8795, loss=0.5026]\u001b[A\n",
      "Training Epoch 18/25:  11%|█         | 32/292 [00:23<02:32,  1.71batch/s, auc=0.8814, loss=0.5512]\u001b[A\n",
      "Training Epoch 18/25:  11%|█▏        | 33/292 [00:23<02:29,  1.73batch/s, auc=0.8814, loss=0.5512]\u001b[A\n",
      "Training Epoch 18/25:  11%|█▏        | 33/292 [00:23<02:29,  1.73batch/s, auc=0.8823, loss=0.5378]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 34/292 [00:23<02:27,  1.75batch/s, auc=0.8823, loss=0.5378]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 34/292 [00:25<02:27,  1.75batch/s, auc=0.8763, loss=1.4539]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 35/292 [00:25<03:05,  1.39batch/s, auc=0.8763, loss=1.4539]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 35/292 [00:25<03:05,  1.39batch/s, auc=0.8774, loss=0.6807]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 36/292 [00:25<02:52,  1.49batch/s, auc=0.8774, loss=0.6807]\u001b[A\n",
      "Training Epoch 18/25:  12%|█▏        | 36/292 [00:26<02:52,  1.49batch/s, auc=0.8789, loss=0.6334]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 37/292 [00:26<02:42,  1.57batch/s, auc=0.8789, loss=0.6334]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 37/292 [00:26<02:42,  1.57batch/s, auc=0.8816, loss=0.3945]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 38/292 [00:26<02:35,  1.63batch/s, auc=0.8816, loss=0.3945]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 38/292 [00:27<02:35,  1.63batch/s, auc=0.8816, loss=0.5497]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 39/292 [00:27<02:30,  1.68batch/s, auc=0.8816, loss=0.5497]\u001b[A\n",
      "Training Epoch 18/25:  13%|█▎        | 39/292 [00:27<02:30,  1.68batch/s, auc=0.8824, loss=0.5896]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▎        | 40/292 [00:27<02:27,  1.71batch/s, auc=0.8824, loss=0.5896]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▎        | 40/292 [00:28<02:27,  1.71batch/s, auc=0.8817, loss=0.8346]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 41/292 [00:28<02:24,  1.73batch/s, auc=0.8817, loss=0.8346]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 41/292 [00:29<02:24,  1.73batch/s, auc=0.8818, loss=0.6787]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 42/292 [00:29<03:01,  1.38batch/s, auc=0.8818, loss=0.6787]\u001b[A\n",
      "Training Epoch 18/25:  14%|█▍        | 42/292 [00:30<03:01,  1.38batch/s, auc=0.8800, loss=0.9181]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▍        | 43/292 [00:30<03:26,  1.20batch/s, auc=0.8800, loss=0.9181]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▍        | 43/292 [00:31<03:26,  1.20batch/s, auc=0.8781, loss=0.9659]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 44/292 [00:31<03:43,  1.11batch/s, auc=0.8781, loss=0.9659]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 44/292 [00:32<03:43,  1.11batch/s, auc=0.8785, loss=0.5556]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 45/292 [00:32<03:17,  1.25batch/s, auc=0.8785, loss=0.5556]\u001b[A\n",
      "Training Epoch 18/25:  15%|█▌        | 45/292 [00:33<03:17,  1.25batch/s, auc=0.8789, loss=0.7986]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 46/292 [00:33<03:36,  1.14batch/s, auc=0.8789, loss=0.7986]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 46/292 [00:33<03:36,  1.14batch/s, auc=0.8792, loss=0.6874]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 47/292 [00:33<03:11,  1.28batch/s, auc=0.8792, loss=0.6874]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▌        | 47/292 [00:34<03:11,  1.28batch/s, auc=0.8758, loss=1.4129]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▋        | 48/292 [00:34<03:31,  1.15batch/s, auc=0.8758, loss=1.4129]\u001b[A\n",
      "Training Epoch 18/25:  16%|█▋        | 48/292 [00:35<03:31,  1.15batch/s, auc=0.8779, loss=0.5868]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 49/292 [00:35<03:08,  1.29batch/s, auc=0.8779, loss=0.5868]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 49/292 [00:36<03:08,  1.29batch/s, auc=0.8765, loss=1.0530]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 50/292 [00:36<03:28,  1.16batch/s, auc=0.8765, loss=1.0530]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 50/292 [00:37<03:28,  1.16batch/s, auc=0.8764, loss=0.9478]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 51/292 [00:37<03:05,  1.30batch/s, auc=0.8764, loss=0.9478]\u001b[A\n",
      "Training Epoch 18/25:  17%|█▋        | 51/292 [00:37<03:05,  1.30batch/s, auc=0.8763, loss=0.6240]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 52/292 [00:37<02:49,  1.41batch/s, auc=0.8763, loss=0.6240]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 52/292 [00:38<02:49,  1.41batch/s, auc=0.8776, loss=0.4902]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 53/292 [00:38<02:38,  1.51batch/s, auc=0.8776, loss=0.4902]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 53/292 [00:38<02:38,  1.51batch/s, auc=0.8768, loss=1.0521]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 54/292 [00:38<02:30,  1.59batch/s, auc=0.8768, loss=1.0521]\u001b[A\n",
      "Training Epoch 18/25:  18%|█▊        | 54/292 [00:39<02:30,  1.59batch/s, auc=0.8766, loss=0.6086]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 55/292 [00:39<02:24,  1.64batch/s, auc=0.8766, loss=0.6086]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 55/292 [00:39<02:24,  1.64batch/s, auc=0.8757, loss=1.0722]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 56/292 [00:39<02:19,  1.69batch/s, auc=0.8757, loss=1.0722]\u001b[A\n",
      "Training Epoch 18/25:  19%|█▉        | 56/292 [00:40<02:19,  1.69batch/s, auc=0.8773, loss=0.4478]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 57/292 [00:40<02:16,  1.72batch/s, auc=0.8773, loss=0.4478]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 57/292 [00:40<02:16,  1.72batch/s, auc=0.8782, loss=0.5509]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 58/292 [00:40<02:14,  1.74batch/s, auc=0.8782, loss=0.5509]\u001b[A\n",
      "Training Epoch 18/25:  20%|█▉        | 58/292 [00:41<02:14,  1.74batch/s, auc=0.8731, loss=1.8285]\u001b[A\n",
      "Training Epoch 18/25:  20%|██        | 59/292 [00:41<02:48,  1.38batch/s, auc=0.8731, loss=1.8285]\u001b[A\n",
      "Training Epoch 18/25:  20%|██        | 59/292 [00:43<02:48,  1.38batch/s, auc=0.8693, loss=1.8713]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 60/292 [00:43<03:12,  1.21batch/s, auc=0.8693, loss=1.8713]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 60/292 [00:43<03:12,  1.21batch/s, auc=0.8714, loss=0.4485]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 61/292 [00:43<02:52,  1.34batch/s, auc=0.8714, loss=0.4485]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 61/292 [00:44<02:52,  1.34batch/s, auc=0.8724, loss=0.8839]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 62/292 [00:44<02:38,  1.45batch/s, auc=0.8724, loss=0.8839]\u001b[A\n",
      "Training Epoch 18/25:  21%|██        | 62/292 [00:44<02:38,  1.45batch/s, auc=0.8733, loss=0.6155]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 63/292 [00:44<02:28,  1.54batch/s, auc=0.8733, loss=0.6155]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 63/292 [00:45<02:28,  1.54batch/s, auc=0.8734, loss=0.9559]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 64/292 [00:45<02:21,  1.61batch/s, auc=0.8734, loss=0.9559]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 64/292 [00:45<02:21,  1.61batch/s, auc=0.8735, loss=0.7669]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 65/292 [00:45<02:16,  1.66batch/s, auc=0.8735, loss=0.7669]\u001b[A\n",
      "Training Epoch 18/25:  22%|██▏       | 65/292 [00:46<02:16,  1.66batch/s, auc=0.8730, loss=0.7414]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 66/292 [00:46<02:13,  1.70batch/s, auc=0.8730, loss=0.7414]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 66/292 [00:46<02:13,  1.70batch/s, auc=0.8748, loss=0.5444]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 67/292 [00:46<02:10,  1.73batch/s, auc=0.8748, loss=0.5444]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 67/292 [00:47<02:10,  1.73batch/s, auc=0.8756, loss=0.4956]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 68/292 [00:47<02:08,  1.75batch/s, auc=0.8756, loss=0.4956]\u001b[A\n",
      "Training Epoch 18/25:  23%|██▎       | 68/292 [00:48<02:08,  1.75batch/s, auc=0.8767, loss=0.5844]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▎       | 69/292 [00:48<02:06,  1.76batch/s, auc=0.8767, loss=0.5844]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▎       | 69/292 [00:48<02:06,  1.76batch/s, auc=0.8762, loss=0.6864]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 70/292 [00:48<02:05,  1.77batch/s, auc=0.8762, loss=0.6864]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 70/292 [00:49<02:05,  1.77batch/s, auc=0.8763, loss=0.7036]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 71/292 [00:49<02:04,  1.78batch/s, auc=0.8763, loss=0.7036]\u001b[A\n",
      "Training Epoch 18/25:  24%|██▍       | 71/292 [00:49<02:04,  1.78batch/s, auc=0.8771, loss=0.6832]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▍       | 72/292 [00:49<02:03,  1.78batch/s, auc=0.8771, loss=0.6832]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▍       | 72/292 [00:50<02:03,  1.78batch/s, auc=0.8785, loss=0.4825]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 73/292 [00:50<02:02,  1.79batch/s, auc=0.8785, loss=0.4825]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 73/292 [00:50<02:02,  1.79batch/s, auc=0.8781, loss=0.8266]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 74/292 [00:50<02:01,  1.79batch/s, auc=0.8781, loss=0.8266]\u001b[A\n",
      "Training Epoch 18/25:  25%|██▌       | 74/292 [00:51<02:01,  1.79batch/s, auc=0.8784, loss=0.5255]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 75/292 [00:51<02:01,  1.79batch/s, auc=0.8784, loss=0.5255]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 75/292 [00:52<02:01,  1.79batch/s, auc=0.8769, loss=1.1476]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 76/292 [00:52<02:34,  1.40batch/s, auc=0.8769, loss=1.1476]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▌       | 76/292 [00:53<02:34,  1.40batch/s, auc=0.8773, loss=0.5513]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▋       | 77/292 [00:53<02:23,  1.50batch/s, auc=0.8773, loss=0.5513]\u001b[A\n",
      "Training Epoch 18/25:  26%|██▋       | 77/292 [00:53<02:23,  1.50batch/s, auc=0.8775, loss=0.5893]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 78/292 [00:53<02:15,  1.58batch/s, auc=0.8775, loss=0.5893]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 78/292 [00:54<02:15,  1.58batch/s, auc=0.8772, loss=0.7040]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 79/292 [00:54<02:09,  1.64batch/s, auc=0.8772, loss=0.7040]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 79/292 [00:55<02:09,  1.64batch/s, auc=0.8764, loss=0.9467]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 80/292 [00:55<02:38,  1.34batch/s, auc=0.8764, loss=0.9467]\u001b[A\n",
      "Training Epoch 18/25:  27%|██▋       | 80/292 [00:55<02:38,  1.34batch/s, auc=0.8766, loss=0.7827]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 81/292 [00:55<02:25,  1.45batch/s, auc=0.8766, loss=0.7827]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 81/292 [00:56<02:25,  1.45batch/s, auc=0.8763, loss=0.7837]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 82/292 [00:56<02:16,  1.54batch/s, auc=0.8763, loss=0.7837]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 82/292 [00:56<02:16,  1.54batch/s, auc=0.8763, loss=0.7881]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 83/292 [00:56<02:10,  1.61batch/s, auc=0.8763, loss=0.7881]\u001b[A\n",
      "Training Epoch 18/25:  28%|██▊       | 83/292 [00:57<02:10,  1.61batch/s, auc=0.8762, loss=0.7252]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 84/292 [00:57<02:05,  1.66batch/s, auc=0.8762, loss=0.7252]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 84/292 [00:58<02:05,  1.66batch/s, auc=0.8756, loss=0.8976]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 85/292 [00:58<02:02,  1.70batch/s, auc=0.8756, loss=0.8976]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 85/292 [00:58<02:02,  1.70batch/s, auc=0.8761, loss=0.5706]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 86/292 [00:58<01:59,  1.73batch/s, auc=0.8761, loss=0.5706]\u001b[A\n",
      "Training Epoch 18/25:  29%|██▉       | 86/292 [00:59<01:59,  1.73batch/s, auc=0.8757, loss=0.7994]\u001b[A\n",
      "Training Epoch 18/25:  30%|██▉       | 87/292 [00:59<01:57,  1.75batch/s, auc=0.8757, loss=0.7994]\u001b[A\n",
      "Training Epoch 18/25:  30%|██▉       | 87/292 [01:00<01:57,  1.75batch/s, auc=0.8746, loss=0.8962]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 88/292 [01:00<02:27,  1.38batch/s, auc=0.8746, loss=0.8962]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 88/292 [01:00<02:27,  1.38batch/s, auc=0.8752, loss=0.5264]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 89/292 [01:00<02:16,  1.48batch/s, auc=0.8752, loss=0.5264]\u001b[A\n",
      "Training Epoch 18/25:  30%|███       | 89/292 [01:01<02:16,  1.48batch/s, auc=0.8758, loss=0.6078]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 90/292 [01:01<02:08,  1.57batch/s, auc=0.8758, loss=0.6078]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 90/292 [01:01<02:08,  1.57batch/s, auc=0.8759, loss=0.6629]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 91/292 [01:01<02:03,  1.63batch/s, auc=0.8759, loss=0.6629]\u001b[A\n",
      "Training Epoch 18/25:  31%|███       | 91/292 [01:02<02:03,  1.63batch/s, auc=0.8742, loss=1.1832]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 92/292 [01:02<02:30,  1.33batch/s, auc=0.8742, loss=1.1832]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 92/292 [01:04<02:30,  1.33batch/s, auc=0.8730, loss=1.1643]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 93/292 [01:04<02:48,  1.18batch/s, auc=0.8730, loss=1.1643]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 93/292 [01:05<02:48,  1.18batch/s, auc=0.8720, loss=0.9065]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 94/292 [01:05<03:01,  1.09batch/s, auc=0.8720, loss=0.9065]\u001b[A\n",
      "Training Epoch 18/25:  32%|███▏      | 94/292 [01:05<03:01,  1.09batch/s, auc=0.8719, loss=0.8280]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 95/292 [01:05<02:39,  1.24batch/s, auc=0.8719, loss=0.8280]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 95/292 [01:06<02:39,  1.24batch/s, auc=0.8719, loss=0.5583]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 96/292 [01:06<02:23,  1.36batch/s, auc=0.8719, loss=0.5583]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 96/292 [01:06<02:23,  1.36batch/s, auc=0.8728, loss=0.6666]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 97/292 [01:06<02:12,  1.47batch/s, auc=0.8728, loss=0.6666]\u001b[A\n",
      "Training Epoch 18/25:  33%|███▎      | 97/292 [01:07<02:12,  1.47batch/s, auc=0.8733, loss=0.5768]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▎      | 98/292 [01:07<02:04,  1.55batch/s, auc=0.8733, loss=0.5768]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▎      | 98/292 [01:07<02:04,  1.55batch/s, auc=0.8737, loss=0.7757]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 99/292 [01:07<01:59,  1.62batch/s, auc=0.8737, loss=0.7757]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 99/292 [01:08<01:59,  1.62batch/s, auc=0.8727, loss=1.0975]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 100/292 [01:08<02:24,  1.33batch/s, auc=0.8727, loss=1.0975]\u001b[A\n",
      "Training Epoch 18/25:  34%|███▍      | 100/292 [01:09<02:24,  1.33batch/s, auc=0.8729, loss=0.4971]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 101/292 [01:09<02:12,  1.44batch/s, auc=0.8729, loss=0.4971]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 101/292 [01:10<02:12,  1.44batch/s, auc=0.8729, loss=0.7645]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 102/292 [01:10<02:04,  1.53batch/s, auc=0.8729, loss=0.7645]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▍      | 102/292 [01:11<02:04,  1.53batch/s, auc=0.8725, loss=0.8209]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▌      | 103/292 [01:11<02:27,  1.28batch/s, auc=0.8725, loss=0.8209]\u001b[A\n",
      "Training Epoch 18/25:  35%|███▌      | 103/292 [01:11<02:27,  1.28batch/s, auc=0.8723, loss=0.5315]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 104/292 [01:11<02:14,  1.40batch/s, auc=0.8723, loss=0.5315]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 104/292 [01:12<02:14,  1.40batch/s, auc=0.8725, loss=0.5910]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 105/292 [01:12<02:04,  1.50batch/s, auc=0.8725, loss=0.5910]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▌      | 105/292 [01:12<02:04,  1.50batch/s, auc=0.8727, loss=0.7084]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▋      | 106/292 [01:12<01:57,  1.58batch/s, auc=0.8727, loss=0.7084]\u001b[A\n",
      "Training Epoch 18/25:  36%|███▋      | 106/292 [01:13<01:57,  1.58batch/s, auc=0.8724, loss=0.6806]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 107/292 [01:13<01:53,  1.64batch/s, auc=0.8724, loss=0.6806]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 107/292 [01:13<01:53,  1.64batch/s, auc=0.8726, loss=0.6203]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 108/292 [01:13<01:49,  1.68batch/s, auc=0.8726, loss=0.6203]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 108/292 [01:14<01:49,  1.68batch/s, auc=0.8734, loss=0.5845]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 109/292 [01:14<01:46,  1.71batch/s, auc=0.8734, loss=0.5845]\u001b[A\n",
      "Training Epoch 18/25:  37%|███▋      | 109/292 [01:15<01:46,  1.71batch/s, auc=0.8713, loss=1.5723]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 110/292 [01:15<02:13,  1.37batch/s, auc=0.8713, loss=1.5723]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 110/292 [01:16<02:13,  1.37batch/s, auc=0.8706, loss=0.9427]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 111/292 [01:16<02:31,  1.20batch/s, auc=0.8706, loss=0.9427]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 111/292 [01:17<02:31,  1.20batch/s, auc=0.8704, loss=0.7615]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 112/292 [01:17<02:15,  1.33batch/s, auc=0.8704, loss=0.7615]\u001b[A\n",
      "Training Epoch 18/25:  38%|███▊      | 112/292 [01:17<02:15,  1.33batch/s, auc=0.8707, loss=0.6025]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▊      | 113/292 [01:17<02:04,  1.44batch/s, auc=0.8707, loss=0.6025]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▊      | 113/292 [01:18<02:04,  1.44batch/s, auc=0.8707, loss=0.8068]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 114/292 [01:18<02:23,  1.24batch/s, auc=0.8707, loss=0.8068]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 114/292 [01:19<02:23,  1.24batch/s, auc=0.8713, loss=0.5270]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 115/292 [01:19<02:09,  1.36batch/s, auc=0.8713, loss=0.5270]\u001b[A\n",
      "Training Epoch 18/25:  39%|███▉      | 115/292 [01:19<02:09,  1.36batch/s, auc=0.8715, loss=0.6661]\u001b[A\n",
      "Training Epoch 18/25:  40%|███▉      | 116/292 [01:19<01:59,  1.47batch/s, auc=0.8715, loss=0.6661]\u001b[A\n",
      "Training Epoch 18/25:  40%|███▉      | 116/292 [01:20<01:59,  1.47batch/s, auc=0.8721, loss=0.6090]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 117/292 [01:20<01:52,  1.55batch/s, auc=0.8721, loss=0.6090]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 117/292 [01:21<01:52,  1.55batch/s, auc=0.8721, loss=0.6035]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 118/292 [01:21<01:47,  1.62batch/s, auc=0.8721, loss=0.6035]\u001b[A\n",
      "Training Epoch 18/25:  40%|████      | 118/292 [01:21<01:47,  1.62batch/s, auc=0.8726, loss=0.7704]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 119/292 [01:21<01:43,  1.67batch/s, auc=0.8726, loss=0.7704]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 119/292 [01:22<01:43,  1.67batch/s, auc=0.8731, loss=0.5864]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 120/292 [01:22<01:41,  1.70batch/s, auc=0.8731, loss=0.5864]\u001b[A\n",
      "Training Epoch 18/25:  41%|████      | 120/292 [01:22<01:41,  1.70batch/s, auc=0.8737, loss=0.5272]\u001b[A\n",
      "Training Epoch 18/25:  41%|████▏     | 121/292 [01:22<01:39,  1.73batch/s, auc=0.8737, loss=0.5272]\u001b[A\n",
      "Training Epoch 18/25:  41%|████▏     | 121/292 [01:23<01:39,  1.73batch/s, auc=0.8736, loss=0.9502]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 122/292 [01:23<01:37,  1.74batch/s, auc=0.8736, loss=0.9502]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 122/292 [01:23<01:37,  1.74batch/s, auc=0.8736, loss=0.7677]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 123/292 [01:23<01:36,  1.76batch/s, auc=0.8736, loss=0.7677]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 123/292 [01:24<01:36,  1.76batch/s, auc=0.8743, loss=0.4575]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 124/292 [01:24<01:34,  1.77batch/s, auc=0.8743, loss=0.4575]\u001b[A\n",
      "Training Epoch 18/25:  42%|████▏     | 124/292 [01:24<01:34,  1.77batch/s, auc=0.8748, loss=0.4851]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 125/292 [01:24<01:34,  1.77batch/s, auc=0.8748, loss=0.4851]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 125/292 [01:26<01:34,  1.77batch/s, auc=0.8746, loss=0.8817]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 126/292 [01:26<01:59,  1.39batch/s, auc=0.8746, loss=0.8817]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 126/292 [01:27<01:59,  1.39batch/s, auc=0.8742, loss=0.9369]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 127/292 [01:27<02:16,  1.21batch/s, auc=0.8742, loss=0.9369]\u001b[A\n",
      "Training Epoch 18/25:  43%|████▎     | 127/292 [01:27<02:16,  1.21batch/s, auc=0.8737, loss=1.0585]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 128/292 [01:27<02:02,  1.34batch/s, auc=0.8737, loss=1.0585]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 128/292 [01:28<02:02,  1.34batch/s, auc=0.8734, loss=0.8777]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 129/292 [01:28<02:17,  1.19batch/s, auc=0.8734, loss=0.8777]\u001b[A\n",
      "Training Epoch 18/25:  44%|████▍     | 129/292 [01:29<02:17,  1.19batch/s, auc=0.8726, loss=0.9104]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 130/292 [01:29<02:27,  1.09batch/s, auc=0.8726, loss=0.9104]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 130/292 [01:30<02:27,  1.09batch/s, auc=0.8735, loss=0.4823]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 131/292 [01:30<02:09,  1.24batch/s, auc=0.8735, loss=0.4823]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▍     | 131/292 [01:30<02:09,  1.24batch/s, auc=0.8742, loss=0.4468]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▌     | 132/292 [01:30<01:57,  1.37batch/s, auc=0.8742, loss=0.4468]\u001b[A\n",
      "Training Epoch 18/25:  45%|████▌     | 132/292 [01:31<01:57,  1.37batch/s, auc=0.8750, loss=0.5463]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 133/292 [01:31<01:48,  1.47batch/s, auc=0.8750, loss=0.5463]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 133/292 [01:32<01:48,  1.47batch/s, auc=0.8756, loss=0.5968]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 134/292 [01:32<01:41,  1.55batch/s, auc=0.8756, loss=0.5968]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 134/292 [01:32<01:41,  1.55batch/s, auc=0.8753, loss=0.8212]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 135/292 [01:32<01:37,  1.62batch/s, auc=0.8753, loss=0.8212]\u001b[A\n",
      "Training Epoch 18/25:  46%|████▌     | 135/292 [01:33<01:37,  1.62batch/s, auc=0.8762, loss=0.4778]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 136/292 [01:33<01:33,  1.66batch/s, auc=0.8762, loss=0.4778]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 136/292 [01:33<01:33,  1.66batch/s, auc=0.8767, loss=0.5024]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 137/292 [01:33<01:31,  1.70batch/s, auc=0.8767, loss=0.5024]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 137/292 [01:34<01:31,  1.70batch/s, auc=0.8765, loss=0.7651]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 138/292 [01:34<01:29,  1.73batch/s, auc=0.8765, loss=0.7651]\u001b[A\n",
      "Training Epoch 18/25:  47%|████▋     | 138/292 [01:34<01:29,  1.73batch/s, auc=0.8764, loss=0.8294]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 139/292 [01:34<01:27,  1.74batch/s, auc=0.8764, loss=0.8294]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 139/292 [01:35<01:27,  1.74batch/s, auc=0.8767, loss=0.4537]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 140/292 [01:35<01:26,  1.76batch/s, auc=0.8767, loss=0.4537]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 140/292 [01:35<01:26,  1.76batch/s, auc=0.8774, loss=0.4933]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 141/292 [01:35<01:25,  1.77batch/s, auc=0.8774, loss=0.4933]\u001b[A\n",
      "Training Epoch 18/25:  48%|████▊     | 141/292 [01:36<01:25,  1.77batch/s, auc=0.8778, loss=0.6143]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▊     | 142/292 [01:36<01:24,  1.77batch/s, auc=0.8778, loss=0.6143]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▊     | 142/292 [01:37<01:24,  1.77batch/s, auc=0.8778, loss=0.5230]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 143/292 [01:37<01:23,  1.78batch/s, auc=0.8778, loss=0.5230]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 143/292 [01:38<01:23,  1.78batch/s, auc=0.8773, loss=0.9345]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 144/292 [01:38<01:46,  1.40batch/s, auc=0.8773, loss=0.9345]\u001b[A\n",
      "Training Epoch 18/25:  49%|████▉     | 144/292 [01:38<01:46,  1.40batch/s, auc=0.8772, loss=0.7826]\u001b[A\n",
      "Training Epoch 18/25:  50%|████▉     | 145/292 [01:38<01:38,  1.49batch/s, auc=0.8772, loss=0.7826]\u001b[A\n",
      "Training Epoch 18/25:  50%|████▉     | 145/292 [01:39<01:38,  1.49batch/s, auc=0.8775, loss=0.5009]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 146/292 [01:39<01:32,  1.57batch/s, auc=0.8775, loss=0.5009]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 146/292 [01:39<01:32,  1.57batch/s, auc=0.8780, loss=0.5591]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 147/292 [01:39<01:28,  1.63batch/s, auc=0.8780, loss=0.5591]\u001b[A\n",
      "Training Epoch 18/25:  50%|█████     | 147/292 [01:40<01:28,  1.63batch/s, auc=0.8784, loss=0.4900]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 148/292 [01:40<01:25,  1.68batch/s, auc=0.8784, loss=0.4900]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 148/292 [01:40<01:25,  1.68batch/s, auc=0.8787, loss=0.5020]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 149/292 [01:40<01:23,  1.71batch/s, auc=0.8787, loss=0.5020]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████     | 149/292 [01:42<01:23,  1.71batch/s, auc=0.8780, loss=0.9132]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████▏    | 150/292 [01:42<01:44,  1.36batch/s, auc=0.8780, loss=0.9132]\u001b[A\n",
      "Training Epoch 18/25:  51%|█████▏    | 150/292 [01:42<01:44,  1.36batch/s, auc=0.8778, loss=0.7117]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 151/292 [01:42<01:36,  1.47batch/s, auc=0.8778, loss=0.7117]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 151/292 [01:43<01:36,  1.47batch/s, auc=0.8776, loss=0.8248]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 152/292 [01:43<01:52,  1.25batch/s, auc=0.8776, loss=0.8248]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 152/292 [01:44<01:52,  1.25batch/s, auc=0.8774, loss=0.9237]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 153/292 [01:44<02:02,  1.13batch/s, auc=0.8774, loss=0.9237]\u001b[A\n",
      "Training Epoch 18/25:  52%|█████▏    | 153/292 [01:45<02:02,  1.13batch/s, auc=0.8769, loss=0.7232]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 154/292 [01:45<01:48,  1.27batch/s, auc=0.8769, loss=0.7232]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 154/292 [01:45<01:48,  1.27batch/s, auc=0.8771, loss=0.5667]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 155/292 [01:45<01:38,  1.39batch/s, auc=0.8771, loss=0.5667]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 155/292 [01:46<01:38,  1.39batch/s, auc=0.8774, loss=0.6498]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 156/292 [01:46<01:31,  1.49batch/s, auc=0.8774, loss=0.6498]\u001b[A\n",
      "Training Epoch 18/25:  53%|█████▎    | 156/292 [01:47<01:31,  1.49batch/s, auc=0.8771, loss=0.8577]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 157/292 [01:47<01:46,  1.26batch/s, auc=0.8771, loss=0.8577]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 157/292 [01:48<01:46,  1.26batch/s, auc=0.8772, loss=0.5752]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 158/292 [01:48<01:36,  1.38batch/s, auc=0.8772, loss=0.5752]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 158/292 [01:48<01:36,  1.38batch/s, auc=0.8775, loss=0.6290]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 159/292 [01:48<01:29,  1.48batch/s, auc=0.8775, loss=0.6290]\u001b[A\n",
      "Training Epoch 18/25:  54%|█████▍    | 159/292 [01:49<01:29,  1.48batch/s, auc=0.8774, loss=0.6578]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▍    | 160/292 [01:49<01:24,  1.56batch/s, auc=0.8774, loss=0.6578]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▍    | 160/292 [01:49<01:24,  1.56batch/s, auc=0.8774, loss=0.5894]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 161/292 [01:49<01:20,  1.62batch/s, auc=0.8774, loss=0.5894]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 161/292 [01:50<01:20,  1.62batch/s, auc=0.8778, loss=0.5587]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 162/292 [01:50<01:17,  1.67batch/s, auc=0.8778, loss=0.5587]\u001b[A\n",
      "Training Epoch 18/25:  55%|█████▌    | 162/292 [01:50<01:17,  1.67batch/s, auc=0.8781, loss=0.7080]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 163/292 [01:50<01:15,  1.70batch/s, auc=0.8781, loss=0.7080]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 163/292 [01:51<01:15,  1.70batch/s, auc=0.8780, loss=0.7741]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 164/292 [01:51<01:14,  1.73batch/s, auc=0.8780, loss=0.7741]\u001b[A\n",
      "Training Epoch 18/25:  56%|█████▌    | 164/292 [01:52<01:14,  1.73batch/s, auc=0.8777, loss=0.7877]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 165/292 [01:52<01:12,  1.74batch/s, auc=0.8777, loss=0.7877]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 165/292 [01:52<01:12,  1.74batch/s, auc=0.8775, loss=0.8638]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 166/292 [01:52<01:11,  1.76batch/s, auc=0.8775, loss=0.8638]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 166/292 [01:53<01:11,  1.76batch/s, auc=0.8755, loss=2.2494]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 167/292 [01:53<01:30,  1.39batch/s, auc=0.8755, loss=2.2494]\u001b[A\n",
      "Training Epoch 18/25:  57%|█████▋    | 167/292 [01:54<01:30,  1.39batch/s, auc=0.8754, loss=0.5321]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 168/292 [01:54<01:23,  1.49batch/s, auc=0.8754, loss=0.5321]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 168/292 [01:54<01:23,  1.49batch/s, auc=0.8750, loss=0.8640]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 169/292 [01:54<01:18,  1.56batch/s, auc=0.8750, loss=0.8640]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 169/292 [01:55<01:18,  1.56batch/s, auc=0.8754, loss=0.5706]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 170/292 [01:55<01:15,  1.62batch/s, auc=0.8754, loss=0.5706]\u001b[A\n",
      "Training Epoch 18/25:  58%|█████▊    | 170/292 [01:55<01:15,  1.62batch/s, auc=0.8757, loss=0.5245]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▊    | 171/292 [01:55<01:12,  1.67batch/s, auc=0.8757, loss=0.5245]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▊    | 171/292 [01:56<01:12,  1.67batch/s, auc=0.8756, loss=0.8059]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 172/292 [01:56<01:10,  1.70batch/s, auc=0.8756, loss=0.8059]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 172/292 [01:57<01:10,  1.70batch/s, auc=0.8751, loss=1.2050]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 173/292 [01:57<01:08,  1.73batch/s, auc=0.8751, loss=1.2050]\u001b[A\n",
      "Training Epoch 18/25:  59%|█████▉    | 173/292 [01:57<01:08,  1.73batch/s, auc=0.8754, loss=0.6230]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 174/292 [01:57<01:07,  1.74batch/s, auc=0.8754, loss=0.6230]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 174/292 [01:58<01:07,  1.74batch/s, auc=0.8759, loss=0.4661]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 175/292 [01:58<01:06,  1.76batch/s, auc=0.8759, loss=0.4661]\u001b[A\n",
      "Training Epoch 18/25:  60%|█████▉    | 175/292 [01:58<01:06,  1.76batch/s, auc=0.8764, loss=0.4864]\u001b[A\n",
      "Training Epoch 18/25:  60%|██████    | 176/292 [01:58<01:05,  1.76batch/s, auc=0.8764, loss=0.4864]\u001b[A\n",
      "Training Epoch 18/25:  60%|██████    | 176/292 [01:59<01:05,  1.76batch/s, auc=0.8767, loss=0.4455]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 177/292 [01:59<01:05,  1.77batch/s, auc=0.8767, loss=0.4455]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 177/292 [02:00<01:05,  1.77batch/s, auc=0.8763, loss=0.8905]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 178/292 [02:00<01:21,  1.39batch/s, auc=0.8763, loss=0.8905]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████    | 178/292 [02:00<01:21,  1.39batch/s, auc=0.8766, loss=0.7459]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████▏   | 179/292 [02:00<01:15,  1.49batch/s, auc=0.8766, loss=0.7459]\u001b[A\n",
      "Training Epoch 18/25:  61%|██████▏   | 179/292 [02:01<01:15,  1.49batch/s, auc=0.8764, loss=0.8720]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 180/292 [02:01<01:11,  1.57batch/s, auc=0.8764, loss=0.8720]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 180/292 [02:02<01:11,  1.57batch/s, auc=0.8761, loss=0.8303]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 181/292 [02:02<01:25,  1.30batch/s, auc=0.8761, loss=0.8303]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 181/292 [02:03<01:25,  1.30batch/s, auc=0.8765, loss=0.6283]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 182/292 [02:03<01:17,  1.41batch/s, auc=0.8765, loss=0.6283]\u001b[A\n",
      "Training Epoch 18/25:  62%|██████▏   | 182/292 [02:03<01:17,  1.41batch/s, auc=0.8761, loss=1.0088]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 183/292 [02:03<01:12,  1.51batch/s, auc=0.8761, loss=1.0088]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 183/292 [02:04<01:12,  1.51batch/s, auc=0.8761, loss=0.7239]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 184/292 [02:04<01:08,  1.58batch/s, auc=0.8761, loss=0.7239]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 184/292 [02:04<01:08,  1.58batch/s, auc=0.8763, loss=0.5812]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 185/292 [02:04<01:05,  1.63batch/s, auc=0.8763, loss=0.5812]\u001b[A\n",
      "Training Epoch 18/25:  63%|██████▎   | 185/292 [02:05<01:05,  1.63batch/s, auc=0.8758, loss=1.2285]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▎   | 186/292 [02:05<01:19,  1.33batch/s, auc=0.8758, loss=1.2285]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▎   | 186/292 [02:06<01:19,  1.33batch/s, auc=0.8759, loss=0.6964]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 187/292 [02:06<01:13,  1.44batch/s, auc=0.8759, loss=0.6964]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 187/292 [02:06<01:13,  1.44batch/s, auc=0.8755, loss=0.8059]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 188/292 [02:06<01:08,  1.53batch/s, auc=0.8755, loss=0.8059]\u001b[A\n",
      "Training Epoch 18/25:  64%|██████▍   | 188/292 [02:07<01:08,  1.53batch/s, auc=0.8754, loss=0.7004]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▍   | 189/292 [02:07<01:04,  1.59batch/s, auc=0.8754, loss=0.7004]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▍   | 189/292 [02:08<01:04,  1.59batch/s, auc=0.8752, loss=0.7601]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 190/292 [02:08<01:02,  1.64batch/s, auc=0.8752, loss=0.7601]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 190/292 [02:09<01:02,  1.64batch/s, auc=0.8746, loss=1.1573]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 191/292 [02:09<01:15,  1.33batch/s, auc=0.8746, loss=1.1573]\u001b[A\n",
      "Training Epoch 18/25:  65%|██████▌   | 191/292 [02:09<01:15,  1.33batch/s, auc=0.8751, loss=0.6415]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 192/292 [02:09<01:09,  1.44batch/s, auc=0.8751, loss=0.6415]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 192/292 [02:10<01:09,  1.44batch/s, auc=0.8752, loss=0.7785]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 193/292 [02:10<01:04,  1.53batch/s, auc=0.8752, loss=0.7785]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▌   | 193/292 [02:10<01:04,  1.53batch/s, auc=0.8754, loss=0.5856]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▋   | 194/292 [02:10<01:01,  1.60batch/s, auc=0.8754, loss=0.5856]\u001b[A\n",
      "Training Epoch 18/25:  66%|██████▋   | 194/292 [02:11<01:01,  1.60batch/s, auc=0.8749, loss=0.9754]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 195/292 [02:11<01:13,  1.31batch/s, auc=0.8749, loss=0.9754]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 195/292 [02:12<01:13,  1.31batch/s, auc=0.8753, loss=0.4882]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 196/292 [02:12<01:07,  1.43batch/s, auc=0.8753, loss=0.4882]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 196/292 [02:13<01:07,  1.43batch/s, auc=0.8750, loss=0.8319]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 197/292 [02:13<01:17,  1.23batch/s, auc=0.8750, loss=0.8319]\u001b[A\n",
      "Training Epoch 18/25:  67%|██████▋   | 197/292 [02:14<01:17,  1.23batch/s, auc=0.8752, loss=0.5580]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 198/292 [02:14<01:09,  1.35batch/s, auc=0.8752, loss=0.5580]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 198/292 [02:14<01:09,  1.35batch/s, auc=0.8751, loss=0.8267]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 199/292 [02:14<01:03,  1.46batch/s, auc=0.8751, loss=0.8267]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 199/292 [02:15<01:03,  1.46batch/s, auc=0.8750, loss=1.0698]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 200/292 [02:15<00:59,  1.54batch/s, auc=0.8750, loss=1.0698]\u001b[A\n",
      "Training Epoch 18/25:  68%|██████▊   | 200/292 [02:15<00:59,  1.54batch/s, auc=0.8749, loss=0.7837]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 201/292 [02:15<00:56,  1.61batch/s, auc=0.8749, loss=0.7837]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 201/292 [02:16<00:56,  1.61batch/s, auc=0.8750, loss=0.7371]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 202/292 [02:16<00:54,  1.65batch/s, auc=0.8750, loss=0.7371]\u001b[A\n",
      "Training Epoch 18/25:  69%|██████▉   | 202/292 [02:16<00:54,  1.65batch/s, auc=0.8749, loss=0.7857]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 203/292 [02:16<00:52,  1.69batch/s, auc=0.8749, loss=0.7857]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 203/292 [02:17<00:52,  1.69batch/s, auc=0.8754, loss=0.5168]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 204/292 [02:17<00:51,  1.71batch/s, auc=0.8754, loss=0.5168]\u001b[A\n",
      "Training Epoch 18/25:  70%|██████▉   | 204/292 [02:18<00:51,  1.71batch/s, auc=0.8756, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  70%|███████   | 205/292 [02:18<00:50,  1.73batch/s, auc=0.8756, loss=0.5338]\u001b[A\n",
      "Training Epoch 18/25:  70%|███████   | 205/292 [02:19<00:50,  1.73batch/s, auc=0.8747, loss=1.2532]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 206/292 [02:19<01:02,  1.38batch/s, auc=0.8747, loss=1.2532]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 206/292 [02:19<01:02,  1.38batch/s, auc=0.8750, loss=0.6743]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 207/292 [02:19<00:57,  1.48batch/s, auc=0.8750, loss=0.6743]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 207/292 [02:20<00:57,  1.48batch/s, auc=0.8752, loss=0.5733]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 208/292 [02:20<00:54,  1.56batch/s, auc=0.8752, loss=0.5733]\u001b[A\n",
      "Training Epoch 18/25:  71%|███████   | 208/292 [02:20<00:54,  1.56batch/s, auc=0.8753, loss=0.6301]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 209/292 [02:20<00:51,  1.61batch/s, auc=0.8753, loss=0.6301]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 209/292 [02:21<00:51,  1.61batch/s, auc=0.8754, loss=0.7999]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 210/292 [02:21<00:49,  1.66batch/s, auc=0.8754, loss=0.7999]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 210/292 [02:21<00:49,  1.66batch/s, auc=0.8752, loss=0.8339]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 211/292 [02:21<00:47,  1.69batch/s, auc=0.8752, loss=0.8339]\u001b[A\n",
      "Training Epoch 18/25:  72%|███████▏  | 211/292 [02:22<00:47,  1.69batch/s, auc=0.8757, loss=0.4223]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 212/292 [02:22<00:46,  1.72batch/s, auc=0.8757, loss=0.4223]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 212/292 [02:23<00:46,  1.72batch/s, auc=0.8759, loss=0.7751]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 213/292 [02:23<00:45,  1.74batch/s, auc=0.8759, loss=0.7751]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 213/292 [02:24<00:45,  1.74batch/s, auc=0.8753, loss=0.9582]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 214/292 [02:24<00:56,  1.38batch/s, auc=0.8753, loss=0.9582]\u001b[A\n",
      "Training Epoch 18/25:  73%|███████▎  | 214/292 [02:24<00:56,  1.38batch/s, auc=0.8754, loss=0.7537]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▎  | 215/292 [02:24<00:52,  1.47batch/s, auc=0.8754, loss=0.7537]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▎  | 215/292 [02:25<00:52,  1.47batch/s, auc=0.8753, loss=0.6164]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 216/292 [02:25<00:48,  1.55batch/s, auc=0.8753, loss=0.6164]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 216/292 [02:25<00:48,  1.55batch/s, auc=0.8754, loss=0.6773]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 217/292 [02:25<00:46,  1.62batch/s, auc=0.8754, loss=0.6773]\u001b[A\n",
      "Training Epoch 18/25:  74%|███████▍  | 217/292 [02:26<00:46,  1.62batch/s, auc=0.8757, loss=0.5243]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▍  | 218/292 [02:26<00:44,  1.66batch/s, auc=0.8757, loss=0.5243]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▍  | 218/292 [02:26<00:44,  1.66batch/s, auc=0.8760, loss=0.6737]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 219/292 [02:26<00:43,  1.70batch/s, auc=0.8760, loss=0.6737]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 219/292 [02:27<00:43,  1.70batch/s, auc=0.8759, loss=0.6425]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 220/292 [02:27<00:41,  1.72batch/s, auc=0.8759, loss=0.6425]\u001b[A\n",
      "Training Epoch 18/25:  75%|███████▌  | 220/292 [02:28<00:41,  1.72batch/s, auc=0.8753, loss=1.1974]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 221/292 [02:28<00:51,  1.37batch/s, auc=0.8753, loss=1.1974]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 221/292 [02:29<00:51,  1.37batch/s, auc=0.8751, loss=0.8915]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 222/292 [02:29<00:58,  1.20batch/s, auc=0.8751, loss=0.8915]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▌  | 222/292 [02:30<00:58,  1.20batch/s, auc=0.8754, loss=0.4615]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▋  | 223/292 [02:30<00:51,  1.33batch/s, auc=0.8754, loss=0.4615]\u001b[A\n",
      "Training Epoch 18/25:  76%|███████▋  | 223/292 [02:30<00:51,  1.33batch/s, auc=0.8757, loss=0.5476]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 224/292 [02:30<00:47,  1.44batch/s, auc=0.8757, loss=0.5476]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 224/292 [02:31<00:47,  1.44batch/s, auc=0.8757, loss=0.7218]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 225/292 [02:31<00:44,  1.52batch/s, auc=0.8757, loss=0.7218]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 225/292 [02:31<00:44,  1.52batch/s, auc=0.8756, loss=0.7188]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 226/292 [02:31<00:41,  1.59batch/s, auc=0.8756, loss=0.7188]\u001b[A\n",
      "Training Epoch 18/25:  77%|███████▋  | 226/292 [02:32<00:41,  1.59batch/s, auc=0.8757, loss=0.5773]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 227/292 [02:32<00:39,  1.64batch/s, auc=0.8757, loss=0.5773]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 227/292 [02:33<00:39,  1.64batch/s, auc=0.8757, loss=0.7509]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 228/292 [02:33<00:47,  1.33batch/s, auc=0.8757, loss=0.7509]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 228/292 [02:34<00:47,  1.33batch/s, auc=0.8758, loss=0.7127]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 229/292 [02:34<00:43,  1.44batch/s, auc=0.8758, loss=0.7127]\u001b[A\n",
      "Training Epoch 18/25:  78%|███████▊  | 229/292 [02:34<00:43,  1.44batch/s, auc=0.8758, loss=0.6672]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 230/292 [02:34<00:40,  1.53batch/s, auc=0.8758, loss=0.6672]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 230/292 [02:35<00:40,  1.53batch/s, auc=0.8753, loss=1.1761]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 231/292 [02:35<00:47,  1.28batch/s, auc=0.8753, loss=1.1761]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 231/292 [02:36<00:47,  1.28batch/s, auc=0.8756, loss=0.5886]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 232/292 [02:36<00:42,  1.40batch/s, auc=0.8756, loss=0.5886]\u001b[A\n",
      "Training Epoch 18/25:  79%|███████▉  | 232/292 [02:36<00:42,  1.40batch/s, auc=0.8759, loss=0.6326]\u001b[A\n",
      "Training Epoch 18/25:  80%|███████▉  | 233/292 [02:36<00:39,  1.49batch/s, auc=0.8759, loss=0.6326]\u001b[A\n",
      "Training Epoch 18/25:  80%|███████▉  | 233/292 [02:37<00:39,  1.49batch/s, auc=0.8759, loss=0.6811]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 234/292 [02:37<00:37,  1.57batch/s, auc=0.8759, loss=0.6811]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 234/292 [02:38<00:37,  1.57batch/s, auc=0.8761, loss=0.7544]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 235/292 [02:38<00:43,  1.30batch/s, auc=0.8761, loss=0.7544]\u001b[A\n",
      "Training Epoch 18/25:  80%|████████  | 235/292 [02:39<00:43,  1.30batch/s, auc=0.8763, loss=0.5455]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 236/292 [02:39<00:39,  1.41batch/s, auc=0.8763, loss=0.5455]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 236/292 [02:39<00:39,  1.41batch/s, auc=0.8765, loss=0.5435]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 237/292 [02:39<00:36,  1.51batch/s, auc=0.8765, loss=0.5435]\u001b[A\n",
      "Training Epoch 18/25:  81%|████████  | 237/292 [02:40<00:36,  1.51batch/s, auc=0.8761, loss=1.0742]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 238/292 [02:40<00:34,  1.58batch/s, auc=0.8761, loss=1.0742]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 238/292 [02:40<00:34,  1.58batch/s, auc=0.8758, loss=0.9270]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 239/292 [02:40<00:32,  1.63batch/s, auc=0.8758, loss=0.9270]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 239/292 [02:41<00:32,  1.63batch/s, auc=0.8761, loss=0.5479]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 240/292 [02:41<00:31,  1.67batch/s, auc=0.8761, loss=0.5479]\u001b[A\n",
      "Training Epoch 18/25:  82%|████████▏ | 240/292 [02:41<00:31,  1.67batch/s, auc=0.8761, loss=0.5162]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 241/292 [02:41<00:29,  1.70batch/s, auc=0.8761, loss=0.5162]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 241/292 [02:42<00:29,  1.70batch/s, auc=0.8760, loss=0.7238]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 242/292 [02:42<00:29,  1.72batch/s, auc=0.8760, loss=0.7238]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 242/292 [02:43<00:29,  1.72batch/s, auc=0.8761, loss=0.8011]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 243/292 [02:43<00:28,  1.74batch/s, auc=0.8761, loss=0.8011]\u001b[A\n",
      "Training Epoch 18/25:  83%|████████▎ | 243/292 [02:44<00:28,  1.74batch/s, auc=0.8759, loss=0.8435]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▎ | 244/292 [02:44<00:34,  1.38batch/s, auc=0.8759, loss=0.8435]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▎ | 244/292 [02:44<00:34,  1.38batch/s, auc=0.8760, loss=0.5801]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 245/292 [02:44<00:31,  1.48batch/s, auc=0.8760, loss=0.5801]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 245/292 [02:45<00:31,  1.48batch/s, auc=0.8760, loss=0.7183]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 246/292 [02:45<00:29,  1.56batch/s, auc=0.8760, loss=0.7183]\u001b[A\n",
      "Training Epoch 18/25:  84%|████████▍ | 246/292 [02:46<00:29,  1.56batch/s, auc=0.8753, loss=1.2697]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 247/292 [02:46<00:34,  1.29batch/s, auc=0.8753, loss=1.2697]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 247/292 [02:46<00:34,  1.29batch/s, auc=0.8755, loss=0.6026]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 248/292 [02:46<00:31,  1.41batch/s, auc=0.8755, loss=0.6026]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▍ | 248/292 [02:47<00:31,  1.41batch/s, auc=0.8753, loss=0.6692]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▌ | 249/292 [02:47<00:28,  1.50batch/s, auc=0.8753, loss=0.6692]\u001b[A\n",
      "Training Epoch 18/25:  85%|████████▌ | 249/292 [02:48<00:28,  1.50batch/s, auc=0.8749, loss=0.9546]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 250/292 [02:48<00:33,  1.26batch/s, auc=0.8749, loss=0.9546]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 250/292 [02:49<00:33,  1.26batch/s, auc=0.8750, loss=0.6196]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 251/292 [02:49<00:29,  1.38batch/s, auc=0.8750, loss=0.6196]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▌ | 251/292 [02:50<00:29,  1.38batch/s, auc=0.8747, loss=0.8472]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▋ | 252/292 [02:50<00:33,  1.20batch/s, auc=0.8747, loss=0.8472]\u001b[A\n",
      "Training Epoch 18/25:  86%|████████▋ | 252/292 [02:50<00:33,  1.20batch/s, auc=0.8748, loss=0.5359]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 253/292 [02:50<00:29,  1.33batch/s, auc=0.8748, loss=0.5359]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 253/292 [02:51<00:29,  1.33batch/s, auc=0.8745, loss=0.9561]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 254/292 [02:51<00:32,  1.18batch/s, auc=0.8745, loss=0.9561]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 254/292 [02:52<00:32,  1.18batch/s, auc=0.8746, loss=0.6608]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 255/292 [02:52<00:34,  1.09batch/s, auc=0.8746, loss=0.6608]\u001b[A\n",
      "Training Epoch 18/25:  87%|████████▋ | 255/292 [02:53<00:34,  1.09batch/s, auc=0.8745, loss=0.7944]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 256/292 [02:53<00:29,  1.23batch/s, auc=0.8745, loss=0.7944]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 256/292 [02:54<00:29,  1.23batch/s, auc=0.8743, loss=0.8117]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 257/292 [02:54<00:25,  1.35batch/s, auc=0.8743, loss=0.8117]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 257/292 [02:54<00:25,  1.35batch/s, auc=0.8742, loss=0.6371]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 258/292 [02:54<00:23,  1.46batch/s, auc=0.8742, loss=0.6371]\u001b[A\n",
      "Training Epoch 18/25:  88%|████████▊ | 258/292 [02:55<00:23,  1.46batch/s, auc=0.8733, loss=1.6400]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▊ | 259/292 [02:55<00:26,  1.24batch/s, auc=0.8733, loss=1.6400]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▊ | 259/292 [02:56<00:26,  1.24batch/s, auc=0.8729, loss=1.2354]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 260/292 [02:56<00:28,  1.13batch/s, auc=0.8729, loss=1.2354]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 260/292 [02:57<00:28,  1.13batch/s, auc=0.8721, loss=1.4140]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 261/292 [02:57<00:29,  1.06batch/s, auc=0.8721, loss=1.4140]\u001b[A\n",
      "Training Epoch 18/25:  89%|████████▉ | 261/292 [02:58<00:29,  1.06batch/s, auc=0.8718, loss=0.8947]\u001b[A\n",
      "Training Epoch 18/25:  90%|████████▉ | 262/292 [02:58<00:29,  1.01batch/s, auc=0.8718, loss=0.8947]\u001b[A\n",
      "Training Epoch 18/25:  90%|████████▉ | 262/292 [02:59<00:29,  1.01batch/s, auc=0.8720, loss=0.5296]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 263/292 [02:59<00:24,  1.16batch/s, auc=0.8720, loss=0.5296]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 263/292 [03:00<00:24,  1.16batch/s, auc=0.8721, loss=0.5294]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 264/292 [03:00<00:21,  1.30batch/s, auc=0.8721, loss=0.5294]\u001b[A\n",
      "Training Epoch 18/25:  90%|█████████ | 264/292 [03:00<00:21,  1.30batch/s, auc=0.8722, loss=0.7695]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 265/292 [03:00<00:19,  1.41batch/s, auc=0.8722, loss=0.7695]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 265/292 [03:01<00:19,  1.41batch/s, auc=0.8719, loss=0.8894]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 266/292 [03:01<00:21,  1.22batch/s, auc=0.8719, loss=0.8894]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████ | 266/292 [03:02<00:21,  1.22batch/s, auc=0.8713, loss=1.3263]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████▏| 267/292 [03:02<00:22,  1.11batch/s, auc=0.8713, loss=1.3263]\u001b[A\n",
      "Training Epoch 18/25:  91%|█████████▏| 267/292 [03:03<00:22,  1.11batch/s, auc=0.8715, loss=0.5634]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 268/292 [03:03<00:19,  1.25batch/s, auc=0.8715, loss=0.5634]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 268/292 [03:04<00:19,  1.25batch/s, auc=0.8712, loss=1.0239]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 269/292 [03:04<00:20,  1.13batch/s, auc=0.8712, loss=1.0239]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 269/292 [03:05<00:20,  1.13batch/s, auc=0.8711, loss=0.8667]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 270/292 [03:05<00:20,  1.06batch/s, auc=0.8711, loss=0.8667]\u001b[A\n",
      "Training Epoch 18/25:  92%|█████████▏| 270/292 [03:06<00:20,  1.06batch/s, auc=0.8709, loss=1.0310]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 271/292 [03:06<00:17,  1.20batch/s, auc=0.8709, loss=1.0310]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 271/292 [03:06<00:17,  1.20batch/s, auc=0.8710, loss=0.7037]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 272/292 [03:06<00:15,  1.33batch/s, auc=0.8710, loss=0.7037]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 272/292 [03:07<00:15,  1.33batch/s, auc=0.8708, loss=1.0857]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 273/292 [03:07<00:16,  1.17batch/s, auc=0.8708, loss=1.0857]\u001b[A\n",
      "Training Epoch 18/25:  93%|█████████▎| 273/292 [03:08<00:16,  1.17batch/s, auc=0.8705, loss=1.1196]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 274/292 [03:08<00:16,  1.09batch/s, auc=0.8705, loss=1.1196]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 274/292 [03:09<00:16,  1.09batch/s, auc=0.8705, loss=0.6867]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 275/292 [03:09<00:13,  1.23batch/s, auc=0.8705, loss=0.6867]\u001b[A\n",
      "Training Epoch 18/25:  94%|█████████▍| 275/292 [03:09<00:13,  1.23batch/s, auc=0.8705, loss=0.8246]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 276/292 [03:09<00:11,  1.35batch/s, auc=0.8705, loss=0.8246]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 276/292 [03:10<00:11,  1.35batch/s, auc=0.8703, loss=0.6944]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 277/292 [03:10<00:10,  1.45batch/s, auc=0.8703, loss=0.6944]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▍| 277/292 [03:11<00:10,  1.45batch/s, auc=0.8704, loss=0.7917]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▌| 278/292 [03:11<00:09,  1.54batch/s, auc=0.8704, loss=0.7917]\u001b[A\n",
      "Training Epoch 18/25:  95%|█████████▌| 278/292 [03:11<00:09,  1.54batch/s, auc=0.8706, loss=0.7006]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 279/292 [03:11<00:08,  1.60batch/s, auc=0.8706, loss=0.7006]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 279/292 [03:12<00:08,  1.60batch/s, auc=0.8703, loss=1.0373]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 280/292 [03:12<00:09,  1.31batch/s, auc=0.8703, loss=1.0373]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 280/292 [03:13<00:09,  1.31batch/s, auc=0.8704, loss=0.7929]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 281/292 [03:13<00:09,  1.16batch/s, auc=0.8704, loss=0.7929]\u001b[A\n",
      "Training Epoch 18/25:  96%|█████████▌| 281/292 [03:14<00:09,  1.16batch/s, auc=0.8706, loss=0.5433]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 282/292 [03:14<00:07,  1.30batch/s, auc=0.8706, loss=0.5433]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 282/292 [03:15<00:07,  1.30batch/s, auc=0.8703, loss=0.7255]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 283/292 [03:15<00:07,  1.16batch/s, auc=0.8703, loss=0.7255]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 283/292 [03:16<00:07,  1.16batch/s, auc=0.8709, loss=0.6421]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 284/292 [03:16<00:06,  1.29batch/s, auc=0.8709, loss=0.6421]\u001b[A\n",
      "Training Epoch 18/25:  97%|█████████▋| 284/292 [03:16<00:06,  1.29batch/s, auc=0.8708, loss=0.7361]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 285/292 [03:16<00:04,  1.41batch/s, auc=0.8708, loss=0.7361]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 285/292 [03:17<00:04,  1.41batch/s, auc=0.8703, loss=1.2402]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 286/292 [03:17<00:04,  1.22batch/s, auc=0.8703, loss=1.2402]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 286/292 [03:18<00:04,  1.22batch/s, auc=0.8705, loss=0.7276]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 287/292 [03:18<00:03,  1.34batch/s, auc=0.8705, loss=0.7276]\u001b[A\n",
      "Training Epoch 18/25:  98%|█████████▊| 287/292 [03:18<00:03,  1.34batch/s, auc=0.8709, loss=0.5793]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▊| 288/292 [03:18<00:02,  1.45batch/s, auc=0.8709, loss=0.5793]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▊| 288/292 [03:19<00:02,  1.45batch/s, auc=0.8712, loss=0.5441]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 289/292 [03:19<00:01,  1.53batch/s, auc=0.8712, loss=0.5441]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 289/292 [03:19<00:01,  1.53batch/s, auc=0.8713, loss=0.6568]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 290/292 [03:19<00:01,  1.60batch/s, auc=0.8713, loss=0.6568]\u001b[A\n",
      "Training Epoch 18/25:  99%|█████████▉| 290/292 [03:20<00:01,  1.60batch/s, auc=0.8713, loss=0.7061]\u001b[A\n",
      "Training Epoch 18/25: 100%|█████████▉| 291/292 [03:20<00:00,  1.65batch/s, auc=0.8713, loss=0.7061]\u001b[A\n",
      "Training Epoch 18/25: 100%|█████████▉| 291/292 [03:20<00:00,  1.65batch/s, auc=0.8713, loss=0.5012]\u001b[A\n",
      "Training Epoch 18/25: 100%|██████████| 292/292 [03:20<00:00,  1.45batch/s, auc=0.8713, loss=0.5012]\u001b[A\n",
      "Epochs:  72%|███████▏  | 18/25 [1:04:16<25:19, 217.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25] Train Loss: 0.7536 | Train AUROC: 0.8713 Val Loss: 0.8554 | Val AUROC: 0.8465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 19/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9509, loss=0.5588]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 1/292 [00:01<07:22,  1.52s/batch, auc=0.9509, loss=0.5588]\u001b[A\n",
      "Training Epoch 19/25:   0%|          | 1/292 [00:02<07:22,  1.52s/batch, auc=0.8542, loss=0.9332]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 2/292 [00:02<06:05,  1.26s/batch, auc=0.8542, loss=0.9332]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 2/292 [00:03<06:05,  1.26s/batch, auc=0.8872, loss=0.5770]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 3/292 [00:03<04:30,  1.07batch/s, auc=0.8872, loss=0.5770]\u001b[A\n",
      "Training Epoch 19/25:   1%|          | 3/292 [00:03<04:30,  1.07batch/s, auc=0.8896, loss=0.7847]\u001b[A\n",
      "Training Epoch 19/25:   1%|▏         | 4/292 [00:03<03:47,  1.27batch/s, auc=0.8896, loss=0.7847]\u001b[A\n",
      "Training Epoch 19/25:   1%|▏         | 4/292 [00:04<03:47,  1.27batch/s, auc=0.8870, loss=0.7931]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 5/292 [00:04<04:15,  1.12batch/s, auc=0.8870, loss=0.7931]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 5/292 [00:05<04:15,  1.12batch/s, auc=0.8908, loss=0.6802]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 6/292 [00:05<03:42,  1.29batch/s, auc=0.8908, loss=0.6802]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 6/292 [00:05<03:42,  1.29batch/s, auc=0.8875, loss=0.7249]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 7/292 [00:05<03:20,  1.42batch/s, auc=0.8875, loss=0.7249]\u001b[A\n",
      "Training Epoch 19/25:   2%|▏         | 7/292 [00:06<03:20,  1.42batch/s, auc=0.8859, loss=0.8072]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 8/292 [00:06<03:06,  1.52batch/s, auc=0.8859, loss=0.8072]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 8/292 [00:07<03:06,  1.52batch/s, auc=0.8737, loss=1.1217]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 9/292 [00:07<03:42,  1.27batch/s, auc=0.8737, loss=1.1217]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 9/292 [00:08<03:42,  1.27batch/s, auc=0.8625, loss=1.1179]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 10/292 [00:08<04:06,  1.14batch/s, auc=0.8625, loss=1.1179]\u001b[A\n",
      "Training Epoch 19/25:   3%|▎         | 10/292 [00:09<04:06,  1.14batch/s, auc=0.8666, loss=0.5762]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 11/292 [00:09<03:38,  1.29batch/s, auc=0.8666, loss=0.5762]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 11/292 [00:09<03:38,  1.29batch/s, auc=0.8656, loss=0.9755]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 12/292 [00:09<03:18,  1.41batch/s, auc=0.8656, loss=0.9755]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 12/292 [00:10<03:18,  1.41batch/s, auc=0.8669, loss=0.7918]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 13/292 [00:10<03:48,  1.22batch/s, auc=0.8669, loss=0.7918]\u001b[A\n",
      "Training Epoch 19/25:   4%|▍         | 13/292 [00:11<03:48,  1.22batch/s, auc=0.8642, loss=0.9406]\u001b[A\n",
      "Training Epoch 19/25:   5%|▍         | 14/292 [00:11<04:08,  1.12batch/s, auc=0.8642, loss=0.9406]\u001b[A\n",
      "Training Epoch 19/25:   5%|▍         | 14/292 [00:12<04:08,  1.12batch/s, auc=0.8650, loss=0.8658]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 15/292 [00:12<04:22,  1.06batch/s, auc=0.8650, loss=0.8658]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 15/292 [00:13<04:22,  1.06batch/s, auc=0.8714, loss=0.4788]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 16/292 [00:13<03:49,  1.20batch/s, auc=0.8714, loss=0.4788]\u001b[A\n",
      "Training Epoch 19/25:   5%|▌         | 16/292 [00:14<03:49,  1.20batch/s, auc=0.8700, loss=0.7994]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 17/292 [00:14<03:25,  1.34batch/s, auc=0.8700, loss=0.7994]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 17/292 [00:14<03:25,  1.34batch/s, auc=0.8694, loss=0.7933]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 18/292 [00:14<03:08,  1.45batch/s, auc=0.8694, loss=0.7933]\u001b[A\n",
      "Training Epoch 19/25:   6%|▌         | 18/292 [00:15<03:08,  1.45batch/s, auc=0.8645, loss=1.1752]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 19/292 [00:15<03:39,  1.24batch/s, auc=0.8645, loss=1.1752]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 19/292 [00:16<03:39,  1.24batch/s, auc=0.8654, loss=0.8251]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 20/292 [00:16<03:18,  1.37batch/s, auc=0.8654, loss=0.8251]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 20/292 [00:17<03:18,  1.37batch/s, auc=0.8634, loss=0.8950]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 21/292 [00:17<03:45,  1.20batch/s, auc=0.8634, loss=0.8950]\u001b[A\n",
      "Training Epoch 19/25:   7%|▋         | 21/292 [00:17<03:45,  1.20batch/s, auc=0.8649, loss=0.7291]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 22/292 [00:17<03:22,  1.34batch/s, auc=0.8649, loss=0.7291]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 22/292 [00:18<03:22,  1.34batch/s, auc=0.8612, loss=1.3132]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 23/292 [00:18<03:47,  1.18batch/s, auc=0.8612, loss=1.3132]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 23/292 [00:19<03:47,  1.18batch/s, auc=0.8548, loss=1.4685]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 24/292 [00:19<04:04,  1.10batch/s, auc=0.8548, loss=1.4685]\u001b[A\n",
      "Training Epoch 19/25:   8%|▊         | 24/292 [00:21<04:04,  1.10batch/s, auc=0.8505, loss=1.0114]\u001b[A\n",
      "Training Epoch 19/25:   9%|▊         | 25/292 [00:21<04:16,  1.04batch/s, auc=0.8505, loss=1.0114]\u001b[A\n",
      "Training Epoch 19/25:   9%|▊         | 25/292 [00:21<04:16,  1.04batch/s, auc=0.8532, loss=0.5846]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 26/292 [00:21<03:43,  1.19batch/s, auc=0.8532, loss=0.5846]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 26/292 [00:22<03:43,  1.19batch/s, auc=0.8575, loss=0.4938]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 27/292 [00:22<03:19,  1.33batch/s, auc=0.8575, loss=0.4938]\u001b[A\n",
      "Training Epoch 19/25:   9%|▉         | 27/292 [00:22<03:19,  1.33batch/s, auc=0.8575, loss=0.6748]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 28/292 [00:22<03:03,  1.44batch/s, auc=0.8575, loss=0.6748]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 28/292 [00:23<03:03,  1.44batch/s, auc=0.8564, loss=0.7860]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 29/292 [00:23<02:51,  1.53batch/s, auc=0.8564, loss=0.7860]\u001b[A\n",
      "Training Epoch 19/25:  10%|▉         | 29/292 [00:23<02:51,  1.53batch/s, auc=0.8578, loss=0.6334]\u001b[A\n",
      "Training Epoch 19/25:  10%|█         | 30/292 [00:23<02:43,  1.60batch/s, auc=0.8578, loss=0.6334]\u001b[A\n",
      "Training Epoch 19/25:  10%|█         | 30/292 [00:24<02:43,  1.60batch/s, auc=0.8601, loss=0.5285]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 31/292 [00:24<02:37,  1.66batch/s, auc=0.8601, loss=0.5285]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 31/292 [00:24<02:37,  1.66batch/s, auc=0.8607, loss=0.6841]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 32/292 [00:24<02:33,  1.70batch/s, auc=0.8607, loss=0.6841]\u001b[A\n",
      "Training Epoch 19/25:  11%|█         | 32/292 [00:25<02:33,  1.70batch/s, auc=0.8636, loss=0.4776]\u001b[A\n",
      "Training Epoch 19/25:  11%|█▏        | 33/292 [00:25<02:29,  1.73batch/s, auc=0.8636, loss=0.4776]\u001b[A\n",
      "Training Epoch 19/25:  11%|█▏        | 33/292 [00:26<02:29,  1.73batch/s, auc=0.8626, loss=0.8705]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 34/292 [00:26<02:27,  1.75batch/s, auc=0.8626, loss=0.8705]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 34/292 [00:26<02:27,  1.75batch/s, auc=0.8621, loss=1.1482]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 35/292 [00:26<02:25,  1.76batch/s, auc=0.8621, loss=1.1482]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 35/292 [00:27<02:25,  1.76batch/s, auc=0.8632, loss=0.6664]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 36/292 [00:27<02:24,  1.77batch/s, auc=0.8632, loss=0.6664]\u001b[A\n",
      "Training Epoch 19/25:  12%|█▏        | 36/292 [00:27<02:24,  1.77batch/s, auc=0.8644, loss=0.6571]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 37/292 [00:27<02:23,  1.78batch/s, auc=0.8644, loss=0.6571]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 37/292 [00:28<02:23,  1.78batch/s, auc=0.8661, loss=0.5873]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 38/292 [00:28<02:22,  1.79batch/s, auc=0.8661, loss=0.5873]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 38/292 [00:28<02:22,  1.79batch/s, auc=0.8644, loss=0.9108]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 39/292 [00:28<02:21,  1.79batch/s, auc=0.8644, loss=0.9108]\u001b[A\n",
      "Training Epoch 19/25:  13%|█▎        | 39/292 [00:29<02:21,  1.79batch/s, auc=0.8673, loss=0.6731]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▎        | 40/292 [00:29<02:20,  1.79batch/s, auc=0.8673, loss=0.6731]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▎        | 40/292 [00:29<02:20,  1.79batch/s, auc=0.8692, loss=0.5478]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 41/292 [00:29<02:19,  1.80batch/s, auc=0.8692, loss=0.5478]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 41/292 [00:30<02:19,  1.80batch/s, auc=0.8695, loss=0.7262]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 42/292 [00:30<02:19,  1.80batch/s, auc=0.8695, loss=0.7262]\u001b[A\n",
      "Training Epoch 19/25:  14%|█▍        | 42/292 [00:31<02:19,  1.80batch/s, auc=0.8703, loss=0.6642]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▍        | 43/292 [00:31<02:18,  1.80batch/s, auc=0.8703, loss=0.6642]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▍        | 43/292 [00:31<02:18,  1.80batch/s, auc=0.8693, loss=0.9753]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 44/292 [00:31<02:17,  1.80batch/s, auc=0.8693, loss=0.9753]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 44/292 [00:32<02:17,  1.80batch/s, auc=0.8703, loss=0.5971]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 45/292 [00:32<02:17,  1.80batch/s, auc=0.8703, loss=0.5971]\u001b[A\n",
      "Training Epoch 19/25:  15%|█▌        | 45/292 [00:32<02:17,  1.80batch/s, auc=0.8702, loss=0.6442]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 46/292 [00:32<02:16,  1.80batch/s, auc=0.8702, loss=0.6442]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 46/292 [00:33<02:16,  1.80batch/s, auc=0.8688, loss=0.7544]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 47/292 [00:33<02:16,  1.80batch/s, auc=0.8688, loss=0.7544]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▌        | 47/292 [00:34<02:16,  1.80batch/s, auc=0.8650, loss=1.3912]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▋        | 48/292 [00:34<02:53,  1.41batch/s, auc=0.8650, loss=1.3912]\u001b[A\n",
      "Training Epoch 19/25:  16%|█▋        | 48/292 [00:34<02:53,  1.41batch/s, auc=0.8678, loss=0.5484]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 49/292 [00:34<02:41,  1.51batch/s, auc=0.8678, loss=0.5484]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 49/292 [00:35<02:41,  1.51batch/s, auc=0.8677, loss=0.8078]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 50/292 [00:35<02:32,  1.58batch/s, auc=0.8677, loss=0.8078]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 50/292 [00:36<02:32,  1.58batch/s, auc=0.8688, loss=0.4800]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 51/292 [00:36<02:27,  1.64batch/s, auc=0.8688, loss=0.4800]\u001b[A\n",
      "Training Epoch 19/25:  17%|█▋        | 51/292 [00:36<02:27,  1.64batch/s, auc=0.8688, loss=0.6873]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 52/292 [00:36<02:22,  1.68batch/s, auc=0.8688, loss=0.6873]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 52/292 [00:37<02:22,  1.68batch/s, auc=0.8683, loss=0.7843]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 53/292 [00:37<02:19,  1.72batch/s, auc=0.8683, loss=0.7843]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 53/292 [00:38<02:19,  1.72batch/s, auc=0.8668, loss=0.9902]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 54/292 [00:38<02:53,  1.37batch/s, auc=0.8668, loss=0.9902]\u001b[A\n",
      "Training Epoch 19/25:  18%|█▊        | 54/292 [00:39<02:53,  1.37batch/s, auc=0.8648, loss=1.1166]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 55/292 [00:39<03:17,  1.20batch/s, auc=0.8648, loss=1.1166]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 55/292 [00:39<03:17,  1.20batch/s, auc=0.8652, loss=0.6112]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 56/292 [00:39<02:56,  1.33batch/s, auc=0.8652, loss=0.6112]\u001b[A\n",
      "Training Epoch 19/25:  19%|█▉        | 56/292 [00:40<02:56,  1.33batch/s, auc=0.8665, loss=0.4600]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 57/292 [00:40<02:42,  1.45batch/s, auc=0.8665, loss=0.4600]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 57/292 [00:40<02:42,  1.45batch/s, auc=0.8682, loss=0.7864]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 58/292 [00:40<02:32,  1.53batch/s, auc=0.8682, loss=0.7864]\u001b[A\n",
      "Training Epoch 19/25:  20%|█▉        | 58/292 [00:41<02:32,  1.53batch/s, auc=0.8693, loss=0.4670]\u001b[A\n",
      "Training Epoch 19/25:  20%|██        | 59/292 [00:41<02:25,  1.60batch/s, auc=0.8693, loss=0.4670]\u001b[A\n",
      "Training Epoch 19/25:  20%|██        | 59/292 [00:42<02:25,  1.60batch/s, auc=0.8705, loss=0.5527]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 60/292 [00:42<02:20,  1.66batch/s, auc=0.8705, loss=0.5527]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 60/292 [00:42<02:20,  1.66batch/s, auc=0.8716, loss=0.5853]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 61/292 [00:42<02:16,  1.69batch/s, auc=0.8716, loss=0.5853]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 61/292 [00:43<02:16,  1.69batch/s, auc=0.8702, loss=1.0184]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 62/292 [00:43<02:13,  1.72batch/s, auc=0.8702, loss=1.0184]\u001b[A\n",
      "Training Epoch 19/25:  21%|██        | 62/292 [00:43<02:13,  1.72batch/s, auc=0.8700, loss=0.8361]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 63/292 [00:43<02:11,  1.74batch/s, auc=0.8700, loss=0.8361]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 63/292 [00:44<02:11,  1.74batch/s, auc=0.8688, loss=0.8872]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 64/292 [00:44<02:44,  1.38batch/s, auc=0.8688, loss=0.8872]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 64/292 [00:45<02:44,  1.38batch/s, auc=0.8698, loss=0.6221]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 65/292 [00:45<02:32,  1.49batch/s, auc=0.8698, loss=0.6221]\u001b[A\n",
      "Training Epoch 19/25:  22%|██▏       | 65/292 [00:45<02:32,  1.49batch/s, auc=0.8709, loss=0.5303]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 66/292 [00:45<02:24,  1.57batch/s, auc=0.8709, loss=0.5303]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 66/292 [00:46<02:24,  1.57batch/s, auc=0.8710, loss=0.6611]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 67/292 [00:46<02:18,  1.63batch/s, auc=0.8710, loss=0.6611]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 67/292 [00:47<02:18,  1.63batch/s, auc=0.8704, loss=0.7372]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 68/292 [00:47<02:48,  1.33batch/s, auc=0.8704, loss=0.7372]\u001b[A\n",
      "Training Epoch 19/25:  23%|██▎       | 68/292 [00:48<02:48,  1.33batch/s, auc=0.8706, loss=0.7380]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▎       | 69/292 [00:48<02:34,  1.44batch/s, auc=0.8706, loss=0.7380]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▎       | 69/292 [00:48<02:34,  1.44batch/s, auc=0.8702, loss=0.6991]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 70/292 [00:48<02:24,  1.54batch/s, auc=0.8702, loss=0.6991]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 70/292 [00:49<02:24,  1.54batch/s, auc=0.8699, loss=0.8386]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 71/292 [00:49<02:51,  1.29batch/s, auc=0.8699, loss=0.8386]\u001b[A\n",
      "Training Epoch 19/25:  24%|██▍       | 71/292 [00:50<02:51,  1.29batch/s, auc=0.8704, loss=0.5275]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▍       | 72/292 [00:50<02:36,  1.41batch/s, auc=0.8704, loss=0.5275]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▍       | 72/292 [00:50<02:36,  1.41batch/s, auc=0.8716, loss=0.6061]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 73/292 [00:50<02:25,  1.50batch/s, auc=0.8716, loss=0.6061]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 73/292 [00:51<02:25,  1.50batch/s, auc=0.8720, loss=0.7435]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 74/292 [00:51<02:17,  1.58batch/s, auc=0.8720, loss=0.7435]\u001b[A\n",
      "Training Epoch 19/25:  25%|██▌       | 74/292 [00:51<02:17,  1.58batch/s, auc=0.8725, loss=0.5470]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 75/292 [00:51<02:12,  1.64batch/s, auc=0.8725, loss=0.5470]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 75/292 [00:52<02:12,  1.64batch/s, auc=0.8726, loss=0.6193]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 76/292 [00:52<02:08,  1.68batch/s, auc=0.8726, loss=0.6193]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▌       | 76/292 [00:53<02:08,  1.68batch/s, auc=0.8730, loss=0.5876]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▋       | 77/292 [00:53<02:05,  1.71batch/s, auc=0.8730, loss=0.5876]\u001b[A\n",
      "Training Epoch 19/25:  26%|██▋       | 77/292 [00:54<02:05,  1.71batch/s, auc=0.8712, loss=1.3028]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 78/292 [00:54<02:36,  1.37batch/s, auc=0.8712, loss=1.3028]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 78/292 [00:54<02:36,  1.37batch/s, auc=0.8721, loss=0.5154]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 79/292 [00:54<02:24,  1.47batch/s, auc=0.8721, loss=0.5154]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 79/292 [00:55<02:24,  1.47batch/s, auc=0.8712, loss=0.9877]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 80/292 [00:55<02:16,  1.56batch/s, auc=0.8712, loss=0.9877]\u001b[A\n",
      "Training Epoch 19/25:  27%|██▋       | 80/292 [00:55<02:16,  1.56batch/s, auc=0.8722, loss=0.4949]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 81/292 [00:55<02:10,  1.62batch/s, auc=0.8722, loss=0.4949]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 81/292 [00:56<02:10,  1.62batch/s, auc=0.8721, loss=0.5543]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 82/292 [00:56<02:05,  1.67batch/s, auc=0.8721, loss=0.5543]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 82/292 [00:57<02:05,  1.67batch/s, auc=0.8717, loss=0.7985]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 83/292 [00:57<02:34,  1.35batch/s, auc=0.8717, loss=0.7985]\u001b[A\n",
      "Training Epoch 19/25:  28%|██▊       | 83/292 [00:57<02:34,  1.35batch/s, auc=0.8714, loss=0.7587]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 84/292 [00:57<02:22,  1.46batch/s, auc=0.8714, loss=0.7587]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 84/292 [00:59<02:22,  1.46batch/s, auc=0.8701, loss=1.0697]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 85/292 [00:59<02:45,  1.25batch/s, auc=0.8701, loss=1.0697]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 85/292 [01:00<02:45,  1.25batch/s, auc=0.8696, loss=0.8436]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 86/292 [01:00<03:01,  1.13batch/s, auc=0.8696, loss=0.8436]\u001b[A\n",
      "Training Epoch 19/25:  29%|██▉       | 86/292 [01:00<03:01,  1.13batch/s, auc=0.8697, loss=0.6694]\u001b[A\n",
      "Training Epoch 19/25:  30%|██▉       | 87/292 [01:00<02:41,  1.27batch/s, auc=0.8697, loss=0.6694]\u001b[A\n",
      "Training Epoch 19/25:  30%|██▉       | 87/292 [01:01<02:41,  1.27batch/s, auc=0.8698, loss=0.7473]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 88/292 [01:01<02:26,  1.39batch/s, auc=0.8698, loss=0.7473]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 88/292 [01:01<02:26,  1.39batch/s, auc=0.8696, loss=0.8625]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 89/292 [01:01<02:15,  1.49batch/s, auc=0.8696, loss=0.8625]\u001b[A\n",
      "Training Epoch 19/25:  30%|███       | 89/292 [01:02<02:15,  1.49batch/s, auc=0.8703, loss=0.5468]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 90/292 [01:02<02:08,  1.57batch/s, auc=0.8703, loss=0.5468]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 90/292 [01:02<02:08,  1.57batch/s, auc=0.8701, loss=0.7682]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 91/292 [01:02<02:03,  1.63batch/s, auc=0.8701, loss=0.7682]\u001b[A\n",
      "Training Epoch 19/25:  31%|███       | 91/292 [01:03<02:03,  1.63batch/s, auc=0.8709, loss=0.6010]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 92/292 [01:03<01:59,  1.68batch/s, auc=0.8709, loss=0.6010]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 92/292 [01:04<01:59,  1.68batch/s, auc=0.8714, loss=0.5983]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 93/292 [01:04<01:56,  1.71batch/s, auc=0.8714, loss=0.5983]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 93/292 [01:04<01:56,  1.71batch/s, auc=0.8701, loss=0.9478]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 94/292 [01:04<01:54,  1.74batch/s, auc=0.8701, loss=0.9478]\u001b[A\n",
      "Training Epoch 19/25:  32%|███▏      | 94/292 [01:05<01:54,  1.74batch/s, auc=0.8706, loss=0.6005]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 95/292 [01:05<01:52,  1.75batch/s, auc=0.8706, loss=0.6005]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 95/292 [01:05<01:52,  1.75batch/s, auc=0.8711, loss=0.5951]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 96/292 [01:05<01:51,  1.77batch/s, auc=0.8711, loss=0.5951]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 96/292 [01:06<01:51,  1.77batch/s, auc=0.8718, loss=0.4929]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 97/292 [01:06<01:49,  1.77batch/s, auc=0.8718, loss=0.4929]\u001b[A\n",
      "Training Epoch 19/25:  33%|███▎      | 97/292 [01:06<01:49,  1.77batch/s, auc=0.8725, loss=0.6849]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▎      | 98/292 [01:06<01:49,  1.78batch/s, auc=0.8725, loss=0.6849]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▎      | 98/292 [01:07<01:49,  1.78batch/s, auc=0.8731, loss=0.5220]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 99/292 [01:07<01:48,  1.78batch/s, auc=0.8731, loss=0.5220]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 99/292 [01:07<01:48,  1.78batch/s, auc=0.8736, loss=0.5015]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 100/292 [01:07<01:47,  1.79batch/s, auc=0.8736, loss=0.5015]\u001b[A\n",
      "Training Epoch 19/25:  34%|███▍      | 100/292 [01:08<01:47,  1.79batch/s, auc=0.8740, loss=0.5364]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 101/292 [01:08<01:46,  1.79batch/s, auc=0.8740, loss=0.5364]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 101/292 [01:09<01:46,  1.79batch/s, auc=0.8742, loss=0.6133]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 102/292 [01:09<01:46,  1.79batch/s, auc=0.8742, loss=0.6133]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▍      | 102/292 [01:09<01:46,  1.79batch/s, auc=0.8749, loss=0.6131]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▌      | 103/292 [01:09<01:45,  1.79batch/s, auc=0.8749, loss=0.6131]\u001b[A\n",
      "Training Epoch 19/25:  35%|███▌      | 103/292 [01:10<01:45,  1.79batch/s, auc=0.8749, loss=0.5975]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 104/292 [01:10<01:45,  1.79batch/s, auc=0.8749, loss=0.5975]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 104/292 [01:10<01:45,  1.79batch/s, auc=0.8748, loss=0.8301]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 105/292 [01:10<01:44,  1.79batch/s, auc=0.8748, loss=0.8301]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▌      | 105/292 [01:11<01:44,  1.79batch/s, auc=0.8757, loss=0.5133]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▋      | 106/292 [01:11<01:43,  1.79batch/s, auc=0.8757, loss=0.5133]\u001b[A\n",
      "Training Epoch 19/25:  36%|███▋      | 106/292 [01:11<01:43,  1.79batch/s, auc=0.8765, loss=0.6020]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 107/292 [01:11<01:43,  1.79batch/s, auc=0.8765, loss=0.6020]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 107/292 [01:12<01:43,  1.79batch/s, auc=0.8776, loss=0.4971]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 108/292 [01:12<01:42,  1.79batch/s, auc=0.8776, loss=0.4971]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 108/292 [01:12<01:42,  1.79batch/s, auc=0.8780, loss=0.4921]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 109/292 [01:12<01:42,  1.79batch/s, auc=0.8780, loss=0.4921]\u001b[A\n",
      "Training Epoch 19/25:  37%|███▋      | 109/292 [01:14<01:42,  1.79batch/s, auc=0.8772, loss=1.0946]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 110/292 [01:14<02:09,  1.40batch/s, auc=0.8772, loss=1.0946]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 110/292 [01:14<02:09,  1.40batch/s, auc=0.8776, loss=0.5536]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 111/292 [01:14<02:00,  1.50batch/s, auc=0.8776, loss=0.5536]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 111/292 [01:15<02:00,  1.50batch/s, auc=0.8780, loss=0.6814]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 112/292 [01:15<01:54,  1.58batch/s, auc=0.8780, loss=0.6814]\u001b[A\n",
      "Training Epoch 19/25:  38%|███▊      | 112/292 [01:15<01:54,  1.58batch/s, auc=0.8784, loss=0.6780]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▊      | 113/292 [01:15<01:49,  1.64batch/s, auc=0.8784, loss=0.6780]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▊      | 113/292 [01:16<01:49,  1.64batch/s, auc=0.8771, loss=1.1127]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 114/292 [01:16<02:13,  1.33batch/s, auc=0.8771, loss=1.1127]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 114/292 [01:17<02:13,  1.33batch/s, auc=0.8777, loss=0.6287]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 115/292 [01:17<02:02,  1.44batch/s, auc=0.8777, loss=0.6287]\u001b[A\n",
      "Training Epoch 19/25:  39%|███▉      | 115/292 [01:17<02:02,  1.44batch/s, auc=0.8779, loss=0.5700]\u001b[A\n",
      "Training Epoch 19/25:  40%|███▉      | 116/292 [01:17<01:54,  1.53batch/s, auc=0.8779, loss=0.5700]\u001b[A\n",
      "Training Epoch 19/25:  40%|███▉      | 116/292 [01:18<01:54,  1.53batch/s, auc=0.8781, loss=0.6695]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 117/292 [01:18<01:49,  1.60batch/s, auc=0.8781, loss=0.6695]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 117/292 [01:19<01:49,  1.60batch/s, auc=0.8765, loss=1.2760]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 118/292 [01:19<02:12,  1.32batch/s, auc=0.8765, loss=1.2760]\u001b[A\n",
      "Training Epoch 19/25:  40%|████      | 118/292 [01:20<02:12,  1.32batch/s, auc=0.8773, loss=0.5811]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 119/292 [01:20<02:00,  1.43batch/s, auc=0.8773, loss=0.5811]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 119/292 [01:20<02:00,  1.43batch/s, auc=0.8780, loss=0.4378]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 120/292 [01:20<01:53,  1.52batch/s, auc=0.8780, loss=0.4378]\u001b[A\n",
      "Training Epoch 19/25:  41%|████      | 120/292 [01:21<01:53,  1.52batch/s, auc=0.8782, loss=0.5614]\u001b[A\n",
      "Training Epoch 19/25:  41%|████▏     | 121/292 [01:21<01:47,  1.59batch/s, auc=0.8782, loss=0.5614]\u001b[A\n",
      "Training Epoch 19/25:  41%|████▏     | 121/292 [01:21<01:47,  1.59batch/s, auc=0.8783, loss=0.6670]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 122/292 [01:21<01:43,  1.65batch/s, auc=0.8783, loss=0.6670]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 122/292 [01:22<01:43,  1.65batch/s, auc=0.8785, loss=0.4823]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 123/292 [01:22<01:40,  1.69batch/s, auc=0.8785, loss=0.4823]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 123/292 [01:23<01:40,  1.69batch/s, auc=0.8782, loss=0.9459]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 124/292 [01:23<02:03,  1.36batch/s, auc=0.8782, loss=0.9459]\u001b[A\n",
      "Training Epoch 19/25:  42%|████▏     | 124/292 [01:23<02:03,  1.36batch/s, auc=0.8782, loss=0.7873]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 125/292 [01:23<01:54,  1.46batch/s, auc=0.8782, loss=0.7873]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 125/292 [01:25<01:54,  1.46batch/s, auc=0.8778, loss=0.9698]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 126/292 [01:25<02:13,  1.25batch/s, auc=0.8778, loss=0.9698]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 126/292 [01:25<02:13,  1.25batch/s, auc=0.8772, loss=0.8270]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 127/292 [01:25<02:00,  1.37batch/s, auc=0.8772, loss=0.8270]\u001b[A\n",
      "Training Epoch 19/25:  43%|████▎     | 127/292 [01:26<02:00,  1.37batch/s, auc=0.8769, loss=0.9939]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 128/292 [01:26<01:51,  1.48batch/s, auc=0.8769, loss=0.9939]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 128/292 [01:26<01:51,  1.48batch/s, auc=0.8771, loss=0.5673]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 129/292 [01:26<01:44,  1.56batch/s, auc=0.8771, loss=0.5673]\u001b[A\n",
      "Training Epoch 19/25:  44%|████▍     | 129/292 [01:27<01:44,  1.56batch/s, auc=0.8775, loss=0.5978]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 130/292 [01:27<01:40,  1.62batch/s, auc=0.8775, loss=0.5978]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 130/292 [01:27<01:40,  1.62batch/s, auc=0.8778, loss=0.5997]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 131/292 [01:27<01:36,  1.67batch/s, auc=0.8778, loss=0.5997]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▍     | 131/292 [01:28<01:36,  1.67batch/s, auc=0.8781, loss=0.5791]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▌     | 132/292 [01:28<01:34,  1.70batch/s, auc=0.8781, loss=0.5791]\u001b[A\n",
      "Training Epoch 19/25:  45%|████▌     | 132/292 [01:28<01:34,  1.70batch/s, auc=0.8777, loss=0.7537]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 133/292 [01:28<01:32,  1.73batch/s, auc=0.8777, loss=0.7537]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 133/292 [01:29<01:32,  1.73batch/s, auc=0.8783, loss=0.4440]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 134/292 [01:29<01:30,  1.74batch/s, auc=0.8783, loss=0.4440]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 134/292 [01:30<01:30,  1.74batch/s, auc=0.8788, loss=0.5787]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 135/292 [01:30<01:29,  1.75batch/s, auc=0.8788, loss=0.5787]\u001b[A\n",
      "Training Epoch 19/25:  46%|████▌     | 135/292 [01:30<01:29,  1.75batch/s, auc=0.8790, loss=0.7654]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 136/292 [01:30<01:28,  1.76batch/s, auc=0.8790, loss=0.7654]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 136/292 [01:31<01:28,  1.76batch/s, auc=0.8783, loss=0.8921]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 137/292 [01:31<01:51,  1.39batch/s, auc=0.8783, loss=0.8921]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 137/292 [01:32<01:51,  1.39batch/s, auc=0.8784, loss=0.7688]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 138/292 [01:32<02:07,  1.21batch/s, auc=0.8784, loss=0.7688]\u001b[A\n",
      "Training Epoch 19/25:  47%|████▋     | 138/292 [01:33<02:07,  1.21batch/s, auc=0.8783, loss=0.7468]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 139/292 [01:33<01:54,  1.34batch/s, auc=0.8783, loss=0.7468]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 139/292 [01:33<01:54,  1.34batch/s, auc=0.8784, loss=0.5379]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 140/292 [01:33<01:44,  1.45batch/s, auc=0.8784, loss=0.5379]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 140/292 [01:34<01:44,  1.45batch/s, auc=0.8786, loss=0.5694]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 141/292 [01:34<01:38,  1.54batch/s, auc=0.8786, loss=0.5694]\u001b[A\n",
      "Training Epoch 19/25:  48%|████▊     | 141/292 [01:35<01:38,  1.54batch/s, auc=0.8790, loss=0.5848]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▊     | 142/292 [01:35<01:33,  1.60batch/s, auc=0.8790, loss=0.5848]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▊     | 142/292 [01:36<01:33,  1.60batch/s, auc=0.8788, loss=0.7701]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 143/292 [01:36<01:53,  1.32batch/s, auc=0.8788, loss=0.7701]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 143/292 [01:36<01:53,  1.32batch/s, auc=0.8794, loss=0.4593]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 144/292 [01:36<01:43,  1.43batch/s, auc=0.8794, loss=0.4593]\u001b[A\n",
      "Training Epoch 19/25:  49%|████▉     | 144/292 [01:37<01:43,  1.43batch/s, auc=0.8798, loss=0.4829]\u001b[A\n",
      "Training Epoch 19/25:  50%|████▉     | 145/292 [01:37<01:36,  1.52batch/s, auc=0.8798, loss=0.4829]\u001b[A\n",
      "Training Epoch 19/25:  50%|████▉     | 145/292 [01:37<01:36,  1.52batch/s, auc=0.8806, loss=0.5475]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 146/292 [01:37<01:31,  1.59batch/s, auc=0.8806, loss=0.5475]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 146/292 [01:38<01:31,  1.59batch/s, auc=0.8811, loss=0.7153]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 147/292 [01:38<01:28,  1.65batch/s, auc=0.8811, loss=0.7153]\u001b[A\n",
      "Training Epoch 19/25:  50%|█████     | 147/292 [01:38<01:28,  1.65batch/s, auc=0.8807, loss=1.0351]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 148/292 [01:38<01:25,  1.69batch/s, auc=0.8807, loss=1.0351]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 148/292 [01:39<01:25,  1.69batch/s, auc=0.8808, loss=0.5312]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 149/292 [01:39<01:23,  1.72batch/s, auc=0.8808, loss=0.5312]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████     | 149/292 [01:40<01:23,  1.72batch/s, auc=0.8813, loss=0.6123]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████▏    | 150/292 [01:40<01:21,  1.74batch/s, auc=0.8813, loss=0.6123]\u001b[A\n",
      "Training Epoch 19/25:  51%|█████▏    | 150/292 [01:40<01:21,  1.74batch/s, auc=0.8811, loss=0.7995]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 151/292 [01:40<01:20,  1.76batch/s, auc=0.8811, loss=0.7995]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 151/292 [01:41<01:20,  1.76batch/s, auc=0.8805, loss=0.9308]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 152/292 [01:41<01:41,  1.39batch/s, auc=0.8805, loss=0.9308]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 152/292 [01:42<01:41,  1.39batch/s, auc=0.8800, loss=0.9672]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 153/292 [01:42<01:55,  1.21batch/s, auc=0.8800, loss=0.9672]\u001b[A\n",
      "Training Epoch 19/25:  52%|█████▏    | 153/292 [01:43<01:55,  1.21batch/s, auc=0.8798, loss=0.7551]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 154/292 [01:43<01:43,  1.34batch/s, auc=0.8798, loss=0.7551]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 154/292 [01:43<01:43,  1.34batch/s, auc=0.8800, loss=0.6670]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 155/292 [01:43<01:34,  1.45batch/s, auc=0.8800, loss=0.6670]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 155/292 [01:44<01:34,  1.45batch/s, auc=0.8799, loss=0.8015]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 156/292 [01:44<01:28,  1.54batch/s, auc=0.8799, loss=0.8015]\u001b[A\n",
      "Training Epoch 19/25:  53%|█████▎    | 156/292 [01:44<01:28,  1.54batch/s, auc=0.8805, loss=0.5065]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 157/292 [01:44<01:24,  1.60batch/s, auc=0.8805, loss=0.5065]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 157/292 [01:45<01:24,  1.60batch/s, auc=0.8807, loss=0.6589]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 158/292 [01:45<01:21,  1.65batch/s, auc=0.8807, loss=0.6589]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 158/292 [01:46<01:21,  1.65batch/s, auc=0.8812, loss=0.4811]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 159/292 [01:46<01:18,  1.69batch/s, auc=0.8812, loss=0.4811]\u001b[A\n",
      "Training Epoch 19/25:  54%|█████▍    | 159/292 [01:47<01:18,  1.69batch/s, auc=0.8799, loss=1.3960]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▍    | 160/292 [01:47<01:37,  1.36batch/s, auc=0.8799, loss=1.3960]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▍    | 160/292 [01:47<01:37,  1.36batch/s, auc=0.8798, loss=0.6537]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 161/292 [01:47<01:29,  1.46batch/s, auc=0.8798, loss=0.6537]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 161/292 [01:48<01:29,  1.46batch/s, auc=0.8795, loss=0.6139]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 162/292 [01:48<01:24,  1.55batch/s, auc=0.8795, loss=0.6139]\u001b[A\n",
      "Training Epoch 19/25:  55%|█████▌    | 162/292 [01:49<01:24,  1.55batch/s, auc=0.8798, loss=0.6623]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 163/292 [01:49<01:40,  1.29batch/s, auc=0.8798, loss=0.6623]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 163/292 [01:50<01:40,  1.29batch/s, auc=0.8796, loss=0.8267]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 164/292 [01:50<01:50,  1.15batch/s, auc=0.8796, loss=0.8267]\u001b[A\n",
      "Training Epoch 19/25:  56%|█████▌    | 164/292 [01:51<01:50,  1.15batch/s, auc=0.8792, loss=0.9584]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 165/292 [01:51<01:58,  1.08batch/s, auc=0.8792, loss=0.9584]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 165/292 [01:52<01:58,  1.08batch/s, auc=0.8797, loss=0.6084]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 166/292 [01:52<01:43,  1.22batch/s, auc=0.8797, loss=0.6084]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 166/292 [01:53<01:43,  1.22batch/s, auc=0.8794, loss=0.8402]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 167/292 [01:53<01:51,  1.12batch/s, auc=0.8794, loss=0.8402]\u001b[A\n",
      "Training Epoch 19/25:  57%|█████▋    | 167/292 [01:53<01:51,  1.12batch/s, auc=0.8790, loss=0.7545]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 168/292 [01:53<01:38,  1.26batch/s, auc=0.8790, loss=0.7545]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 168/292 [01:54<01:38,  1.26batch/s, auc=0.8792, loss=0.4924]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 169/292 [01:54<01:29,  1.38batch/s, auc=0.8792, loss=0.4924]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 169/292 [01:54<01:29,  1.38batch/s, auc=0.8797, loss=0.4616]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 170/292 [01:54<01:22,  1.48batch/s, auc=0.8797, loss=0.4616]\u001b[A\n",
      "Training Epoch 19/25:  58%|█████▊    | 170/292 [01:55<01:22,  1.48batch/s, auc=0.8793, loss=1.1015]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▊    | 171/292 [01:55<01:17,  1.56batch/s, auc=0.8793, loss=1.1015]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▊    | 171/292 [01:55<01:17,  1.56batch/s, auc=0.8796, loss=0.5576]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 172/292 [01:55<01:13,  1.62batch/s, auc=0.8796, loss=0.5576]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 172/292 [01:56<01:13,  1.62batch/s, auc=0.8798, loss=0.5526]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 173/292 [01:56<01:11,  1.67batch/s, auc=0.8798, loss=0.5526]\u001b[A\n",
      "Training Epoch 19/25:  59%|█████▉    | 173/292 [01:57<01:11,  1.67batch/s, auc=0.8805, loss=0.3929]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 174/292 [01:57<01:09,  1.70batch/s, auc=0.8805, loss=0.3929]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 174/292 [01:57<01:09,  1.70batch/s, auc=0.8809, loss=0.5592]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 175/292 [01:57<01:07,  1.72batch/s, auc=0.8809, loss=0.5592]\u001b[A\n",
      "Training Epoch 19/25:  60%|█████▉    | 175/292 [01:58<01:07,  1.72batch/s, auc=0.8811, loss=0.7158]\u001b[A\n",
      "Training Epoch 19/25:  60%|██████    | 176/292 [01:58<01:06,  1.74batch/s, auc=0.8811, loss=0.7158]\u001b[A\n",
      "Training Epoch 19/25:  60%|██████    | 176/292 [01:58<01:06,  1.74batch/s, auc=0.8813, loss=0.5747]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 177/292 [01:58<01:05,  1.76batch/s, auc=0.8813, loss=0.5747]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 177/292 [01:59<01:05,  1.76batch/s, auc=0.8809, loss=0.8400]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 178/292 [01:59<01:04,  1.76batch/s, auc=0.8809, loss=0.8400]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████    | 178/292 [01:59<01:04,  1.76batch/s, auc=0.8809, loss=0.7359]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████▏   | 179/292 [01:59<01:03,  1.77batch/s, auc=0.8809, loss=0.7359]\u001b[A\n",
      "Training Epoch 19/25:  61%|██████▏   | 179/292 [02:00<01:03,  1.77batch/s, auc=0.8803, loss=1.3393]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 180/292 [02:00<01:03,  1.78batch/s, auc=0.8803, loss=1.3393]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 180/292 [02:01<01:03,  1.78batch/s, auc=0.8804, loss=0.7915]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 181/292 [02:01<01:02,  1.78batch/s, auc=0.8804, loss=0.7915]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 181/292 [02:01<01:02,  1.78batch/s, auc=0.8805, loss=0.6954]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 182/292 [02:01<01:01,  1.78batch/s, auc=0.8805, loss=0.6954]\u001b[A\n",
      "Training Epoch 19/25:  62%|██████▏   | 182/292 [02:02<01:01,  1.78batch/s, auc=0.8808, loss=0.6445]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 183/292 [02:02<01:01,  1.78batch/s, auc=0.8808, loss=0.6445]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 183/292 [02:02<01:01,  1.78batch/s, auc=0.8810, loss=0.5266]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 184/292 [02:02<01:00,  1.78batch/s, auc=0.8810, loss=0.5266]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 184/292 [02:03<01:00,  1.78batch/s, auc=0.8804, loss=1.0485]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 185/292 [02:03<01:16,  1.40batch/s, auc=0.8804, loss=1.0485]\u001b[A\n",
      "Training Epoch 19/25:  63%|██████▎   | 185/292 [02:04<01:16,  1.40batch/s, auc=0.8806, loss=0.6755]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▎   | 186/292 [02:04<01:10,  1.49batch/s, auc=0.8806, loss=0.6755]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▎   | 186/292 [02:04<01:10,  1.49batch/s, auc=0.8804, loss=0.5568]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 187/292 [02:04<01:06,  1.57batch/s, auc=0.8804, loss=0.5568]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 187/292 [02:05<01:06,  1.57batch/s, auc=0.8806, loss=0.5488]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 188/292 [02:05<01:03,  1.63batch/s, auc=0.8806, loss=0.5488]\u001b[A\n",
      "Training Epoch 19/25:  64%|██████▍   | 188/292 [02:06<01:03,  1.63batch/s, auc=0.8806, loss=0.7371]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▍   | 189/292 [02:06<01:01,  1.67batch/s, auc=0.8806, loss=0.7371]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▍   | 189/292 [02:06<01:01,  1.67batch/s, auc=0.8806, loss=0.6296]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 190/292 [02:06<00:59,  1.70batch/s, auc=0.8806, loss=0.6296]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 190/292 [02:07<00:59,  1.70batch/s, auc=0.8804, loss=0.7567]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 191/292 [02:07<00:58,  1.73batch/s, auc=0.8804, loss=0.7567]\u001b[A\n",
      "Training Epoch 19/25:  65%|██████▌   | 191/292 [02:08<00:58,  1.73batch/s, auc=0.8802, loss=0.9711]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 192/292 [02:08<01:12,  1.37batch/s, auc=0.8802, loss=0.9711]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 192/292 [02:08<01:12,  1.37batch/s, auc=0.8800, loss=0.7573]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 193/292 [02:08<01:07,  1.47batch/s, auc=0.8800, loss=0.7573]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▌   | 193/292 [02:09<01:07,  1.47batch/s, auc=0.8801, loss=0.5873]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▋   | 194/292 [02:09<01:03,  1.55batch/s, auc=0.8801, loss=0.5873]\u001b[A\n",
      "Training Epoch 19/25:  66%|██████▋   | 194/292 [02:09<01:03,  1.55batch/s, auc=0.8800, loss=0.9812]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 195/292 [02:09<01:00,  1.62batch/s, auc=0.8800, loss=0.9812]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 195/292 [02:10<01:00,  1.62batch/s, auc=0.8802, loss=0.6180]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 196/292 [02:10<00:57,  1.66batch/s, auc=0.8802, loss=0.6180]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 196/292 [02:11<00:57,  1.66batch/s, auc=0.8802, loss=0.5312]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 197/292 [02:11<00:56,  1.70batch/s, auc=0.8802, loss=0.5312]\u001b[A\n",
      "Training Epoch 19/25:  67%|██████▋   | 197/292 [02:12<00:56,  1.70batch/s, auc=0.8800, loss=0.8243]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 198/292 [02:12<01:09,  1.36batch/s, auc=0.8800, loss=0.8243]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 198/292 [02:12<01:09,  1.36batch/s, auc=0.8799, loss=0.8182]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 199/292 [02:12<01:03,  1.46batch/s, auc=0.8799, loss=0.8182]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 199/292 [02:13<01:03,  1.46batch/s, auc=0.8803, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 200/292 [02:13<00:59,  1.55batch/s, auc=0.8803, loss=0.5283]\u001b[A\n",
      "Training Epoch 19/25:  68%|██████▊   | 200/292 [02:13<00:59,  1.55batch/s, auc=0.8807, loss=0.4892]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 201/292 [02:13<00:56,  1.61batch/s, auc=0.8807, loss=0.4892]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 201/292 [02:14<00:56,  1.61batch/s, auc=0.8810, loss=0.6805]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 202/292 [02:14<00:54,  1.66batch/s, auc=0.8810, loss=0.6805]\u001b[A\n",
      "Training Epoch 19/25:  69%|██████▉   | 202/292 [02:14<00:54,  1.66batch/s, auc=0.8807, loss=0.8682]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 203/292 [02:14<00:52,  1.69batch/s, auc=0.8807, loss=0.8682]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 203/292 [02:15<00:52,  1.69batch/s, auc=0.8809, loss=0.5743]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 204/292 [02:15<00:51,  1.72batch/s, auc=0.8809, loss=0.5743]\u001b[A\n",
      "Training Epoch 19/25:  70%|██████▉   | 204/292 [02:16<00:51,  1.72batch/s, auc=0.8810, loss=0.6170]\u001b[A\n",
      "Training Epoch 19/25:  70%|███████   | 205/292 [02:16<00:50,  1.74batch/s, auc=0.8810, loss=0.6170]\u001b[A\n",
      "Training Epoch 19/25:  70%|███████   | 205/292 [02:16<00:50,  1.74batch/s, auc=0.8811, loss=0.5951]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 206/292 [02:16<00:49,  1.75batch/s, auc=0.8811, loss=0.5951]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 206/292 [02:17<00:49,  1.75batch/s, auc=0.8806, loss=1.0754]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 207/292 [02:17<01:01,  1.38batch/s, auc=0.8806, loss=1.0754]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 207/292 [02:18<01:01,  1.38batch/s, auc=0.8804, loss=0.8571]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 208/292 [02:18<00:56,  1.48batch/s, auc=0.8804, loss=0.8571]\u001b[A\n",
      "Training Epoch 19/25:  71%|███████   | 208/292 [02:18<00:56,  1.48batch/s, auc=0.8804, loss=0.5305]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 209/292 [02:18<00:53,  1.56batch/s, auc=0.8804, loss=0.5305]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 209/292 [02:19<00:53,  1.56batch/s, auc=0.8808, loss=0.4773]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 210/292 [02:19<00:50,  1.62batch/s, auc=0.8808, loss=0.4773]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 210/292 [02:19<00:50,  1.62batch/s, auc=0.8809, loss=0.5968]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 211/292 [02:19<00:48,  1.66batch/s, auc=0.8809, loss=0.5968]\u001b[A\n",
      "Training Epoch 19/25:  72%|███████▏  | 211/292 [02:20<00:48,  1.66batch/s, auc=0.8813, loss=0.4509]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 212/292 [02:20<00:47,  1.70batch/s, auc=0.8813, loss=0.4509]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 212/292 [02:21<00:47,  1.70batch/s, auc=0.8812, loss=0.8418]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 213/292 [02:21<00:45,  1.72batch/s, auc=0.8812, loss=0.8418]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 213/292 [02:21<00:45,  1.72batch/s, auc=0.8811, loss=0.6234]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 214/292 [02:21<00:44,  1.74batch/s, auc=0.8811, loss=0.6234]\u001b[A\n",
      "Training Epoch 19/25:  73%|███████▎  | 214/292 [02:22<00:44,  1.74batch/s, auc=0.8816, loss=0.4293]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▎  | 215/292 [02:22<00:43,  1.75batch/s, auc=0.8816, loss=0.4293]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▎  | 215/292 [02:22<00:43,  1.75batch/s, auc=0.8817, loss=0.6566]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 216/292 [02:22<00:43,  1.76batch/s, auc=0.8817, loss=0.6566]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 216/292 [02:23<00:43,  1.76batch/s, auc=0.8818, loss=0.5707]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 217/292 [02:23<00:42,  1.76batch/s, auc=0.8818, loss=0.5707]\u001b[A\n",
      "Training Epoch 19/25:  74%|███████▍  | 217/292 [02:23<00:42,  1.76batch/s, auc=0.8816, loss=0.7940]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▍  | 218/292 [02:23<00:41,  1.77batch/s, auc=0.8816, loss=0.7940]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▍  | 218/292 [02:24<00:41,  1.77batch/s, auc=0.8817, loss=0.6486]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 219/292 [02:24<00:41,  1.77batch/s, auc=0.8817, loss=0.6486]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 219/292 [02:24<00:41,  1.77batch/s, auc=0.8819, loss=0.4699]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 220/292 [02:24<00:40,  1.77batch/s, auc=0.8819, loss=0.4699]\u001b[A\n",
      "Training Epoch 19/25:  75%|███████▌  | 220/292 [02:25<00:40,  1.77batch/s, auc=0.8820, loss=0.5961]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 221/292 [02:25<00:40,  1.77batch/s, auc=0.8820, loss=0.5961]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 221/292 [02:26<00:40,  1.77batch/s, auc=0.8821, loss=0.6259]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 222/292 [02:26<00:39,  1.77batch/s, auc=0.8821, loss=0.6259]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▌  | 222/292 [02:26<00:39,  1.77batch/s, auc=0.8819, loss=0.6692]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▋  | 223/292 [02:26<00:38,  1.77batch/s, auc=0.8819, loss=0.6692]\u001b[A\n",
      "Training Epoch 19/25:  76%|███████▋  | 223/292 [02:27<00:38,  1.77batch/s, auc=0.8820, loss=0.6585]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 224/292 [02:27<00:38,  1.78batch/s, auc=0.8820, loss=0.6585]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 224/292 [02:27<00:38,  1.78batch/s, auc=0.8822, loss=0.5368]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 225/292 [02:27<00:37,  1.78batch/s, auc=0.8822, loss=0.5368]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 225/292 [02:28<00:37,  1.78batch/s, auc=0.8817, loss=1.1067]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 226/292 [02:28<00:37,  1.78batch/s, auc=0.8817, loss=1.1067]\u001b[A\n",
      "Training Epoch 19/25:  77%|███████▋  | 226/292 [02:28<00:37,  1.78batch/s, auc=0.8813, loss=1.0020]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 227/292 [02:28<00:36,  1.78batch/s, auc=0.8813, loss=1.0020]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 227/292 [02:29<00:36,  1.78batch/s, auc=0.8813, loss=0.7077]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 228/292 [02:29<00:35,  1.78batch/s, auc=0.8813, loss=0.7077]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 228/292 [02:30<00:35,  1.78batch/s, auc=0.8814, loss=0.7499]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 229/292 [02:30<00:35,  1.78batch/s, auc=0.8814, loss=0.7499]\u001b[A\n",
      "Training Epoch 19/25:  78%|███████▊  | 229/292 [02:31<00:35,  1.78batch/s, auc=0.8809, loss=1.1259]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 230/292 [02:31<00:44,  1.39batch/s, auc=0.8809, loss=1.1259]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 230/292 [02:31<00:44,  1.39batch/s, auc=0.8806, loss=0.7602]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 231/292 [02:31<00:40,  1.49batch/s, auc=0.8806, loss=0.7602]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 231/292 [02:32<00:40,  1.49batch/s, auc=0.8810, loss=0.5187]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 232/292 [02:32<00:38,  1.56batch/s, auc=0.8810, loss=0.5187]\u001b[A\n",
      "Training Epoch 19/25:  79%|███████▉  | 232/292 [02:32<00:38,  1.56batch/s, auc=0.8811, loss=0.5938]\u001b[A\n",
      "Training Epoch 19/25:  80%|███████▉  | 233/292 [02:32<00:36,  1.62batch/s, auc=0.8811, loss=0.5938]\u001b[A\n",
      "Training Epoch 19/25:  80%|███████▉  | 233/292 [02:33<00:36,  1.62batch/s, auc=0.8812, loss=0.7882]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 234/292 [02:33<00:34,  1.67batch/s, auc=0.8812, loss=0.7882]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 234/292 [02:34<00:34,  1.67batch/s, auc=0.8808, loss=0.9451]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 235/292 [02:34<00:42,  1.34batch/s, auc=0.8808, loss=0.9451]\u001b[A\n",
      "Training Epoch 19/25:  80%|████████  | 235/292 [02:35<00:42,  1.34batch/s, auc=0.8811, loss=0.6372]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 236/292 [02:35<00:38,  1.45batch/s, auc=0.8811, loss=0.6372]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 236/292 [02:35<00:38,  1.45batch/s, auc=0.8814, loss=0.4797]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 237/292 [02:35<00:35,  1.53batch/s, auc=0.8814, loss=0.4797]\u001b[A\n",
      "Training Epoch 19/25:  81%|████████  | 237/292 [02:36<00:35,  1.53batch/s, auc=0.8812, loss=0.8546]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 238/292 [02:36<00:42,  1.28batch/s, auc=0.8812, loss=0.8546]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 238/292 [02:37<00:42,  1.28batch/s, auc=0.8811, loss=0.8763]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 239/292 [02:37<00:37,  1.40batch/s, auc=0.8811, loss=0.8763]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 239/292 [02:37<00:37,  1.40batch/s, auc=0.8813, loss=0.5665]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 240/292 [02:37<00:34,  1.49batch/s, auc=0.8813, loss=0.5665]\u001b[A\n",
      "Training Epoch 19/25:  82%|████████▏ | 240/292 [02:38<00:34,  1.49batch/s, auc=0.8813, loss=0.6188]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 241/292 [02:38<00:40,  1.26batch/s, auc=0.8813, loss=0.6188]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 241/292 [02:39<00:40,  1.26batch/s, auc=0.8813, loss=0.7506]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 242/292 [02:39<00:36,  1.38batch/s, auc=0.8813, loss=0.7506]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 242/292 [02:39<00:36,  1.38batch/s, auc=0.8813, loss=0.8029]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 243/292 [02:39<00:33,  1.48batch/s, auc=0.8813, loss=0.8029]\u001b[A\n",
      "Training Epoch 19/25:  83%|████████▎ | 243/292 [02:40<00:33,  1.48batch/s, auc=0.8812, loss=0.5667]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▎ | 244/292 [02:40<00:30,  1.56batch/s, auc=0.8812, loss=0.5667]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▎ | 244/292 [02:41<00:30,  1.56batch/s, auc=0.8808, loss=1.1052]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 245/292 [02:41<00:36,  1.29batch/s, auc=0.8808, loss=1.1052]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 245/292 [02:42<00:36,  1.29batch/s, auc=0.8808, loss=0.5961]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 246/292 [02:42<00:32,  1.41batch/s, auc=0.8808, loss=0.5961]\u001b[A\n",
      "Training Epoch 19/25:  84%|████████▍ | 246/292 [02:42<00:32,  1.41batch/s, auc=0.8806, loss=0.8785]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 247/292 [02:42<00:29,  1.50batch/s, auc=0.8806, loss=0.8785]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 247/292 [02:43<00:29,  1.50batch/s, auc=0.8806, loss=0.5971]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 248/292 [02:43<00:27,  1.58batch/s, auc=0.8806, loss=0.5971]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▍ | 248/292 [02:43<00:27,  1.58batch/s, auc=0.8804, loss=0.9421]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▌ | 249/292 [02:43<00:26,  1.63batch/s, auc=0.8804, loss=0.9421]\u001b[A\n",
      "Training Epoch 19/25:  85%|████████▌ | 249/292 [02:44<00:26,  1.63batch/s, auc=0.8805, loss=0.5149]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 250/292 [02:44<00:25,  1.67batch/s, auc=0.8805, loss=0.5149]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 250/292 [02:45<00:25,  1.67batch/s, auc=0.8808, loss=0.6774]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 251/292 [02:45<00:24,  1.70batch/s, auc=0.8808, loss=0.6774]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▌ | 251/292 [02:45<00:24,  1.70batch/s, auc=0.8809, loss=0.5918]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▋ | 252/292 [02:45<00:23,  1.72batch/s, auc=0.8809, loss=0.5918]\u001b[A\n",
      "Training Epoch 19/25:  86%|████████▋ | 252/292 [02:46<00:23,  1.72batch/s, auc=0.8808, loss=0.7211]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 253/292 [02:46<00:22,  1.74batch/s, auc=0.8808, loss=0.7211]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 253/292 [02:47<00:22,  1.74batch/s, auc=0.8799, loss=1.4297]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 254/292 [02:47<00:27,  1.38batch/s, auc=0.8799, loss=1.4297]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 254/292 [02:47<00:27,  1.38batch/s, auc=0.8801, loss=0.6724]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 255/292 [02:47<00:25,  1.47batch/s, auc=0.8801, loss=0.6724]\u001b[A\n",
      "Training Epoch 19/25:  87%|████████▋ | 255/292 [02:48<00:25,  1.47batch/s, auc=0.8804, loss=0.5732]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 256/292 [02:48<00:23,  1.55batch/s, auc=0.8804, loss=0.5732]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 256/292 [02:48<00:23,  1.55batch/s, auc=0.8807, loss=0.5355]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 257/292 [02:48<00:21,  1.61batch/s, auc=0.8807, loss=0.5355]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 257/292 [02:49<00:21,  1.61batch/s, auc=0.8804, loss=0.8457]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 258/292 [02:49<00:20,  1.66batch/s, auc=0.8804, loss=0.8457]\u001b[A\n",
      "Training Epoch 19/25:  88%|████████▊ | 258/292 [02:50<00:20,  1.66batch/s, auc=0.8804, loss=0.7439]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▊ | 259/292 [02:50<00:24,  1.34batch/s, auc=0.8804, loss=0.7439]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▊ | 259/292 [02:51<00:24,  1.34batch/s, auc=0.8803, loss=0.8161]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 260/292 [02:51<00:22,  1.45batch/s, auc=0.8803, loss=0.8161]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 260/292 [02:51<00:22,  1.45batch/s, auc=0.8806, loss=0.4249]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 261/292 [02:51<00:20,  1.53batch/s, auc=0.8806, loss=0.4249]\u001b[A\n",
      "Training Epoch 19/25:  89%|████████▉ | 261/292 [02:52<00:20,  1.53batch/s, auc=0.8809, loss=0.7508]\u001b[A\n",
      "Training Epoch 19/25:  90%|████████▉ | 262/292 [02:52<00:18,  1.60batch/s, auc=0.8809, loss=0.7508]\u001b[A\n",
      "Training Epoch 19/25:  90%|████████▉ | 262/292 [02:53<00:18,  1.60batch/s, auc=0.8806, loss=0.9068]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 263/292 [02:53<00:22,  1.31batch/s, auc=0.8806, loss=0.9068]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 263/292 [02:53<00:22,  1.31batch/s, auc=0.8806, loss=0.7048]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 264/292 [02:53<00:19,  1.42batch/s, auc=0.8806, loss=0.7048]\u001b[A\n",
      "Training Epoch 19/25:  90%|█████████ | 264/292 [02:54<00:19,  1.42batch/s, auc=0.8806, loss=0.6922]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 265/292 [02:54<00:17,  1.51batch/s, auc=0.8806, loss=0.6922]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 265/292 [02:55<00:17,  1.51batch/s, auc=0.8805, loss=0.7625]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 266/292 [02:55<00:16,  1.58batch/s, auc=0.8805, loss=0.7625]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████ | 266/292 [02:55<00:16,  1.58batch/s, auc=0.8806, loss=0.7283]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████▏| 267/292 [02:55<00:15,  1.63batch/s, auc=0.8806, loss=0.7283]\u001b[A\n",
      "Training Epoch 19/25:  91%|█████████▏| 267/292 [02:56<00:15,  1.63batch/s, auc=0.8806, loss=0.6100]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 268/292 [02:56<00:14,  1.67batch/s, auc=0.8806, loss=0.6100]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 268/292 [02:56<00:14,  1.67batch/s, auc=0.8805, loss=0.8215]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 269/292 [02:56<00:13,  1.70batch/s, auc=0.8805, loss=0.8215]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 269/292 [02:57<00:13,  1.70batch/s, auc=0.8807, loss=0.5275]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 270/292 [02:57<00:12,  1.72batch/s, auc=0.8807, loss=0.5275]\u001b[A\n",
      "Training Epoch 19/25:  92%|█████████▏| 270/292 [02:57<00:12,  1.72batch/s, auc=0.8808, loss=0.5262]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 271/292 [02:57<00:12,  1.74batch/s, auc=0.8808, loss=0.5262]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 271/292 [02:58<00:12,  1.74batch/s, auc=0.8805, loss=0.8437]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 272/292 [02:58<00:14,  1.37batch/s, auc=0.8805, loss=0.8437]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 272/292 [02:59<00:14,  1.37batch/s, auc=0.8808, loss=0.6504]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 273/292 [02:59<00:12,  1.47batch/s, auc=0.8808, loss=0.6504]\u001b[A\n",
      "Training Epoch 19/25:  93%|█████████▎| 273/292 [03:00<00:12,  1.47batch/s, auc=0.8808, loss=0.8056]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 274/292 [03:00<00:11,  1.55batch/s, auc=0.8808, loss=0.8056]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 274/292 [03:00<00:11,  1.55batch/s, auc=0.8808, loss=0.5576]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 275/292 [03:00<00:10,  1.61batch/s, auc=0.8808, loss=0.5576]\u001b[A\n",
      "Training Epoch 19/25:  94%|█████████▍| 275/292 [03:01<00:10,  1.61batch/s, auc=0.8806, loss=0.8848]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 276/292 [03:01<00:12,  1.32batch/s, auc=0.8806, loss=0.8848]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 276/292 [03:02<00:12,  1.32batch/s, auc=0.8803, loss=0.9893]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 277/292 [03:02<00:12,  1.17batch/s, auc=0.8803, loss=0.9893]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▍| 277/292 [03:03<00:12,  1.17batch/s, auc=0.8797, loss=1.2317]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▌| 278/292 [03:03<00:12,  1.08batch/s, auc=0.8797, loss=1.2317]\u001b[A\n",
      "Training Epoch 19/25:  95%|█████████▌| 278/292 [03:04<00:12,  1.08batch/s, auc=0.8801, loss=0.4878]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 279/292 [03:04<00:10,  1.23batch/s, auc=0.8801, loss=0.4878]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 279/292 [03:04<00:10,  1.23batch/s, auc=0.8801, loss=0.5536]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 280/292 [03:04<00:08,  1.35batch/s, auc=0.8801, loss=0.5536]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 280/292 [03:05<00:08,  1.35batch/s, auc=0.8804, loss=0.5160]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 281/292 [03:05<00:07,  1.45batch/s, auc=0.8804, loss=0.5160]\u001b[A\n",
      "Training Epoch 19/25:  96%|█████████▌| 281/292 [03:06<00:07,  1.45batch/s, auc=0.8805, loss=0.7583]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 282/292 [03:06<00:06,  1.54batch/s, auc=0.8805, loss=0.7583]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 282/292 [03:07<00:06,  1.54batch/s, auc=0.8803, loss=0.9660]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 283/292 [03:07<00:07,  1.28batch/s, auc=0.8803, loss=0.9660]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 283/292 [03:07<00:07,  1.28batch/s, auc=0.8806, loss=0.4995]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 284/292 [03:07<00:05,  1.40batch/s, auc=0.8806, loss=0.4995]\u001b[A\n",
      "Training Epoch 19/25:  97%|█████████▋| 284/292 [03:08<00:05,  1.40batch/s, auc=0.8807, loss=0.8295]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.49batch/s, auc=0.8807, loss=0.8295]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 285/292 [03:08<00:04,  1.49batch/s, auc=0.8809, loss=0.8170]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 286/292 [03:08<00:03,  1.57batch/s, auc=0.8809, loss=0.8170]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 286/292 [03:09<00:03,  1.57batch/s, auc=0.8811, loss=0.6380]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 287/292 [03:09<00:03,  1.63batch/s, auc=0.8811, loss=0.6380]\u001b[A\n",
      "Training Epoch 19/25:  98%|█████████▊| 287/292 [03:10<00:03,  1.63batch/s, auc=0.8811, loss=0.6086]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.67batch/s, auc=0.8811, loss=0.6086]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▊| 288/292 [03:10<00:02,  1.67batch/s, auc=0.8813, loss=0.5764]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 289/292 [03:10<00:01,  1.70batch/s, auc=0.8813, loss=0.5764]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 289/292 [03:11<00:01,  1.70batch/s, auc=0.8814, loss=0.7998]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.72batch/s, auc=0.8814, loss=0.7998]\u001b[A\n",
      "Training Epoch 19/25:  99%|█████████▉| 290/292 [03:11<00:01,  1.72batch/s, auc=0.8818, loss=0.4960]\u001b[A\n",
      "Training Epoch 19/25: 100%|█████████▉| 291/292 [03:11<00:00,  1.73batch/s, auc=0.8818, loss=0.4960]\u001b[A\n",
      "Training Epoch 19/25: 100%|█████████▉| 291/292 [03:12<00:00,  1.73batch/s, auc=0.8817, loss=0.8473]\u001b[A\n",
      "Training Epoch 19/25: 100%|██████████| 292/292 [03:12<00:00,  1.52batch/s, auc=0.8817, loss=0.8473]\u001b[A\n",
      "Epochs:  76%|███████▌  | 19/25 [1:07:47<21:32, 215.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25] Train Loss: 0.7218 | Train AUROC: 0.8817 Val Loss: 0.8109 | Val AUROC: 0.8612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 20/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9253, loss=0.5391]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 1/292 [00:01<07:08,  1.47s/batch, auc=0.9253, loss=0.5391]\u001b[A\n",
      "Training Epoch 20/25:   0%|          | 1/292 [00:02<07:08,  1.47s/batch, auc=0.9237, loss=0.5158]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.9237, loss=0.5158]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 2/292 [00:02<04:30,  1.07batch/s, auc=0.9215, loss=0.5300]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 3/292 [00:02<03:39,  1.32batch/s, auc=0.9215, loss=0.5300]\u001b[A\n",
      "Training Epoch 20/25:   1%|          | 3/292 [00:03<03:39,  1.32batch/s, auc=0.9197, loss=0.6840]\u001b[A\n",
      "Training Epoch 20/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.9197, loss=0.6840]\u001b[A\n",
      "Training Epoch 20/25:   1%|▏         | 4/292 [00:03<03:15,  1.47batch/s, auc=0.9036, loss=0.7347]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 5/292 [00:03<03:02,  1.57batch/s, auc=0.9036, loss=0.7347]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 5/292 [00:04<03:02,  1.57batch/s, auc=0.9032, loss=0.6133]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.9032, loss=0.6133]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.9091, loss=0.6424]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 7/292 [00:04<02:48,  1.69batch/s, auc=0.9091, loss=0.6424]\u001b[A\n",
      "Training Epoch 20/25:   2%|▏         | 7/292 [00:05<02:48,  1.69batch/s, auc=0.8941, loss=0.9573]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 8/292 [00:05<03:31,  1.34batch/s, auc=0.8941, loss=0.9573]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 8/292 [00:06<03:31,  1.34batch/s, auc=0.8991, loss=0.5614]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 9/292 [00:06<03:13,  1.46batch/s, auc=0.8991, loss=0.5614]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 9/292 [00:07<03:13,  1.46batch/s, auc=0.8692, loss=1.8472]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 10/292 [00:07<03:46,  1.25batch/s, auc=0.8692, loss=1.8472]\u001b[A\n",
      "Training Epoch 20/25:   3%|▎         | 10/292 [00:08<03:46,  1.25batch/s, auc=0.8598, loss=1.0481]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 11/292 [00:08<04:08,  1.13batch/s, auc=0.8598, loss=1.0481]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 11/292 [00:09<04:08,  1.13batch/s, auc=0.8626, loss=0.6277]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 12/292 [00:09<03:39,  1.27batch/s, auc=0.8626, loss=0.6277]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 12/292 [00:09<03:39,  1.27batch/s, auc=0.8602, loss=0.7200]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 13/292 [00:09<03:19,  1.40batch/s, auc=0.8602, loss=0.7200]\u001b[A\n",
      "Training Epoch 20/25:   4%|▍         | 13/292 [00:10<03:19,  1.40batch/s, auc=0.8661, loss=0.5268]\u001b[A\n",
      "Training Epoch 20/25:   5%|▍         | 14/292 [00:10<03:05,  1.50batch/s, auc=0.8661, loss=0.5268]\u001b[A\n",
      "Training Epoch 20/25:   5%|▍         | 14/292 [00:10<03:05,  1.50batch/s, auc=0.8678, loss=0.6368]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 15/292 [00:10<02:55,  1.58batch/s, auc=0.8678, loss=0.6368]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 15/292 [00:11<02:55,  1.58batch/s, auc=0.8648, loss=0.8534]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 16/292 [00:11<02:48,  1.64batch/s, auc=0.8648, loss=0.8534]\u001b[A\n",
      "Training Epoch 20/25:   5%|▌         | 16/292 [00:11<02:48,  1.64batch/s, auc=0.8682, loss=0.5993]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 17/292 [00:11<02:43,  1.68batch/s, auc=0.8682, loss=0.5993]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 17/292 [00:12<02:43,  1.68batch/s, auc=0.8521, loss=1.7857]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 18/292 [00:12<03:21,  1.36batch/s, auc=0.8521, loss=1.7857]\u001b[A\n",
      "Training Epoch 20/25:   6%|▌         | 18/292 [00:13<03:21,  1.36batch/s, auc=0.8519, loss=0.9034]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 19/292 [00:13<03:06,  1.47batch/s, auc=0.8519, loss=0.9034]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 19/292 [00:14<03:06,  1.47batch/s, auc=0.8540, loss=0.6698]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 20/292 [00:14<02:55,  1.55batch/s, auc=0.8540, loss=0.6698]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 20/292 [00:14<02:55,  1.55batch/s, auc=0.8570, loss=0.5767]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 21/292 [00:14<02:47,  1.62batch/s, auc=0.8570, loss=0.5767]\u001b[A\n",
      "Training Epoch 20/25:   7%|▋         | 21/292 [00:15<02:47,  1.62batch/s, auc=0.8590, loss=0.6193]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 22/292 [00:15<02:41,  1.67batch/s, auc=0.8590, loss=0.6193]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 22/292 [00:16<02:41,  1.67batch/s, auc=0.8559, loss=1.0678]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 23/292 [00:16<03:19,  1.35batch/s, auc=0.8559, loss=1.0678]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 23/292 [00:16<03:19,  1.35batch/s, auc=0.8559, loss=0.8830]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 24/292 [00:16<03:03,  1.46batch/s, auc=0.8559, loss=0.8830]\u001b[A\n",
      "Training Epoch 20/25:   8%|▊         | 24/292 [00:17<03:03,  1.46batch/s, auc=0.8611, loss=0.5053]\u001b[A\n",
      "Training Epoch 20/25:   9%|▊         | 25/292 [00:17<02:52,  1.55batch/s, auc=0.8611, loss=0.5053]\u001b[A\n",
      "Training Epoch 20/25:   9%|▊         | 25/292 [00:18<02:52,  1.55batch/s, auc=0.8603, loss=0.7675]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 26/292 [00:18<03:25,  1.29batch/s, auc=0.8603, loss=0.7675]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 26/292 [00:18<03:25,  1.29batch/s, auc=0.8639, loss=0.5548]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 27/292 [00:18<03:07,  1.41batch/s, auc=0.8639, loss=0.5548]\u001b[A\n",
      "Training Epoch 20/25:   9%|▉         | 27/292 [00:20<03:07,  1.41batch/s, auc=0.8614, loss=0.8931]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 28/292 [00:20<03:35,  1.22batch/s, auc=0.8614, loss=0.8931]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 28/292 [00:21<03:35,  1.22batch/s, auc=0.8580, loss=1.0831]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 29/292 [00:21<03:55,  1.12batch/s, auc=0.8580, loss=1.0831]\u001b[A\n",
      "Training Epoch 20/25:  10%|▉         | 29/292 [00:21<03:55,  1.12batch/s, auc=0.8587, loss=0.6538]\u001b[A\n",
      "Training Epoch 20/25:  10%|█         | 30/292 [00:21<03:27,  1.26batch/s, auc=0.8587, loss=0.6538]\u001b[A\n",
      "Training Epoch 20/25:  10%|█         | 30/292 [00:22<03:27,  1.26batch/s, auc=0.8622, loss=0.4991]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 31/292 [00:22<03:08,  1.39batch/s, auc=0.8622, loss=0.4991]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 31/292 [00:22<03:08,  1.39batch/s, auc=0.8643, loss=0.5505]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 32/292 [00:22<02:54,  1.49batch/s, auc=0.8643, loss=0.5505]\u001b[A\n",
      "Training Epoch 20/25:  11%|█         | 32/292 [00:23<02:54,  1.49batch/s, auc=0.8663, loss=0.6230]\u001b[A\n",
      "Training Epoch 20/25:  11%|█▏        | 33/292 [00:23<02:44,  1.57batch/s, auc=0.8663, loss=0.6230]\u001b[A\n",
      "Training Epoch 20/25:  11%|█▏        | 33/292 [00:23<02:44,  1.57batch/s, auc=0.8662, loss=0.5197]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 34/292 [00:23<02:38,  1.63batch/s, auc=0.8662, loss=0.5197]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 34/292 [00:24<02:38,  1.63batch/s, auc=0.8626, loss=1.1757]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 35/292 [00:24<03:12,  1.33batch/s, auc=0.8626, loss=1.1757]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 35/292 [00:25<03:12,  1.33batch/s, auc=0.8664, loss=0.4596]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 36/292 [00:25<02:57,  1.45batch/s, auc=0.8664, loss=0.4596]\u001b[A\n",
      "Training Epoch 20/25:  12%|█▏        | 36/292 [00:26<02:57,  1.45batch/s, auc=0.8688, loss=0.5440]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 37/292 [00:26<02:45,  1.54batch/s, auc=0.8688, loss=0.5440]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 37/292 [00:26<02:45,  1.54batch/s, auc=0.8706, loss=0.4934]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 38/292 [00:26<02:37,  1.61batch/s, auc=0.8706, loss=0.4934]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 38/292 [00:27<02:37,  1.61batch/s, auc=0.8708, loss=0.7454]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 39/292 [00:27<03:11,  1.32batch/s, auc=0.8708, loss=0.7454]\u001b[A\n",
      "Training Epoch 20/25:  13%|█▎        | 39/292 [00:28<03:11,  1.32batch/s, auc=0.8727, loss=0.4878]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▎        | 40/292 [00:28<02:55,  1.44batch/s, auc=0.8727, loss=0.4878]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▎        | 40/292 [00:28<02:55,  1.44batch/s, auc=0.8747, loss=0.5859]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 41/292 [00:28<02:44,  1.53batch/s, auc=0.8747, loss=0.5859]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 41/292 [00:29<02:44,  1.53batch/s, auc=0.8696, loss=1.3506]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 42/292 [00:29<03:15,  1.28batch/s, auc=0.8696, loss=1.3506]\u001b[A\n",
      "Training Epoch 20/25:  14%|█▍        | 42/292 [00:30<03:15,  1.28batch/s, auc=0.8676, loss=1.0351]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▍        | 43/292 [00:30<03:36,  1.15batch/s, auc=0.8676, loss=1.0351]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▍        | 43/292 [00:32<03:36,  1.15batch/s, auc=0.8678, loss=0.7926]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 44/292 [00:32<03:50,  1.08batch/s, auc=0.8678, loss=0.7926]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 44/292 [00:33<03:50,  1.08batch/s, auc=0.8672, loss=0.7189]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 45/292 [00:33<04:00,  1.03batch/s, auc=0.8672, loss=0.7189]\u001b[A\n",
      "Training Epoch 20/25:  15%|█▌        | 45/292 [00:34<04:00,  1.03batch/s, auc=0.8659, loss=0.9210]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 46/292 [00:34<04:06,  1.00s/batch, auc=0.8659, loss=0.9210]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 46/292 [00:35<04:06,  1.00s/batch, auc=0.8638, loss=0.9857]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 47/292 [00:35<04:10,  1.02s/batch, auc=0.8638, loss=0.9857]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▌        | 47/292 [00:35<04:10,  1.02s/batch, auc=0.8657, loss=0.5551]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▋        | 48/292 [00:35<03:35,  1.13batch/s, auc=0.8657, loss=0.5551]\u001b[A\n",
      "Training Epoch 20/25:  16%|█▋        | 48/292 [00:36<03:35,  1.13batch/s, auc=0.8629, loss=1.1781]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 49/292 [00:36<03:48,  1.06batch/s, auc=0.8629, loss=1.1781]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 49/292 [00:37<03:48,  1.06batch/s, auc=0.8619, loss=0.9329]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 50/292 [00:37<03:19,  1.21batch/s, auc=0.8619, loss=0.9329]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 50/292 [00:38<03:19,  1.21batch/s, auc=0.8651, loss=0.4182]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 51/292 [00:38<02:59,  1.34batch/s, auc=0.8651, loss=0.4182]\u001b[A\n",
      "Training Epoch 20/25:  17%|█▋        | 51/292 [00:38<02:59,  1.34batch/s, auc=0.8654, loss=0.7381]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 52/292 [00:38<02:45,  1.45batch/s, auc=0.8654, loss=0.7381]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 52/292 [00:39<02:45,  1.45batch/s, auc=0.8669, loss=0.4720]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 53/292 [00:39<02:35,  1.54batch/s, auc=0.8669, loss=0.4720]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 53/292 [00:39<02:35,  1.54batch/s, auc=0.8672, loss=0.6648]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 54/292 [00:39<02:27,  1.61batch/s, auc=0.8672, loss=0.6648]\u001b[A\n",
      "Training Epoch 20/25:  18%|█▊        | 54/292 [00:40<02:27,  1.61batch/s, auc=0.8681, loss=0.6065]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 55/292 [00:40<02:22,  1.66batch/s, auc=0.8681, loss=0.6065]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 55/292 [00:40<02:22,  1.66batch/s, auc=0.8689, loss=0.5614]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 56/292 [00:40<02:18,  1.70batch/s, auc=0.8689, loss=0.5614]\u001b[A\n",
      "Training Epoch 20/25:  19%|█▉        | 56/292 [00:41<02:18,  1.70batch/s, auc=0.8696, loss=0.6179]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 57/292 [00:41<02:15,  1.73batch/s, auc=0.8696, loss=0.6179]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 57/292 [00:42<02:15,  1.73batch/s, auc=0.8697, loss=0.8466]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 58/292 [00:42<02:50,  1.37batch/s, auc=0.8697, loss=0.8466]\u001b[A\n",
      "Training Epoch 20/25:  20%|█▉        | 58/292 [00:42<02:50,  1.37batch/s, auc=0.8687, loss=0.8831]\u001b[A\n",
      "Training Epoch 20/25:  20%|██        | 59/292 [00:42<02:37,  1.48batch/s, auc=0.8687, loss=0.8831]\u001b[A\n",
      "Training Epoch 20/25:  20%|██        | 59/292 [00:43<02:37,  1.48batch/s, auc=0.8696, loss=0.6961]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 60/292 [00:43<02:28,  1.56batch/s, auc=0.8696, loss=0.6961]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 60/292 [00:44<02:28,  1.56batch/s, auc=0.8713, loss=0.5158]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 61/292 [00:44<02:22,  1.63batch/s, auc=0.8713, loss=0.5158]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 61/292 [00:44<02:22,  1.63batch/s, auc=0.8720, loss=0.4907]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 62/292 [00:44<02:17,  1.67batch/s, auc=0.8720, loss=0.4907]\u001b[A\n",
      "Training Epoch 20/25:  21%|██        | 62/292 [00:45<02:17,  1.67batch/s, auc=0.8710, loss=0.7635]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 63/292 [00:45<02:49,  1.35batch/s, auc=0.8710, loss=0.7635]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 63/292 [00:46<02:49,  1.35batch/s, auc=0.8689, loss=1.1358]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 64/292 [00:46<03:11,  1.19batch/s, auc=0.8689, loss=1.1358]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 64/292 [00:47<03:11,  1.19batch/s, auc=0.8679, loss=0.9488]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 65/292 [00:47<02:51,  1.32batch/s, auc=0.8679, loss=0.9488]\u001b[A\n",
      "Training Epoch 20/25:  22%|██▏       | 65/292 [00:47<02:51,  1.32batch/s, auc=0.8696, loss=0.4939]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 66/292 [00:47<02:37,  1.44batch/s, auc=0.8696, loss=0.4939]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 66/292 [00:48<02:37,  1.44batch/s, auc=0.8702, loss=0.5222]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 67/292 [00:48<02:27,  1.53batch/s, auc=0.8702, loss=0.5222]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 67/292 [00:49<02:27,  1.53batch/s, auc=0.8721, loss=0.4854]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 68/292 [00:49<02:20,  1.60batch/s, auc=0.8721, loss=0.4854]\u001b[A\n",
      "Training Epoch 20/25:  23%|██▎       | 68/292 [00:49<02:20,  1.60batch/s, auc=0.8713, loss=0.9245]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▎       | 69/292 [00:49<02:15,  1.65batch/s, auc=0.8713, loss=0.9245]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▎       | 69/292 [00:50<02:15,  1.65batch/s, auc=0.8715, loss=0.7216]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 70/292 [00:50<02:11,  1.69batch/s, auc=0.8715, loss=0.7216]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 70/292 [00:50<02:11,  1.69batch/s, auc=0.8723, loss=0.6252]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 71/292 [00:50<02:08,  1.72batch/s, auc=0.8723, loss=0.6252]\u001b[A\n",
      "Training Epoch 20/25:  24%|██▍       | 71/292 [00:51<02:08,  1.72batch/s, auc=0.8728, loss=0.5869]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▍       | 72/292 [00:51<02:06,  1.74batch/s, auc=0.8728, loss=0.5869]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▍       | 72/292 [00:51<02:06,  1.74batch/s, auc=0.8722, loss=0.6871]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 73/292 [00:51<02:04,  1.76batch/s, auc=0.8722, loss=0.6871]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 73/292 [00:52<02:04,  1.76batch/s, auc=0.8712, loss=1.0144]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 74/292 [00:52<02:36,  1.39batch/s, auc=0.8712, loss=1.0144]\u001b[A\n",
      "Training Epoch 20/25:  25%|██▌       | 74/292 [00:53<02:36,  1.39batch/s, auc=0.8693, loss=1.2885]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 75/292 [00:53<02:59,  1.21batch/s, auc=0.8693, loss=1.2885]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 75/292 [00:54<02:59,  1.21batch/s, auc=0.8710, loss=0.4798]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 76/292 [00:54<02:40,  1.34batch/s, auc=0.8710, loss=0.4798]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▌       | 76/292 [00:55<02:40,  1.34batch/s, auc=0.8716, loss=0.8195]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▋       | 77/292 [00:55<02:28,  1.45batch/s, auc=0.8716, loss=0.8195]\u001b[A\n",
      "Training Epoch 20/25:  26%|██▋       | 77/292 [00:56<02:28,  1.45batch/s, auc=0.8685, loss=1.6396]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 78/292 [00:56<02:52,  1.24batch/s, auc=0.8685, loss=1.6396]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 78/292 [00:56<02:52,  1.24batch/s, auc=0.8686, loss=0.5231]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 79/292 [00:56<02:35,  1.37batch/s, auc=0.8686, loss=0.5231]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 79/292 [00:57<02:35,  1.37batch/s, auc=0.8673, loss=1.0411]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 80/292 [00:57<02:56,  1.20batch/s, auc=0.8673, loss=1.0411]\u001b[A\n",
      "Training Epoch 20/25:  27%|██▋       | 80/292 [00:58<02:56,  1.20batch/s, auc=0.8692, loss=0.3982]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 81/292 [00:58<02:38,  1.33batch/s, auc=0.8692, loss=0.3982]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 81/292 [00:58<02:38,  1.33batch/s, auc=0.8696, loss=0.8363]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 82/292 [00:58<02:25,  1.45batch/s, auc=0.8696, loss=0.8363]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 82/292 [00:59<02:25,  1.45batch/s, auc=0.8698, loss=0.7164]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 83/292 [00:59<02:16,  1.53batch/s, auc=0.8698, loss=0.7164]\u001b[A\n",
      "Training Epoch 20/25:  28%|██▊       | 83/292 [01:00<02:16,  1.53batch/s, auc=0.8703, loss=0.5959]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 84/292 [01:00<02:09,  1.60batch/s, auc=0.8703, loss=0.5959]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 84/292 [01:01<02:09,  1.60batch/s, auc=0.8690, loss=1.1033]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 85/292 [01:01<02:37,  1.32batch/s, auc=0.8690, loss=1.1033]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 85/292 [01:01<02:37,  1.32batch/s, auc=0.8693, loss=0.7524]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 86/292 [01:01<02:23,  1.43batch/s, auc=0.8693, loss=0.7524]\u001b[A\n",
      "Training Epoch 20/25:  29%|██▉       | 86/292 [01:02<02:23,  1.43batch/s, auc=0.8694, loss=0.7030]\u001b[A\n",
      "Training Epoch 20/25:  30%|██▉       | 87/292 [01:02<02:14,  1.52batch/s, auc=0.8694, loss=0.7030]\u001b[A\n",
      "Training Epoch 20/25:  30%|██▉       | 87/292 [01:02<02:14,  1.52batch/s, auc=0.8691, loss=0.8091]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 88/292 [01:02<02:07,  1.60batch/s, auc=0.8691, loss=0.8091]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 88/292 [01:03<02:07,  1.60batch/s, auc=0.8691, loss=0.7235]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 89/292 [01:03<02:02,  1.65batch/s, auc=0.8691, loss=0.7235]\u001b[A\n",
      "Training Epoch 20/25:  30%|███       | 89/292 [01:03<02:02,  1.65batch/s, auc=0.8697, loss=0.5641]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 90/292 [01:03<01:59,  1.69batch/s, auc=0.8697, loss=0.5641]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 90/292 [01:04<01:59,  1.69batch/s, auc=0.8699, loss=0.5819]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 91/292 [01:04<01:56,  1.72batch/s, auc=0.8699, loss=0.5819]\u001b[A\n",
      "Training Epoch 20/25:  31%|███       | 91/292 [01:04<01:56,  1.72batch/s, auc=0.8702, loss=0.6439]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 92/292 [01:04<01:54,  1.74batch/s, auc=0.8702, loss=0.6439]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 92/292 [01:05<01:54,  1.74batch/s, auc=0.8704, loss=0.5927]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 93/292 [01:05<01:53,  1.76batch/s, auc=0.8704, loss=0.5927]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 93/292 [01:06<01:53,  1.76batch/s, auc=0.8697, loss=1.0027]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 94/292 [01:06<01:52,  1.77batch/s, auc=0.8697, loss=1.0027]\u001b[A\n",
      "Training Epoch 20/25:  32%|███▏      | 94/292 [01:06<01:52,  1.77batch/s, auc=0.8700, loss=0.6618]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 95/292 [01:06<01:51,  1.77batch/s, auc=0.8700, loss=0.6618]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 95/292 [01:07<01:51,  1.77batch/s, auc=0.8699, loss=0.7698]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 96/292 [01:07<01:50,  1.78batch/s, auc=0.8699, loss=0.7698]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 96/292 [01:08<01:50,  1.78batch/s, auc=0.8690, loss=1.2113]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 97/292 [01:08<02:19,  1.40batch/s, auc=0.8690, loss=1.2113]\u001b[A\n",
      "Training Epoch 20/25:  33%|███▎      | 97/292 [01:08<02:19,  1.40batch/s, auc=0.8696, loss=0.5070]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▎      | 98/292 [01:08<02:09,  1.50batch/s, auc=0.8696, loss=0.5070]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▎      | 98/292 [01:09<02:09,  1.50batch/s, auc=0.8696, loss=0.9074]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 99/292 [01:09<02:02,  1.57batch/s, auc=0.8696, loss=0.9074]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 99/292 [01:09<02:02,  1.57batch/s, auc=0.8702, loss=0.6656]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 100/292 [01:09<01:57,  1.63batch/s, auc=0.8702, loss=0.6656]\u001b[A\n",
      "Training Epoch 20/25:  34%|███▍      | 100/292 [01:10<01:57,  1.63batch/s, auc=0.8707, loss=0.5940]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 101/292 [01:10<01:53,  1.68batch/s, auc=0.8707, loss=0.5940]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 101/292 [01:11<01:53,  1.68batch/s, auc=0.8711, loss=0.5809]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 102/292 [01:11<01:51,  1.71batch/s, auc=0.8711, loss=0.5809]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▍      | 102/292 [01:11<01:51,  1.71batch/s, auc=0.8712, loss=0.6540]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▌      | 103/292 [01:11<01:49,  1.73batch/s, auc=0.8712, loss=0.6540]\u001b[A\n",
      "Training Epoch 20/25:  35%|███▌      | 103/292 [01:12<01:49,  1.73batch/s, auc=0.8721, loss=0.5746]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 104/292 [01:12<01:47,  1.75batch/s, auc=0.8721, loss=0.5746]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 104/292 [01:12<01:47,  1.75batch/s, auc=0.8721, loss=0.7228]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 105/292 [01:12<01:46,  1.76batch/s, auc=0.8721, loss=0.7228]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▌      | 105/292 [01:13<01:46,  1.76batch/s, auc=0.8711, loss=0.9799]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▋      | 106/292 [01:13<02:13,  1.39batch/s, auc=0.8711, loss=0.9799]\u001b[A\n",
      "Training Epoch 20/25:  36%|███▋      | 106/292 [01:14<02:13,  1.39batch/s, auc=0.8711, loss=0.8363]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 107/292 [01:14<02:04,  1.49batch/s, auc=0.8711, loss=0.8363]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 107/292 [01:14<02:04,  1.49batch/s, auc=0.8722, loss=0.5197]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 108/292 [01:14<01:57,  1.57batch/s, auc=0.8722, loss=0.5197]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 108/292 [01:15<01:57,  1.57batch/s, auc=0.8726, loss=0.6825]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 109/292 [01:15<01:52,  1.63batch/s, auc=0.8726, loss=0.6825]\u001b[A\n",
      "Training Epoch 20/25:  37%|███▋      | 109/292 [01:16<01:52,  1.63batch/s, auc=0.8729, loss=0.5773]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 110/292 [01:16<01:48,  1.68batch/s, auc=0.8729, loss=0.5773]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 110/292 [01:16<01:48,  1.68batch/s, auc=0.8735, loss=0.4942]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 111/292 [01:16<01:45,  1.71batch/s, auc=0.8735, loss=0.4942]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 111/292 [01:17<01:45,  1.71batch/s, auc=0.8729, loss=0.8950]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 112/292 [01:17<02:11,  1.37batch/s, auc=0.8729, loss=0.8950]\u001b[A\n",
      "Training Epoch 20/25:  38%|███▊      | 112/292 [01:18<02:11,  1.37batch/s, auc=0.8740, loss=0.3974]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▊      | 113/292 [01:18<02:01,  1.47batch/s, auc=0.8740, loss=0.3974]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▊      | 113/292 [01:18<02:01,  1.47batch/s, auc=0.8747, loss=0.6937]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 114/292 [01:18<01:54,  1.55batch/s, auc=0.8747, loss=0.6937]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 114/292 [01:19<01:54,  1.55batch/s, auc=0.8752, loss=0.6324]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 115/292 [01:19<01:49,  1.62batch/s, auc=0.8752, loss=0.6324]\u001b[A\n",
      "Training Epoch 20/25:  39%|███▉      | 115/292 [01:19<01:49,  1.62batch/s, auc=0.8752, loss=0.6836]\u001b[A\n",
      "Training Epoch 20/25:  40%|███▉      | 116/292 [01:19<01:45,  1.67batch/s, auc=0.8752, loss=0.6836]\u001b[A\n",
      "Training Epoch 20/25:  40%|███▉      | 116/292 [01:20<01:45,  1.67batch/s, auc=0.8759, loss=0.7964]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 117/292 [01:20<01:42,  1.70batch/s, auc=0.8759, loss=0.7964]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 117/292 [01:21<01:42,  1.70batch/s, auc=0.8764, loss=0.5880]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 118/292 [01:21<01:40,  1.73batch/s, auc=0.8764, loss=0.5880]\u001b[A\n",
      "Training Epoch 20/25:  40%|████      | 118/292 [01:22<01:40,  1.73batch/s, auc=0.8760, loss=0.8950]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 119/292 [01:22<02:06,  1.37batch/s, auc=0.8760, loss=0.8950]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 119/292 [01:23<02:06,  1.37batch/s, auc=0.8758, loss=0.8226]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 120/292 [01:23<02:23,  1.20batch/s, auc=0.8758, loss=0.8226]\u001b[A\n",
      "Training Epoch 20/25:  41%|████      | 120/292 [01:23<02:23,  1.20batch/s, auc=0.8763, loss=0.5728]\u001b[A\n",
      "Training Epoch 20/25:  41%|████▏     | 121/292 [01:23<02:08,  1.33batch/s, auc=0.8763, loss=0.5728]\u001b[A\n",
      "Training Epoch 20/25:  41%|████▏     | 121/292 [01:24<02:08,  1.33batch/s, auc=0.8763, loss=0.7769]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 122/292 [01:24<01:57,  1.44batch/s, auc=0.8763, loss=0.7769]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 122/292 [01:24<01:57,  1.44batch/s, auc=0.8759, loss=0.8439]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 123/292 [01:24<01:50,  1.53batch/s, auc=0.8759, loss=0.8439]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 123/292 [01:25<01:50,  1.53batch/s, auc=0.8763, loss=0.5548]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 124/292 [01:25<01:44,  1.60batch/s, auc=0.8763, loss=0.5548]\u001b[A\n",
      "Training Epoch 20/25:  42%|████▏     | 124/292 [01:26<01:44,  1.60batch/s, auc=0.8770, loss=0.6138]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 125/292 [01:26<01:40,  1.66batch/s, auc=0.8770, loss=0.6138]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 125/292 [01:26<01:40,  1.66batch/s, auc=0.8764, loss=0.9412]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 126/292 [01:26<01:38,  1.69batch/s, auc=0.8764, loss=0.9412]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 126/292 [01:27<01:38,  1.69batch/s, auc=0.8770, loss=0.7777]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 127/292 [01:27<01:35,  1.72batch/s, auc=0.8770, loss=0.7777]\u001b[A\n",
      "Training Epoch 20/25:  43%|████▎     | 127/292 [01:27<01:35,  1.72batch/s, auc=0.8774, loss=0.5996]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 128/292 [01:27<01:34,  1.74batch/s, auc=0.8774, loss=0.5996]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 128/292 [01:28<01:34,  1.74batch/s, auc=0.8770, loss=0.7230]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 129/292 [01:28<01:32,  1.75batch/s, auc=0.8770, loss=0.7230]\u001b[A\n",
      "Training Epoch 20/25:  44%|████▍     | 129/292 [01:28<01:32,  1.75batch/s, auc=0.8773, loss=0.5410]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 130/292 [01:28<01:31,  1.76batch/s, auc=0.8773, loss=0.5410]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 130/292 [01:29<01:31,  1.76batch/s, auc=0.8775, loss=0.5117]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 131/292 [01:29<01:30,  1.77batch/s, auc=0.8775, loss=0.5117]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▍     | 131/292 [01:29<01:30,  1.77batch/s, auc=0.8775, loss=0.6408]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▌     | 132/292 [01:29<01:30,  1.78batch/s, auc=0.8775, loss=0.6408]\u001b[A\n",
      "Training Epoch 20/25:  45%|████▌     | 132/292 [01:30<01:30,  1.78batch/s, auc=0.8780, loss=0.6086]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 133/292 [01:30<01:29,  1.78batch/s, auc=0.8780, loss=0.6086]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 133/292 [01:31<01:29,  1.78batch/s, auc=0.8776, loss=1.0247]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 134/292 [01:31<01:53,  1.40batch/s, auc=0.8776, loss=1.0247]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 134/292 [01:32<01:53,  1.40batch/s, auc=0.8779, loss=0.5141]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 135/292 [01:32<01:45,  1.49batch/s, auc=0.8779, loss=0.5141]\u001b[A\n",
      "Training Epoch 20/25:  46%|████▌     | 135/292 [01:33<01:45,  1.49batch/s, auc=0.8772, loss=1.1041]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 136/292 [01:33<02:03,  1.26batch/s, auc=0.8772, loss=1.1041]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 136/292 [01:33<02:03,  1.26batch/s, auc=0.8776, loss=0.6225]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 137/292 [01:33<01:51,  1.39batch/s, auc=0.8776, loss=0.6225]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 137/292 [01:34<01:51,  1.39batch/s, auc=0.8779, loss=0.6134]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 138/292 [01:34<01:43,  1.49batch/s, auc=0.8779, loss=0.6134]\u001b[A\n",
      "Training Epoch 20/25:  47%|████▋     | 138/292 [01:34<01:43,  1.49batch/s, auc=0.8778, loss=0.5919]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 139/292 [01:34<01:37,  1.57batch/s, auc=0.8778, loss=0.5919]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 139/292 [01:35<01:37,  1.57batch/s, auc=0.8782, loss=0.6451]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 140/292 [01:35<01:33,  1.63batch/s, auc=0.8782, loss=0.6451]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 140/292 [01:35<01:33,  1.63batch/s, auc=0.8783, loss=0.8099]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 141/292 [01:35<01:30,  1.67batch/s, auc=0.8783, loss=0.8099]\u001b[A\n",
      "Training Epoch 20/25:  48%|████▊     | 141/292 [01:36<01:30,  1.67batch/s, auc=0.8786, loss=0.5850]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▊     | 142/292 [01:36<01:27,  1.71batch/s, auc=0.8786, loss=0.5850]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▊     | 142/292 [01:37<01:27,  1.71batch/s, auc=0.8784, loss=0.8120]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 143/292 [01:37<01:26,  1.73batch/s, auc=0.8784, loss=0.8120]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 143/292 [01:38<01:26,  1.73batch/s, auc=0.8775, loss=1.1897]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 144/292 [01:38<01:47,  1.37batch/s, auc=0.8775, loss=1.1897]\u001b[A\n",
      "Training Epoch 20/25:  49%|████▉     | 144/292 [01:38<01:47,  1.37batch/s, auc=0.8770, loss=0.7597]\u001b[A\n",
      "Training Epoch 20/25:  50%|████▉     | 145/292 [01:38<01:39,  1.48batch/s, auc=0.8770, loss=0.7597]\u001b[A\n",
      "Training Epoch 20/25:  50%|████▉     | 145/292 [01:39<01:39,  1.48batch/s, auc=0.8775, loss=0.5709]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 146/292 [01:39<01:33,  1.56batch/s, auc=0.8775, loss=0.5709]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 146/292 [01:40<01:33,  1.56batch/s, auc=0.8770, loss=0.9916]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 147/292 [01:40<01:51,  1.30batch/s, auc=0.8770, loss=0.9916]\u001b[A\n",
      "Training Epoch 20/25:  50%|█████     | 147/292 [01:40<01:51,  1.30batch/s, auc=0.8773, loss=0.5329]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 148/292 [01:40<01:41,  1.41batch/s, auc=0.8773, loss=0.5329]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 148/292 [01:41<01:41,  1.41batch/s, auc=0.8777, loss=0.6614]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 149/292 [01:41<01:34,  1.51batch/s, auc=0.8777, loss=0.6614]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████     | 149/292 [01:42<01:34,  1.51batch/s, auc=0.8766, loss=1.1551]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████▏    | 150/292 [01:42<01:51,  1.27batch/s, auc=0.8766, loss=1.1551]\u001b[A\n",
      "Training Epoch 20/25:  51%|█████▏    | 150/292 [01:43<01:51,  1.27batch/s, auc=0.8765, loss=0.6073]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 151/292 [01:43<01:41,  1.39batch/s, auc=0.8765, loss=0.6073]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 151/292 [01:43<01:41,  1.39batch/s, auc=0.8769, loss=0.4399]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 152/292 [01:43<01:33,  1.49batch/s, auc=0.8769, loss=0.4399]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 152/292 [01:44<01:33,  1.49batch/s, auc=0.8767, loss=0.9308]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 153/292 [01:44<01:50,  1.26batch/s, auc=0.8767, loss=0.9308]\u001b[A\n",
      "Training Epoch 20/25:  52%|█████▏    | 153/292 [01:45<01:50,  1.26batch/s, auc=0.8769, loss=0.5305]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 154/292 [01:45<01:39,  1.38batch/s, auc=0.8769, loss=0.5305]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 154/292 [01:46<01:39,  1.38batch/s, auc=0.8759, loss=1.2457]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 155/292 [01:46<01:53,  1.21batch/s, auc=0.8759, loss=1.2457]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 155/292 [01:46<01:53,  1.21batch/s, auc=0.8763, loss=0.5128]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 156/292 [01:46<01:41,  1.34batch/s, auc=0.8763, loss=0.5128]\u001b[A\n",
      "Training Epoch 20/25:  53%|█████▎    | 156/292 [01:47<01:41,  1.34batch/s, auc=0.8759, loss=0.7321]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 157/292 [01:47<01:33,  1.45batch/s, auc=0.8759, loss=0.7321]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 157/292 [01:48<01:33,  1.45batch/s, auc=0.8765, loss=0.5418]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 158/292 [01:48<01:27,  1.53batch/s, auc=0.8765, loss=0.5418]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 158/292 [01:49<01:27,  1.53batch/s, auc=0.8764, loss=0.6780]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 159/292 [01:49<01:43,  1.28batch/s, auc=0.8764, loss=0.6780]\u001b[A\n",
      "Training Epoch 20/25:  54%|█████▍    | 159/292 [01:49<01:43,  1.28batch/s, auc=0.8764, loss=0.7051]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▍    | 160/292 [01:49<01:34,  1.40batch/s, auc=0.8764, loss=0.7051]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▍    | 160/292 [01:50<01:34,  1.40batch/s, auc=0.8752, loss=1.2278]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 161/292 [01:50<01:47,  1.22batch/s, auc=0.8752, loss=1.2278]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 161/292 [01:51<01:47,  1.22batch/s, auc=0.8751, loss=0.7756]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 162/292 [01:51<01:36,  1.34batch/s, auc=0.8751, loss=0.7756]\u001b[A\n",
      "Training Epoch 20/25:  55%|█████▌    | 162/292 [01:51<01:36,  1.34batch/s, auc=0.8753, loss=0.5738]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 163/292 [01:51<01:28,  1.45batch/s, auc=0.8753, loss=0.5738]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 163/292 [01:52<01:28,  1.45batch/s, auc=0.8757, loss=0.5891]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 164/292 [01:52<01:23,  1.54batch/s, auc=0.8757, loss=0.5891]\u001b[A\n",
      "Training Epoch 20/25:  56%|█████▌    | 164/292 [01:53<01:23,  1.54batch/s, auc=0.8755, loss=0.8676]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 165/292 [01:53<01:19,  1.60batch/s, auc=0.8755, loss=0.8676]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 165/292 [01:53<01:19,  1.60batch/s, auc=0.8756, loss=0.6861]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 166/292 [01:53<01:16,  1.66batch/s, auc=0.8756, loss=0.6861]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 166/292 [01:54<01:16,  1.66batch/s, auc=0.8754, loss=0.9498]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 167/292 [01:54<01:33,  1.34batch/s, auc=0.8754, loss=0.9498]\u001b[A\n",
      "Training Epoch 20/25:  57%|█████▋    | 167/292 [01:55<01:33,  1.34batch/s, auc=0.8757, loss=0.5601]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 168/292 [01:55<01:25,  1.45batch/s, auc=0.8757, loss=0.5601]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 168/292 [01:56<01:25,  1.45batch/s, auc=0.8747, loss=1.2616]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 169/292 [01:56<01:39,  1.24batch/s, auc=0.8747, loss=1.2616]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 169/292 [01:56<01:39,  1.24batch/s, auc=0.8750, loss=0.6969]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 170/292 [01:56<01:29,  1.36batch/s, auc=0.8750, loss=0.6969]\u001b[A\n",
      "Training Epoch 20/25:  58%|█████▊    | 170/292 [01:57<01:29,  1.36batch/s, auc=0.8742, loss=1.0680]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▊    | 171/292 [01:57<01:41,  1.20batch/s, auc=0.8742, loss=1.0680]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▊    | 171/292 [01:58<01:41,  1.20batch/s, auc=0.8744, loss=0.6999]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 172/292 [01:58<01:30,  1.33batch/s, auc=0.8744, loss=0.6999]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 172/292 [01:59<01:30,  1.33batch/s, auc=0.8742, loss=0.8214]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 173/292 [01:59<01:22,  1.44batch/s, auc=0.8742, loss=0.8214]\u001b[A\n",
      "Training Epoch 20/25:  59%|█████▉    | 173/292 [02:00<01:22,  1.44batch/s, auc=0.8739, loss=0.7849]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 174/292 [02:00<01:35,  1.23batch/s, auc=0.8739, loss=0.7849]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 174/292 [02:00<01:35,  1.23batch/s, auc=0.8746, loss=0.5875]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 175/292 [02:00<01:26,  1.36batch/s, auc=0.8746, loss=0.5875]\u001b[A\n",
      "Training Epoch 20/25:  60%|█████▉    | 175/292 [02:01<01:26,  1.36batch/s, auc=0.8744, loss=0.8082]\u001b[A\n",
      "Training Epoch 20/25:  60%|██████    | 176/292 [02:01<01:19,  1.46batch/s, auc=0.8744, loss=0.8082]\u001b[A\n",
      "Training Epoch 20/25:  60%|██████    | 176/292 [02:01<01:19,  1.46batch/s, auc=0.8745, loss=0.5330]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 177/292 [02:01<01:14,  1.55batch/s, auc=0.8745, loss=0.5330]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 177/292 [02:02<01:14,  1.55batch/s, auc=0.8751, loss=0.5016]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 178/292 [02:02<01:10,  1.61batch/s, auc=0.8751, loss=0.5016]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████    | 178/292 [02:02<01:10,  1.61batch/s, auc=0.8752, loss=0.7522]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████▏   | 179/292 [02:02<01:08,  1.66batch/s, auc=0.8752, loss=0.7522]\u001b[A\n",
      "Training Epoch 20/25:  61%|██████▏   | 179/292 [02:03<01:08,  1.66batch/s, auc=0.8750, loss=0.7888]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 180/292 [02:03<01:06,  1.70batch/s, auc=0.8750, loss=0.7888]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 180/292 [02:04<01:06,  1.70batch/s, auc=0.8754, loss=0.4599]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 181/292 [02:04<01:04,  1.72batch/s, auc=0.8754, loss=0.4599]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 181/292 [02:04<01:04,  1.72batch/s, auc=0.8754, loss=0.7327]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 182/292 [02:04<01:03,  1.74batch/s, auc=0.8754, loss=0.7327]\u001b[A\n",
      "Training Epoch 20/25:  62%|██████▏   | 182/292 [02:05<01:03,  1.74batch/s, auc=0.8749, loss=1.0063]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 183/292 [02:05<01:19,  1.38batch/s, auc=0.8749, loss=1.0063]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 183/292 [02:06<01:19,  1.38batch/s, auc=0.8748, loss=0.7582]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 184/292 [02:06<01:13,  1.48batch/s, auc=0.8748, loss=0.7582]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 184/292 [02:06<01:13,  1.48batch/s, auc=0.8751, loss=0.5558]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 185/292 [02:06<01:08,  1.56batch/s, auc=0.8751, loss=0.5558]\u001b[A\n",
      "Training Epoch 20/25:  63%|██████▎   | 185/292 [02:07<01:08,  1.56batch/s, auc=0.8754, loss=0.4608]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▎   | 186/292 [02:07<01:05,  1.62batch/s, auc=0.8754, loss=0.4608]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▎   | 186/292 [02:08<01:05,  1.62batch/s, auc=0.8744, loss=1.2078]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 187/292 [02:08<01:19,  1.32batch/s, auc=0.8744, loss=1.2078]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 187/292 [02:09<01:19,  1.32batch/s, auc=0.8738, loss=1.0631]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 188/292 [02:09<01:28,  1.17batch/s, auc=0.8738, loss=1.0631]\u001b[A\n",
      "Training Epoch 20/25:  64%|██████▍   | 188/292 [02:10<01:28,  1.17batch/s, auc=0.8735, loss=0.8418]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▍   | 189/292 [02:10<01:18,  1.31batch/s, auc=0.8735, loss=0.8418]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▍   | 189/292 [02:10<01:18,  1.31batch/s, auc=0.8736, loss=0.6209]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 190/292 [02:10<01:11,  1.42batch/s, auc=0.8736, loss=0.6209]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 190/292 [02:11<01:11,  1.42batch/s, auc=0.8744, loss=0.5634]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 191/292 [02:11<01:06,  1.51batch/s, auc=0.8744, loss=0.5634]\u001b[A\n",
      "Training Epoch 20/25:  65%|██████▌   | 191/292 [02:11<01:06,  1.51batch/s, auc=0.8742, loss=0.9542]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 192/292 [02:11<01:03,  1.59batch/s, auc=0.8742, loss=0.9542]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 192/292 [02:12<01:03,  1.59batch/s, auc=0.8734, loss=1.1812]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 193/292 [02:12<01:15,  1.31batch/s, auc=0.8734, loss=1.1812]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▌   | 193/292 [02:13<01:15,  1.31batch/s, auc=0.8727, loss=1.1239]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▋   | 194/292 [02:13<01:24,  1.16batch/s, auc=0.8727, loss=1.1239]\u001b[A\n",
      "Training Epoch 20/25:  66%|██████▋   | 194/292 [02:14<01:24,  1.16batch/s, auc=0.8728, loss=0.6971]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 195/292 [02:14<01:14,  1.30batch/s, auc=0.8728, loss=0.6971]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 195/292 [02:15<01:14,  1.30batch/s, auc=0.8729, loss=0.7286]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 196/292 [02:15<01:07,  1.42batch/s, auc=0.8729, loss=0.7286]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 196/292 [02:15<01:07,  1.42batch/s, auc=0.8731, loss=0.6021]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 197/292 [02:15<01:02,  1.51batch/s, auc=0.8731, loss=0.6021]\u001b[A\n",
      "Training Epoch 20/25:  67%|██████▋   | 197/292 [02:16<01:02,  1.51batch/s, auc=0.8733, loss=0.5203]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 198/292 [02:16<00:59,  1.58batch/s, auc=0.8733, loss=0.5203]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 198/292 [02:17<00:59,  1.58batch/s, auc=0.8729, loss=0.9516]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 199/292 [02:17<01:11,  1.31batch/s, auc=0.8729, loss=0.9516]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 199/292 [02:17<01:11,  1.31batch/s, auc=0.8733, loss=0.5550]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 200/292 [02:17<01:04,  1.42batch/s, auc=0.8733, loss=0.5550]\u001b[A\n",
      "Training Epoch 20/25:  68%|██████▊   | 200/292 [02:18<01:04,  1.42batch/s, auc=0.8734, loss=0.7285]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 201/292 [02:18<01:00,  1.51batch/s, auc=0.8734, loss=0.7285]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 201/292 [02:18<01:00,  1.51batch/s, auc=0.8733, loss=0.6356]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 202/292 [02:18<00:56,  1.58batch/s, auc=0.8733, loss=0.6356]\u001b[A\n",
      "Training Epoch 20/25:  69%|██████▉   | 202/292 [02:19<00:56,  1.58batch/s, auc=0.8735, loss=0.6700]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 203/292 [02:19<00:54,  1.64batch/s, auc=0.8735, loss=0.6700]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 203/292 [02:20<00:54,  1.64batch/s, auc=0.8734, loss=0.7872]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 204/292 [02:20<00:52,  1.68batch/s, auc=0.8734, loss=0.7872]\u001b[A\n",
      "Training Epoch 20/25:  70%|██████▉   | 204/292 [02:20<00:52,  1.68batch/s, auc=0.8735, loss=0.5318]\u001b[A\n",
      "Training Epoch 20/25:  70%|███████   | 205/292 [02:20<00:51,  1.71batch/s, auc=0.8735, loss=0.5318]\u001b[A\n",
      "Training Epoch 20/25:  70%|███████   | 205/292 [02:21<00:51,  1.71batch/s, auc=0.8737, loss=0.6309]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 206/292 [02:21<00:49,  1.73batch/s, auc=0.8737, loss=0.6309]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 206/292 [02:21<00:49,  1.73batch/s, auc=0.8740, loss=0.4544]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 207/292 [02:21<00:48,  1.74batch/s, auc=0.8740, loss=0.4544]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 207/292 [02:22<00:48,  1.74batch/s, auc=0.8741, loss=0.6915]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 208/292 [02:22<00:47,  1.75batch/s, auc=0.8741, loss=0.6915]\u001b[A\n",
      "Training Epoch 20/25:  71%|███████   | 208/292 [02:22<00:47,  1.75batch/s, auc=0.8736, loss=1.0016]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 209/292 [02:22<00:47,  1.76batch/s, auc=0.8736, loss=1.0016]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 209/292 [02:23<00:47,  1.76batch/s, auc=0.8734, loss=0.8807]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 210/292 [02:23<00:46,  1.76batch/s, auc=0.8734, loss=0.8807]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 210/292 [02:24<00:46,  1.76batch/s, auc=0.8737, loss=0.6041]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 211/292 [02:24<00:45,  1.77batch/s, auc=0.8737, loss=0.6041]\u001b[A\n",
      "Training Epoch 20/25:  72%|███████▏  | 211/292 [02:25<00:45,  1.77batch/s, auc=0.8736, loss=0.7991]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 212/292 [02:25<00:57,  1.39batch/s, auc=0.8736, loss=0.7991]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 212/292 [02:25<00:57,  1.39batch/s, auc=0.8739, loss=0.6495]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 213/292 [02:25<00:53,  1.49batch/s, auc=0.8739, loss=0.6495]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 213/292 [02:26<00:53,  1.49batch/s, auc=0.8739, loss=0.6625]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 214/292 [02:26<00:49,  1.57batch/s, auc=0.8739, loss=0.6625]\u001b[A\n",
      "Training Epoch 20/25:  73%|███████▎  | 214/292 [02:26<00:49,  1.57batch/s, auc=0.8742, loss=0.5218]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▎  | 215/292 [02:26<00:47,  1.62batch/s, auc=0.8742, loss=0.5218]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▎  | 215/292 [02:27<00:47,  1.62batch/s, auc=0.8735, loss=1.1903]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 216/292 [02:27<00:57,  1.33batch/s, auc=0.8735, loss=1.1903]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 216/292 [02:28<00:57,  1.33batch/s, auc=0.8739, loss=0.4592]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 217/292 [02:28<00:52,  1.44batch/s, auc=0.8739, loss=0.4592]\u001b[A\n",
      "Training Epoch 20/25:  74%|███████▍  | 217/292 [02:28<00:52,  1.44batch/s, auc=0.8743, loss=0.4526]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▍  | 218/292 [02:28<00:48,  1.52batch/s, auc=0.8743, loss=0.4526]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▍  | 218/292 [02:30<00:48,  1.52batch/s, auc=0.8741, loss=0.9176]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 219/292 [02:30<00:57,  1.28batch/s, auc=0.8741, loss=0.9176]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 219/292 [02:30<00:57,  1.28batch/s, auc=0.8742, loss=0.5499]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 220/292 [02:30<00:51,  1.40batch/s, auc=0.8742, loss=0.5499]\u001b[A\n",
      "Training Epoch 20/25:  75%|███████▌  | 220/292 [02:31<00:51,  1.40batch/s, auc=0.8745, loss=0.6323]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 221/292 [02:31<00:47,  1.49batch/s, auc=0.8745, loss=0.6323]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 221/292 [02:31<00:47,  1.49batch/s, auc=0.8746, loss=0.6556]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 222/292 [02:31<00:44,  1.57batch/s, auc=0.8746, loss=0.6556]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▌  | 222/292 [02:32<00:44,  1.57batch/s, auc=0.8747, loss=0.6155]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▋  | 223/292 [02:32<00:42,  1.63batch/s, auc=0.8747, loss=0.6155]\u001b[A\n",
      "Training Epoch 20/25:  76%|███████▋  | 223/292 [02:32<00:42,  1.63batch/s, auc=0.8747, loss=0.5583]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 224/292 [02:32<00:40,  1.67batch/s, auc=0.8747, loss=0.5583]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 224/292 [02:33<00:40,  1.67batch/s, auc=0.8751, loss=0.5155]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 225/292 [02:33<00:39,  1.70batch/s, auc=0.8751, loss=0.5155]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 225/292 [02:33<00:39,  1.70batch/s, auc=0.8753, loss=0.5974]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 226/292 [02:33<00:38,  1.72batch/s, auc=0.8753, loss=0.5974]\u001b[A\n",
      "Training Epoch 20/25:  77%|███████▋  | 226/292 [02:34<00:38,  1.72batch/s, auc=0.8757, loss=0.5225]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 227/292 [02:34<00:37,  1.74batch/s, auc=0.8757, loss=0.5225]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 227/292 [02:35<00:37,  1.74batch/s, auc=0.8764, loss=0.3847]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 228/292 [02:35<00:36,  1.75batch/s, auc=0.8764, loss=0.3847]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 228/292 [02:35<00:36,  1.75batch/s, auc=0.8764, loss=0.7190]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 229/292 [02:35<00:35,  1.76batch/s, auc=0.8764, loss=0.7190]\u001b[A\n",
      "Training Epoch 20/25:  78%|███████▊  | 229/292 [02:36<00:35,  1.76batch/s, auc=0.8766, loss=0.5717]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 230/292 [02:36<00:35,  1.76batch/s, auc=0.8766, loss=0.5717]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 230/292 [02:37<00:35,  1.76batch/s, auc=0.8761, loss=1.0479]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 231/292 [02:37<00:43,  1.39batch/s, auc=0.8761, loss=1.0479]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 231/292 [02:37<00:43,  1.39batch/s, auc=0.8761, loss=0.6646]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 232/292 [02:37<00:40,  1.49batch/s, auc=0.8761, loss=0.6646]\u001b[A\n",
      "Training Epoch 20/25:  79%|███████▉  | 232/292 [02:38<00:40,  1.49batch/s, auc=0.8761, loss=0.6532]\u001b[A\n",
      "Training Epoch 20/25:  80%|███████▉  | 233/292 [02:38<00:37,  1.56batch/s, auc=0.8761, loss=0.6532]\u001b[A\n",
      "Training Epoch 20/25:  80%|███████▉  | 233/292 [02:39<00:37,  1.56batch/s, auc=0.8760, loss=0.9512]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 234/292 [02:39<00:44,  1.29batch/s, auc=0.8760, loss=0.9512]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 234/292 [02:40<00:44,  1.29batch/s, auc=0.8760, loss=0.8729]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 235/292 [02:40<00:49,  1.16batch/s, auc=0.8760, loss=0.8729]\u001b[A\n",
      "Training Epoch 20/25:  80%|████████  | 235/292 [02:41<00:49,  1.16batch/s, auc=0.8760, loss=0.6147]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 236/292 [02:41<00:43,  1.29batch/s, auc=0.8760, loss=0.6147]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 236/292 [02:41<00:43,  1.29batch/s, auc=0.8762, loss=0.8090]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 237/292 [02:41<00:39,  1.41batch/s, auc=0.8762, loss=0.8090]\u001b[A\n",
      "Training Epoch 20/25:  81%|████████  | 237/292 [02:42<00:39,  1.41batch/s, auc=0.8764, loss=0.6424]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 238/292 [02:42<00:36,  1.50batch/s, auc=0.8764, loss=0.6424]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 238/292 [02:42<00:36,  1.50batch/s, auc=0.8767, loss=0.6073]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 239/292 [02:42<00:33,  1.57batch/s, auc=0.8767, loss=0.6073]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 239/292 [02:43<00:33,  1.57batch/s, auc=0.8768, loss=0.5452]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 240/292 [02:43<00:31,  1.63batch/s, auc=0.8768, loss=0.5452]\u001b[A\n",
      "Training Epoch 20/25:  82%|████████▏ | 240/292 [02:43<00:31,  1.63batch/s, auc=0.8769, loss=0.6745]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 241/292 [02:43<00:30,  1.67batch/s, auc=0.8769, loss=0.6745]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 241/292 [02:44<00:30,  1.67batch/s, auc=0.8769, loss=0.7344]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 242/292 [02:44<00:29,  1.70batch/s, auc=0.8769, loss=0.7344]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 242/292 [02:45<00:29,  1.70batch/s, auc=0.8768, loss=0.7190]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 243/292 [02:45<00:28,  1.72batch/s, auc=0.8768, loss=0.7190]\u001b[A\n",
      "Training Epoch 20/25:  83%|████████▎ | 243/292 [02:45<00:28,  1.72batch/s, auc=0.8769, loss=0.6707]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▎ | 244/292 [02:45<00:27,  1.74batch/s, auc=0.8769, loss=0.6707]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▎ | 244/292 [02:46<00:27,  1.74batch/s, auc=0.8770, loss=0.6103]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 245/292 [02:46<00:26,  1.75batch/s, auc=0.8770, loss=0.6103]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 245/292 [02:46<00:26,  1.75batch/s, auc=0.8775, loss=0.4929]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 246/292 [02:46<00:26,  1.76batch/s, auc=0.8775, loss=0.4929]\u001b[A\n",
      "Training Epoch 20/25:  84%|████████▍ | 246/292 [02:47<00:26,  1.76batch/s, auc=0.8776, loss=0.5763]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 247/292 [02:47<00:25,  1.76batch/s, auc=0.8776, loss=0.5763]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 247/292 [02:47<00:25,  1.76batch/s, auc=0.8778, loss=0.6503]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 248/292 [02:47<00:24,  1.76batch/s, auc=0.8778, loss=0.6503]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▍ | 248/292 [02:48<00:24,  1.76batch/s, auc=0.8781, loss=0.4786]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▌ | 249/292 [02:48<00:24,  1.77batch/s, auc=0.8781, loss=0.4786]\u001b[A\n",
      "Training Epoch 20/25:  85%|████████▌ | 249/292 [02:49<00:24,  1.77batch/s, auc=0.8784, loss=0.5443]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 250/292 [02:49<00:23,  1.77batch/s, auc=0.8784, loss=0.5443]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 250/292 [02:49<00:23,  1.77batch/s, auc=0.8786, loss=0.6306]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 251/292 [02:49<00:23,  1.77batch/s, auc=0.8786, loss=0.6306]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▌ | 251/292 [02:50<00:23,  1.77batch/s, auc=0.8784, loss=0.6527]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▋ | 252/292 [02:50<00:22,  1.77batch/s, auc=0.8784, loss=0.6527]\u001b[A\n",
      "Training Epoch 20/25:  86%|████████▋ | 252/292 [02:50<00:22,  1.77batch/s, auc=0.8787, loss=0.5345]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 253/292 [02:50<00:22,  1.77batch/s, auc=0.8787, loss=0.5345]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 253/292 [02:51<00:22,  1.77batch/s, auc=0.8788, loss=0.6512]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 254/292 [02:51<00:21,  1.77batch/s, auc=0.8788, loss=0.6512]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 254/292 [02:51<00:21,  1.77batch/s, auc=0.8789, loss=0.6140]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 255/292 [02:51<00:20,  1.77batch/s, auc=0.8789, loss=0.6140]\u001b[A\n",
      "Training Epoch 20/25:  87%|████████▋ | 255/292 [02:52<00:20,  1.77batch/s, auc=0.8785, loss=1.4112]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 256/292 [02:52<00:25,  1.39batch/s, auc=0.8785, loss=1.4112]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 256/292 [02:54<00:25,  1.39batch/s, auc=0.8777, loss=1.4497]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 257/292 [02:54<00:28,  1.21batch/s, auc=0.8777, loss=1.4497]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 257/292 [02:55<00:28,  1.21batch/s, auc=0.8771, loss=1.2910]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 258/292 [02:55<00:30,  1.11batch/s, auc=0.8771, loss=1.2910]\u001b[A\n",
      "Training Epoch 20/25:  88%|████████▊ | 258/292 [02:55<00:30,  1.11batch/s, auc=0.8771, loss=0.8803]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▊ | 259/292 [02:55<00:26,  1.25batch/s, auc=0.8771, loss=0.8803]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▊ | 259/292 [02:56<00:26,  1.25batch/s, auc=0.8769, loss=0.9374]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 260/292 [02:56<00:23,  1.37batch/s, auc=0.8769, loss=0.9374]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 260/292 [02:56<00:23,  1.37batch/s, auc=0.8771, loss=0.5346]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 261/292 [02:56<00:21,  1.47batch/s, auc=0.8771, loss=0.5346]\u001b[A\n",
      "Training Epoch 20/25:  89%|████████▉ | 261/292 [02:57<00:21,  1.47batch/s, auc=0.8771, loss=0.7979]\u001b[A\n",
      "Training Epoch 20/25:  90%|████████▉ | 262/292 [02:57<00:19,  1.55batch/s, auc=0.8771, loss=0.7979]\u001b[A\n",
      "Training Epoch 20/25:  90%|████████▉ | 262/292 [02:57<00:19,  1.55batch/s, auc=0.8770, loss=0.8193]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 263/292 [02:57<00:18,  1.61batch/s, auc=0.8770, loss=0.8193]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 263/292 [02:59<00:18,  1.61batch/s, auc=0.8768, loss=0.8758]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 264/292 [02:59<00:21,  1.32batch/s, auc=0.8768, loss=0.8758]\u001b[A\n",
      "Training Epoch 20/25:  90%|█████████ | 264/292 [03:00<00:21,  1.32batch/s, auc=0.8770, loss=0.7568]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 265/292 [03:00<00:23,  1.17batch/s, auc=0.8770, loss=0.7568]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 265/292 [03:00<00:23,  1.17batch/s, auc=0.8769, loss=0.6783]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 266/292 [03:00<00:19,  1.30batch/s, auc=0.8769, loss=0.6783]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████ | 266/292 [03:01<00:19,  1.30batch/s, auc=0.8769, loss=0.6733]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████▏| 267/292 [03:01<00:17,  1.41batch/s, auc=0.8769, loss=0.6733]\u001b[A\n",
      "Training Epoch 20/25:  91%|█████████▏| 267/292 [03:01<00:17,  1.41batch/s, auc=0.8772, loss=0.5043]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 268/292 [03:01<00:15,  1.50batch/s, auc=0.8772, loss=0.5043]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 268/292 [03:02<00:15,  1.50batch/s, auc=0.8774, loss=0.5314]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 269/292 [03:02<00:14,  1.57batch/s, auc=0.8774, loss=0.5314]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 269/292 [03:02<00:14,  1.57batch/s, auc=0.8775, loss=0.6609]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 270/292 [03:02<00:13,  1.63batch/s, auc=0.8775, loss=0.6609]\u001b[A\n",
      "Training Epoch 20/25:  92%|█████████▏| 270/292 [03:03<00:13,  1.63batch/s, auc=0.8773, loss=0.9734]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 271/292 [03:03<00:12,  1.67batch/s, auc=0.8773, loss=0.9734]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 271/292 [03:04<00:12,  1.67batch/s, auc=0.8776, loss=0.6400]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 272/292 [03:04<00:11,  1.70batch/s, auc=0.8776, loss=0.6400]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 272/292 [03:04<00:11,  1.70batch/s, auc=0.8777, loss=0.6636]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 273/292 [03:04<00:11,  1.72batch/s, auc=0.8777, loss=0.6636]\u001b[A\n",
      "Training Epoch 20/25:  93%|█████████▎| 273/292 [03:05<00:11,  1.72batch/s, auc=0.8777, loss=0.5856]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 274/292 [03:05<00:10,  1.73batch/s, auc=0.8777, loss=0.5856]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 274/292 [03:06<00:10,  1.73batch/s, auc=0.8773, loss=0.9908]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 275/292 [03:06<00:12,  1.37batch/s, auc=0.8773, loss=0.9908]\u001b[A\n",
      "Training Epoch 20/25:  94%|█████████▍| 275/292 [03:06<00:12,  1.37batch/s, auc=0.8773, loss=0.8490]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 276/292 [03:06<00:10,  1.47batch/s, auc=0.8773, loss=0.8490]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 276/292 [03:07<00:10,  1.47batch/s, auc=0.8768, loss=1.0202]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 277/292 [03:07<00:12,  1.25batch/s, auc=0.8768, loss=1.0202]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▍| 277/292 [03:08<00:12,  1.25batch/s, auc=0.8768, loss=0.5957]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▌| 278/292 [03:08<00:10,  1.37batch/s, auc=0.8768, loss=0.5957]\u001b[A\n",
      "Training Epoch 20/25:  95%|█████████▌| 278/292 [03:09<00:10,  1.37batch/s, auc=0.8769, loss=0.6556]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 279/292 [03:09<00:08,  1.47batch/s, auc=0.8769, loss=0.6556]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 279/292 [03:09<00:08,  1.47batch/s, auc=0.8770, loss=0.6496]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 280/292 [03:09<00:07,  1.55batch/s, auc=0.8770, loss=0.6496]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 280/292 [03:10<00:07,  1.55batch/s, auc=0.8773, loss=0.5394]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 281/292 [03:10<00:06,  1.61batch/s, auc=0.8773, loss=0.5394]\u001b[A\n",
      "Training Epoch 20/25:  96%|█████████▌| 281/292 [03:10<00:06,  1.61batch/s, auc=0.8773, loss=0.8932]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 282/292 [03:10<00:06,  1.65batch/s, auc=0.8773, loss=0.8932]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 282/292 [03:11<00:06,  1.65batch/s, auc=0.8774, loss=0.6545]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 283/292 [03:11<00:05,  1.68batch/s, auc=0.8774, loss=0.6545]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 283/292 [03:12<00:05,  1.68batch/s, auc=0.8772, loss=0.9075]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.35batch/s, auc=0.8772, loss=0.9075]\u001b[A\n",
      "Training Epoch 20/25:  97%|█████████▋| 284/292 [03:12<00:05,  1.35batch/s, auc=0.8773, loss=0.8131]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 285/292 [03:12<00:04,  1.45batch/s, auc=0.8773, loss=0.8131]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 285/292 [03:13<00:04,  1.45batch/s, auc=0.8775, loss=0.5753]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 286/292 [03:13<00:03,  1.54batch/s, auc=0.8775, loss=0.5753]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 286/292 [03:14<00:03,  1.54batch/s, auc=0.8775, loss=0.8231]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.60batch/s, auc=0.8775, loss=0.8231]\u001b[A\n",
      "Training Epoch 20/25:  98%|█████████▊| 287/292 [03:14<00:03,  1.60batch/s, auc=0.8775, loss=0.7192]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▊| 288/292 [03:14<00:02,  1.65batch/s, auc=0.8775, loss=0.7192]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▊| 288/292 [03:15<00:02,  1.65batch/s, auc=0.8774, loss=0.7397]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 289/292 [03:15<00:01,  1.68batch/s, auc=0.8774, loss=0.7397]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 289/292 [03:15<00:01,  1.68batch/s, auc=0.8776, loss=0.4737]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 290/292 [03:15<00:01,  1.71batch/s, auc=0.8776, loss=0.4737]\u001b[A\n",
      "Training Epoch 20/25:  99%|█████████▉| 290/292 [03:16<00:01,  1.71batch/s, auc=0.8778, loss=0.6774]\u001b[A\n",
      "Training Epoch 20/25: 100%|█████████▉| 291/292 [03:16<00:00,  1.73batch/s, auc=0.8778, loss=0.6774]\u001b[A\n",
      "Training Epoch 20/25: 100%|█████████▉| 291/292 [03:16<00:00,  1.73batch/s, auc=0.8778, loss=0.6776]\u001b[A\n",
      "Training Epoch 20/25: 100%|██████████| 292/292 [03:16<00:00,  1.48batch/s, auc=0.8778, loss=0.6776]\u001b[A\n",
      "Epochs:  80%|████████  | 20/25 [1:11:23<17:57, 215.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25] Train Loss: 0.7343 | Train AUROC: 0.8778 Val Loss: 0.9163 | Val AUROC: 0.8346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 21/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8630, loss=0.8470]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 1/292 [00:01<07:30,  1.55s/batch, auc=0.8630, loss=0.8470]\u001b[A\n",
      "Training Epoch 21/25:   0%|          | 1/292 [00:02<07:30,  1.55s/batch, auc=0.8814, loss=0.5694]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 2/292 [00:02<04:39,  1.04batch/s, auc=0.8814, loss=0.5694]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 2/292 [00:02<04:39,  1.04batch/s, auc=0.9071, loss=0.5107]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 3/292 [00:02<03:44,  1.29batch/s, auc=0.9071, loss=0.5107]\u001b[A\n",
      "Training Epoch 21/25:   1%|          | 3/292 [00:03<03:44,  1.29batch/s, auc=0.9067, loss=0.6118]\u001b[A\n",
      "Training Epoch 21/25:   1%|▏         | 4/292 [00:03<03:18,  1.45batch/s, auc=0.9067, loss=0.6118]\u001b[A\n",
      "Training Epoch 21/25:   1%|▏         | 4/292 [00:03<03:18,  1.45batch/s, auc=0.9210, loss=0.4516]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 5/292 [00:03<03:03,  1.56batch/s, auc=0.9210, loss=0.4516]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 5/292 [00:04<03:03,  1.56batch/s, auc=0.9268, loss=0.4929]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.9268, loss=0.4929]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 6/292 [00:05<02:54,  1.64batch/s, auc=0.9133, loss=0.9770]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 7/292 [00:05<03:36,  1.31batch/s, auc=0.9133, loss=0.9770]\u001b[A\n",
      "Training Epoch 21/25:   2%|▏         | 7/292 [00:05<03:36,  1.31batch/s, auc=0.9224, loss=0.4629]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 8/292 [00:05<03:17,  1.44batch/s, auc=0.9224, loss=0.4629]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 8/292 [00:06<03:17,  1.44batch/s, auc=0.9254, loss=0.5191]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 9/292 [00:06<03:04,  1.54batch/s, auc=0.9254, loss=0.5191]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 9/292 [00:07<03:04,  1.54batch/s, auc=0.9224, loss=0.6733]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 10/292 [00:07<02:55,  1.61batch/s, auc=0.9224, loss=0.6733]\u001b[A\n",
      "Training Epoch 21/25:   3%|▎         | 10/292 [00:07<02:55,  1.61batch/s, auc=0.9297, loss=0.3564]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 11/292 [00:07<02:48,  1.66batch/s, auc=0.9297, loss=0.3564]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 11/292 [00:08<02:48,  1.66batch/s, auc=0.9196, loss=0.9397]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 12/292 [00:08<03:28,  1.34batch/s, auc=0.9196, loss=0.9397]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 12/292 [00:09<03:28,  1.34batch/s, auc=0.9154, loss=0.7899]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 13/292 [00:09<03:11,  1.46batch/s, auc=0.9154, loss=0.7899]\u001b[A\n",
      "Training Epoch 21/25:   4%|▍         | 13/292 [00:09<03:11,  1.46batch/s, auc=0.9124, loss=0.6723]\u001b[A\n",
      "Training Epoch 21/25:   5%|▍         | 14/292 [00:09<02:59,  1.55batch/s, auc=0.9124, loss=0.6723]\u001b[A\n",
      "Training Epoch 21/25:   5%|▍         | 14/292 [00:10<02:59,  1.55batch/s, auc=0.9109, loss=0.6509]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 15/292 [00:10<02:51,  1.62batch/s, auc=0.9109, loss=0.6509]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 15/292 [00:10<02:51,  1.62batch/s, auc=0.9049, loss=0.9709]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 16/292 [00:10<02:45,  1.67batch/s, auc=0.9049, loss=0.9709]\u001b[A\n",
      "Training Epoch 21/25:   5%|▌         | 16/292 [00:11<02:45,  1.67batch/s, auc=0.9040, loss=0.5555]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 17/292 [00:11<02:41,  1.71batch/s, auc=0.9040, loss=0.5555]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 17/292 [00:12<02:41,  1.71batch/s, auc=0.9066, loss=0.5294]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 18/292 [00:12<02:37,  1.74batch/s, auc=0.9066, loss=0.5294]\u001b[A\n",
      "Training Epoch 21/25:   6%|▌         | 18/292 [00:12<02:37,  1.74batch/s, auc=0.9077, loss=0.4878]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 19/292 [00:12<02:35,  1.76batch/s, auc=0.9077, loss=0.4878]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 19/292 [00:13<02:35,  1.76batch/s, auc=0.9069, loss=0.6493]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 20/292 [00:13<02:33,  1.77batch/s, auc=0.9069, loss=0.6493]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 20/292 [00:13<02:33,  1.77batch/s, auc=0.9083, loss=0.5582]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 21/292 [00:13<02:32,  1.78batch/s, auc=0.9083, loss=0.5582]\u001b[A\n",
      "Training Epoch 21/25:   7%|▋         | 21/292 [00:14<02:32,  1.78batch/s, auc=0.9071, loss=0.6861]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 22/292 [00:14<02:31,  1.79batch/s, auc=0.9071, loss=0.6861]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 22/292 [00:14<02:31,  1.79batch/s, auc=0.9068, loss=0.7809]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 23/292 [00:14<02:30,  1.79batch/s, auc=0.9068, loss=0.7809]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 23/292 [00:15<02:30,  1.79batch/s, auc=0.9082, loss=0.5486]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 24/292 [00:15<02:29,  1.79batch/s, auc=0.9082, loss=0.5486]\u001b[A\n",
      "Training Epoch 21/25:   8%|▊         | 24/292 [00:16<02:29,  1.79batch/s, auc=0.9038, loss=1.0104]\u001b[A\n",
      "Training Epoch 21/25:   9%|▊         | 25/292 [00:16<03:10,  1.40batch/s, auc=0.9038, loss=1.0104]\u001b[A\n",
      "Training Epoch 21/25:   9%|▊         | 25/292 [00:17<03:10,  1.40batch/s, auc=0.9036, loss=0.6767]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 26/292 [00:17<03:37,  1.22batch/s, auc=0.9036, loss=0.6767]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 26/292 [00:18<03:37,  1.22batch/s, auc=0.9012, loss=0.7705]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 27/292 [00:18<03:16,  1.35batch/s, auc=0.9012, loss=0.7705]\u001b[A\n",
      "Training Epoch 21/25:   9%|▉         | 27/292 [00:18<03:16,  1.35batch/s, auc=0.9014, loss=0.6197]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 28/292 [00:18<03:00,  1.46batch/s, auc=0.9014, loss=0.6197]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 28/292 [00:19<03:00,  1.46batch/s, auc=0.8989, loss=0.8879]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 29/292 [00:19<03:30,  1.25batch/s, auc=0.8989, loss=0.8879]\u001b[A\n",
      "Training Epoch 21/25:  10%|▉         | 29/292 [00:20<03:30,  1.25batch/s, auc=0.8935, loss=1.2702]\u001b[A\n",
      "Training Epoch 21/25:  10%|█         | 30/292 [00:20<03:50,  1.13batch/s, auc=0.8935, loss=1.2702]\u001b[A\n",
      "Training Epoch 21/25:  10%|█         | 30/292 [00:21<03:50,  1.13batch/s, auc=0.8928, loss=0.8269]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 31/292 [00:21<04:05,  1.07batch/s, auc=0.8928, loss=0.8269]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 31/292 [00:22<04:05,  1.07batch/s, auc=0.8929, loss=0.6417]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 32/292 [00:22<03:34,  1.21batch/s, auc=0.8929, loss=0.6417]\u001b[A\n",
      "Training Epoch 21/25:  11%|█         | 32/292 [00:22<03:34,  1.21batch/s, auc=0.8941, loss=0.5558]\u001b[A\n",
      "Training Epoch 21/25:  11%|█▏        | 33/292 [00:22<03:12,  1.35batch/s, auc=0.8941, loss=0.5558]\u001b[A\n",
      "Training Epoch 21/25:  11%|█▏        | 33/292 [00:23<03:12,  1.35batch/s, auc=0.8962, loss=0.5903]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 34/292 [00:23<02:57,  1.46batch/s, auc=0.8962, loss=0.5903]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 34/292 [00:24<02:57,  1.46batch/s, auc=0.8937, loss=0.9118]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 35/292 [00:24<03:26,  1.25batch/s, auc=0.8937, loss=0.9118]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 35/292 [00:25<03:26,  1.25batch/s, auc=0.8908, loss=0.9792]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 36/292 [00:25<03:46,  1.13batch/s, auc=0.8908, loss=0.9792]\u001b[A\n",
      "Training Epoch 21/25:  12%|█▏        | 36/292 [00:26<03:46,  1.13batch/s, auc=0.8924, loss=0.4127]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 37/292 [00:26<03:20,  1.27batch/s, auc=0.8924, loss=0.4127]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 37/292 [00:26<03:20,  1.27batch/s, auc=0.8921, loss=0.6138]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 38/292 [00:26<03:01,  1.40batch/s, auc=0.8921, loss=0.6138]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 38/292 [00:27<03:01,  1.40batch/s, auc=0.8928, loss=0.6631]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 39/292 [00:27<02:49,  1.50batch/s, auc=0.8928, loss=0.6631]\u001b[A\n",
      "Training Epoch 21/25:  13%|█▎        | 39/292 [00:27<02:49,  1.50batch/s, auc=0.8941, loss=0.5588]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▎        | 40/292 [00:27<02:40,  1.57batch/s, auc=0.8941, loss=0.5588]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▎        | 40/292 [00:28<02:40,  1.57batch/s, auc=0.8948, loss=0.6206]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 41/292 [00:28<02:33,  1.64batch/s, auc=0.8948, loss=0.6206]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 41/292 [00:28<02:33,  1.64batch/s, auc=0.8954, loss=0.5455]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 42/292 [00:28<02:28,  1.68batch/s, auc=0.8954, loss=0.5455]\u001b[A\n",
      "Training Epoch 21/25:  14%|█▍        | 42/292 [00:29<02:28,  1.68batch/s, auc=0.8956, loss=0.6071]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▍        | 43/292 [00:29<02:25,  1.72batch/s, auc=0.8956, loss=0.6071]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▍        | 43/292 [00:30<02:25,  1.72batch/s, auc=0.8958, loss=0.5925]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 44/292 [00:30<02:22,  1.74batch/s, auc=0.8958, loss=0.5925]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 44/292 [00:30<02:22,  1.74batch/s, auc=0.8949, loss=0.8597]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 45/292 [00:30<02:20,  1.76batch/s, auc=0.8949, loss=0.8597]\u001b[A\n",
      "Training Epoch 21/25:  15%|█▌        | 45/292 [00:31<02:20,  1.76batch/s, auc=0.8959, loss=0.5337]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 46/292 [00:31<02:19,  1.77batch/s, auc=0.8959, loss=0.5337]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 46/292 [00:31<02:19,  1.77batch/s, auc=0.8967, loss=0.7953]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 47/292 [00:31<02:17,  1.78batch/s, auc=0.8967, loss=0.7953]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▌        | 47/292 [00:32<02:17,  1.78batch/s, auc=0.8979, loss=0.4220]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▋        | 48/292 [00:32<02:16,  1.78batch/s, auc=0.8979, loss=0.4220]\u001b[A\n",
      "Training Epoch 21/25:  16%|█▋        | 48/292 [00:33<02:16,  1.78batch/s, auc=0.8953, loss=1.1672]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 49/292 [00:33<02:53,  1.40batch/s, auc=0.8953, loss=1.1672]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 49/292 [00:33<02:53,  1.40batch/s, auc=0.8941, loss=0.8196]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 50/292 [00:33<02:41,  1.50batch/s, auc=0.8941, loss=0.8196]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 50/292 [00:34<02:41,  1.50batch/s, auc=0.8933, loss=0.7058]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 51/292 [00:34<02:32,  1.58batch/s, auc=0.8933, loss=0.7058]\u001b[A\n",
      "Training Epoch 21/25:  17%|█▋        | 51/292 [00:35<02:32,  1.58batch/s, auc=0.8948, loss=0.4430]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 52/292 [00:35<02:26,  1.64batch/s, auc=0.8948, loss=0.4430]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 52/292 [00:35<02:26,  1.64batch/s, auc=0.8946, loss=0.6446]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 53/292 [00:35<02:22,  1.68batch/s, auc=0.8946, loss=0.6446]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 53/292 [00:36<02:22,  1.68batch/s, auc=0.8940, loss=0.9037]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 54/292 [00:36<02:18,  1.71batch/s, auc=0.8940, loss=0.9037]\u001b[A\n",
      "Training Epoch 21/25:  18%|█▊        | 54/292 [00:36<02:18,  1.71batch/s, auc=0.8930, loss=0.7616]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 55/292 [00:36<02:16,  1.74batch/s, auc=0.8930, loss=0.7616]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 55/292 [00:37<02:16,  1.74batch/s, auc=0.8907, loss=1.2474]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 56/292 [00:37<02:14,  1.76batch/s, auc=0.8907, loss=1.2474]\u001b[A\n",
      "Training Epoch 21/25:  19%|█▉        | 56/292 [00:37<02:14,  1.76batch/s, auc=0.8920, loss=0.4172]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 57/292 [00:37<02:12,  1.77batch/s, auc=0.8920, loss=0.4172]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 57/292 [00:38<02:12,  1.77batch/s, auc=0.8898, loss=1.1563]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 58/292 [00:38<02:47,  1.39batch/s, auc=0.8898, loss=1.1563]\u001b[A\n",
      "Training Epoch 21/25:  20%|█▉        | 58/292 [00:39<02:47,  1.39batch/s, auc=0.8897, loss=0.5053]\u001b[A\n",
      "Training Epoch 21/25:  20%|██        | 59/292 [00:39<02:35,  1.49batch/s, auc=0.8897, loss=0.5053]\u001b[A\n",
      "Training Epoch 21/25:  20%|██        | 59/292 [00:39<02:35,  1.49batch/s, auc=0.8898, loss=0.6639]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 60/292 [00:39<02:27,  1.57batch/s, auc=0.8898, loss=0.6639]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 60/292 [00:40<02:27,  1.57batch/s, auc=0.8900, loss=0.6157]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 61/292 [00:40<02:21,  1.63batch/s, auc=0.8900, loss=0.6157]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 61/292 [00:41<02:21,  1.63batch/s, auc=0.8912, loss=0.5373]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 62/292 [00:41<02:16,  1.68batch/s, auc=0.8912, loss=0.5373]\u001b[A\n",
      "Training Epoch 21/25:  21%|██        | 62/292 [00:41<02:16,  1.68batch/s, auc=0.8907, loss=0.8871]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 63/292 [00:41<02:13,  1.71batch/s, auc=0.8907, loss=0.8871]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 63/292 [00:42<02:13,  1.71batch/s, auc=0.8908, loss=0.5954]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 64/292 [00:42<02:11,  1.74batch/s, auc=0.8908, loss=0.5954]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 64/292 [00:42<02:11,  1.74batch/s, auc=0.8897, loss=1.0034]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 65/292 [00:42<02:09,  1.75batch/s, auc=0.8897, loss=1.0034]\u001b[A\n",
      "Training Epoch 21/25:  22%|██▏       | 65/292 [00:43<02:09,  1.75batch/s, auc=0.8900, loss=0.5770]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 66/292 [00:43<02:07,  1.77batch/s, auc=0.8900, loss=0.5770]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 66/292 [00:43<02:07,  1.77batch/s, auc=0.8888, loss=0.8617]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 67/292 [00:43<02:06,  1.78batch/s, auc=0.8888, loss=0.8617]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 67/292 [00:44<02:06,  1.78batch/s, auc=0.8886, loss=0.7973]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 68/292 [00:44<02:05,  1.78batch/s, auc=0.8886, loss=0.7973]\u001b[A\n",
      "Training Epoch 21/25:  23%|██▎       | 68/292 [00:45<02:05,  1.78batch/s, auc=0.8887, loss=0.7234]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▎       | 69/292 [00:45<02:04,  1.78batch/s, auc=0.8887, loss=0.7234]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▎       | 69/292 [00:45<02:04,  1.78batch/s, auc=0.8892, loss=0.5342]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 70/292 [00:45<02:04,  1.79batch/s, auc=0.8892, loss=0.5342]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 70/292 [00:46<02:04,  1.79batch/s, auc=0.8870, loss=1.0744]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 71/292 [00:46<02:37,  1.40batch/s, auc=0.8870, loss=1.0744]\u001b[A\n",
      "Training Epoch 21/25:  24%|██▍       | 71/292 [00:47<02:37,  1.40batch/s, auc=0.8872, loss=0.7098]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▍       | 72/292 [00:47<02:26,  1.50batch/s, auc=0.8872, loss=0.7098]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▍       | 72/292 [00:47<02:26,  1.50batch/s, auc=0.8869, loss=0.7471]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 73/292 [00:47<02:18,  1.58batch/s, auc=0.8869, loss=0.7471]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 73/292 [00:48<02:18,  1.58batch/s, auc=0.8846, loss=1.3462]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 74/292 [00:48<02:46,  1.31batch/s, auc=0.8846, loss=1.3462]\u001b[A\n",
      "Training Epoch 21/25:  25%|██▌       | 74/292 [00:49<02:46,  1.31batch/s, auc=0.8834, loss=0.9531]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 75/292 [00:49<03:06,  1.16batch/s, auc=0.8834, loss=0.9531]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 75/292 [00:50<03:06,  1.16batch/s, auc=0.8843, loss=0.4053]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 76/292 [00:50<02:45,  1.30batch/s, auc=0.8843, loss=0.4053]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▌       | 76/292 [00:51<02:45,  1.30batch/s, auc=0.8831, loss=0.8992]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▋       | 77/292 [00:51<03:04,  1.16batch/s, auc=0.8831, loss=0.8992]\u001b[A\n",
      "Training Epoch 21/25:  26%|██▋       | 77/292 [00:52<03:04,  1.16batch/s, auc=0.8822, loss=0.9036]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 78/292 [00:52<03:17,  1.08batch/s, auc=0.8822, loss=0.9036]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 78/292 [00:53<03:17,  1.08batch/s, auc=0.8830, loss=0.5794]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 79/292 [00:53<02:53,  1.23batch/s, auc=0.8830, loss=0.5794]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 79/292 [00:53<02:53,  1.23batch/s, auc=0.8834, loss=0.7485]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 80/292 [00:53<02:36,  1.36batch/s, auc=0.8834, loss=0.7485]\u001b[A\n",
      "Training Epoch 21/25:  27%|██▋       | 80/292 [00:54<02:36,  1.36batch/s, auc=0.8841, loss=0.6509]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 81/292 [00:54<02:24,  1.46batch/s, auc=0.8841, loss=0.6509]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 81/292 [00:55<02:24,  1.46batch/s, auc=0.8833, loss=0.9458]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 82/292 [00:55<02:48,  1.25batch/s, auc=0.8833, loss=0.9458]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 82/292 [00:55<02:48,  1.25batch/s, auc=0.8830, loss=0.6935]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 83/292 [00:55<02:32,  1.37batch/s, auc=0.8830, loss=0.6935]\u001b[A\n",
      "Training Epoch 21/25:  28%|██▊       | 83/292 [00:56<02:32,  1.37batch/s, auc=0.8833, loss=0.6640]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 84/292 [00:56<02:20,  1.48batch/s, auc=0.8833, loss=0.6640]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 84/292 [00:57<02:20,  1.48batch/s, auc=0.8830, loss=0.7462]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 85/292 [00:57<02:12,  1.56batch/s, auc=0.8830, loss=0.7462]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 85/292 [00:57<02:12,  1.56batch/s, auc=0.8821, loss=1.0354]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 86/292 [00:57<02:06,  1.63batch/s, auc=0.8821, loss=1.0354]\u001b[A\n",
      "Training Epoch 21/25:  29%|██▉       | 86/292 [00:58<02:06,  1.63batch/s, auc=0.8823, loss=0.5904]\u001b[A\n",
      "Training Epoch 21/25:  30%|██▉       | 87/292 [00:58<02:02,  1.67batch/s, auc=0.8823, loss=0.5904]\u001b[A\n",
      "Training Epoch 21/25:  30%|██▉       | 87/292 [00:58<02:02,  1.67batch/s, auc=0.8817, loss=0.7416]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 88/292 [00:58<01:59,  1.71batch/s, auc=0.8817, loss=0.7416]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 88/292 [00:59<01:59,  1.71batch/s, auc=0.8820, loss=0.8094]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 89/292 [00:59<01:57,  1.73batch/s, auc=0.8820, loss=0.8094]\u001b[A\n",
      "Training Epoch 21/25:  30%|███       | 89/292 [01:00<01:57,  1.73batch/s, auc=0.8812, loss=1.0511]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 90/292 [01:00<02:26,  1.38batch/s, auc=0.8812, loss=1.0511]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 90/292 [01:00<02:26,  1.38batch/s, auc=0.8811, loss=0.7630]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 91/292 [01:00<02:15,  1.48batch/s, auc=0.8811, loss=0.7630]\u001b[A\n",
      "Training Epoch 21/25:  31%|███       | 91/292 [01:01<02:15,  1.48batch/s, auc=0.8820, loss=0.5774]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 92/292 [01:01<02:08,  1.56batch/s, auc=0.8820, loss=0.5774]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 92/292 [01:01<02:08,  1.56batch/s, auc=0.8827, loss=0.6918]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 93/292 [01:01<02:02,  1.62batch/s, auc=0.8827, loss=0.6918]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 93/292 [01:03<02:02,  1.62batch/s, auc=0.8820, loss=0.8366]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 94/292 [01:03<02:29,  1.33batch/s, auc=0.8820, loss=0.8366]\u001b[A\n",
      "Training Epoch 21/25:  32%|███▏      | 94/292 [01:03<02:29,  1.33batch/s, auc=0.8828, loss=0.5844]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 95/292 [01:03<02:16,  1.44batch/s, auc=0.8828, loss=0.5844]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 95/292 [01:04<02:16,  1.44batch/s, auc=0.8830, loss=0.6039]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 96/292 [01:04<02:07,  1.53batch/s, auc=0.8830, loss=0.6039]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 96/292 [01:04<02:07,  1.53batch/s, auc=0.8831, loss=0.5661]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 97/292 [01:04<02:01,  1.60batch/s, auc=0.8831, loss=0.5661]\u001b[A\n",
      "Training Epoch 21/25:  33%|███▎      | 97/292 [01:05<02:01,  1.60batch/s, auc=0.8824, loss=0.8795]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▎      | 98/292 [01:05<02:27,  1.32batch/s, auc=0.8824, loss=0.8795]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▎      | 98/292 [01:06<02:27,  1.32batch/s, auc=0.8820, loss=0.7937]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 99/292 [01:06<02:44,  1.17batch/s, auc=0.8820, loss=0.7937]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 99/292 [01:07<02:44,  1.17batch/s, auc=0.8823, loss=0.5254]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 100/292 [01:07<02:26,  1.31batch/s, auc=0.8823, loss=0.5254]\u001b[A\n",
      "Training Epoch 21/25:  34%|███▍      | 100/292 [01:08<02:26,  1.31batch/s, auc=0.8827, loss=0.5530]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 101/292 [01:08<02:14,  1.42batch/s, auc=0.8827, loss=0.5530]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 101/292 [01:08<02:14,  1.42batch/s, auc=0.8836, loss=0.6110]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 102/292 [01:08<02:05,  1.51batch/s, auc=0.8836, loss=0.6110]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▍      | 102/292 [01:09<02:05,  1.51batch/s, auc=0.8826, loss=1.0455]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▌      | 103/292 [01:09<02:28,  1.28batch/s, auc=0.8826, loss=1.0455]\u001b[A\n",
      "Training Epoch 21/25:  35%|███▌      | 103/292 [01:10<02:28,  1.28batch/s, auc=0.8819, loss=1.0324]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 104/292 [01:10<02:43,  1.15batch/s, auc=0.8819, loss=1.0324]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 104/292 [01:11<02:43,  1.15batch/s, auc=0.8822, loss=0.4885]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 105/292 [01:11<02:25,  1.29batch/s, auc=0.8822, loss=0.4885]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▌      | 105/292 [01:11<02:25,  1.29batch/s, auc=0.8825, loss=0.4484]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▋      | 106/292 [01:11<02:12,  1.41batch/s, auc=0.8825, loss=0.4484]\u001b[A\n",
      "Training Epoch 21/25:  36%|███▋      | 106/292 [01:12<02:12,  1.41batch/s, auc=0.8811, loss=1.1578]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 107/292 [01:12<02:31,  1.22batch/s, auc=0.8811, loss=1.1578]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 107/292 [01:13<02:31,  1.22batch/s, auc=0.8802, loss=1.0205]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 108/292 [01:13<02:16,  1.35batch/s, auc=0.8802, loss=1.0205]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 108/292 [01:14<02:16,  1.35batch/s, auc=0.8804, loss=0.6068]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 109/292 [01:14<02:05,  1.45batch/s, auc=0.8804, loss=0.6068]\u001b[A\n",
      "Training Epoch 21/25:  37%|███▋      | 109/292 [01:14<02:05,  1.45batch/s, auc=0.8813, loss=0.5556]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 110/292 [01:14<01:58,  1.54batch/s, auc=0.8813, loss=0.5556]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 110/292 [01:15<01:58,  1.54batch/s, auc=0.8810, loss=0.7688]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 111/292 [01:15<02:20,  1.29batch/s, auc=0.8810, loss=0.7688]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 111/292 [01:16<02:20,  1.29batch/s, auc=0.8809, loss=0.8214]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 112/292 [01:16<02:07,  1.41batch/s, auc=0.8809, loss=0.8214]\u001b[A\n",
      "Training Epoch 21/25:  38%|███▊      | 112/292 [01:16<02:07,  1.41batch/s, auc=0.8802, loss=0.7219]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▊      | 113/292 [01:16<01:59,  1.50batch/s, auc=0.8802, loss=0.7219]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▊      | 113/292 [01:17<01:59,  1.50batch/s, auc=0.8807, loss=0.5180]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 114/292 [01:17<01:52,  1.58batch/s, auc=0.8807, loss=0.5180]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 114/292 [01:17<01:52,  1.58batch/s, auc=0.8812, loss=0.7788]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 115/292 [01:17<01:48,  1.64batch/s, auc=0.8812, loss=0.7788]\u001b[A\n",
      "Training Epoch 21/25:  39%|███▉      | 115/292 [01:18<01:48,  1.64batch/s, auc=0.8813, loss=0.7335]\u001b[A\n",
      "Training Epoch 21/25:  40%|███▉      | 116/292 [01:18<01:44,  1.68batch/s, auc=0.8813, loss=0.7335]\u001b[A\n",
      "Training Epoch 21/25:  40%|███▉      | 116/292 [01:19<01:44,  1.68batch/s, auc=0.8824, loss=0.4128]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 117/292 [01:19<01:42,  1.71batch/s, auc=0.8824, loss=0.4128]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 117/292 [01:19<01:42,  1.71batch/s, auc=0.8817, loss=0.9355]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 118/292 [01:19<01:40,  1.73batch/s, auc=0.8817, loss=0.9355]\u001b[A\n",
      "Training Epoch 21/25:  40%|████      | 118/292 [01:20<01:40,  1.73batch/s, auc=0.8810, loss=0.8069]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 119/292 [01:20<01:38,  1.75batch/s, auc=0.8810, loss=0.8069]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 119/292 [01:20<01:38,  1.75batch/s, auc=0.8811, loss=0.7214]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 120/292 [01:20<01:37,  1.76batch/s, auc=0.8811, loss=0.7214]\u001b[A\n",
      "Training Epoch 21/25:  41%|████      | 120/292 [01:21<01:37,  1.76batch/s, auc=0.8818, loss=0.6019]\u001b[A\n",
      "Training Epoch 21/25:  41%|████▏     | 121/292 [01:21<01:36,  1.77batch/s, auc=0.8818, loss=0.6019]\u001b[A\n",
      "Training Epoch 21/25:  41%|████▏     | 121/292 [01:21<01:36,  1.77batch/s, auc=0.8819, loss=0.7415]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 122/292 [01:21<01:35,  1.77batch/s, auc=0.8819, loss=0.7415]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 122/292 [01:22<01:35,  1.77batch/s, auc=0.8814, loss=0.8203]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 123/292 [01:22<01:35,  1.78batch/s, auc=0.8814, loss=0.8203]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 123/292 [01:23<01:35,  1.78batch/s, auc=0.8802, loss=1.0803]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 124/292 [01:23<02:00,  1.40batch/s, auc=0.8802, loss=1.0803]\u001b[A\n",
      "Training Epoch 21/25:  42%|████▏     | 124/292 [01:24<02:00,  1.40batch/s, auc=0.8792, loss=1.1094]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 125/292 [01:24<01:51,  1.49batch/s, auc=0.8792, loss=1.1094]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 125/292 [01:24<01:51,  1.49batch/s, auc=0.8796, loss=0.6368]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 126/292 [01:24<01:45,  1.57batch/s, auc=0.8796, loss=0.6368]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 126/292 [01:25<01:45,  1.57batch/s, auc=0.8799, loss=0.5454]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 127/292 [01:25<01:41,  1.63batch/s, auc=0.8799, loss=0.5454]\u001b[A\n",
      "Training Epoch 21/25:  43%|████▎     | 127/292 [01:25<01:41,  1.63batch/s, auc=0.8795, loss=0.8037]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 128/292 [01:25<01:38,  1.67batch/s, auc=0.8795, loss=0.8037]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 128/292 [01:26<01:38,  1.67batch/s, auc=0.8796, loss=0.6207]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 129/292 [01:26<01:35,  1.71batch/s, auc=0.8796, loss=0.6207]\u001b[A\n",
      "Training Epoch 21/25:  44%|████▍     | 129/292 [01:26<01:35,  1.71batch/s, auc=0.8799, loss=0.5753]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 130/292 [01:26<01:33,  1.73batch/s, auc=0.8799, loss=0.5753]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 130/292 [01:27<01:33,  1.73batch/s, auc=0.8801, loss=0.8211]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 131/292 [01:27<01:32,  1.75batch/s, auc=0.8801, loss=0.8211]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▍     | 131/292 [01:27<01:32,  1.75batch/s, auc=0.8804, loss=0.6107]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▌     | 132/292 [01:27<01:30,  1.76batch/s, auc=0.8804, loss=0.6107]\u001b[A\n",
      "Training Epoch 21/25:  45%|████▌     | 132/292 [01:28<01:30,  1.76batch/s, auc=0.8807, loss=0.7628]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 133/292 [01:28<01:30,  1.77batch/s, auc=0.8807, loss=0.7628]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 133/292 [01:29<01:30,  1.77batch/s, auc=0.8813, loss=0.5690]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 134/292 [01:29<01:29,  1.77batch/s, auc=0.8813, loss=0.5690]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 134/292 [01:29<01:29,  1.77batch/s, auc=0.8810, loss=0.6978]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 135/292 [01:29<01:28,  1.78batch/s, auc=0.8810, loss=0.6978]\u001b[A\n",
      "Training Epoch 21/25:  46%|████▌     | 135/292 [01:30<01:28,  1.78batch/s, auc=0.8815, loss=0.5005]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 136/292 [01:30<01:27,  1.78batch/s, auc=0.8815, loss=0.5005]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 136/292 [01:30<01:27,  1.78batch/s, auc=0.8822, loss=0.4768]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 137/292 [01:30<01:26,  1.79batch/s, auc=0.8822, loss=0.4768]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 137/292 [01:31<01:26,  1.79batch/s, auc=0.8811, loss=1.2653]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 138/292 [01:31<01:50,  1.40batch/s, auc=0.8811, loss=1.2653]\u001b[A\n",
      "Training Epoch 21/25:  47%|████▋     | 138/292 [01:32<01:50,  1.40batch/s, auc=0.8806, loss=0.8884]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 139/292 [01:32<02:06,  1.21batch/s, auc=0.8806, loss=0.8884]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 139/292 [01:33<02:06,  1.21batch/s, auc=0.8805, loss=0.7727]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 140/292 [01:33<01:53,  1.34batch/s, auc=0.8805, loss=0.7727]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 140/292 [01:33<01:53,  1.34batch/s, auc=0.8803, loss=0.7450]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 141/292 [01:33<01:44,  1.45batch/s, auc=0.8803, loss=0.7450]\u001b[A\n",
      "Training Epoch 21/25:  48%|████▊     | 141/292 [01:34<01:44,  1.45batch/s, auc=0.8801, loss=0.8233]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▊     | 142/292 [01:34<01:37,  1.54batch/s, auc=0.8801, loss=0.8233]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▊     | 142/292 [01:35<01:37,  1.54batch/s, auc=0.8808, loss=0.5579]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 143/292 [01:35<01:32,  1.60batch/s, auc=0.8808, loss=0.5579]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 143/292 [01:35<01:32,  1.60batch/s, auc=0.8810, loss=0.8269]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 144/292 [01:35<01:29,  1.65batch/s, auc=0.8810, loss=0.8269]\u001b[A\n",
      "Training Epoch 21/25:  49%|████▉     | 144/292 [01:36<01:29,  1.65batch/s, auc=0.8812, loss=0.5904]\u001b[A\n",
      "Training Epoch 21/25:  50%|████▉     | 145/292 [01:36<01:26,  1.69batch/s, auc=0.8812, loss=0.5904]\u001b[A\n",
      "Training Epoch 21/25:  50%|████▉     | 145/292 [01:36<01:26,  1.69batch/s, auc=0.8810, loss=0.7049]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 146/292 [01:36<01:24,  1.72batch/s, auc=0.8810, loss=0.7049]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 146/292 [01:37<01:24,  1.72batch/s, auc=0.8806, loss=0.9015]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 147/292 [01:37<01:23,  1.74batch/s, auc=0.8806, loss=0.9015]\u001b[A\n",
      "Training Epoch 21/25:  50%|█████     | 147/292 [01:37<01:23,  1.74batch/s, auc=0.8810, loss=0.5710]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 148/292 [01:37<01:22,  1.75batch/s, auc=0.8810, loss=0.5710]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 148/292 [01:38<01:22,  1.75batch/s, auc=0.8814, loss=0.6506]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 149/292 [01:38<01:21,  1.76batch/s, auc=0.8814, loss=0.6506]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████     | 149/292 [01:39<01:21,  1.76batch/s, auc=0.8818, loss=0.5053]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████▏    | 150/292 [01:39<01:20,  1.77batch/s, auc=0.8818, loss=0.5053]\u001b[A\n",
      "Training Epoch 21/25:  51%|█████▏    | 150/292 [01:39<01:20,  1.77batch/s, auc=0.8820, loss=0.5301]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 151/292 [01:39<01:19,  1.78batch/s, auc=0.8820, loss=0.5301]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 151/292 [01:40<01:19,  1.78batch/s, auc=0.8818, loss=0.7435]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 152/292 [01:40<01:18,  1.78batch/s, auc=0.8818, loss=0.7435]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 152/292 [01:41<01:18,  1.78batch/s, auc=0.8813, loss=0.9260]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 153/292 [01:41<01:39,  1.40batch/s, auc=0.8813, loss=0.9260]\u001b[A\n",
      "Training Epoch 21/25:  52%|█████▏    | 153/292 [01:41<01:39,  1.40batch/s, auc=0.8812, loss=0.5756]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 154/292 [01:41<01:32,  1.49batch/s, auc=0.8812, loss=0.5756]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 154/292 [01:42<01:32,  1.49batch/s, auc=0.8816, loss=0.6085]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 155/292 [01:42<01:27,  1.57batch/s, auc=0.8816, loss=0.6085]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 155/292 [01:42<01:27,  1.57batch/s, auc=0.8814, loss=0.7553]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 156/292 [01:42<01:23,  1.63batch/s, auc=0.8814, loss=0.7553]\u001b[A\n",
      "Training Epoch 21/25:  53%|█████▎    | 156/292 [01:43<01:23,  1.63batch/s, auc=0.8813, loss=0.6256]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 157/292 [01:43<01:20,  1.67batch/s, auc=0.8813, loss=0.6256]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 157/292 [01:44<01:20,  1.67batch/s, auc=0.8813, loss=0.8989]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 158/292 [01:44<01:18,  1.70batch/s, auc=0.8813, loss=0.8989]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 158/292 [01:44<01:18,  1.70batch/s, auc=0.8812, loss=0.7632]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 159/292 [01:44<01:16,  1.73batch/s, auc=0.8812, loss=0.7632]\u001b[A\n",
      "Training Epoch 21/25:  54%|█████▍    | 159/292 [01:45<01:16,  1.73batch/s, auc=0.8817, loss=0.6470]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▍    | 160/292 [01:45<01:15,  1.74batch/s, auc=0.8817, loss=0.6470]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▍    | 160/292 [01:45<01:15,  1.74batch/s, auc=0.8816, loss=0.6307]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 161/292 [01:45<01:14,  1.76batch/s, auc=0.8816, loss=0.6307]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 161/292 [01:46<01:14,  1.76batch/s, auc=0.8813, loss=0.8430]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 162/292 [01:46<01:13,  1.77batch/s, auc=0.8813, loss=0.8430]\u001b[A\n",
      "Training Epoch 21/25:  55%|█████▌    | 162/292 [01:46<01:13,  1.77batch/s, auc=0.8808, loss=1.0345]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 163/292 [01:46<01:12,  1.77batch/s, auc=0.8808, loss=1.0345]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 163/292 [01:47<01:12,  1.77batch/s, auc=0.8807, loss=0.7554]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 164/292 [01:47<01:12,  1.77batch/s, auc=0.8807, loss=0.7554]\u001b[A\n",
      "Training Epoch 21/25:  56%|█████▌    | 164/292 [01:47<01:12,  1.77batch/s, auc=0.8808, loss=0.8208]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 165/292 [01:47<01:11,  1.78batch/s, auc=0.8808, loss=0.8208]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 165/292 [01:48<01:11,  1.78batch/s, auc=0.8809, loss=0.6957]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 166/292 [01:48<01:10,  1.78batch/s, auc=0.8809, loss=0.6957]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 166/292 [01:49<01:10,  1.78batch/s, auc=0.8809, loss=0.6363]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 167/292 [01:49<01:10,  1.78batch/s, auc=0.8809, loss=0.6363]\u001b[A\n",
      "Training Epoch 21/25:  57%|█████▋    | 167/292 [01:49<01:10,  1.78batch/s, auc=0.8807, loss=0.7103]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 168/292 [01:49<01:09,  1.78batch/s, auc=0.8807, loss=0.7103]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 168/292 [01:50<01:09,  1.78batch/s, auc=0.8803, loss=0.8427]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 169/292 [01:50<01:08,  1.78batch/s, auc=0.8803, loss=0.8427]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 169/292 [01:50<01:08,  1.78batch/s, auc=0.8803, loss=0.6093]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 170/292 [01:50<01:08,  1.78batch/s, auc=0.8803, loss=0.6093]\u001b[A\n",
      "Training Epoch 21/25:  58%|█████▊    | 170/292 [01:51<01:08,  1.78batch/s, auc=0.8803, loss=0.7227]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▊    | 171/292 [01:51<01:07,  1.78batch/s, auc=0.8803, loss=0.7227]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▊    | 171/292 [01:51<01:07,  1.78batch/s, auc=0.8808, loss=0.4971]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 172/292 [01:51<01:07,  1.78batch/s, auc=0.8808, loss=0.4971]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 172/292 [01:52<01:07,  1.78batch/s, auc=0.8806, loss=0.8880]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 173/292 [01:52<01:25,  1.40batch/s, auc=0.8806, loss=0.8880]\u001b[A\n",
      "Training Epoch 21/25:  59%|█████▉    | 173/292 [01:53<01:25,  1.40batch/s, auc=0.8807, loss=0.7035]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 174/292 [01:53<01:18,  1.49batch/s, auc=0.8807, loss=0.7035]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 174/292 [01:54<01:18,  1.49batch/s, auc=0.8808, loss=0.6120]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 175/292 [01:54<01:14,  1.57batch/s, auc=0.8808, loss=0.6120]\u001b[A\n",
      "Training Epoch 21/25:  60%|█████▉    | 175/292 [01:54<01:14,  1.57batch/s, auc=0.8811, loss=0.6227]\u001b[A\n",
      "Training Epoch 21/25:  60%|██████    | 176/292 [01:54<01:11,  1.63batch/s, auc=0.8811, loss=0.6227]\u001b[A\n",
      "Training Epoch 21/25:  60%|██████    | 176/292 [01:55<01:11,  1.63batch/s, auc=0.8812, loss=0.7288]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 177/292 [01:55<01:08,  1.67batch/s, auc=0.8812, loss=0.7288]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 177/292 [01:55<01:08,  1.67batch/s, auc=0.8814, loss=0.6025]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 178/292 [01:55<01:06,  1.70batch/s, auc=0.8814, loss=0.6025]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████    | 178/292 [01:56<01:06,  1.70batch/s, auc=0.8817, loss=0.7747]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████▏   | 179/292 [01:56<01:05,  1.73batch/s, auc=0.8817, loss=0.7747]\u001b[A\n",
      "Training Epoch 21/25:  61%|██████▏   | 179/292 [01:56<01:05,  1.73batch/s, auc=0.8814, loss=0.7661]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 180/292 [01:56<01:04,  1.74batch/s, auc=0.8814, loss=0.7661]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 180/292 [01:57<01:04,  1.74batch/s, auc=0.8812, loss=0.7411]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 181/292 [01:57<01:20,  1.38batch/s, auc=0.8812, loss=0.7411]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 181/292 [01:58<01:20,  1.38batch/s, auc=0.8814, loss=0.5873]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 182/292 [01:58<01:14,  1.48batch/s, auc=0.8814, loss=0.5873]\u001b[A\n",
      "Training Epoch 21/25:  62%|██████▏   | 182/292 [01:59<01:14,  1.48batch/s, auc=0.8815, loss=0.6312]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 183/292 [01:59<01:09,  1.56batch/s, auc=0.8815, loss=0.6312]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 183/292 [01:59<01:09,  1.56batch/s, auc=0.8815, loss=0.6386]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 184/292 [01:59<01:06,  1.62batch/s, auc=0.8815, loss=0.6386]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 184/292 [02:00<01:06,  1.62batch/s, auc=0.8815, loss=0.6434]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 185/292 [02:00<01:04,  1.66batch/s, auc=0.8815, loss=0.6434]\u001b[A\n",
      "Training Epoch 21/25:  63%|██████▎   | 185/292 [02:00<01:04,  1.66batch/s, auc=0.8817, loss=0.5288]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▎   | 186/292 [02:00<01:02,  1.69batch/s, auc=0.8817, loss=0.5288]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▎   | 186/292 [02:01<01:02,  1.69batch/s, auc=0.8819, loss=0.5491]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 187/292 [02:01<01:01,  1.72batch/s, auc=0.8819, loss=0.5491]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 187/292 [02:02<01:01,  1.72batch/s, auc=0.8812, loss=1.1054]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 188/292 [02:02<01:15,  1.37batch/s, auc=0.8812, loss=1.1054]\u001b[A\n",
      "Training Epoch 21/25:  64%|██████▍   | 188/292 [02:02<01:15,  1.37batch/s, auc=0.8816, loss=0.5443]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▍   | 189/292 [02:02<01:10,  1.47batch/s, auc=0.8816, loss=0.5443]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▍   | 189/292 [02:03<01:10,  1.47batch/s, auc=0.8814, loss=0.9461]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 190/292 [02:03<01:05,  1.55batch/s, auc=0.8814, loss=0.9461]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 190/292 [02:04<01:05,  1.55batch/s, auc=0.8810, loss=0.8124]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 191/292 [02:04<01:18,  1.29batch/s, auc=0.8810, loss=0.8124]\u001b[A\n",
      "Training Epoch 21/25:  65%|██████▌   | 191/292 [02:05<01:18,  1.29batch/s, auc=0.8812, loss=0.6566]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 192/292 [02:05<01:11,  1.41batch/s, auc=0.8812, loss=0.6566]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 192/292 [02:05<01:11,  1.41batch/s, auc=0.8816, loss=0.5575]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 193/292 [02:05<01:05,  1.50batch/s, auc=0.8816, loss=0.5575]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▌   | 193/292 [02:06<01:05,  1.50batch/s, auc=0.8820, loss=0.5307]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▋   | 194/292 [02:06<01:02,  1.58batch/s, auc=0.8820, loss=0.5307]\u001b[A\n",
      "Training Epoch 21/25:  66%|██████▋   | 194/292 [02:06<01:02,  1.58batch/s, auc=0.8822, loss=0.6568]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 195/292 [02:06<00:59,  1.63batch/s, auc=0.8822, loss=0.6568]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 195/292 [02:07<00:59,  1.63batch/s, auc=0.8820, loss=0.7791]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 196/292 [02:07<00:57,  1.67batch/s, auc=0.8820, loss=0.7791]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 196/292 [02:07<00:57,  1.67batch/s, auc=0.8820, loss=0.6623]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 197/292 [02:07<00:55,  1.71batch/s, auc=0.8820, loss=0.6623]\u001b[A\n",
      "Training Epoch 21/25:  67%|██████▋   | 197/292 [02:08<00:55,  1.71batch/s, auc=0.8819, loss=0.7662]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 198/292 [02:08<00:54,  1.73batch/s, auc=0.8819, loss=0.7662]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 198/292 [02:09<00:54,  1.73batch/s, auc=0.8821, loss=0.7283]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 199/292 [02:09<00:53,  1.74batch/s, auc=0.8821, loss=0.7283]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 199/292 [02:09<00:53,  1.74batch/s, auc=0.8826, loss=0.5143]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 200/292 [02:09<00:52,  1.75batch/s, auc=0.8826, loss=0.5143]\u001b[A\n",
      "Training Epoch 21/25:  68%|██████▊   | 200/292 [02:10<00:52,  1.75batch/s, auc=0.8826, loss=0.6863]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 201/292 [02:10<00:51,  1.76batch/s, auc=0.8826, loss=0.6863]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 201/292 [02:10<00:51,  1.76batch/s, auc=0.8826, loss=0.5399]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 202/292 [02:10<00:50,  1.77batch/s, auc=0.8826, loss=0.5399]\u001b[A\n",
      "Training Epoch 21/25:  69%|██████▉   | 202/292 [02:11<00:50,  1.77batch/s, auc=0.8829, loss=0.5531]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 203/292 [02:11<00:50,  1.77batch/s, auc=0.8829, loss=0.5531]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 203/292 [02:12<00:50,  1.77batch/s, auc=0.8824, loss=0.9607]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 204/292 [02:12<01:03,  1.39batch/s, auc=0.8824, loss=0.9607]\u001b[A\n",
      "Training Epoch 21/25:  70%|██████▉   | 204/292 [02:12<01:03,  1.39batch/s, auc=0.8827, loss=0.6961]\u001b[A\n",
      "Training Epoch 21/25:  70%|███████   | 205/292 [02:12<00:58,  1.49batch/s, auc=0.8827, loss=0.6961]\u001b[A\n",
      "Training Epoch 21/25:  70%|███████   | 205/292 [02:13<00:58,  1.49batch/s, auc=0.8831, loss=0.4400]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 206/292 [02:13<00:54,  1.57batch/s, auc=0.8831, loss=0.4400]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 206/292 [02:14<00:54,  1.57batch/s, auc=0.8830, loss=0.7908]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 207/292 [02:14<01:05,  1.30batch/s, auc=0.8830, loss=0.7908]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 207/292 [02:15<01:05,  1.30batch/s, auc=0.8830, loss=0.5376]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 208/292 [02:15<00:59,  1.41batch/s, auc=0.8830, loss=0.5376]\u001b[A\n",
      "Training Epoch 21/25:  71%|███████   | 208/292 [02:15<00:59,  1.41batch/s, auc=0.8831, loss=0.6356]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 209/292 [02:15<00:55,  1.50batch/s, auc=0.8831, loss=0.6356]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 209/292 [02:16<00:55,  1.50batch/s, auc=0.8828, loss=0.8851]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 210/292 [02:16<01:04,  1.27batch/s, auc=0.8828, loss=0.8851]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 210/292 [02:17<01:04,  1.27batch/s, auc=0.8824, loss=0.9290]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 211/292 [02:17<01:10,  1.14batch/s, auc=0.8824, loss=0.9290]\u001b[A\n",
      "Training Epoch 21/25:  72%|███████▏  | 211/292 [02:18<01:10,  1.14batch/s, auc=0.8825, loss=0.6555]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 212/292 [02:18<01:02,  1.28batch/s, auc=0.8825, loss=0.6555]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 212/292 [02:19<01:02,  1.28batch/s, auc=0.8827, loss=0.7319]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 213/292 [02:19<00:56,  1.40batch/s, auc=0.8827, loss=0.7319]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 213/292 [02:19<00:56,  1.40batch/s, auc=0.8829, loss=0.6832]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 214/292 [02:19<00:52,  1.49batch/s, auc=0.8829, loss=0.6832]\u001b[A\n",
      "Training Epoch 21/25:  73%|███████▎  | 214/292 [02:20<00:52,  1.49batch/s, auc=0.8829, loss=0.7907]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▎  | 215/292 [02:20<01:01,  1.26batch/s, auc=0.8829, loss=0.7907]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▎  | 215/292 [02:21<01:01,  1.26batch/s, auc=0.8830, loss=0.4933]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 216/292 [02:21<00:54,  1.38batch/s, auc=0.8830, loss=0.4933]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 216/292 [02:21<00:54,  1.38batch/s, auc=0.8828, loss=0.7067]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 217/292 [02:21<00:50,  1.48batch/s, auc=0.8828, loss=0.7067]\u001b[A\n",
      "Training Epoch 21/25:  74%|███████▍  | 217/292 [02:22<00:50,  1.48batch/s, auc=0.8833, loss=0.3667]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▍  | 218/292 [02:22<00:47,  1.56batch/s, auc=0.8833, loss=0.3667]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▍  | 218/292 [02:22<00:47,  1.56batch/s, auc=0.8834, loss=0.5618]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 219/292 [02:22<00:45,  1.62batch/s, auc=0.8834, loss=0.5618]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 219/292 [02:24<00:45,  1.62batch/s, auc=0.8830, loss=0.8684]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 220/292 [02:24<00:54,  1.32batch/s, auc=0.8830, loss=0.8684]\u001b[A\n",
      "Training Epoch 21/25:  75%|███████▌  | 220/292 [02:24<00:54,  1.32batch/s, auc=0.8829, loss=0.7179]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 221/292 [02:24<00:49,  1.43batch/s, auc=0.8829, loss=0.7179]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 221/292 [02:25<00:49,  1.43batch/s, auc=0.8833, loss=0.4765]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 222/292 [02:25<00:46,  1.52batch/s, auc=0.8833, loss=0.4765]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▌  | 222/292 [02:25<00:46,  1.52batch/s, auc=0.8836, loss=0.6262]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▋  | 223/292 [02:25<00:43,  1.59batch/s, auc=0.8836, loss=0.6262]\u001b[A\n",
      "Training Epoch 21/25:  76%|███████▋  | 223/292 [02:26<00:43,  1.59batch/s, auc=0.8836, loss=0.7606]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 224/292 [02:26<00:41,  1.64batch/s, auc=0.8836, loss=0.7606]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 224/292 [02:26<00:41,  1.64batch/s, auc=0.8835, loss=0.7182]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 225/292 [02:26<00:39,  1.68batch/s, auc=0.8835, loss=0.7182]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 225/292 [02:27<00:39,  1.68batch/s, auc=0.8837, loss=0.5235]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 226/292 [02:27<00:38,  1.71batch/s, auc=0.8837, loss=0.5235]\u001b[A\n",
      "Training Epoch 21/25:  77%|███████▋  | 226/292 [02:27<00:38,  1.71batch/s, auc=0.8841, loss=0.4928]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 227/292 [02:27<00:37,  1.73batch/s, auc=0.8841, loss=0.4928]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 227/292 [02:28<00:37,  1.73batch/s, auc=0.8841, loss=0.7121]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 228/292 [02:28<00:36,  1.75batch/s, auc=0.8841, loss=0.7121]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 228/292 [02:29<00:36,  1.75batch/s, auc=0.8841, loss=0.6980]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 229/292 [02:29<00:35,  1.75batch/s, auc=0.8841, loss=0.6980]\u001b[A\n",
      "Training Epoch 21/25:  78%|███████▊  | 229/292 [02:29<00:35,  1.75batch/s, auc=0.8840, loss=0.7742]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 230/292 [02:29<00:35,  1.76batch/s, auc=0.8840, loss=0.7742]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 230/292 [02:30<00:35,  1.76batch/s, auc=0.8839, loss=0.8616]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 231/292 [02:30<00:34,  1.76batch/s, auc=0.8839, loss=0.8616]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 231/292 [02:30<00:34,  1.76batch/s, auc=0.8840, loss=0.5698]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 232/292 [02:30<00:33,  1.77batch/s, auc=0.8840, loss=0.5698]\u001b[A\n",
      "Training Epoch 21/25:  79%|███████▉  | 232/292 [02:31<00:33,  1.77batch/s, auc=0.8843, loss=0.5803]\u001b[A\n",
      "Training Epoch 21/25:  80%|███████▉  | 233/292 [02:31<00:33,  1.77batch/s, auc=0.8843, loss=0.5803]\u001b[A\n",
      "Training Epoch 21/25:  80%|███████▉  | 233/292 [02:31<00:33,  1.77batch/s, auc=0.8843, loss=0.6893]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 234/292 [02:31<00:32,  1.78batch/s, auc=0.8843, loss=0.6893]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 234/292 [02:32<00:32,  1.78batch/s, auc=0.8847, loss=0.4206]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 235/292 [02:32<00:32,  1.78batch/s, auc=0.8847, loss=0.4206]\u001b[A\n",
      "Training Epoch 21/25:  80%|████████  | 235/292 [02:33<00:32,  1.78batch/s, auc=0.8846, loss=0.8457]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 236/292 [02:33<00:31,  1.78batch/s, auc=0.8846, loss=0.8457]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 236/292 [02:33<00:31,  1.78batch/s, auc=0.8848, loss=0.5727]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 237/292 [02:33<00:30,  1.78batch/s, auc=0.8848, loss=0.5727]\u001b[A\n",
      "Training Epoch 21/25:  81%|████████  | 237/292 [02:34<00:30,  1.78batch/s, auc=0.8851, loss=0.5444]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 238/292 [02:34<00:30,  1.78batch/s, auc=0.8851, loss=0.5444]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 238/292 [02:34<00:30,  1.78batch/s, auc=0.8854, loss=0.5821]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 239/292 [02:34<00:29,  1.78batch/s, auc=0.8854, loss=0.5821]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 239/292 [02:35<00:29,  1.78batch/s, auc=0.8856, loss=0.4988]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 240/292 [02:35<00:29,  1.78batch/s, auc=0.8856, loss=0.4988]\u001b[A\n",
      "Training Epoch 21/25:  82%|████████▏ | 240/292 [02:35<00:29,  1.78batch/s, auc=0.8854, loss=0.7394]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 241/292 [02:35<00:28,  1.78batch/s, auc=0.8854, loss=0.7394]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 241/292 [02:36<00:28,  1.78batch/s, auc=0.8856, loss=0.5691]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 242/292 [02:36<00:28,  1.78batch/s, auc=0.8856, loss=0.5691]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 242/292 [02:36<00:28,  1.78batch/s, auc=0.8856, loss=0.5418]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 243/292 [02:36<00:27,  1.77batch/s, auc=0.8856, loss=0.5418]\u001b[A\n",
      "Training Epoch 21/25:  83%|████████▎ | 243/292 [02:37<00:27,  1.77batch/s, auc=0.8857, loss=0.7223]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▎ | 244/292 [02:37<00:27,  1.78batch/s, auc=0.8857, loss=0.7223]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▎ | 244/292 [02:38<00:27,  1.78batch/s, auc=0.8860, loss=0.5034]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 245/292 [02:38<00:26,  1.78batch/s, auc=0.8860, loss=0.5034]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 245/292 [02:38<00:26,  1.78batch/s, auc=0.8859, loss=0.7141]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 246/292 [02:38<00:25,  1.77batch/s, auc=0.8859, loss=0.7141]\u001b[A\n",
      "Training Epoch 21/25:  84%|████████▍ | 246/292 [02:39<00:25,  1.77batch/s, auc=0.8861, loss=0.6978]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 247/292 [02:39<00:25,  1.77batch/s, auc=0.8861, loss=0.6978]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 247/292 [02:39<00:25,  1.77batch/s, auc=0.8863, loss=0.5540]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 248/292 [02:39<00:24,  1.77batch/s, auc=0.8863, loss=0.5540]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▍ | 248/292 [02:40<00:24,  1.77batch/s, auc=0.8864, loss=0.6030]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▌ | 249/292 [02:40<00:24,  1.77batch/s, auc=0.8864, loss=0.6030]\u001b[A\n",
      "Training Epoch 21/25:  85%|████████▌ | 249/292 [02:40<00:24,  1.77batch/s, auc=0.8865, loss=0.5776]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 250/292 [02:40<00:23,  1.77batch/s, auc=0.8865, loss=0.5776]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 250/292 [02:41<00:23,  1.77batch/s, auc=0.8864, loss=0.7437]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 251/292 [02:41<00:23,  1.77batch/s, auc=0.8864, loss=0.7437]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▌ | 251/292 [02:42<00:23,  1.77batch/s, auc=0.8863, loss=0.8590]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▋ | 252/292 [02:42<00:22,  1.77batch/s, auc=0.8863, loss=0.8590]\u001b[A\n",
      "Training Epoch 21/25:  86%|████████▋ | 252/292 [02:42<00:22,  1.77batch/s, auc=0.8864, loss=0.7429]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 253/292 [02:42<00:21,  1.77batch/s, auc=0.8864, loss=0.7429]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 253/292 [02:43<00:21,  1.77batch/s, auc=0.8866, loss=0.4274]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 254/292 [02:43<00:21,  1.77batch/s, auc=0.8866, loss=0.4274]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 254/292 [02:43<00:21,  1.77batch/s, auc=0.8866, loss=0.6402]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 255/292 [02:43<00:20,  1.77batch/s, auc=0.8866, loss=0.6402]\u001b[A\n",
      "Training Epoch 21/25:  87%|████████▋ | 255/292 [02:44<00:20,  1.77batch/s, auc=0.8867, loss=0.5378]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 256/292 [02:44<00:20,  1.77batch/s, auc=0.8867, loss=0.5378]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 256/292 [02:45<00:20,  1.77batch/s, auc=0.8861, loss=1.3189]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 257/292 [02:45<00:25,  1.39batch/s, auc=0.8861, loss=1.3189]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 257/292 [02:46<00:25,  1.39batch/s, auc=0.8856, loss=1.1143]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 258/292 [02:46<00:28,  1.21batch/s, auc=0.8856, loss=1.1143]\u001b[A\n",
      "Training Epoch 21/25:  88%|████████▊ | 258/292 [02:47<00:28,  1.21batch/s, auc=0.8857, loss=0.5330]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▊ | 259/292 [02:47<00:24,  1.34batch/s, auc=0.8857, loss=0.5330]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▊ | 259/292 [02:47<00:24,  1.34batch/s, auc=0.8856, loss=0.7650]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 260/292 [02:47<00:22,  1.45batch/s, auc=0.8856, loss=0.7650]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 260/292 [02:48<00:22,  1.45batch/s, auc=0.8856, loss=0.7685]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 261/292 [02:48<00:20,  1.53batch/s, auc=0.8856, loss=0.7685]\u001b[A\n",
      "Training Epoch 21/25:  89%|████████▉ | 261/292 [02:48<00:20,  1.53batch/s, auc=0.8859, loss=0.4827]\u001b[A\n",
      "Training Epoch 21/25:  90%|████████▉ | 262/292 [02:48<00:18,  1.60batch/s, auc=0.8859, loss=0.4827]\u001b[A\n",
      "Training Epoch 21/25:  90%|████████▉ | 262/292 [02:49<00:18,  1.60batch/s, auc=0.8861, loss=0.5981]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 263/292 [02:49<00:17,  1.64batch/s, auc=0.8861, loss=0.5981]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 263/292 [02:49<00:17,  1.64batch/s, auc=0.8862, loss=0.5962]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 264/292 [02:49<00:16,  1.68batch/s, auc=0.8862, loss=0.5962]\u001b[A\n",
      "Training Epoch 21/25:  90%|█████████ | 264/292 [02:50<00:16,  1.68batch/s, auc=0.8864, loss=0.5958]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 265/292 [02:50<00:15,  1.71batch/s, auc=0.8864, loss=0.5958]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 265/292 [02:50<00:15,  1.71batch/s, auc=0.8864, loss=0.5236]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 266/292 [02:50<00:15,  1.73batch/s, auc=0.8864, loss=0.5236]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████ | 266/292 [02:52<00:15,  1.73batch/s, auc=0.8861, loss=0.9423]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████▏| 267/292 [02:52<00:18,  1.37batch/s, auc=0.8861, loss=0.9423]\u001b[A\n",
      "Training Epoch 21/25:  91%|█████████▏| 267/292 [02:52<00:18,  1.37batch/s, auc=0.8857, loss=1.2044]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 268/292 [02:52<00:16,  1.47batch/s, auc=0.8857, loss=1.2044]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 268/292 [02:53<00:16,  1.47batch/s, auc=0.8854, loss=0.9732]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 269/292 [02:53<00:18,  1.25batch/s, auc=0.8854, loss=0.9732]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 269/292 [02:54<00:18,  1.25batch/s, auc=0.8858, loss=0.4453]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 270/292 [02:54<00:16,  1.37batch/s, auc=0.8858, loss=0.4453]\u001b[A\n",
      "Training Epoch 21/25:  92%|█████████▏| 270/292 [02:55<00:16,  1.37batch/s, auc=0.8856, loss=0.8029]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 271/292 [02:55<00:17,  1.20batch/s, auc=0.8856, loss=0.8029]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 271/292 [02:56<00:17,  1.20batch/s, auc=0.8849, loss=1.1412]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 272/292 [02:56<00:18,  1.10batch/s, auc=0.8849, loss=1.1412]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 272/292 [02:56<00:18,  1.10batch/s, auc=0.8848, loss=0.8539]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 273/292 [02:56<00:15,  1.24batch/s, auc=0.8848, loss=0.8539]\u001b[A\n",
      "Training Epoch 21/25:  93%|█████████▎| 273/292 [02:58<00:15,  1.24batch/s, auc=0.8846, loss=0.9546]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 274/292 [02:58<00:16,  1.12batch/s, auc=0.8846, loss=0.9546]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 274/292 [02:58<00:16,  1.12batch/s, auc=0.8848, loss=0.6651]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 275/292 [02:58<00:13,  1.26batch/s, auc=0.8848, loss=0.6651]\u001b[A\n",
      "Training Epoch 21/25:  94%|█████████▍| 275/292 [02:59<00:13,  1.26batch/s, auc=0.8848, loss=0.7403]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 276/292 [02:59<00:11,  1.38batch/s, auc=0.8848, loss=0.7403]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 276/292 [03:00<00:11,  1.38batch/s, auc=0.8844, loss=1.0411]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 277/292 [03:00<00:12,  1.20batch/s, auc=0.8844, loss=1.0411]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▍| 277/292 [03:00<00:12,  1.20batch/s, auc=0.8844, loss=0.6084]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▌| 278/292 [03:00<00:10,  1.33batch/s, auc=0.8844, loss=0.6084]\u001b[A\n",
      "Training Epoch 21/25:  95%|█████████▌| 278/292 [03:01<00:10,  1.33batch/s, auc=0.8844, loss=0.9514]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 279/292 [03:01<00:09,  1.43batch/s, auc=0.8844, loss=0.9514]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 279/292 [03:01<00:09,  1.43batch/s, auc=0.8845, loss=0.5060]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 280/292 [03:01<00:07,  1.52batch/s, auc=0.8845, loss=0.5060]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 280/292 [03:02<00:07,  1.52batch/s, auc=0.8847, loss=0.4339]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 281/292 [03:02<00:06,  1.59batch/s, auc=0.8847, loss=0.4339]\u001b[A\n",
      "Training Epoch 21/25:  96%|█████████▌| 281/292 [03:03<00:06,  1.59batch/s, auc=0.8849, loss=0.4995]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 282/292 [03:03<00:06,  1.64batch/s, auc=0.8849, loss=0.4995]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 282/292 [03:03<00:06,  1.64batch/s, auc=0.8847, loss=0.7048]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 283/292 [03:03<00:05,  1.68batch/s, auc=0.8847, loss=0.7048]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 283/292 [03:04<00:05,  1.68batch/s, auc=0.8850, loss=0.4610]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 284/292 [03:04<00:04,  1.70batch/s, auc=0.8850, loss=0.4610]\u001b[A\n",
      "Training Epoch 21/25:  97%|█████████▋| 284/292 [03:04<00:04,  1.70batch/s, auc=0.8850, loss=0.6986]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 285/292 [03:04<00:04,  1.72batch/s, auc=0.8850, loss=0.6986]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 285/292 [03:05<00:04,  1.72batch/s, auc=0.8849, loss=0.7971]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 286/292 [03:05<00:03,  1.74batch/s, auc=0.8849, loss=0.7971]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 286/292 [03:05<00:03,  1.74batch/s, auc=0.8848, loss=0.9680]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 287/292 [03:05<00:02,  1.75batch/s, auc=0.8848, loss=0.9680]\u001b[A\n",
      "Training Epoch 21/25:  98%|█████████▊| 287/292 [03:06<00:02,  1.75batch/s, auc=0.8846, loss=0.8851]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▊| 288/292 [03:06<00:02,  1.76batch/s, auc=0.8846, loss=0.8851]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▊| 288/292 [03:07<00:02,  1.76batch/s, auc=0.8847, loss=0.6081]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 289/292 [03:07<00:01,  1.76batch/s, auc=0.8847, loss=0.6081]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 289/292 [03:07<00:01,  1.76batch/s, auc=0.8845, loss=0.8396]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 290/292 [03:07<00:01,  1.76batch/s, auc=0.8845, loss=0.8396]\u001b[A\n",
      "Training Epoch 21/25:  99%|█████████▉| 290/292 [03:08<00:01,  1.76batch/s, auc=0.8844, loss=0.7623]\u001b[A\n",
      "Training Epoch 21/25: 100%|█████████▉| 291/292 [03:08<00:00,  1.39batch/s, auc=0.8844, loss=0.7623]\u001b[A\n",
      "Training Epoch 21/25: 100%|█████████▉| 291/292 [03:09<00:00,  1.39batch/s, auc=0.8845, loss=0.6533]\u001b[A\n",
      "Training Epoch 21/25: 100%|██████████| 292/292 [03:09<00:00,  1.54batch/s, auc=0.8845, loss=0.6533]\u001b[A\n",
      "Epochs:  84%|████████▍ | 21/25 [1:14:52<14:13, 213.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25] Train Loss: 0.7103 | Train AUROC: 0.8845 Val Loss: 0.7979 | Val AUROC: 0.8515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 22/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9458, loss=0.4885]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 1/292 [00:01<07:04,  1.46s/batch, auc=0.9458, loss=0.4885]\u001b[A\n",
      "Training Epoch 22/25:   0%|          | 1/292 [00:02<07:04,  1.46s/batch, auc=0.9271, loss=0.5454]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 2/292 [00:02<04:29,  1.08batch/s, auc=0.9271, loss=0.5454]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 2/292 [00:02<04:29,  1.08batch/s, auc=0.9082, loss=0.8075]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 3/292 [00:02<03:39,  1.32batch/s, auc=0.9082, loss=0.8075]\u001b[A\n",
      "Training Epoch 22/25:   1%|          | 3/292 [00:03<03:39,  1.32batch/s, auc=0.9120, loss=0.6081]\u001b[A\n",
      "Training Epoch 22/25:   1%|▏         | 4/292 [00:03<03:15,  1.48batch/s, auc=0.9120, loss=0.6081]\u001b[A\n",
      "Training Epoch 22/25:   1%|▏         | 4/292 [00:03<03:15,  1.48batch/s, auc=0.9129, loss=0.6658]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 5/292 [00:03<03:01,  1.58batch/s, auc=0.9129, loss=0.6658]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 5/292 [00:04<03:01,  1.58batch/s, auc=0.9020, loss=0.7975]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.9020, loss=0.7975]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 6/292 [00:04<02:53,  1.65batch/s, auc=0.9008, loss=0.7747]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 7/292 [00:04<02:48,  1.69batch/s, auc=0.9008, loss=0.7747]\u001b[A\n",
      "Training Epoch 22/25:   2%|▏         | 7/292 [00:05<02:48,  1.69batch/s, auc=0.8993, loss=0.5238]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 8/292 [00:05<02:44,  1.73batch/s, auc=0.8993, loss=0.5238]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 8/292 [00:05<02:44,  1.73batch/s, auc=0.8951, loss=0.7489]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 9/292 [00:05<02:41,  1.75batch/s, auc=0.8951, loss=0.7489]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 9/292 [00:06<02:41,  1.75batch/s, auc=0.8980, loss=0.6113]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 10/292 [00:06<02:39,  1.77batch/s, auc=0.8980, loss=0.6113]\u001b[A\n",
      "Training Epoch 22/25:   3%|▎         | 10/292 [00:07<02:39,  1.77batch/s, auc=0.8924, loss=0.9266]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 11/292 [00:07<02:38,  1.78batch/s, auc=0.8924, loss=0.9266]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 11/292 [00:08<02:38,  1.78batch/s, auc=0.8857, loss=0.9438]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 12/292 [00:08<03:20,  1.39batch/s, auc=0.8857, loss=0.9438]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 12/292 [00:08<03:20,  1.39batch/s, auc=0.8787, loss=0.8112]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 13/292 [00:08<03:06,  1.50batch/s, auc=0.8787, loss=0.8112]\u001b[A\n",
      "Training Epoch 22/25:   4%|▍         | 13/292 [00:09<03:06,  1.50batch/s, auc=0.8688, loss=1.0860]\u001b[A\n",
      "Training Epoch 22/25:   5%|▍         | 14/292 [00:09<03:39,  1.27batch/s, auc=0.8688, loss=1.0860]\u001b[A\n",
      "Training Epoch 22/25:   5%|▍         | 14/292 [00:10<03:39,  1.27batch/s, auc=0.8676, loss=0.5218]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 15/292 [00:10<03:19,  1.39batch/s, auc=0.8676, loss=0.5218]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 15/292 [00:10<03:19,  1.39batch/s, auc=0.8715, loss=0.5266]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 16/292 [00:10<03:04,  1.49batch/s, auc=0.8715, loss=0.5266]\u001b[A\n",
      "Training Epoch 22/25:   5%|▌         | 16/292 [00:11<03:04,  1.49batch/s, auc=0.8672, loss=0.8779]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 17/292 [00:11<03:37,  1.26batch/s, auc=0.8672, loss=0.8779]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 17/292 [00:12<03:37,  1.26batch/s, auc=0.8661, loss=0.6013]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 18/292 [00:12<03:17,  1.39batch/s, auc=0.8661, loss=0.6013]\u001b[A\n",
      "Training Epoch 22/25:   6%|▌         | 18/292 [00:12<03:17,  1.39batch/s, auc=0.8705, loss=0.5243]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 19/292 [00:12<03:02,  1.49batch/s, auc=0.8705, loss=0.5243]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 19/292 [00:13<03:02,  1.49batch/s, auc=0.8761, loss=0.5500]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 20/292 [00:13<02:52,  1.57batch/s, auc=0.8761, loss=0.5500]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 20/292 [00:14<02:52,  1.57batch/s, auc=0.8766, loss=0.7277]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 21/292 [00:14<02:45,  1.64batch/s, auc=0.8766, loss=0.7277]\u001b[A\n",
      "Training Epoch 22/25:   7%|▋         | 21/292 [00:15<02:45,  1.64batch/s, auc=0.8756, loss=0.7731]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 22/292 [00:15<03:22,  1.33batch/s, auc=0.8756, loss=0.7731]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 22/292 [00:15<03:22,  1.33batch/s, auc=0.8772, loss=0.5729]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 23/292 [00:15<03:05,  1.45batch/s, auc=0.8772, loss=0.5729]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 23/292 [00:16<03:05,  1.45batch/s, auc=0.8786, loss=0.4660]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 24/292 [00:16<02:54,  1.54batch/s, auc=0.8786, loss=0.4660]\u001b[A\n",
      "Training Epoch 22/25:   8%|▊         | 24/292 [00:16<02:54,  1.54batch/s, auc=0.8795, loss=0.6311]\u001b[A\n",
      "Training Epoch 22/25:   9%|▊         | 25/292 [00:16<02:45,  1.61batch/s, auc=0.8795, loss=0.6311]\u001b[A\n",
      "Training Epoch 22/25:   9%|▊         | 25/292 [00:17<02:45,  1.61batch/s, auc=0.8783, loss=0.7314]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 26/292 [00:17<02:40,  1.66batch/s, auc=0.8783, loss=0.7314]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 26/292 [00:18<02:40,  1.66batch/s, auc=0.8720, loss=1.1412]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 27/292 [00:18<03:17,  1.34batch/s, auc=0.8720, loss=1.1412]\u001b[A\n",
      "Training Epoch 22/25:   9%|▉         | 27/292 [00:19<03:17,  1.34batch/s, auc=0.8671, loss=0.9684]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 28/292 [00:19<03:42,  1.19batch/s, auc=0.8671, loss=0.9684]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 28/292 [00:20<03:42,  1.19batch/s, auc=0.8665, loss=0.7874]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 29/292 [00:20<03:59,  1.10batch/s, auc=0.8665, loss=0.7874]\u001b[A\n",
      "Training Epoch 22/25:  10%|▉         | 29/292 [00:21<03:59,  1.10batch/s, auc=0.8652, loss=0.7564]\u001b[A\n",
      "Training Epoch 22/25:  10%|█         | 30/292 [00:21<04:11,  1.04batch/s, auc=0.8652, loss=0.7564]\u001b[A\n",
      "Training Epoch 22/25:  10%|█         | 30/292 [00:22<04:11,  1.04batch/s, auc=0.8650, loss=0.8352]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 31/292 [00:22<03:38,  1.19batch/s, auc=0.8650, loss=0.8352]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 31/292 [00:22<03:38,  1.19batch/s, auc=0.8668, loss=0.5864]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 32/292 [00:22<03:15,  1.33batch/s, auc=0.8668, loss=0.5864]\u001b[A\n",
      "Training Epoch 22/25:  11%|█         | 32/292 [00:23<03:15,  1.33batch/s, auc=0.8683, loss=0.5237]\u001b[A\n",
      "Training Epoch 22/25:  11%|█▏        | 33/292 [00:23<02:59,  1.44batch/s, auc=0.8683, loss=0.5237]\u001b[A\n",
      "Training Epoch 22/25:  11%|█▏        | 33/292 [00:23<02:59,  1.44batch/s, auc=0.8675, loss=0.8841]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 34/292 [00:23<02:48,  1.53batch/s, auc=0.8675, loss=0.8841]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 34/292 [00:24<02:48,  1.53batch/s, auc=0.8668, loss=0.6607]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 35/292 [00:24<03:19,  1.29batch/s, auc=0.8668, loss=0.6607]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 35/292 [00:25<03:19,  1.29batch/s, auc=0.8686, loss=0.5935]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 36/292 [00:25<03:01,  1.41batch/s, auc=0.8686, loss=0.5935]\u001b[A\n",
      "Training Epoch 22/25:  12%|█▏        | 36/292 [00:26<03:01,  1.41batch/s, auc=0.8693, loss=0.6594]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 37/292 [00:26<02:49,  1.51batch/s, auc=0.8693, loss=0.6594]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 37/292 [00:26<02:49,  1.51batch/s, auc=0.8726, loss=0.4845]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 38/292 [00:26<02:40,  1.58batch/s, auc=0.8726, loss=0.4845]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 38/292 [00:27<02:40,  1.58batch/s, auc=0.8694, loss=1.0984]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 39/292 [00:27<03:13,  1.31batch/s, auc=0.8694, loss=1.0984]\u001b[A\n",
      "Training Epoch 22/25:  13%|█▎        | 39/292 [00:28<03:13,  1.31batch/s, auc=0.8722, loss=0.5711]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▎        | 40/292 [00:28<02:56,  1.43batch/s, auc=0.8722, loss=0.5711]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▎        | 40/292 [00:28<02:56,  1.43batch/s, auc=0.8730, loss=0.5348]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 41/292 [00:28<02:45,  1.52batch/s, auc=0.8730, loss=0.5348]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 41/292 [00:29<02:45,  1.52batch/s, auc=0.8742, loss=0.6653]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 42/292 [00:29<02:36,  1.59batch/s, auc=0.8742, loss=0.6653]\u001b[A\n",
      "Training Epoch 22/25:  14%|█▍        | 42/292 [00:29<02:36,  1.59batch/s, auc=0.8754, loss=0.5245]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▍        | 43/292 [00:29<02:30,  1.65batch/s, auc=0.8754, loss=0.5245]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▍        | 43/292 [00:30<02:30,  1.65batch/s, auc=0.8781, loss=0.6067]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 44/292 [00:30<02:26,  1.70batch/s, auc=0.8781, loss=0.6067]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 44/292 [00:31<02:26,  1.70batch/s, auc=0.8772, loss=0.7177]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 45/292 [00:31<02:23,  1.73batch/s, auc=0.8772, loss=0.7177]\u001b[A\n",
      "Training Epoch 22/25:  15%|█▌        | 45/292 [00:31<02:23,  1.73batch/s, auc=0.8774, loss=0.7411]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 46/292 [00:31<02:20,  1.75batch/s, auc=0.8774, loss=0.7411]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 46/292 [00:32<02:20,  1.75batch/s, auc=0.8789, loss=0.5368]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 47/292 [00:32<02:18,  1.76batch/s, auc=0.8789, loss=0.5368]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▌        | 47/292 [00:32<02:18,  1.76batch/s, auc=0.8776, loss=0.9585]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▋        | 48/292 [00:32<02:17,  1.78batch/s, auc=0.8776, loss=0.9585]\u001b[A\n",
      "Training Epoch 22/25:  16%|█▋        | 48/292 [00:33<02:17,  1.78batch/s, auc=0.8790, loss=0.5198]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 49/292 [00:33<02:16,  1.78batch/s, auc=0.8790, loss=0.5198]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 49/292 [00:33<02:16,  1.78batch/s, auc=0.8805, loss=0.6307]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 50/292 [00:33<02:15,  1.79batch/s, auc=0.8805, loss=0.6307]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 50/292 [00:34<02:15,  1.79batch/s, auc=0.8792, loss=0.8689]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 51/292 [00:34<02:51,  1.40batch/s, auc=0.8792, loss=0.8689]\u001b[A\n",
      "Training Epoch 22/25:  17%|█▋        | 51/292 [00:35<02:51,  1.40batch/s, auc=0.8766, loss=1.0332]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 52/292 [00:35<03:16,  1.22batch/s, auc=0.8766, loss=1.0332]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 52/292 [00:37<03:16,  1.22batch/s, auc=0.8757, loss=0.9177]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 53/292 [00:37<03:34,  1.12batch/s, auc=0.8757, loss=0.9177]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 53/292 [00:37<03:34,  1.12batch/s, auc=0.8776, loss=0.5156]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 54/292 [00:37<03:08,  1.26batch/s, auc=0.8776, loss=0.5156]\u001b[A\n",
      "Training Epoch 22/25:  18%|█▊        | 54/292 [00:38<03:08,  1.26batch/s, auc=0.8776, loss=0.6489]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 55/292 [00:38<02:51,  1.38batch/s, auc=0.8776, loss=0.6489]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 55/292 [00:38<02:51,  1.38batch/s, auc=0.8792, loss=0.5039]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 56/292 [00:38<02:38,  1.49batch/s, auc=0.8792, loss=0.5039]\u001b[A\n",
      "Training Epoch 22/25:  19%|█▉        | 56/292 [00:39<02:38,  1.49batch/s, auc=0.8797, loss=0.5450]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 57/292 [00:39<02:30,  1.57batch/s, auc=0.8797, loss=0.5450]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 57/292 [00:39<02:30,  1.57batch/s, auc=0.8798, loss=0.8235]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 58/292 [00:39<02:23,  1.63batch/s, auc=0.8798, loss=0.8235]\u001b[A\n",
      "Training Epoch 22/25:  20%|█▉        | 58/292 [00:40<02:23,  1.63batch/s, auc=0.8790, loss=0.8623]\u001b[A\n",
      "Training Epoch 22/25:  20%|██        | 59/292 [00:40<02:19,  1.67batch/s, auc=0.8790, loss=0.8623]\u001b[A\n",
      "Training Epoch 22/25:  20%|██        | 59/292 [00:40<02:19,  1.67batch/s, auc=0.8788, loss=0.8378]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 60/292 [00:40<02:15,  1.71batch/s, auc=0.8788, loss=0.8378]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 60/292 [00:41<02:15,  1.71batch/s, auc=0.8798, loss=0.6323]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 61/292 [00:41<02:13,  1.73batch/s, auc=0.8798, loss=0.6323]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 61/292 [00:42<02:13,  1.73batch/s, auc=0.8797, loss=0.6588]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 62/292 [00:42<02:11,  1.75batch/s, auc=0.8797, loss=0.6588]\u001b[A\n",
      "Training Epoch 22/25:  21%|██        | 62/292 [00:42<02:11,  1.75batch/s, auc=0.8807, loss=0.5354]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 63/292 [00:42<02:09,  1.76batch/s, auc=0.8807, loss=0.5354]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 63/292 [00:43<02:09,  1.76batch/s, auc=0.8818, loss=0.5307]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 64/292 [00:43<02:08,  1.77batch/s, auc=0.8818, loss=0.5307]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 64/292 [00:44<02:08,  1.77batch/s, auc=0.8800, loss=0.8729]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 65/292 [00:44<02:42,  1.40batch/s, auc=0.8800, loss=0.8729]\u001b[A\n",
      "Training Epoch 22/25:  22%|██▏       | 65/292 [00:44<02:42,  1.40batch/s, auc=0.8813, loss=0.4803]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 66/292 [00:44<02:31,  1.50batch/s, auc=0.8813, loss=0.4803]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 66/292 [00:45<02:31,  1.50batch/s, auc=0.8813, loss=0.7482]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 67/292 [00:45<02:22,  1.57batch/s, auc=0.8813, loss=0.7482]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 67/292 [00:45<02:22,  1.57batch/s, auc=0.8824, loss=0.4931]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 68/292 [00:45<02:16,  1.64batch/s, auc=0.8824, loss=0.4931]\u001b[A\n",
      "Training Epoch 22/25:  23%|██▎       | 68/292 [00:46<02:16,  1.64batch/s, auc=0.8829, loss=0.5210]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▎       | 69/292 [00:46<02:12,  1.68batch/s, auc=0.8829, loss=0.5210]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▎       | 69/292 [00:47<02:12,  1.68batch/s, auc=0.8812, loss=0.9069]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 70/292 [00:47<02:09,  1.71batch/s, auc=0.8812, loss=0.9069]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 70/292 [00:47<02:09,  1.71batch/s, auc=0.8786, loss=1.1343]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 71/292 [00:47<02:07,  1.74batch/s, auc=0.8786, loss=1.1343]\u001b[A\n",
      "Training Epoch 22/25:  24%|██▍       | 71/292 [00:48<02:07,  1.74batch/s, auc=0.8785, loss=0.8225]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▍       | 72/292 [00:48<02:05,  1.75batch/s, auc=0.8785, loss=0.8225]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▍       | 72/292 [00:48<02:05,  1.75batch/s, auc=0.8783, loss=0.5531]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 73/292 [00:48<02:04,  1.77batch/s, auc=0.8783, loss=0.5531]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 73/292 [00:49<02:04,  1.77batch/s, auc=0.8781, loss=0.6941]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 74/292 [00:49<02:02,  1.77batch/s, auc=0.8781, loss=0.6941]\u001b[A\n",
      "Training Epoch 22/25:  25%|██▌       | 74/292 [00:49<02:02,  1.77batch/s, auc=0.8791, loss=0.5671]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 75/292 [00:49<02:01,  1.78batch/s, auc=0.8791, loss=0.5671]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 75/292 [00:50<02:01,  1.78batch/s, auc=0.8785, loss=0.8626]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 76/292 [00:50<02:34,  1.40batch/s, auc=0.8785, loss=0.8626]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▌       | 76/292 [00:51<02:34,  1.40batch/s, auc=0.8786, loss=0.7055]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▋       | 77/292 [00:51<02:23,  1.50batch/s, auc=0.8786, loss=0.7055]\u001b[A\n",
      "Training Epoch 22/25:  26%|██▋       | 77/292 [00:52<02:23,  1.50batch/s, auc=0.8765, loss=1.1154]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 78/292 [00:52<02:49,  1.27batch/s, auc=0.8765, loss=1.1154]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 78/292 [00:53<02:49,  1.27batch/s, auc=0.8724, loss=1.7220]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 79/292 [00:53<03:06,  1.14batch/s, auc=0.8724, loss=1.7220]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 79/292 [00:54<03:06,  1.14batch/s, auc=0.8734, loss=0.6525]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 80/292 [00:54<02:45,  1.28batch/s, auc=0.8734, loss=0.6525]\u001b[A\n",
      "Training Epoch 22/25:  27%|██▋       | 80/292 [00:54<02:45,  1.28batch/s, auc=0.8725, loss=0.8959]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 81/292 [00:54<02:30,  1.40batch/s, auc=0.8725, loss=0.8959]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 81/292 [00:55<02:30,  1.40batch/s, auc=0.8722, loss=0.5028]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 82/292 [00:55<02:19,  1.50batch/s, auc=0.8722, loss=0.5028]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 82/292 [00:55<02:19,  1.50batch/s, auc=0.8728, loss=0.6771]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 83/292 [00:55<02:12,  1.58batch/s, auc=0.8728, loss=0.6771]\u001b[A\n",
      "Training Epoch 22/25:  28%|██▊       | 83/292 [00:56<02:12,  1.58batch/s, auc=0.8720, loss=0.9367]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 84/292 [00:56<02:39,  1.31batch/s, auc=0.8720, loss=0.9367]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 84/292 [00:57<02:39,  1.31batch/s, auc=0.8722, loss=0.6219]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 85/292 [00:57<02:25,  1.42batch/s, auc=0.8722, loss=0.6219]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 85/292 [00:57<02:25,  1.42batch/s, auc=0.8724, loss=0.7218]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 86/292 [00:57<02:15,  1.52batch/s, auc=0.8724, loss=0.7218]\u001b[A\n",
      "Training Epoch 22/25:  29%|██▉       | 86/292 [00:58<02:15,  1.52batch/s, auc=0.8718, loss=0.9485]\u001b[A\n",
      "Training Epoch 22/25:  30%|██▉       | 87/292 [00:58<02:08,  1.59batch/s, auc=0.8718, loss=0.9485]\u001b[A\n",
      "Training Epoch 22/25:  30%|██▉       | 87/292 [00:59<02:08,  1.59batch/s, auc=0.8733, loss=0.5482]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 88/292 [00:59<02:03,  1.65batch/s, auc=0.8733, loss=0.5482]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 88/292 [00:59<02:03,  1.65batch/s, auc=0.8732, loss=1.0697]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 89/292 [00:59<02:00,  1.69batch/s, auc=0.8732, loss=1.0697]\u001b[A\n",
      "Training Epoch 22/25:  30%|███       | 89/292 [01:00<02:00,  1.69batch/s, auc=0.8739, loss=0.5699]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 90/292 [01:00<01:57,  1.72batch/s, auc=0.8739, loss=0.5699]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 90/292 [01:00<01:57,  1.72batch/s, auc=0.8753, loss=0.5736]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 91/292 [01:00<01:55,  1.74batch/s, auc=0.8753, loss=0.5736]\u001b[A\n",
      "Training Epoch 22/25:  31%|███       | 91/292 [01:01<01:55,  1.74batch/s, auc=0.8756, loss=0.5628]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 92/292 [01:01<01:54,  1.75batch/s, auc=0.8756, loss=0.5628]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 92/292 [01:01<01:54,  1.75batch/s, auc=0.8765, loss=0.6450]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 93/292 [01:01<01:52,  1.77batch/s, auc=0.8765, loss=0.6450]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 93/292 [01:02<01:52,  1.77batch/s, auc=0.8753, loss=1.2072]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 94/292 [01:02<02:22,  1.39batch/s, auc=0.8753, loss=1.2072]\u001b[A\n",
      "Training Epoch 22/25:  32%|███▏      | 94/292 [01:04<02:22,  1.39batch/s, auc=0.8738, loss=1.0993]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 95/292 [01:04<02:42,  1.21batch/s, auc=0.8738, loss=1.0993]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 95/292 [01:05<02:42,  1.21batch/s, auc=0.8724, loss=1.0907]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 96/292 [01:05<02:56,  1.11batch/s, auc=0.8724, loss=1.0907]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 96/292 [01:05<02:56,  1.11batch/s, auc=0.8730, loss=0.6407]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 97/292 [01:05<02:35,  1.25batch/s, auc=0.8730, loss=0.6407]\u001b[A\n",
      "Training Epoch 22/25:  33%|███▎      | 97/292 [01:06<02:35,  1.25batch/s, auc=0.8738, loss=0.6959]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▎      | 98/292 [01:06<02:20,  1.38batch/s, auc=0.8738, loss=0.6959]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▎      | 98/292 [01:06<02:20,  1.38batch/s, auc=0.8740, loss=0.7434]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 99/292 [01:06<02:10,  1.48batch/s, auc=0.8740, loss=0.7434]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 99/292 [01:07<02:10,  1.48batch/s, auc=0.8734, loss=0.9183]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 100/292 [01:07<02:32,  1.26batch/s, auc=0.8734, loss=0.9183]\u001b[A\n",
      "Training Epoch 22/25:  34%|███▍      | 100/292 [01:08<02:32,  1.26batch/s, auc=0.8729, loss=0.9861]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 101/292 [01:08<02:18,  1.38batch/s, auc=0.8729, loss=0.9861]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 101/292 [01:08<02:18,  1.38batch/s, auc=0.8745, loss=0.6005]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 102/292 [01:08<02:08,  1.48batch/s, auc=0.8745, loss=0.6005]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▍      | 102/292 [01:09<02:08,  1.48batch/s, auc=0.8744, loss=0.8851]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▌      | 103/292 [01:09<02:00,  1.56batch/s, auc=0.8744, loss=0.8851]\u001b[A\n",
      "Training Epoch 22/25:  35%|███▌      | 103/292 [01:10<02:00,  1.56batch/s, auc=0.8745, loss=0.6100]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 104/292 [01:10<01:55,  1.63batch/s, auc=0.8745, loss=0.6100]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 104/292 [01:10<01:55,  1.63batch/s, auc=0.8745, loss=0.7949]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 105/292 [01:10<01:51,  1.67batch/s, auc=0.8745, loss=0.7949]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▌      | 105/292 [01:11<01:51,  1.67batch/s, auc=0.8751, loss=0.8207]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▋      | 106/292 [01:11<01:49,  1.70batch/s, auc=0.8751, loss=0.8207]\u001b[A\n",
      "Training Epoch 22/25:  36%|███▋      | 106/292 [01:11<01:49,  1.70batch/s, auc=0.8752, loss=0.6965]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 107/292 [01:11<01:47,  1.73batch/s, auc=0.8752, loss=0.6965]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 107/292 [01:12<01:47,  1.73batch/s, auc=0.8754, loss=0.8388]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 108/292 [01:12<01:45,  1.75batch/s, auc=0.8754, loss=0.8388]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 108/292 [01:12<01:45,  1.75batch/s, auc=0.8751, loss=0.6932]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 109/292 [01:12<01:43,  1.76batch/s, auc=0.8751, loss=0.6932]\u001b[A\n",
      "Training Epoch 22/25:  37%|███▋      | 109/292 [01:13<01:43,  1.76batch/s, auc=0.8755, loss=0.7716]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 110/292 [01:13<01:42,  1.77batch/s, auc=0.8755, loss=0.7716]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 110/292 [01:14<01:42,  1.77batch/s, auc=0.8763, loss=0.6849]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 111/292 [01:14<01:41,  1.77batch/s, auc=0.8763, loss=0.6849]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 111/292 [01:14<01:41,  1.77batch/s, auc=0.8761, loss=0.7645]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 112/292 [01:14<01:41,  1.78batch/s, auc=0.8761, loss=0.7645]\u001b[A\n",
      "Training Epoch 22/25:  38%|███▊      | 112/292 [01:15<01:41,  1.78batch/s, auc=0.8764, loss=0.6187]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▊      | 113/292 [01:15<01:40,  1.78batch/s, auc=0.8764, loss=0.6187]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▊      | 113/292 [01:16<01:40,  1.78batch/s, auc=0.8749, loss=1.3220]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 114/292 [01:16<02:07,  1.40batch/s, auc=0.8749, loss=1.3220]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 114/292 [01:16<02:07,  1.40batch/s, auc=0.8752, loss=0.6616]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 115/292 [01:16<01:58,  1.50batch/s, auc=0.8752, loss=0.6616]\u001b[A\n",
      "Training Epoch 22/25:  39%|███▉      | 115/292 [01:17<01:58,  1.50batch/s, auc=0.8748, loss=0.7403]\u001b[A\n",
      "Training Epoch 22/25:  40%|███▉      | 116/292 [01:17<01:51,  1.57batch/s, auc=0.8748, loss=0.7403]\u001b[A\n",
      "Training Epoch 22/25:  40%|███▉      | 116/292 [01:17<01:51,  1.57batch/s, auc=0.8751, loss=0.6446]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 117/292 [01:17<01:47,  1.63batch/s, auc=0.8751, loss=0.6446]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 117/292 [01:18<01:47,  1.63batch/s, auc=0.8748, loss=0.7810]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 118/292 [01:18<01:43,  1.68batch/s, auc=0.8748, loss=0.7810]\u001b[A\n",
      "Training Epoch 22/25:  40%|████      | 118/292 [01:19<01:43,  1.68batch/s, auc=0.8745, loss=0.7282]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 119/292 [01:19<01:41,  1.71batch/s, auc=0.8745, loss=0.7282]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 119/292 [01:19<01:41,  1.71batch/s, auc=0.8744, loss=0.6083]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 120/292 [01:19<01:39,  1.73batch/s, auc=0.8744, loss=0.6083]\u001b[A\n",
      "Training Epoch 22/25:  41%|████      | 120/292 [01:20<01:39,  1.73batch/s, auc=0.8749, loss=0.5564]\u001b[A\n",
      "Training Epoch 22/25:  41%|████▏     | 121/292 [01:20<01:37,  1.75batch/s, auc=0.8749, loss=0.5564]\u001b[A\n",
      "Training Epoch 22/25:  41%|████▏     | 121/292 [01:20<01:37,  1.75batch/s, auc=0.8752, loss=0.7422]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 122/292 [01:20<01:36,  1.76batch/s, auc=0.8752, loss=0.7422]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 122/292 [01:21<01:36,  1.76batch/s, auc=0.8753, loss=0.6604]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 123/292 [01:21<01:35,  1.77batch/s, auc=0.8753, loss=0.6604]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 123/292 [01:21<01:35,  1.77batch/s, auc=0.8753, loss=0.7720]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 124/292 [01:21<01:34,  1.78batch/s, auc=0.8753, loss=0.7720]\u001b[A\n",
      "Training Epoch 22/25:  42%|████▏     | 124/292 [01:22<01:34,  1.78batch/s, auc=0.8754, loss=0.7162]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 125/292 [01:22<01:33,  1.78batch/s, auc=0.8754, loss=0.7162]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 125/292 [01:22<01:33,  1.78batch/s, auc=0.8760, loss=0.5987]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 126/292 [01:22<01:33,  1.78batch/s, auc=0.8760, loss=0.5987]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 126/292 [01:23<01:33,  1.78batch/s, auc=0.8747, loss=1.2472]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 127/292 [01:23<01:58,  1.40batch/s, auc=0.8747, loss=1.2472]\u001b[A\n",
      "Training Epoch 22/25:  43%|████▎     | 127/292 [01:24<01:58,  1.40batch/s, auc=0.8753, loss=0.5301]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 128/292 [01:24<01:49,  1.49batch/s, auc=0.8753, loss=0.5301]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 128/292 [01:25<01:49,  1.49batch/s, auc=0.8754, loss=0.5921]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 129/292 [01:25<01:43,  1.57batch/s, auc=0.8754, loss=0.5921]\u001b[A\n",
      "Training Epoch 22/25:  44%|████▍     | 129/292 [01:25<01:43,  1.57batch/s, auc=0.8757, loss=0.6731]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 130/292 [01:25<01:39,  1.63batch/s, auc=0.8757, loss=0.6731]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 130/292 [01:26<01:39,  1.63batch/s, auc=0.8759, loss=0.6024]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 131/292 [01:26<01:36,  1.67batch/s, auc=0.8759, loss=0.6024]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▍     | 131/292 [01:26<01:36,  1.67batch/s, auc=0.8761, loss=0.6043]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▌     | 132/292 [01:26<01:33,  1.71batch/s, auc=0.8761, loss=0.6043]\u001b[A\n",
      "Training Epoch 22/25:  45%|████▌     | 132/292 [01:27<01:33,  1.71batch/s, auc=0.8764, loss=0.5859]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 133/292 [01:27<01:31,  1.73batch/s, auc=0.8764, loss=0.5859]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 133/292 [01:28<01:31,  1.73batch/s, auc=0.8759, loss=0.9007]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 134/292 [01:28<01:54,  1.37batch/s, auc=0.8759, loss=0.9007]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 134/292 [01:28<01:54,  1.37batch/s, auc=0.8759, loss=0.6521]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 135/292 [01:28<01:46,  1.48batch/s, auc=0.8759, loss=0.6521]\u001b[A\n",
      "Training Epoch 22/25:  46%|████▌     | 135/292 [01:29<01:46,  1.48batch/s, auc=0.8762, loss=0.5679]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 136/292 [01:29<01:40,  1.56batch/s, auc=0.8762, loss=0.5679]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 136/292 [01:30<01:40,  1.56batch/s, auc=0.8752, loss=1.1521]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 137/292 [01:30<01:59,  1.30batch/s, auc=0.8752, loss=1.1521]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 137/292 [01:31<01:59,  1.30batch/s, auc=0.8752, loss=0.8422]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 138/292 [01:31<02:12,  1.16batch/s, auc=0.8752, loss=0.8422]\u001b[A\n",
      "Training Epoch 22/25:  47%|████▋     | 138/292 [01:32<02:12,  1.16batch/s, auc=0.8757, loss=0.6759]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 139/292 [01:32<01:58,  1.30batch/s, auc=0.8757, loss=0.6759]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 139/292 [01:32<01:58,  1.30batch/s, auc=0.8759, loss=0.7145]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 140/292 [01:32<01:47,  1.41batch/s, auc=0.8759, loss=0.7145]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 140/292 [01:33<01:47,  1.41batch/s, auc=0.8762, loss=0.5812]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 141/292 [01:33<01:40,  1.51batch/s, auc=0.8762, loss=0.5812]\u001b[A\n",
      "Training Epoch 22/25:  48%|████▊     | 141/292 [01:33<01:40,  1.51batch/s, auc=0.8763, loss=0.7443]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▊     | 142/292 [01:33<01:34,  1.58batch/s, auc=0.8763, loss=0.7443]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▊     | 142/292 [01:34<01:34,  1.58batch/s, auc=0.8764, loss=0.6001]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 143/292 [01:34<01:30,  1.64batch/s, auc=0.8764, loss=0.6001]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 143/292 [01:35<01:30,  1.64batch/s, auc=0.8765, loss=0.6659]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 144/292 [01:35<01:28,  1.68batch/s, auc=0.8765, loss=0.6659]\u001b[A\n",
      "Training Epoch 22/25:  49%|████▉     | 144/292 [01:35<01:28,  1.68batch/s, auc=0.8769, loss=0.6902]\u001b[A\n",
      "Training Epoch 22/25:  50%|████▉     | 145/292 [01:35<01:26,  1.71batch/s, auc=0.8769, loss=0.6902]\u001b[A\n",
      "Training Epoch 22/25:  50%|████▉     | 145/292 [01:36<01:26,  1.71batch/s, auc=0.8773, loss=0.7135]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 146/292 [01:36<01:24,  1.73batch/s, auc=0.8773, loss=0.7135]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 146/292 [01:36<01:24,  1.73batch/s, auc=0.8775, loss=0.5892]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 147/292 [01:36<01:22,  1.75batch/s, auc=0.8775, loss=0.5892]\u001b[A\n",
      "Training Epoch 22/25:  50%|█████     | 147/292 [01:37<01:22,  1.75batch/s, auc=0.8774, loss=0.7275]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 148/292 [01:37<01:21,  1.76batch/s, auc=0.8774, loss=0.7275]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 148/292 [01:37<01:21,  1.76batch/s, auc=0.8780, loss=0.5646]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 149/292 [01:37<01:21,  1.76batch/s, auc=0.8780, loss=0.5646]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████     | 149/292 [01:38<01:21,  1.76batch/s, auc=0.8772, loss=0.9946]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████▏    | 150/292 [01:38<01:42,  1.39batch/s, auc=0.8772, loss=0.9946]\u001b[A\n",
      "Training Epoch 22/25:  51%|█████▏    | 150/292 [01:39<01:42,  1.39batch/s, auc=0.8775, loss=0.4821]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 151/292 [01:39<01:34,  1.49batch/s, auc=0.8775, loss=0.4821]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 151/292 [01:40<01:34,  1.49batch/s, auc=0.8775, loss=0.7627]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 152/292 [01:40<01:29,  1.57batch/s, auc=0.8775, loss=0.7627]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 152/292 [01:40<01:29,  1.57batch/s, auc=0.8776, loss=0.7417]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 153/292 [01:40<01:25,  1.63batch/s, auc=0.8776, loss=0.7417]\u001b[A\n",
      "Training Epoch 22/25:  52%|█████▏    | 153/292 [01:41<01:25,  1.63batch/s, auc=0.8778, loss=0.7089]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 154/292 [01:41<01:22,  1.67batch/s, auc=0.8778, loss=0.7089]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 154/292 [01:41<01:22,  1.67batch/s, auc=0.8780, loss=0.5841]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 155/292 [01:41<01:20,  1.71batch/s, auc=0.8780, loss=0.5841]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 155/292 [01:42<01:20,  1.71batch/s, auc=0.8784, loss=0.8305]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 156/292 [01:42<01:18,  1.73batch/s, auc=0.8784, loss=0.8305]\u001b[A\n",
      "Training Epoch 22/25:  53%|█████▎    | 156/292 [01:42<01:18,  1.73batch/s, auc=0.8792, loss=0.4650]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 157/292 [01:42<01:17,  1.75batch/s, auc=0.8792, loss=0.4650]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 157/292 [01:43<01:17,  1.75batch/s, auc=0.8788, loss=0.8908]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 158/292 [01:43<01:37,  1.38batch/s, auc=0.8788, loss=0.8908]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 158/292 [01:44<01:37,  1.38batch/s, auc=0.8788, loss=0.6877]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 159/292 [01:44<01:29,  1.48batch/s, auc=0.8788, loss=0.6877]\u001b[A\n",
      "Training Epoch 22/25:  54%|█████▍    | 159/292 [01:45<01:29,  1.48batch/s, auc=0.8794, loss=0.5019]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▍    | 160/292 [01:45<01:24,  1.56batch/s, auc=0.8794, loss=0.5019]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▍    | 160/292 [01:45<01:24,  1.56batch/s, auc=0.8794, loss=0.6174]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 161/292 [01:45<01:20,  1.62batch/s, auc=0.8794, loss=0.6174]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 161/292 [01:46<01:20,  1.62batch/s, auc=0.8795, loss=0.5633]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 162/292 [01:46<01:17,  1.67batch/s, auc=0.8795, loss=0.5633]\u001b[A\n",
      "Training Epoch 22/25:  55%|█████▌    | 162/292 [01:46<01:17,  1.67batch/s, auc=0.8795, loss=0.6095]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 163/292 [01:46<01:15,  1.70batch/s, auc=0.8795, loss=0.6095]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 163/292 [01:47<01:15,  1.70batch/s, auc=0.8801, loss=0.4529]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 164/292 [01:47<01:14,  1.72batch/s, auc=0.8801, loss=0.4529]\u001b[A\n",
      "Training Epoch 22/25:  56%|█████▌    | 164/292 [01:47<01:14,  1.72batch/s, auc=0.8805, loss=0.7002]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 165/292 [01:47<01:12,  1.74batch/s, auc=0.8805, loss=0.7002]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 165/292 [01:48<01:12,  1.74batch/s, auc=0.8797, loss=1.0716]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 166/292 [01:48<01:31,  1.38batch/s, auc=0.8797, loss=1.0716]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 166/292 [01:49<01:31,  1.38batch/s, auc=0.8800, loss=0.5202]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 167/292 [01:49<01:24,  1.48batch/s, auc=0.8800, loss=0.5202]\u001b[A\n",
      "Training Epoch 22/25:  57%|█████▋    | 167/292 [01:50<01:24,  1.48batch/s, auc=0.8806, loss=0.4948]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 168/292 [01:50<01:19,  1.56batch/s, auc=0.8806, loss=0.4948]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 168/292 [01:51<01:19,  1.56batch/s, auc=0.8806, loss=0.8382]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 169/292 [01:51<01:34,  1.30batch/s, auc=0.8806, loss=0.8382]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 169/292 [01:51<01:34,  1.30batch/s, auc=0.8803, loss=0.7731]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 170/292 [01:51<01:26,  1.41batch/s, auc=0.8803, loss=0.7731]\u001b[A\n",
      "Training Epoch 22/25:  58%|█████▊    | 170/292 [01:52<01:26,  1.41batch/s, auc=0.8805, loss=0.5476]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▊    | 171/292 [01:52<01:20,  1.51batch/s, auc=0.8805, loss=0.5476]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▊    | 171/292 [01:52<01:20,  1.51batch/s, auc=0.8804, loss=0.8582]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 172/292 [01:52<01:15,  1.58batch/s, auc=0.8804, loss=0.8582]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 172/292 [01:53<01:15,  1.58batch/s, auc=0.8803, loss=0.7969]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 173/292 [01:53<01:12,  1.64batch/s, auc=0.8803, loss=0.7969]\u001b[A\n",
      "Training Epoch 22/25:  59%|█████▉    | 173/292 [01:54<01:12,  1.64batch/s, auc=0.8805, loss=0.7113]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 174/292 [01:54<01:28,  1.33batch/s, auc=0.8805, loss=0.7113]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 174/292 [01:55<01:28,  1.33batch/s, auc=0.8809, loss=0.6144]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 175/292 [01:55<01:21,  1.44batch/s, auc=0.8809, loss=0.6144]\u001b[A\n",
      "Training Epoch 22/25:  60%|█████▉    | 175/292 [01:55<01:21,  1.44batch/s, auc=0.8816, loss=0.4685]\u001b[A\n",
      "Training Epoch 22/25:  60%|██████    | 176/292 [01:55<01:15,  1.53batch/s, auc=0.8816, loss=0.4685]\u001b[A\n",
      "Training Epoch 22/25:  60%|██████    | 176/292 [01:56<01:15,  1.53batch/s, auc=0.8819, loss=0.5123]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 177/292 [01:56<01:12,  1.60batch/s, auc=0.8819, loss=0.5123]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 177/292 [01:56<01:12,  1.60batch/s, auc=0.8824, loss=0.3981]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 178/292 [01:56<01:09,  1.65batch/s, auc=0.8824, loss=0.3981]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████    | 178/292 [01:57<01:09,  1.65batch/s, auc=0.8823, loss=0.7717]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████▏   | 179/292 [01:57<01:06,  1.69batch/s, auc=0.8823, loss=0.7717]\u001b[A\n",
      "Training Epoch 22/25:  61%|██████▏   | 179/292 [01:57<01:06,  1.69batch/s, auc=0.8823, loss=0.9348]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 180/292 [01:57<01:05,  1.72batch/s, auc=0.8823, loss=0.9348]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 180/292 [01:58<01:05,  1.72batch/s, auc=0.8826, loss=0.6140]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 181/292 [01:58<01:03,  1.74batch/s, auc=0.8826, loss=0.6140]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 181/292 [01:58<01:03,  1.74batch/s, auc=0.8822, loss=0.7285]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 182/292 [01:58<01:02,  1.75batch/s, auc=0.8822, loss=0.7285]\u001b[A\n",
      "Training Epoch 22/25:  62%|██████▏   | 182/292 [01:59<01:02,  1.75batch/s, auc=0.8820, loss=0.6813]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 183/292 [01:59<01:01,  1.76batch/s, auc=0.8820, loss=0.6813]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 183/292 [02:00<01:01,  1.76batch/s, auc=0.8818, loss=0.8835]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 184/292 [02:00<01:17,  1.39batch/s, auc=0.8818, loss=0.8835]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 184/292 [02:01<01:17,  1.39batch/s, auc=0.8819, loss=0.6073]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 185/292 [02:01<01:11,  1.49batch/s, auc=0.8819, loss=0.6073]\u001b[A\n",
      "Training Epoch 22/25:  63%|██████▎   | 185/292 [02:01<01:11,  1.49batch/s, auc=0.8819, loss=0.6681]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▎   | 186/292 [02:01<01:07,  1.56batch/s, auc=0.8819, loss=0.6681]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▎   | 186/292 [02:02<01:07,  1.56batch/s, auc=0.8822, loss=0.6668]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 187/292 [02:02<01:04,  1.62batch/s, auc=0.8822, loss=0.6668]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 187/292 [02:02<01:04,  1.62batch/s, auc=0.8821, loss=0.9186]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 188/292 [02:02<01:02,  1.67batch/s, auc=0.8821, loss=0.9186]\u001b[A\n",
      "Training Epoch 22/25:  64%|██████▍   | 188/292 [02:03<01:02,  1.67batch/s, auc=0.8823, loss=0.5537]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▍   | 189/292 [02:03<01:00,  1.70batch/s, auc=0.8823, loss=0.5537]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▍   | 189/292 [02:03<01:00,  1.70batch/s, auc=0.8827, loss=0.5006]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 190/292 [02:03<00:59,  1.72batch/s, auc=0.8827, loss=0.5006]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 190/292 [02:05<00:59,  1.72batch/s, auc=0.8824, loss=0.8483]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 191/292 [02:05<01:13,  1.37batch/s, auc=0.8824, loss=0.8483]\u001b[A\n",
      "Training Epoch 22/25:  65%|██████▌   | 191/292 [02:05<01:13,  1.37batch/s, auc=0.8823, loss=0.7481]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 192/292 [02:05<01:07,  1.47batch/s, auc=0.8823, loss=0.7481]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 192/292 [02:06<01:07,  1.47batch/s, auc=0.8819, loss=0.9795]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 193/292 [02:06<01:19,  1.25batch/s, auc=0.8819, loss=0.9795]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▌   | 193/292 [02:07<01:19,  1.25batch/s, auc=0.8819, loss=0.6748]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▋   | 194/292 [02:07<01:11,  1.37batch/s, auc=0.8819, loss=0.6748]\u001b[A\n",
      "Training Epoch 22/25:  66%|██████▋   | 194/292 [02:08<01:11,  1.37batch/s, auc=0.8811, loss=1.2239]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 195/292 [02:08<01:20,  1.20batch/s, auc=0.8811, loss=1.2239]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 195/292 [02:09<01:20,  1.20batch/s, auc=0.8803, loss=1.1297]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 196/292 [02:09<01:27,  1.10batch/s, auc=0.8803, loss=1.1297]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 196/292 [02:10<01:27,  1.10batch/s, auc=0.8799, loss=0.9556]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 197/292 [02:10<01:31,  1.04batch/s, auc=0.8799, loss=0.9556]\u001b[A\n",
      "Training Epoch 22/25:  67%|██████▋   | 197/292 [02:11<01:31,  1.04batch/s, auc=0.8801, loss=0.5873]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 198/292 [02:11<01:18,  1.19batch/s, auc=0.8801, loss=0.5873]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 198/292 [02:11<01:18,  1.19batch/s, auc=0.8803, loss=0.5097]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 199/292 [02:11<01:10,  1.32batch/s, auc=0.8803, loss=0.5097]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 199/292 [02:12<01:10,  1.32batch/s, auc=0.8805, loss=0.6998]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 200/292 [02:12<01:04,  1.43batch/s, auc=0.8805, loss=0.6998]\u001b[A\n",
      "Training Epoch 22/25:  68%|██████▊   | 200/292 [02:12<01:04,  1.43batch/s, auc=0.8806, loss=0.7428]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 201/292 [02:12<00:59,  1.52batch/s, auc=0.8806, loss=0.7428]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 201/292 [02:13<00:59,  1.52batch/s, auc=0.8806, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 202/292 [02:13<00:56,  1.59batch/s, auc=0.8806, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  69%|██████▉   | 202/292 [02:13<00:56,  1.59batch/s, auc=0.8805, loss=0.8386]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 203/292 [02:13<00:54,  1.64batch/s, auc=0.8805, loss=0.8386]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 203/292 [02:14<00:54,  1.64batch/s, auc=0.8806, loss=0.6693]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 204/292 [02:14<00:52,  1.68batch/s, auc=0.8806, loss=0.6693]\u001b[A\n",
      "Training Epoch 22/25:  70%|██████▉   | 204/292 [02:14<00:52,  1.68batch/s, auc=0.8807, loss=0.6207]\u001b[A\n",
      "Training Epoch 22/25:  70%|███████   | 205/292 [02:14<00:50,  1.71batch/s, auc=0.8807, loss=0.6207]\u001b[A\n",
      "Training Epoch 22/25:  70%|███████   | 205/292 [02:15<00:50,  1.71batch/s, auc=0.8806, loss=0.7404]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 206/292 [02:15<00:49,  1.73batch/s, auc=0.8806, loss=0.7404]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 206/292 [02:16<00:49,  1.73batch/s, auc=0.8798, loss=1.1224]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 207/292 [02:16<01:01,  1.37batch/s, auc=0.8798, loss=1.1224]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 207/292 [02:17<01:01,  1.37batch/s, auc=0.8798, loss=0.7387]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 208/292 [02:17<00:56,  1.48batch/s, auc=0.8798, loss=0.7387]\u001b[A\n",
      "Training Epoch 22/25:  71%|███████   | 208/292 [02:18<00:56,  1.48batch/s, auc=0.8793, loss=0.9597]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 209/292 [02:18<01:06,  1.25batch/s, auc=0.8793, loss=0.9597]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 209/292 [02:18<01:06,  1.25batch/s, auc=0.8791, loss=0.7684]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 210/292 [02:18<00:59,  1.38batch/s, auc=0.8791, loss=0.7684]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 210/292 [02:19<00:59,  1.38batch/s, auc=0.8789, loss=0.8620]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 211/292 [02:19<01:07,  1.20batch/s, auc=0.8789, loss=0.8620]\u001b[A\n",
      "Training Epoch 22/25:  72%|███████▏  | 211/292 [02:20<01:07,  1.20batch/s, auc=0.8789, loss=0.6544]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 212/292 [02:20<01:00,  1.33batch/s, auc=0.8789, loss=0.6544]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 212/292 [02:20<01:00,  1.33batch/s, auc=0.8789, loss=0.8157]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 213/292 [02:20<00:54,  1.44batch/s, auc=0.8789, loss=0.8157]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 213/292 [02:21<00:54,  1.44batch/s, auc=0.8791, loss=0.5767]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 214/292 [02:21<00:51,  1.53batch/s, auc=0.8791, loss=0.5767]\u001b[A\n",
      "Training Epoch 22/25:  73%|███████▎  | 214/292 [02:22<00:51,  1.53batch/s, auc=0.8789, loss=0.6165]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▎  | 215/292 [02:22<00:48,  1.60batch/s, auc=0.8789, loss=0.6165]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▎  | 215/292 [02:22<00:48,  1.60batch/s, auc=0.8789, loss=0.7249]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 216/292 [02:22<00:46,  1.65batch/s, auc=0.8789, loss=0.7249]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 216/292 [02:23<00:46,  1.65batch/s, auc=0.8790, loss=0.7605]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 217/292 [02:23<00:44,  1.69batch/s, auc=0.8790, loss=0.7605]\u001b[A\n",
      "Training Epoch 22/25:  74%|███████▍  | 217/292 [02:24<00:44,  1.69batch/s, auc=0.8783, loss=1.3882]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▍  | 218/292 [02:24<00:54,  1.35batch/s, auc=0.8783, loss=1.3882]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▍  | 218/292 [02:24<00:54,  1.35batch/s, auc=0.8781, loss=0.8080]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 219/292 [02:24<00:50,  1.46batch/s, auc=0.8781, loss=0.8080]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 219/292 [02:25<00:50,  1.46batch/s, auc=0.8784, loss=0.7226]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 220/292 [02:25<00:46,  1.54batch/s, auc=0.8784, loss=0.7226]\u001b[A\n",
      "Training Epoch 22/25:  75%|███████▌  | 220/292 [02:25<00:46,  1.54batch/s, auc=0.8787, loss=0.5471]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 221/292 [02:25<00:44,  1.60batch/s, auc=0.8787, loss=0.5471]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 221/292 [02:26<00:44,  1.60batch/s, auc=0.8787, loss=0.6136]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 222/292 [02:26<00:42,  1.65batch/s, auc=0.8787, loss=0.6136]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▌  | 222/292 [02:27<00:42,  1.65batch/s, auc=0.8791, loss=0.4594]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▋  | 223/292 [02:27<00:40,  1.69batch/s, auc=0.8791, loss=0.4594]\u001b[A\n",
      "Training Epoch 22/25:  76%|███████▋  | 223/292 [02:27<00:40,  1.69batch/s, auc=0.8793, loss=0.5444]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 224/292 [02:27<00:39,  1.72batch/s, auc=0.8793, loss=0.5444]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 224/292 [02:28<00:39,  1.72batch/s, auc=0.8795, loss=0.6879]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 225/292 [02:28<00:38,  1.73batch/s, auc=0.8795, loss=0.6879]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 225/292 [02:28<00:38,  1.73batch/s, auc=0.8798, loss=0.5184]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 226/292 [02:28<00:37,  1.75batch/s, auc=0.8798, loss=0.5184]\u001b[A\n",
      "Training Epoch 22/25:  77%|███████▋  | 226/292 [02:29<00:37,  1.75batch/s, auc=0.8793, loss=1.0940]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 227/292 [02:29<00:47,  1.38batch/s, auc=0.8793, loss=1.0940]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 227/292 [02:30<00:47,  1.38batch/s, auc=0.8792, loss=0.7120]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 228/292 [02:30<00:43,  1.48batch/s, auc=0.8792, loss=0.7120]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 228/292 [02:31<00:43,  1.48batch/s, auc=0.8794, loss=0.7084]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 229/292 [02:31<00:40,  1.56batch/s, auc=0.8794, loss=0.7084]\u001b[A\n",
      "Training Epoch 22/25:  78%|███████▊  | 229/292 [02:31<00:40,  1.56batch/s, auc=0.8792, loss=0.8265]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 230/292 [02:31<00:38,  1.62batch/s, auc=0.8792, loss=0.8265]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 230/292 [02:32<00:38,  1.62batch/s, auc=0.8794, loss=0.5460]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 231/292 [02:32<00:36,  1.66batch/s, auc=0.8794, loss=0.5460]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 231/292 [02:32<00:36,  1.66batch/s, auc=0.8795, loss=0.8440]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 232/292 [02:32<00:35,  1.70batch/s, auc=0.8795, loss=0.8440]\u001b[A\n",
      "Training Epoch 22/25:  79%|███████▉  | 232/292 [02:33<00:35,  1.70batch/s, auc=0.8796, loss=0.6483]\u001b[A\n",
      "Training Epoch 22/25:  80%|███████▉  | 233/292 [02:33<00:34,  1.72batch/s, auc=0.8796, loss=0.6483]\u001b[A\n",
      "Training Epoch 22/25:  80%|███████▉  | 233/292 [02:33<00:34,  1.72batch/s, auc=0.8799, loss=0.5451]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 234/292 [02:33<00:33,  1.74batch/s, auc=0.8799, loss=0.5451]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 234/292 [02:34<00:33,  1.74batch/s, auc=0.8802, loss=0.5651]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 235/292 [02:34<00:32,  1.75batch/s, auc=0.8802, loss=0.5651]\u001b[A\n",
      "Training Epoch 22/25:  80%|████████  | 235/292 [02:34<00:32,  1.75batch/s, auc=0.8803, loss=0.5991]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 236/292 [02:34<00:31,  1.76batch/s, auc=0.8803, loss=0.5991]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 236/292 [02:35<00:31,  1.76batch/s, auc=0.8807, loss=0.7059]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 237/292 [02:35<00:31,  1.76batch/s, auc=0.8807, loss=0.7059]\u001b[A\n",
      "Training Epoch 22/25:  81%|████████  | 237/292 [02:36<00:31,  1.76batch/s, auc=0.8807, loss=0.6918]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 238/292 [02:36<00:30,  1.77batch/s, auc=0.8807, loss=0.6918]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 238/292 [02:37<00:30,  1.77batch/s, auc=0.8804, loss=0.9045]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 239/292 [02:37<00:38,  1.39batch/s, auc=0.8804, loss=0.9045]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 239/292 [02:37<00:38,  1.39batch/s, auc=0.8805, loss=0.6022]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 240/292 [02:37<00:34,  1.49batch/s, auc=0.8805, loss=0.6022]\u001b[A\n",
      "Training Epoch 22/25:  82%|████████▏ | 240/292 [02:38<00:34,  1.49batch/s, auc=0.8807, loss=0.6345]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 241/292 [02:38<00:32,  1.56batch/s, auc=0.8807, loss=0.6345]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 241/292 [02:38<00:32,  1.56batch/s, auc=0.8807, loss=0.6422]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 242/292 [02:38<00:30,  1.62batch/s, auc=0.8807, loss=0.6422]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 242/292 [02:39<00:30,  1.62batch/s, auc=0.8809, loss=0.5933]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 243/292 [02:39<00:29,  1.66batch/s, auc=0.8809, loss=0.5933]\u001b[A\n",
      "Training Epoch 22/25:  83%|████████▎ | 243/292 [02:39<00:29,  1.66batch/s, auc=0.8809, loss=0.7249]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▎ | 244/292 [02:39<00:28,  1.70batch/s, auc=0.8809, loss=0.7249]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▎ | 244/292 [02:40<00:28,  1.70batch/s, auc=0.8810, loss=0.5650]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 245/292 [02:40<00:27,  1.72batch/s, auc=0.8810, loss=0.5650]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 245/292 [02:41<00:27,  1.72batch/s, auc=0.8804, loss=1.1756]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 246/292 [02:41<00:33,  1.37batch/s, auc=0.8804, loss=1.1756]\u001b[A\n",
      "Training Epoch 22/25:  84%|████████▍ | 246/292 [02:42<00:33,  1.37batch/s, auc=0.8805, loss=0.6145]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 247/292 [02:42<00:30,  1.47batch/s, auc=0.8805, loss=0.6145]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 247/292 [02:43<00:30,  1.47batch/s, auc=0.8804, loss=0.7849]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 248/292 [02:43<00:35,  1.25batch/s, auc=0.8804, loss=0.7849]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▍ | 248/292 [02:43<00:35,  1.25batch/s, auc=0.8805, loss=0.7275]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▌ | 249/292 [02:43<00:31,  1.37batch/s, auc=0.8805, loss=0.7275]\u001b[A\n",
      "Training Epoch 22/25:  85%|████████▌ | 249/292 [02:44<00:31,  1.37batch/s, auc=0.8804, loss=0.7323]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 250/292 [02:44<00:28,  1.47batch/s, auc=0.8804, loss=0.7323]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 250/292 [02:44<00:28,  1.47batch/s, auc=0.8809, loss=0.4993]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 251/292 [02:44<00:26,  1.55batch/s, auc=0.8809, loss=0.4993]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▌ | 251/292 [02:45<00:26,  1.55batch/s, auc=0.8807, loss=0.7871]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▋ | 252/292 [02:45<00:24,  1.61batch/s, auc=0.8807, loss=0.7871]\u001b[A\n",
      "Training Epoch 22/25:  86%|████████▋ | 252/292 [02:46<00:24,  1.61batch/s, auc=0.8808, loss=0.5945]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 253/292 [02:46<00:23,  1.66batch/s, auc=0.8808, loss=0.5945]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 253/292 [02:47<00:23,  1.66batch/s, auc=0.8805, loss=1.0293]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 254/292 [02:47<00:28,  1.34batch/s, auc=0.8805, loss=1.0293]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 254/292 [02:48<00:28,  1.34batch/s, auc=0.8802, loss=0.9444]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 255/292 [02:48<00:31,  1.18batch/s, auc=0.8802, loss=0.9444]\u001b[A\n",
      "Training Epoch 22/25:  87%|████████▋ | 255/292 [02:49<00:31,  1.18batch/s, auc=0.8799, loss=0.9810]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 256/292 [02:49<00:33,  1.09batch/s, auc=0.8799, loss=0.9810]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 256/292 [02:50<00:33,  1.09batch/s, auc=0.8797, loss=0.8765]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 257/292 [02:50<00:33,  1.04batch/s, auc=0.8797, loss=0.8765]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 257/292 [02:50<00:33,  1.04batch/s, auc=0.8798, loss=0.6245]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 258/292 [02:50<00:28,  1.18batch/s, auc=0.8798, loss=0.6245]\u001b[A\n",
      "Training Epoch 22/25:  88%|████████▊ | 258/292 [02:51<00:28,  1.18batch/s, auc=0.8799, loss=0.6246]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▊ | 259/292 [02:51<00:25,  1.31batch/s, auc=0.8799, loss=0.6246]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▊ | 259/292 [02:52<00:25,  1.31batch/s, auc=0.8801, loss=0.5659]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 260/292 [02:52<00:22,  1.42batch/s, auc=0.8801, loss=0.5659]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 260/292 [02:52<00:22,  1.42batch/s, auc=0.8801, loss=0.7229]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 261/292 [02:52<00:20,  1.51batch/s, auc=0.8801, loss=0.7229]\u001b[A\n",
      "Training Epoch 22/25:  89%|████████▉ | 261/292 [02:53<00:20,  1.51batch/s, auc=0.8800, loss=0.6337]\u001b[A\n",
      "Training Epoch 22/25:  90%|████████▉ | 262/292 [02:53<00:18,  1.58batch/s, auc=0.8800, loss=0.6337]\u001b[A\n",
      "Training Epoch 22/25:  90%|████████▉ | 262/292 [02:53<00:18,  1.58batch/s, auc=0.8800, loss=0.5369]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 263/292 [02:53<00:17,  1.64batch/s, auc=0.8800, loss=0.5369]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 263/292 [02:54<00:17,  1.64batch/s, auc=0.8801, loss=0.7721]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 264/292 [02:54<00:16,  1.67batch/s, auc=0.8801, loss=0.7721]\u001b[A\n",
      "Training Epoch 22/25:  90%|█████████ | 264/292 [02:54<00:16,  1.67batch/s, auc=0.8802, loss=0.5468]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 265/292 [02:54<00:15,  1.70batch/s, auc=0.8802, loss=0.5468]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 265/292 [02:55<00:15,  1.70batch/s, auc=0.8804, loss=0.6123]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 266/292 [02:55<00:15,  1.72batch/s, auc=0.8804, loss=0.6123]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████ | 266/292 [02:56<00:15,  1.72batch/s, auc=0.8807, loss=0.4613]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████▏| 267/292 [02:56<00:14,  1.74batch/s, auc=0.8807, loss=0.4613]\u001b[A\n",
      "Training Epoch 22/25:  91%|█████████▏| 267/292 [02:56<00:14,  1.74batch/s, auc=0.8808, loss=0.5205]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 268/292 [02:56<00:13,  1.75batch/s, auc=0.8808, loss=0.5205]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 268/292 [02:57<00:13,  1.75batch/s, auc=0.8811, loss=0.4242]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 269/292 [02:57<00:13,  1.75batch/s, auc=0.8811, loss=0.4242]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 269/292 [02:57<00:13,  1.75batch/s, auc=0.8812, loss=0.5685]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 270/292 [02:57<00:12,  1.76batch/s, auc=0.8812, loss=0.5685]\u001b[A\n",
      "Training Epoch 22/25:  92%|█████████▏| 270/292 [02:58<00:12,  1.76batch/s, auc=0.8811, loss=0.7062]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 271/292 [02:58<00:11,  1.76batch/s, auc=0.8811, loss=0.7062]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 271/292 [02:58<00:11,  1.76batch/s, auc=0.8814, loss=0.5660]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 272/292 [02:58<00:11,  1.76batch/s, auc=0.8814, loss=0.5660]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 272/292 [02:59<00:11,  1.76batch/s, auc=0.8810, loss=0.9427]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 273/292 [02:59<00:10,  1.77batch/s, auc=0.8810, loss=0.9427]\u001b[A\n",
      "Training Epoch 22/25:  93%|█████████▎| 273/292 [03:00<00:10,  1.77batch/s, auc=0.8810, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 274/292 [03:00<00:10,  1.77batch/s, auc=0.8810, loss=0.6710]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 274/292 [03:00<00:10,  1.77batch/s, auc=0.8811, loss=0.6021]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 275/292 [03:00<00:09,  1.77batch/s, auc=0.8811, loss=0.6021]\u001b[A\n",
      "Training Epoch 22/25:  94%|█████████▍| 275/292 [03:01<00:09,  1.77batch/s, auc=0.8811, loss=0.6875]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 276/292 [03:01<00:09,  1.77batch/s, auc=0.8811, loss=0.6875]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 276/292 [03:01<00:09,  1.77batch/s, auc=0.8812, loss=0.5913]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 277/292 [03:01<00:08,  1.77batch/s, auc=0.8812, loss=0.5913]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▍| 277/292 [03:02<00:08,  1.77batch/s, auc=0.8813, loss=0.4707]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▌| 278/292 [03:02<00:07,  1.77batch/s, auc=0.8813, loss=0.4707]\u001b[A\n",
      "Training Epoch 22/25:  95%|█████████▌| 278/292 [03:02<00:07,  1.77batch/s, auc=0.8812, loss=0.6153]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 279/292 [03:02<00:07,  1.77batch/s, auc=0.8812, loss=0.6153]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 279/292 [03:03<00:07,  1.77batch/s, auc=0.8813, loss=0.6403]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 280/292 [03:03<00:06,  1.77batch/s, auc=0.8813, loss=0.6403]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 280/292 [03:04<00:06,  1.77batch/s, auc=0.8813, loss=0.7531]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 281/292 [03:04<00:07,  1.39batch/s, auc=0.8813, loss=0.7531]\u001b[A\n",
      "Training Epoch 22/25:  96%|█████████▌| 281/292 [03:05<00:07,  1.39batch/s, auc=0.8812, loss=0.9915]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 282/292 [03:05<00:06,  1.49batch/s, auc=0.8812, loss=0.9915]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 282/292 [03:05<00:06,  1.49batch/s, auc=0.8814, loss=0.5244]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 283/292 [03:05<00:05,  1.56batch/s, auc=0.8814, loss=0.5244]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 283/292 [03:06<00:05,  1.56batch/s, auc=0.8816, loss=0.5267]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 284/292 [03:06<00:04,  1.62batch/s, auc=0.8816, loss=0.5267]\u001b[A\n",
      "Training Epoch 22/25:  97%|█████████▋| 284/292 [03:06<00:04,  1.62batch/s, auc=0.8816, loss=0.5931]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 285/292 [03:06<00:04,  1.66batch/s, auc=0.8816, loss=0.5931]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 285/292 [03:07<00:04,  1.66batch/s, auc=0.8816, loss=0.6073]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 286/292 [03:07<00:03,  1.70batch/s, auc=0.8816, loss=0.6073]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 286/292 [03:07<00:03,  1.70batch/s, auc=0.8815, loss=0.6915]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 287/292 [03:07<00:02,  1.72batch/s, auc=0.8815, loss=0.6915]\u001b[A\n",
      "Training Epoch 22/25:  98%|█████████▊| 287/292 [03:08<00:02,  1.72batch/s, auc=0.8815, loss=0.7648]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▊| 288/292 [03:08<00:02,  1.73batch/s, auc=0.8815, loss=0.7648]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▊| 288/292 [03:08<00:02,  1.73batch/s, auc=0.8815, loss=0.6768]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 289/292 [03:08<00:01,  1.74batch/s, auc=0.8815, loss=0.6768]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 289/292 [03:09<00:01,  1.74batch/s, auc=0.8813, loss=0.7558]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 290/292 [03:09<00:01,  1.75batch/s, auc=0.8813, loss=0.7558]\u001b[A\n",
      "Training Epoch 22/25:  99%|█████████▉| 290/292 [03:10<00:01,  1.75batch/s, auc=0.8812, loss=0.7196]\u001b[A\n",
      "Training Epoch 22/25: 100%|█████████▉| 291/292 [03:10<00:00,  1.76batch/s, auc=0.8812, loss=0.7196]\u001b[A\n",
      "Training Epoch 22/25: 100%|█████████▉| 291/292 [03:10<00:00,  1.76batch/s, auc=0.8812, loss=0.7714]\u001b[A\n",
      "Training Epoch 22/25: 100%|██████████| 292/292 [03:10<00:00,  1.53batch/s, auc=0.8812, loss=0.7714]\u001b[A\n",
      "Epochs:  88%|████████▊ | 22/25 [1:18:22<10:37, 212.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25] Train Loss: 0.7172 | Train AUROC: 0.8812 Val Loss: 1.0042 | Val AUROC: 0.8304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 23/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.8707, loss=0.7651]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 1/292 [00:01<06:59,  1.44s/batch, auc=0.8707, loss=0.7651]\u001b[A\n",
      "Training Epoch 23/25:   0%|          | 1/292 [00:01<06:59,  1.44s/batch, auc=0.8791, loss=0.6018]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 2/292 [00:01<04:26,  1.09batch/s, auc=0.8791, loss=0.6018]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 2/292 [00:03<04:26,  1.09batch/s, auc=0.8599, loss=0.7837]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 3/292 [00:03<04:45,  1.01batch/s, auc=0.8599, loss=0.7837]\u001b[A\n",
      "Training Epoch 23/25:   1%|          | 3/292 [00:04<04:45,  1.01batch/s, auc=0.8737, loss=0.7074]\u001b[A\n",
      "Training Epoch 23/25:   1%|▏         | 4/292 [00:04<04:54,  1.02s/batch, auc=0.8737, loss=0.7074]\u001b[A\n",
      "Training Epoch 23/25:   1%|▏         | 4/292 [00:05<04:54,  1.02s/batch, auc=0.7975, loss=2.2745]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 5/292 [00:05<04:58,  1.04s/batch, auc=0.7975, loss=2.2745]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 5/292 [00:05<04:58,  1.04s/batch, auc=0.8099, loss=0.9035]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 6/292 [00:05<04:10,  1.14batch/s, auc=0.8099, loss=0.9035]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 6/292 [00:06<04:10,  1.14batch/s, auc=0.7920, loss=1.2344]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 7/292 [00:06<04:27,  1.06batch/s, auc=0.7920, loss=1.2344]\u001b[A\n",
      "Training Epoch 23/25:   2%|▏         | 7/292 [00:07<04:27,  1.06batch/s, auc=0.8042, loss=0.4158]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 8/292 [00:07<03:52,  1.22batch/s, auc=0.8042, loss=0.4158]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 8/292 [00:07<03:52,  1.22batch/s, auc=0.8152, loss=0.6900]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 9/292 [00:07<03:27,  1.36batch/s, auc=0.8152, loss=0.6900]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 9/292 [00:08<03:27,  1.36batch/s, auc=0.8216, loss=0.9090]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 10/292 [00:08<03:11,  1.47batch/s, auc=0.8216, loss=0.9090]\u001b[A\n",
      "Training Epoch 23/25:   3%|▎         | 10/292 [00:09<03:11,  1.47batch/s, auc=0.8401, loss=0.5626]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 11/292 [00:09<03:00,  1.56batch/s, auc=0.8401, loss=0.5626]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 11/292 [00:09<03:00,  1.56batch/s, auc=0.8528, loss=0.5125]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 12/292 [00:09<02:52,  1.63batch/s, auc=0.8528, loss=0.5125]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 12/292 [00:10<02:52,  1.63batch/s, auc=0.8563, loss=0.4733]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 13/292 [00:10<02:46,  1.68batch/s, auc=0.8563, loss=0.4733]\u001b[A\n",
      "Training Epoch 23/25:   4%|▍         | 13/292 [00:10<02:46,  1.68batch/s, auc=0.8568, loss=0.5705]\u001b[A\n",
      "Training Epoch 23/25:   5%|▍         | 14/292 [00:10<02:43,  1.70batch/s, auc=0.8568, loss=0.5705]\u001b[A\n",
      "Training Epoch 23/25:   5%|▍         | 14/292 [00:11<02:43,  1.70batch/s, auc=0.8474, loss=1.1394]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 15/292 [00:11<03:23,  1.36batch/s, auc=0.8474, loss=1.1394]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 15/292 [00:12<03:23,  1.36batch/s, auc=0.8517, loss=0.4726]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 16/292 [00:12<03:08,  1.47batch/s, auc=0.8517, loss=0.4726]\u001b[A\n",
      "Training Epoch 23/25:   5%|▌         | 16/292 [00:13<03:08,  1.47batch/s, auc=0.8347, loss=1.6403]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 17/292 [00:13<03:39,  1.25batch/s, auc=0.8347, loss=1.6403]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 17/292 [00:13<03:39,  1.25batch/s, auc=0.8329, loss=0.7717]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 18/292 [00:13<03:18,  1.38batch/s, auc=0.8329, loss=0.7717]\u001b[A\n",
      "Training Epoch 23/25:   6%|▌         | 18/292 [00:14<03:18,  1.38batch/s, auc=0.8442, loss=0.5722]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 19/292 [00:14<03:04,  1.48batch/s, auc=0.8442, loss=0.5722]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 19/292 [00:15<03:04,  1.48batch/s, auc=0.8450, loss=0.7245]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 20/292 [00:15<02:53,  1.57batch/s, auc=0.8450, loss=0.7245]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 20/292 [00:15<02:53,  1.57batch/s, auc=0.8469, loss=0.6248]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 21/292 [00:15<02:46,  1.63batch/s, auc=0.8469, loss=0.6248]\u001b[A\n",
      "Training Epoch 23/25:   7%|▋         | 21/292 [00:16<02:46,  1.63batch/s, auc=0.8501, loss=0.7110]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 22/292 [00:16<02:41,  1.68batch/s, auc=0.8501, loss=0.7110]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 22/292 [00:16<02:41,  1.68batch/s, auc=0.8496, loss=0.9974]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 23/292 [00:16<02:37,  1.71batch/s, auc=0.8496, loss=0.9974]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 23/292 [00:17<02:37,  1.71batch/s, auc=0.8495, loss=0.7234]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 24/292 [00:17<02:34,  1.74batch/s, auc=0.8495, loss=0.7234]\u001b[A\n",
      "Training Epoch 23/25:   8%|▊         | 24/292 [00:17<02:34,  1.74batch/s, auc=0.8519, loss=0.6616]\u001b[A\n",
      "Training Epoch 23/25:   9%|▊         | 25/292 [00:17<02:32,  1.75batch/s, auc=0.8519, loss=0.6616]\u001b[A\n",
      "Training Epoch 23/25:   9%|▊         | 25/292 [00:18<02:32,  1.75batch/s, auc=0.8541, loss=0.6967]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 26/292 [00:18<02:30,  1.77batch/s, auc=0.8541, loss=0.6967]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 26/292 [00:18<02:30,  1.77batch/s, auc=0.8544, loss=0.6502]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 27/292 [00:18<02:29,  1.78batch/s, auc=0.8544, loss=0.6502]\u001b[A\n",
      "Training Epoch 23/25:   9%|▉         | 27/292 [00:20<02:29,  1.78batch/s, auc=0.8537, loss=0.8078]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 28/292 [00:20<03:09,  1.40batch/s, auc=0.8537, loss=0.8078]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 28/292 [00:20<03:09,  1.40batch/s, auc=0.8542, loss=0.6462]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 29/292 [00:20<02:55,  1.50batch/s, auc=0.8542, loss=0.6462]\u001b[A\n",
      "Training Epoch 23/25:  10%|▉         | 29/292 [00:21<02:55,  1.50batch/s, auc=0.8564, loss=0.5624]\u001b[A\n",
      "Training Epoch 23/25:  10%|█         | 30/292 [00:21<02:46,  1.57batch/s, auc=0.8564, loss=0.5624]\u001b[A\n",
      "Training Epoch 23/25:  10%|█         | 30/292 [00:21<02:46,  1.57batch/s, auc=0.8602, loss=0.5430]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 31/292 [00:21<02:39,  1.64batch/s, auc=0.8602, loss=0.5430]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 31/292 [00:22<02:39,  1.64batch/s, auc=0.8610, loss=0.6535]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 32/292 [00:22<02:34,  1.68batch/s, auc=0.8610, loss=0.6535]\u001b[A\n",
      "Training Epoch 23/25:  11%|█         | 32/292 [00:22<02:34,  1.68batch/s, auc=0.8603, loss=0.6179]\u001b[A\n",
      "Training Epoch 23/25:  11%|█▏        | 33/292 [00:22<02:30,  1.72batch/s, auc=0.8603, loss=0.6179]\u001b[A\n",
      "Training Epoch 23/25:  11%|█▏        | 33/292 [00:23<02:30,  1.72batch/s, auc=0.8630, loss=0.7562]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 34/292 [00:23<02:28,  1.74batch/s, auc=0.8630, loss=0.7562]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 34/292 [00:24<02:28,  1.74batch/s, auc=0.8597, loss=1.1981]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 35/292 [00:24<03:06,  1.38batch/s, auc=0.8597, loss=1.1981]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 35/292 [00:25<03:06,  1.38batch/s, auc=0.8595, loss=1.0234]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 36/292 [00:25<02:52,  1.48batch/s, auc=0.8595, loss=1.0234]\u001b[A\n",
      "Training Epoch 23/25:  12%|█▏        | 36/292 [00:25<02:52,  1.48batch/s, auc=0.8616, loss=0.7069]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 37/292 [00:25<02:42,  1.57batch/s, auc=0.8616, loss=0.7069]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 37/292 [00:26<02:42,  1.57batch/s, auc=0.8633, loss=0.5312]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 38/292 [00:26<02:35,  1.63batch/s, auc=0.8633, loss=0.5312]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 38/292 [00:26<02:35,  1.63batch/s, auc=0.8648, loss=0.6463]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 39/292 [00:26<02:31,  1.67batch/s, auc=0.8648, loss=0.6463]\u001b[A\n",
      "Training Epoch 23/25:  13%|█▎        | 39/292 [00:27<02:31,  1.67batch/s, auc=0.8638, loss=0.8356]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▎        | 40/292 [00:27<03:06,  1.35batch/s, auc=0.8638, loss=0.8356]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▎        | 40/292 [00:28<03:06,  1.35batch/s, auc=0.8627, loss=0.9993]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 41/292 [00:28<03:30,  1.19batch/s, auc=0.8627, loss=0.9993]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 41/292 [00:29<03:30,  1.19batch/s, auc=0.8607, loss=1.1421]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 42/292 [00:29<03:47,  1.10batch/s, auc=0.8607, loss=1.1421]\u001b[A\n",
      "Training Epoch 23/25:  14%|█▍        | 42/292 [00:30<03:47,  1.10batch/s, auc=0.8618, loss=0.5520]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▍        | 43/292 [00:30<03:20,  1.24batch/s, auc=0.8618, loss=0.5520]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▍        | 43/292 [00:31<03:20,  1.24batch/s, auc=0.8613, loss=0.7378]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 44/292 [00:31<03:00,  1.37batch/s, auc=0.8613, loss=0.7378]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 44/292 [00:31<03:00,  1.37batch/s, auc=0.8611, loss=0.7563]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 45/292 [00:31<02:47,  1.48batch/s, auc=0.8611, loss=0.7563]\u001b[A\n",
      "Training Epoch 23/25:  15%|█▌        | 45/292 [00:32<02:47,  1.48batch/s, auc=0.8607, loss=0.9234]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 46/292 [00:32<02:37,  1.56batch/s, auc=0.8607, loss=0.9234]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 46/292 [00:32<02:37,  1.56batch/s, auc=0.8612, loss=0.5188]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 47/292 [00:32<02:30,  1.62batch/s, auc=0.8612, loss=0.5188]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▌        | 47/292 [00:33<02:30,  1.62batch/s, auc=0.8615, loss=0.6408]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▋        | 48/292 [00:33<02:25,  1.67batch/s, auc=0.8615, loss=0.6408]\u001b[A\n",
      "Training Epoch 23/25:  16%|█▋        | 48/292 [00:33<02:25,  1.67batch/s, auc=0.8608, loss=0.7522]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 49/292 [00:33<02:22,  1.71batch/s, auc=0.8608, loss=0.7522]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 49/292 [00:34<02:22,  1.71batch/s, auc=0.8618, loss=0.5941]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 50/292 [00:34<02:19,  1.73batch/s, auc=0.8618, loss=0.5941]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 50/292 [00:34<02:19,  1.73batch/s, auc=0.8644, loss=0.4384]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 51/292 [00:34<02:17,  1.75batch/s, auc=0.8644, loss=0.4384]\u001b[A\n",
      "Training Epoch 23/25:  17%|█▋        | 51/292 [00:35<02:17,  1.75batch/s, auc=0.8645, loss=0.7526]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 52/292 [00:35<02:15,  1.77batch/s, auc=0.8645, loss=0.7526]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 52/292 [00:36<02:15,  1.77batch/s, auc=0.8656, loss=0.6524]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 53/292 [00:36<02:14,  1.77batch/s, auc=0.8656, loss=0.6524]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 53/292 [00:36<02:14,  1.77batch/s, auc=0.8668, loss=0.6095]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 54/292 [00:36<02:13,  1.78batch/s, auc=0.8668, loss=0.6095]\u001b[A\n",
      "Training Epoch 23/25:  18%|█▊        | 54/292 [00:37<02:13,  1.78batch/s, auc=0.8672, loss=0.9463]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 55/292 [00:37<02:12,  1.78batch/s, auc=0.8672, loss=0.9463]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 55/292 [00:38<02:12,  1.78batch/s, auc=0.8659, loss=0.9771]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 56/292 [00:38<02:48,  1.40batch/s, auc=0.8659, loss=0.9771]\u001b[A\n",
      "Training Epoch 23/25:  19%|█▉        | 56/292 [00:38<02:48,  1.40batch/s, auc=0.8657, loss=0.6824]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 57/292 [00:38<02:36,  1.50batch/s, auc=0.8657, loss=0.6824]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 57/292 [00:39<02:36,  1.50batch/s, auc=0.8665, loss=0.4502]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 58/292 [00:39<02:28,  1.58batch/s, auc=0.8665, loss=0.4502]\u001b[A\n",
      "Training Epoch 23/25:  20%|█▉        | 58/292 [00:39<02:28,  1.58batch/s, auc=0.8650, loss=1.1337]\u001b[A\n",
      "Training Epoch 23/25:  20%|██        | 59/292 [00:39<02:22,  1.64batch/s, auc=0.8650, loss=1.1337]\u001b[A\n",
      "Training Epoch 23/25:  20%|██        | 59/292 [00:40<02:22,  1.64batch/s, auc=0.8639, loss=0.9035]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 60/292 [00:40<02:53,  1.33batch/s, auc=0.8639, loss=0.9035]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 60/292 [00:42<02:53,  1.33batch/s, auc=0.8632, loss=0.8738]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 61/292 [00:42<03:15,  1.18batch/s, auc=0.8632, loss=0.8738]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 61/292 [00:42<03:15,  1.18batch/s, auc=0.8627, loss=0.6746]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 62/292 [00:42<02:54,  1.31batch/s, auc=0.8627, loss=0.6746]\u001b[A\n",
      "Training Epoch 23/25:  21%|██        | 62/292 [00:43<02:54,  1.31batch/s, auc=0.8627, loss=0.7580]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 63/292 [00:43<02:40,  1.43batch/s, auc=0.8627, loss=0.7580]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 63/292 [00:43<02:40,  1.43batch/s, auc=0.8633, loss=0.6161]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 64/292 [00:43<02:29,  1.52batch/s, auc=0.8633, loss=0.6161]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 64/292 [00:44<02:29,  1.52batch/s, auc=0.8644, loss=0.5185]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 65/292 [00:44<02:22,  1.60batch/s, auc=0.8644, loss=0.5185]\u001b[A\n",
      "Training Epoch 23/25:  22%|██▏       | 65/292 [00:45<02:22,  1.60batch/s, auc=0.8644, loss=0.8555]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 66/292 [00:45<02:51,  1.31batch/s, auc=0.8644, loss=0.8555]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 66/292 [00:45<02:51,  1.31batch/s, auc=0.8637, loss=0.8954]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 67/292 [00:45<02:37,  1.43batch/s, auc=0.8637, loss=0.8954]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 67/292 [00:46<02:37,  1.43batch/s, auc=0.8643, loss=0.6971]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 68/292 [00:46<02:27,  1.52batch/s, auc=0.8643, loss=0.6971]\u001b[A\n",
      "Training Epoch 23/25:  23%|██▎       | 68/292 [00:47<02:27,  1.52batch/s, auc=0.8659, loss=0.4882]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▎       | 69/292 [00:47<02:20,  1.59batch/s, auc=0.8659, loss=0.4882]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▎       | 69/292 [00:47<02:20,  1.59batch/s, auc=0.8663, loss=0.7993]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 70/292 [00:47<02:14,  1.65batch/s, auc=0.8663, loss=0.7993]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 70/292 [00:48<02:14,  1.65batch/s, auc=0.8658, loss=0.8373]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 71/292 [00:48<02:45,  1.34batch/s, auc=0.8658, loss=0.8373]\u001b[A\n",
      "Training Epoch 23/25:  24%|██▍       | 71/292 [00:49<02:45,  1.34batch/s, auc=0.8656, loss=0.5791]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▍       | 72/292 [00:49<02:31,  1.45batch/s, auc=0.8656, loss=0.5791]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▍       | 72/292 [00:49<02:31,  1.45batch/s, auc=0.8676, loss=0.4394]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 73/292 [00:49<02:22,  1.54batch/s, auc=0.8676, loss=0.4394]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 73/292 [00:50<02:22,  1.54batch/s, auc=0.8657, loss=1.1633]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 74/292 [00:50<02:49,  1.29batch/s, auc=0.8657, loss=1.1633]\u001b[A\n",
      "Training Epoch 23/25:  25%|██▌       | 74/292 [00:51<02:49,  1.29batch/s, auc=0.8656, loss=0.8548]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 75/292 [00:51<02:34,  1.41batch/s, auc=0.8656, loss=0.8548]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 75/292 [00:51<02:34,  1.41batch/s, auc=0.8656, loss=0.6446]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 76/292 [00:51<02:23,  1.50batch/s, auc=0.8656, loss=0.6446]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▌       | 76/292 [00:53<02:23,  1.50batch/s, auc=0.8646, loss=0.9552]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▋       | 77/292 [00:53<02:49,  1.27batch/s, auc=0.8646, loss=0.9552]\u001b[A\n",
      "Training Epoch 23/25:  26%|██▋       | 77/292 [00:54<02:49,  1.27batch/s, auc=0.8642, loss=0.7786]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 78/292 [00:54<03:06,  1.15batch/s, auc=0.8642, loss=0.7786]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 78/292 [00:54<03:06,  1.15batch/s, auc=0.8650, loss=0.7248]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 79/292 [00:54<02:45,  1.28batch/s, auc=0.8650, loss=0.7248]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 79/292 [00:55<02:45,  1.28batch/s, auc=0.8647, loss=0.8857]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 80/292 [00:55<02:30,  1.40batch/s, auc=0.8647, loss=0.8857]\u001b[A\n",
      "Training Epoch 23/25:  27%|██▋       | 80/292 [00:56<02:30,  1.40batch/s, auc=0.8614, loss=1.7503]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 81/292 [00:56<02:53,  1.22batch/s, auc=0.8614, loss=1.7503]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 81/292 [00:56<02:53,  1.22batch/s, auc=0.8617, loss=0.6166]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 82/292 [00:56<02:35,  1.35batch/s, auc=0.8617, loss=0.6166]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 82/292 [00:57<02:35,  1.35batch/s, auc=0.8623, loss=0.6367]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 83/292 [00:57<02:23,  1.46batch/s, auc=0.8623, loss=0.6367]\u001b[A\n",
      "Training Epoch 23/25:  28%|██▊       | 83/292 [00:58<02:23,  1.46batch/s, auc=0.8620, loss=0.7405]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 84/292 [00:58<02:46,  1.25batch/s, auc=0.8620, loss=0.7405]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 84/292 [00:59<02:46,  1.25batch/s, auc=0.8636, loss=0.4952]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 85/292 [00:59<02:31,  1.37batch/s, auc=0.8636, loss=0.4952]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 85/292 [00:59<02:31,  1.37batch/s, auc=0.8638, loss=0.8718]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 86/292 [00:59<02:19,  1.48batch/s, auc=0.8638, loss=0.8718]\u001b[A\n",
      "Training Epoch 23/25:  29%|██▉       | 86/292 [01:00<02:19,  1.48batch/s, auc=0.8640, loss=0.6834]\u001b[A\n",
      "Training Epoch 23/25:  30%|██▉       | 87/292 [01:00<02:11,  1.56batch/s, auc=0.8640, loss=0.6834]\u001b[A\n",
      "Training Epoch 23/25:  30%|██▉       | 87/292 [01:00<02:11,  1.56batch/s, auc=0.8643, loss=0.5676]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 88/292 [01:00<02:05,  1.62batch/s, auc=0.8643, loss=0.5676]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 88/292 [01:01<02:05,  1.62batch/s, auc=0.8648, loss=0.6743]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 89/292 [01:01<02:01,  1.67batch/s, auc=0.8648, loss=0.6743]\u001b[A\n",
      "Training Epoch 23/25:  30%|███       | 89/292 [01:01<02:01,  1.67batch/s, auc=0.8658, loss=0.5777]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 90/292 [01:01<01:58,  1.71batch/s, auc=0.8658, loss=0.5777]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 90/292 [01:02<01:58,  1.71batch/s, auc=0.8655, loss=0.8579]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 91/292 [01:02<01:56,  1.73batch/s, auc=0.8655, loss=0.8579]\u001b[A\n",
      "Training Epoch 23/25:  31%|███       | 91/292 [01:02<01:56,  1.73batch/s, auc=0.8666, loss=0.8349]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 92/292 [01:02<01:54,  1.75batch/s, auc=0.8666, loss=0.8349]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 92/292 [01:03<01:54,  1.75batch/s, auc=0.8672, loss=0.5882]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 93/292 [01:03<01:52,  1.76batch/s, auc=0.8672, loss=0.5882]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 93/292 [01:04<01:52,  1.76batch/s, auc=0.8669, loss=0.9180]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 94/292 [01:04<01:51,  1.77batch/s, auc=0.8669, loss=0.9180]\u001b[A\n",
      "Training Epoch 23/25:  32%|███▏      | 94/292 [01:04<01:51,  1.77batch/s, auc=0.8670, loss=0.6017]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 95/292 [01:04<01:50,  1.78batch/s, auc=0.8670, loss=0.6017]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 95/292 [01:05<01:50,  1.78batch/s, auc=0.8675, loss=0.4933]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 96/292 [01:05<01:49,  1.78batch/s, auc=0.8675, loss=0.4933]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 96/292 [01:06<01:49,  1.78batch/s, auc=0.8656, loss=1.3740]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 97/292 [01:06<02:19,  1.40batch/s, auc=0.8656, loss=1.3740]\u001b[A\n",
      "Training Epoch 23/25:  33%|███▎      | 97/292 [01:06<02:19,  1.40batch/s, auc=0.8652, loss=0.9832]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▎      | 98/292 [01:06<02:09,  1.50batch/s, auc=0.8652, loss=0.9832]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▎      | 98/292 [01:07<02:09,  1.50batch/s, auc=0.8655, loss=0.6452]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 99/292 [01:07<02:02,  1.58batch/s, auc=0.8655, loss=0.6452]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 99/292 [01:07<02:02,  1.58batch/s, auc=0.8662, loss=0.4872]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 100/292 [01:07<01:57,  1.64batch/s, auc=0.8662, loss=0.4872]\u001b[A\n",
      "Training Epoch 23/25:  34%|███▍      | 100/292 [01:08<01:57,  1.64batch/s, auc=0.8669, loss=0.4820]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 101/292 [01:08<01:53,  1.68batch/s, auc=0.8669, loss=0.4820]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 101/292 [01:09<01:53,  1.68batch/s, auc=0.8655, loss=1.3146]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 102/292 [01:09<02:20,  1.35batch/s, auc=0.8655, loss=1.3146]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▍      | 102/292 [01:10<02:20,  1.35batch/s, auc=0.8657, loss=0.6059]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▌      | 103/292 [01:10<02:09,  1.46batch/s, auc=0.8657, loss=0.6059]\u001b[A\n",
      "Training Epoch 23/25:  35%|███▌      | 103/292 [01:10<02:09,  1.46batch/s, auc=0.8662, loss=0.6666]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 104/292 [01:10<02:01,  1.55batch/s, auc=0.8662, loss=0.6666]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 104/292 [01:11<02:01,  1.55batch/s, auc=0.8672, loss=0.5457]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 105/292 [01:11<01:55,  1.61batch/s, auc=0.8672, loss=0.5457]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▌      | 105/292 [01:12<01:55,  1.61batch/s, auc=0.8667, loss=0.7991]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▋      | 106/292 [01:12<02:20,  1.32batch/s, auc=0.8667, loss=0.7991]\u001b[A\n",
      "Training Epoch 23/25:  36%|███▋      | 106/292 [01:13<02:20,  1.32batch/s, auc=0.8658, loss=1.0464]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 107/292 [01:13<02:37,  1.17batch/s, auc=0.8658, loss=1.0464]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 107/292 [01:13<02:37,  1.17batch/s, auc=0.8660, loss=0.6250]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 108/292 [01:13<02:20,  1.31batch/s, auc=0.8660, loss=0.6250]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 108/292 [01:14<02:20,  1.31batch/s, auc=0.8659, loss=0.8184]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 109/292 [01:14<02:08,  1.42batch/s, auc=0.8659, loss=0.8184]\u001b[A\n",
      "Training Epoch 23/25:  37%|███▋      | 109/292 [01:15<02:08,  1.42batch/s, auc=0.8653, loss=1.0551]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 110/292 [01:15<01:59,  1.52batch/s, auc=0.8653, loss=1.0551]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 110/292 [01:16<01:59,  1.52batch/s, auc=0.8641, loss=1.1423]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 111/292 [01:16<02:21,  1.28batch/s, auc=0.8641, loss=1.1423]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 111/292 [01:16<02:21,  1.28batch/s, auc=0.8649, loss=0.5413]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 112/292 [01:16<02:08,  1.40batch/s, auc=0.8649, loss=0.5413]\u001b[A\n",
      "Training Epoch 23/25:  38%|███▊      | 112/292 [01:17<02:08,  1.40batch/s, auc=0.8654, loss=0.4838]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▊      | 113/292 [01:17<01:59,  1.49batch/s, auc=0.8654, loss=0.4838]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▊      | 113/292 [01:17<01:59,  1.49batch/s, auc=0.8653, loss=0.7519]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 114/292 [01:17<01:53,  1.57batch/s, auc=0.8653, loss=0.7519]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 114/292 [01:18<01:53,  1.57batch/s, auc=0.8646, loss=0.9127]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 115/292 [01:18<02:15,  1.30batch/s, auc=0.8646, loss=0.9127]\u001b[A\n",
      "Training Epoch 23/25:  39%|███▉      | 115/292 [01:19<02:15,  1.30batch/s, auc=0.8653, loss=0.8003]\u001b[A\n",
      "Training Epoch 23/25:  40%|███▉      | 116/292 [01:19<02:03,  1.42batch/s, auc=0.8653, loss=0.8003]\u001b[A\n",
      "Training Epoch 23/25:  40%|███▉      | 116/292 [01:19<02:03,  1.42batch/s, auc=0.8662, loss=0.5722]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 117/292 [01:19<01:55,  1.51batch/s, auc=0.8662, loss=0.5722]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 117/292 [01:20<01:55,  1.51batch/s, auc=0.8659, loss=0.9301]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 118/292 [01:20<01:49,  1.59batch/s, auc=0.8659, loss=0.9301]\u001b[A\n",
      "Training Epoch 23/25:  40%|████      | 118/292 [01:21<01:49,  1.59batch/s, auc=0.8651, loss=1.0823]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 119/292 [01:21<02:12,  1.31batch/s, auc=0.8651, loss=1.0823]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 119/292 [01:22<02:12,  1.31batch/s, auc=0.8657, loss=0.4161]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 120/292 [01:22<02:00,  1.42batch/s, auc=0.8657, loss=0.4161]\u001b[A\n",
      "Training Epoch 23/25:  41%|████      | 120/292 [01:22<02:00,  1.42batch/s, auc=0.8663, loss=0.5310]\u001b[A\n",
      "Training Epoch 23/25:  41%|████▏     | 121/292 [01:22<01:52,  1.52batch/s, auc=0.8663, loss=0.5310]\u001b[A\n",
      "Training Epoch 23/25:  41%|████▏     | 121/292 [01:23<01:52,  1.52batch/s, auc=0.8660, loss=0.7862]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 122/292 [01:23<02:13,  1.28batch/s, auc=0.8660, loss=0.7862]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 122/292 [01:24<02:13,  1.28batch/s, auc=0.8654, loss=0.8681]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 123/292 [01:24<02:27,  1.15batch/s, auc=0.8654, loss=0.8681]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 123/292 [01:25<02:27,  1.15batch/s, auc=0.8656, loss=0.8456]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 124/292 [01:25<02:10,  1.29batch/s, auc=0.8656, loss=0.8456]\u001b[A\n",
      "Training Epoch 23/25:  42%|████▏     | 124/292 [01:26<02:10,  1.29batch/s, auc=0.8651, loss=0.7576]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 125/292 [01:26<01:58,  1.40batch/s, auc=0.8651, loss=0.7576]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 125/292 [01:26<01:58,  1.40batch/s, auc=0.8658, loss=0.6261]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 126/292 [01:26<01:50,  1.50batch/s, auc=0.8658, loss=0.6261]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 126/292 [01:27<01:50,  1.50batch/s, auc=0.8662, loss=0.5302]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 127/292 [01:27<01:44,  1.58batch/s, auc=0.8662, loss=0.5302]\u001b[A\n",
      "Training Epoch 23/25:  43%|████▎     | 127/292 [01:27<01:44,  1.58batch/s, auc=0.8664, loss=0.8029]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 128/292 [01:27<01:40,  1.64batch/s, auc=0.8664, loss=0.8029]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 128/292 [01:28<01:40,  1.64batch/s, auc=0.8672, loss=0.5312]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 129/292 [01:28<01:37,  1.68batch/s, auc=0.8672, loss=0.5312]\u001b[A\n",
      "Training Epoch 23/25:  44%|████▍     | 129/292 [01:28<01:37,  1.68batch/s, auc=0.8675, loss=0.4400]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 130/292 [01:28<01:34,  1.71batch/s, auc=0.8675, loss=0.4400]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 130/292 [01:29<01:34,  1.71batch/s, auc=0.8677, loss=0.6884]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 131/292 [01:29<01:32,  1.74batch/s, auc=0.8677, loss=0.6884]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▍     | 131/292 [01:29<01:32,  1.74batch/s, auc=0.8678, loss=0.7979]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▌     | 132/292 [01:29<01:31,  1.75batch/s, auc=0.8678, loss=0.7979]\u001b[A\n",
      "Training Epoch 23/25:  45%|████▌     | 132/292 [01:30<01:31,  1.75batch/s, auc=0.8673, loss=0.6566]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 133/292 [01:30<01:30,  1.76batch/s, auc=0.8673, loss=0.6566]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 133/292 [01:31<01:30,  1.76batch/s, auc=0.8673, loss=0.8378]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 134/292 [01:31<01:29,  1.77batch/s, auc=0.8673, loss=0.8378]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 134/292 [01:31<01:29,  1.77batch/s, auc=0.8675, loss=0.6868]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 135/292 [01:31<01:28,  1.78batch/s, auc=0.8675, loss=0.6868]\u001b[A\n",
      "Training Epoch 23/25:  46%|████▌     | 135/292 [01:32<01:28,  1.78batch/s, auc=0.8683, loss=0.6951]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 136/292 [01:32<01:27,  1.78batch/s, auc=0.8683, loss=0.6951]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 136/292 [01:32<01:27,  1.78batch/s, auc=0.8688, loss=0.5513]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 137/292 [01:32<01:27,  1.78batch/s, auc=0.8688, loss=0.5513]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 137/292 [01:33<01:27,  1.78batch/s, auc=0.8686, loss=0.8822]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 138/292 [01:33<01:26,  1.78batch/s, auc=0.8686, loss=0.8822]\u001b[A\n",
      "Training Epoch 23/25:  47%|████▋     | 138/292 [01:34<01:26,  1.78batch/s, auc=0.8673, loss=1.5880]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 139/292 [01:34<01:49,  1.40batch/s, auc=0.8673, loss=1.5880]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 139/292 [01:35<01:49,  1.40batch/s, auc=0.8671, loss=0.8427]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 140/292 [01:35<02:05,  1.21batch/s, auc=0.8671, loss=0.8427]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 140/292 [01:36<02:05,  1.21batch/s, auc=0.8667, loss=0.9223]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 141/292 [01:36<02:15,  1.11batch/s, auc=0.8667, loss=0.9223]\u001b[A\n",
      "Training Epoch 23/25:  48%|████▊     | 141/292 [01:37<02:15,  1.11batch/s, auc=0.8665, loss=0.8623]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▊     | 142/292 [01:37<02:22,  1.05batch/s, auc=0.8665, loss=0.8623]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▊     | 142/292 [01:38<02:22,  1.05batch/s, auc=0.8666, loss=0.6907]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 143/292 [01:38<02:04,  1.20batch/s, auc=0.8666, loss=0.6907]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 143/292 [01:38<02:04,  1.20batch/s, auc=0.8664, loss=0.6949]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 144/292 [01:38<01:51,  1.33batch/s, auc=0.8664, loss=0.6949]\u001b[A\n",
      "Training Epoch 23/25:  49%|████▉     | 144/292 [01:39<01:51,  1.33batch/s, auc=0.8669, loss=0.5069]\u001b[A\n",
      "Training Epoch 23/25:  50%|████▉     | 145/292 [01:39<01:42,  1.44batch/s, auc=0.8669, loss=0.5069]\u001b[A\n",
      "Training Epoch 23/25:  50%|████▉     | 145/292 [01:39<01:42,  1.44batch/s, auc=0.8672, loss=0.7159]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 146/292 [01:39<01:35,  1.53batch/s, auc=0.8672, loss=0.7159]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 146/292 [01:40<01:35,  1.53batch/s, auc=0.8676, loss=0.5261]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 147/292 [01:40<01:30,  1.60batch/s, auc=0.8676, loss=0.5261]\u001b[A\n",
      "Training Epoch 23/25:  50%|█████     | 147/292 [01:40<01:30,  1.60batch/s, auc=0.8684, loss=0.6002]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 148/292 [01:40<01:27,  1.65batch/s, auc=0.8684, loss=0.6002]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 148/292 [01:41<01:27,  1.65batch/s, auc=0.8678, loss=0.8306]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 149/292 [01:41<01:24,  1.69batch/s, auc=0.8678, loss=0.8306]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████     | 149/292 [01:42<01:24,  1.69batch/s, auc=0.8685, loss=0.4877]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████▏    | 150/292 [01:42<01:22,  1.71batch/s, auc=0.8685, loss=0.4877]\u001b[A\n",
      "Training Epoch 23/25:  51%|█████▏    | 150/292 [01:42<01:22,  1.71batch/s, auc=0.8687, loss=0.6243]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 151/292 [01:42<01:21,  1.73batch/s, auc=0.8687, loss=0.6243]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 151/292 [01:43<01:21,  1.73batch/s, auc=0.8692, loss=0.4788]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 152/292 [01:43<01:20,  1.75batch/s, auc=0.8692, loss=0.4788]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 152/292 [01:43<01:20,  1.75batch/s, auc=0.8698, loss=0.4980]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 153/292 [01:43<01:18,  1.76batch/s, auc=0.8698, loss=0.4980]\u001b[A\n",
      "Training Epoch 23/25:  52%|█████▏    | 153/292 [01:44<01:18,  1.76batch/s, auc=0.8706, loss=0.5744]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 154/292 [01:44<01:18,  1.77batch/s, auc=0.8706, loss=0.5744]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 154/292 [01:44<01:18,  1.77batch/s, auc=0.8708, loss=0.4515]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 155/292 [01:44<01:17,  1.77batch/s, auc=0.8708, loss=0.4515]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 155/292 [01:45<01:17,  1.77batch/s, auc=0.8709, loss=0.7041]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 156/292 [01:45<01:16,  1.77batch/s, auc=0.8709, loss=0.7041]\u001b[A\n",
      "Training Epoch 23/25:  53%|█████▎    | 156/292 [01:45<01:16,  1.77batch/s, auc=0.8709, loss=0.7502]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 157/292 [01:45<01:15,  1.78batch/s, auc=0.8709, loss=0.7502]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 157/292 [01:46<01:15,  1.78batch/s, auc=0.8715, loss=0.4612]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 158/292 [01:46<01:15,  1.78batch/s, auc=0.8715, loss=0.4612]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 158/292 [01:47<01:15,  1.78batch/s, auc=0.8717, loss=0.6588]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 159/292 [01:47<01:14,  1.78batch/s, auc=0.8717, loss=0.6588]\u001b[A\n",
      "Training Epoch 23/25:  54%|█████▍    | 159/292 [01:47<01:14,  1.78batch/s, auc=0.8727, loss=0.3364]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▍    | 160/292 [01:47<01:14,  1.78batch/s, auc=0.8727, loss=0.3364]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▍    | 160/292 [01:48<01:14,  1.78batch/s, auc=0.8715, loss=1.3598]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 161/292 [01:48<01:33,  1.40batch/s, auc=0.8715, loss=1.3598]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 161/292 [01:49<01:33,  1.40batch/s, auc=0.8711, loss=0.9420]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 162/292 [01:49<01:47,  1.21batch/s, auc=0.8711, loss=0.9420]\u001b[A\n",
      "Training Epoch 23/25:  55%|█████▌    | 162/292 [01:50<01:47,  1.21batch/s, auc=0.8710, loss=0.7175]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 163/292 [01:50<01:56,  1.11batch/s, auc=0.8710, loss=0.7175]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 163/292 [01:51<01:56,  1.11batch/s, auc=0.8710, loss=1.0242]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 164/292 [01:51<01:42,  1.25batch/s, auc=0.8710, loss=1.0242]\u001b[A\n",
      "Training Epoch 23/25:  56%|█████▌    | 164/292 [01:52<01:42,  1.25batch/s, auc=0.8713, loss=0.5594]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 165/292 [01:52<01:32,  1.37batch/s, auc=0.8713, loss=0.5594]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 165/292 [01:52<01:32,  1.37batch/s, auc=0.8714, loss=0.6205]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 166/292 [01:52<01:25,  1.48batch/s, auc=0.8714, loss=0.6205]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 166/292 [01:53<01:25,  1.48batch/s, auc=0.8720, loss=0.4809]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 167/292 [01:53<01:20,  1.55batch/s, auc=0.8720, loss=0.4809]\u001b[A\n",
      "Training Epoch 23/25:  57%|█████▋    | 167/292 [01:53<01:20,  1.55batch/s, auc=0.8722, loss=0.5609]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 168/292 [01:53<01:16,  1.62batch/s, auc=0.8722, loss=0.5609]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 168/292 [01:54<01:16,  1.62batch/s, auc=0.8717, loss=0.9583]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 169/292 [01:54<01:33,  1.32batch/s, auc=0.8717, loss=0.9583]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 169/292 [01:55<01:33,  1.32batch/s, auc=0.8713, loss=1.0809]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 170/292 [01:55<01:44,  1.17batch/s, auc=0.8713, loss=1.0809]\u001b[A\n",
      "Training Epoch 23/25:  58%|█████▊    | 170/292 [01:56<01:44,  1.17batch/s, auc=0.8717, loss=0.4872]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▊    | 171/292 [01:56<01:32,  1.31batch/s, auc=0.8717, loss=0.4872]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▊    | 171/292 [01:57<01:32,  1.31batch/s, auc=0.8716, loss=0.7172]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 172/292 [01:57<01:43,  1.16batch/s, auc=0.8716, loss=0.7172]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 172/292 [01:58<01:43,  1.16batch/s, auc=0.8717, loss=0.5222]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 173/292 [01:58<01:31,  1.30batch/s, auc=0.8717, loss=0.5222]\u001b[A\n",
      "Training Epoch 23/25:  59%|█████▉    | 173/292 [01:59<01:31,  1.30batch/s, auc=0.8708, loss=1.1119]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 174/292 [01:59<01:41,  1.16batch/s, auc=0.8708, loss=1.1119]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 174/292 [01:59<01:41,  1.16batch/s, auc=0.8714, loss=0.6225]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 175/292 [01:59<01:30,  1.29batch/s, auc=0.8714, loss=0.6225]\u001b[A\n",
      "Training Epoch 23/25:  60%|█████▉    | 175/292 [02:00<01:30,  1.29batch/s, auc=0.8709, loss=1.1358]\u001b[A\n",
      "Training Epoch 23/25:  60%|██████    | 176/292 [02:00<01:22,  1.41batch/s, auc=0.8709, loss=1.1358]\u001b[A\n",
      "Training Epoch 23/25:  60%|██████    | 176/292 [02:00<01:22,  1.41batch/s, auc=0.8710, loss=0.5964]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 177/292 [02:00<01:16,  1.51batch/s, auc=0.8710, loss=0.5964]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 177/292 [02:01<01:16,  1.51batch/s, auc=0.8711, loss=0.6305]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 178/292 [02:01<01:12,  1.58batch/s, auc=0.8711, loss=0.6305]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████    | 178/292 [02:02<01:12,  1.58batch/s, auc=0.8702, loss=1.2105]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████▏   | 179/292 [02:02<01:26,  1.31batch/s, auc=0.8702, loss=1.2105]\u001b[A\n",
      "Training Epoch 23/25:  61%|██████▏   | 179/292 [02:03<01:26,  1.31batch/s, auc=0.8707, loss=0.4596]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 180/292 [02:03<01:18,  1.42batch/s, auc=0.8707, loss=0.4596]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 180/292 [02:04<01:18,  1.42batch/s, auc=0.8705, loss=0.9127]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 181/292 [02:04<01:30,  1.22batch/s, auc=0.8705, loss=0.9127]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 181/292 [02:04<01:30,  1.22batch/s, auc=0.8708, loss=0.4795]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 182/292 [02:04<01:21,  1.35batch/s, auc=0.8708, loss=0.4795]\u001b[A\n",
      "Training Epoch 23/25:  62%|██████▏   | 182/292 [02:05<01:21,  1.35batch/s, auc=0.8707, loss=0.7303]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 183/292 [02:05<01:14,  1.46batch/s, auc=0.8707, loss=0.7303]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 183/292 [02:06<01:14,  1.46batch/s, auc=0.8706, loss=0.6977]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 184/292 [02:06<01:26,  1.24batch/s, auc=0.8706, loss=0.6977]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 184/292 [02:06<01:26,  1.24batch/s, auc=0.8711, loss=0.4119]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 185/292 [02:06<01:18,  1.37batch/s, auc=0.8711, loss=0.4119]\u001b[A\n",
      "Training Epoch 23/25:  63%|██████▎   | 185/292 [02:07<01:18,  1.37batch/s, auc=0.8712, loss=0.8770]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▎   | 186/292 [02:07<01:12,  1.47batch/s, auc=0.8712, loss=0.8770]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▎   | 186/292 [02:07<01:12,  1.47batch/s, auc=0.8712, loss=0.6648]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 187/292 [02:08<01:07,  1.55batch/s, auc=0.8712, loss=0.6648]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 187/292 [02:09<01:07,  1.55batch/s, auc=0.8706, loss=1.1073]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 188/292 [02:09<01:20,  1.29batch/s, auc=0.8706, loss=1.1073]\u001b[A\n",
      "Training Epoch 23/25:  64%|██████▍   | 188/292 [02:09<01:20,  1.29batch/s, auc=0.8708, loss=0.4886]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▍   | 189/292 [02:09<01:13,  1.41batch/s, auc=0.8708, loss=0.4886]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▍   | 189/292 [02:10<01:13,  1.41batch/s, auc=0.8707, loss=0.8906]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 190/292 [02:10<01:23,  1.22batch/s, auc=0.8707, loss=0.8906]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 190/292 [02:11<01:23,  1.22batch/s, auc=0.8705, loss=0.9559]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 191/292 [02:11<01:15,  1.35batch/s, auc=0.8705, loss=0.9559]\u001b[A\n",
      "Training Epoch 23/25:  65%|██████▌   | 191/292 [02:11<01:15,  1.35batch/s, auc=0.8702, loss=1.0343]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 192/292 [02:11<01:08,  1.45batch/s, auc=0.8702, loss=1.0343]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 192/292 [02:12<01:08,  1.45batch/s, auc=0.8696, loss=1.1482]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 193/292 [02:12<01:19,  1.24batch/s, auc=0.8696, loss=1.1482]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▌   | 193/292 [02:13<01:19,  1.24batch/s, auc=0.8688, loss=1.1187]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▋   | 194/292 [02:13<01:26,  1.13batch/s, auc=0.8688, loss=1.1187]\u001b[A\n",
      "Training Epoch 23/25:  66%|██████▋   | 194/292 [02:14<01:26,  1.13batch/s, auc=0.8690, loss=0.5989]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 195/292 [02:14<01:16,  1.27batch/s, auc=0.8690, loss=0.5989]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 195/292 [02:15<01:16,  1.27batch/s, auc=0.8690, loss=1.0178]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 196/292 [02:15<01:24,  1.14batch/s, auc=0.8690, loss=1.0178]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 196/292 [02:16<01:24,  1.14batch/s, auc=0.8694, loss=0.5290]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 197/292 [02:16<01:14,  1.28batch/s, auc=0.8694, loss=0.5290]\u001b[A\n",
      "Training Epoch 23/25:  67%|██████▋   | 197/292 [02:16<01:14,  1.28batch/s, auc=0.8693, loss=0.5736]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 198/292 [02:16<01:07,  1.40batch/s, auc=0.8693, loss=0.5736]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 198/292 [02:17<01:07,  1.40batch/s, auc=0.8696, loss=0.7051]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 199/292 [02:17<01:02,  1.49batch/s, auc=0.8696, loss=0.7051]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 199/292 [02:17<01:02,  1.49batch/s, auc=0.8701, loss=0.5210]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 200/292 [02:17<00:58,  1.57batch/s, auc=0.8701, loss=0.5210]\u001b[A\n",
      "Training Epoch 23/25:  68%|██████▊   | 200/292 [02:18<00:58,  1.57batch/s, auc=0.8704, loss=0.6670]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 201/292 [02:18<00:55,  1.63batch/s, auc=0.8704, loss=0.6670]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 201/292 [02:19<00:55,  1.63batch/s, auc=0.8700, loss=0.9616]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 202/292 [02:19<01:07,  1.33batch/s, auc=0.8700, loss=0.9616]\u001b[A\n",
      "Training Epoch 23/25:  69%|██████▉   | 202/292 [02:20<01:07,  1.33batch/s, auc=0.8703, loss=0.7261]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 203/292 [02:20<01:01,  1.44batch/s, auc=0.8703, loss=0.7261]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 203/292 [02:20<01:01,  1.44batch/s, auc=0.8710, loss=0.4579]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 204/292 [02:20<00:57,  1.52batch/s, auc=0.8710, loss=0.4579]\u001b[A\n",
      "Training Epoch 23/25:  70%|██████▉   | 204/292 [02:21<00:57,  1.52batch/s, auc=0.8708, loss=0.9273]\u001b[A\n",
      "Training Epoch 23/25:  70%|███████   | 205/292 [02:21<01:08,  1.28batch/s, auc=0.8708, loss=0.9273]\u001b[A\n",
      "Training Epoch 23/25:  70%|███████   | 205/292 [02:22<01:08,  1.28batch/s, auc=0.8710, loss=0.7461]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 206/292 [02:22<01:01,  1.40batch/s, auc=0.8710, loss=0.7461]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 206/292 [02:22<01:01,  1.40batch/s, auc=0.8710, loss=0.6915]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 207/292 [02:22<00:57,  1.49batch/s, auc=0.8710, loss=0.6915]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 207/292 [02:23<00:57,  1.49batch/s, auc=0.8707, loss=0.8446]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 208/292 [02:23<00:53,  1.56batch/s, auc=0.8707, loss=0.8446]\u001b[A\n",
      "Training Epoch 23/25:  71%|███████   | 208/292 [02:23<00:53,  1.56batch/s, auc=0.8708, loss=0.6441]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 209/292 [02:23<00:51,  1.62batch/s, auc=0.8708, loss=0.6441]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 209/292 [02:24<00:51,  1.62batch/s, auc=0.8710, loss=0.5646]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 210/292 [02:24<00:49,  1.67batch/s, auc=0.8710, loss=0.5646]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 210/292 [02:25<00:49,  1.67batch/s, auc=0.8710, loss=0.7972]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 211/292 [02:25<00:47,  1.70batch/s, auc=0.8710, loss=0.7972]\u001b[A\n",
      "Training Epoch 23/25:  72%|███████▏  | 211/292 [02:25<00:47,  1.70batch/s, auc=0.8709, loss=0.7513]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 212/292 [02:25<00:46,  1.72batch/s, auc=0.8709, loss=0.7513]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 212/292 [02:26<00:46,  1.72batch/s, auc=0.8700, loss=1.3443]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 213/292 [02:26<00:57,  1.37batch/s, auc=0.8700, loss=1.3443]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 213/292 [02:27<00:57,  1.37batch/s, auc=0.8705, loss=0.5967]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 214/292 [02:27<00:53,  1.47batch/s, auc=0.8705, loss=0.5967]\u001b[A\n",
      "Training Epoch 23/25:  73%|███████▎  | 214/292 [02:28<00:53,  1.47batch/s, auc=0.8700, loss=1.0162]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▎  | 215/292 [02:28<01:01,  1.25batch/s, auc=0.8700, loss=1.0162]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▎  | 215/292 [02:29<01:01,  1.25batch/s, auc=0.8697, loss=0.9368]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 216/292 [02:29<01:07,  1.13batch/s, auc=0.8697, loss=0.9368]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 216/292 [02:30<01:07,  1.13batch/s, auc=0.8698, loss=0.5935]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 217/292 [02:30<00:59,  1.27batch/s, auc=0.8698, loss=0.5935]\u001b[A\n",
      "Training Epoch 23/25:  74%|███████▍  | 217/292 [02:30<00:59,  1.27batch/s, auc=0.8699, loss=0.6678]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▍  | 218/292 [02:30<00:53,  1.39batch/s, auc=0.8699, loss=0.6678]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▍  | 218/292 [02:31<00:53,  1.39batch/s, auc=0.8703, loss=0.5947]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 219/292 [02:31<00:49,  1.49batch/s, auc=0.8703, loss=0.5947]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 219/292 [02:31<00:49,  1.49batch/s, auc=0.8706, loss=0.6407]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 220/292 [02:31<00:46,  1.56batch/s, auc=0.8706, loss=0.6407]\u001b[A\n",
      "Training Epoch 23/25:  75%|███████▌  | 220/292 [02:32<00:46,  1.56batch/s, auc=0.8702, loss=0.8807]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 221/292 [02:32<00:54,  1.30batch/s, auc=0.8702, loss=0.8807]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 221/292 [02:33<00:54,  1.30batch/s, auc=0.8706, loss=0.5269]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 222/292 [02:33<00:49,  1.41batch/s, auc=0.8706, loss=0.5269]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▌  | 222/292 [02:34<00:49,  1.41batch/s, auc=0.8703, loss=0.8908]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▋  | 223/292 [02:34<00:56,  1.22batch/s, auc=0.8703, loss=0.8908]\u001b[A\n",
      "Training Epoch 23/25:  76%|███████▋  | 223/292 [02:35<00:56,  1.22batch/s, auc=0.8703, loss=0.6965]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 224/292 [02:35<00:50,  1.35batch/s, auc=0.8703, loss=0.6965]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 224/292 [02:35<00:50,  1.35batch/s, auc=0.8705, loss=0.4372]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 225/292 [02:35<00:46,  1.45batch/s, auc=0.8705, loss=0.4372]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 225/292 [02:36<00:46,  1.45batch/s, auc=0.8704, loss=0.8365]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 226/292 [02:36<00:53,  1.24batch/s, auc=0.8704, loss=0.8365]\u001b[A\n",
      "Training Epoch 23/25:  77%|███████▋  | 226/292 [02:37<00:53,  1.24batch/s, auc=0.8705, loss=0.6414]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 227/292 [02:37<00:47,  1.36batch/s, auc=0.8705, loss=0.6414]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 227/292 [02:38<00:47,  1.36batch/s, auc=0.8705, loss=0.7227]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 228/292 [02:38<00:53,  1.19batch/s, auc=0.8705, loss=0.7227]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 228/292 [02:39<00:53,  1.19batch/s, auc=0.8704, loss=0.9256]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 229/292 [02:39<00:57,  1.10batch/s, auc=0.8704, loss=0.9256]\u001b[A\n",
      "Training Epoch 23/25:  78%|███████▊  | 229/292 [02:39<00:57,  1.10batch/s, auc=0.8704, loss=0.7736]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 230/292 [02:39<00:49,  1.24batch/s, auc=0.8704, loss=0.7736]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 230/292 [02:40<00:49,  1.24batch/s, auc=0.8706, loss=0.8645]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 231/292 [02:40<00:44,  1.36batch/s, auc=0.8706, loss=0.8645]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 231/292 [02:41<00:44,  1.36batch/s, auc=0.8707, loss=0.6464]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 232/292 [02:41<00:40,  1.47batch/s, auc=0.8707, loss=0.6464]\u001b[A\n",
      "Training Epoch 23/25:  79%|███████▉  | 232/292 [02:41<00:40,  1.47batch/s, auc=0.8713, loss=0.5487]\u001b[A\n",
      "Training Epoch 23/25:  80%|███████▉  | 233/292 [02:41<00:38,  1.55batch/s, auc=0.8713, loss=0.5487]\u001b[A\n",
      "Training Epoch 23/25:  80%|███████▉  | 233/292 [02:42<00:38,  1.55batch/s, auc=0.8708, loss=1.0707]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 234/292 [02:42<00:45,  1.29batch/s, auc=0.8708, loss=1.0707]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 234/292 [02:43<00:45,  1.29batch/s, auc=0.8710, loss=0.5982]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 235/292 [02:43<00:40,  1.40batch/s, auc=0.8710, loss=0.5982]\u001b[A\n",
      "Training Epoch 23/25:  80%|████████  | 235/292 [02:44<00:40,  1.40batch/s, auc=0.8706, loss=0.9541]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 236/292 [02:44<00:46,  1.22batch/s, auc=0.8706, loss=0.9541]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 236/292 [02:44<00:46,  1.22batch/s, auc=0.8706, loss=0.7323]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 237/292 [02:44<00:41,  1.34batch/s, auc=0.8706, loss=0.7323]\u001b[A\n",
      "Training Epoch 23/25:  81%|████████  | 237/292 [02:45<00:41,  1.34batch/s, auc=0.8710, loss=0.4753]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 238/292 [02:45<00:37,  1.45batch/s, auc=0.8710, loss=0.4753]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 238/292 [02:46<00:37,  1.45batch/s, auc=0.8712, loss=0.5271]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 239/292 [02:46<00:34,  1.53batch/s, auc=0.8712, loss=0.5271]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 239/292 [02:46<00:34,  1.53batch/s, auc=0.8718, loss=0.4333]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 240/292 [02:46<00:32,  1.60batch/s, auc=0.8718, loss=0.4333]\u001b[A\n",
      "Training Epoch 23/25:  82%|████████▏ | 240/292 [02:47<00:32,  1.60batch/s, auc=0.8713, loss=0.9803]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 241/292 [02:47<00:38,  1.31batch/s, auc=0.8713, loss=0.9803]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 241/292 [02:48<00:38,  1.31batch/s, auc=0.8714, loss=0.6112]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 242/292 [02:48<00:35,  1.42batch/s, auc=0.8714, loss=0.6112]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 242/292 [02:48<00:35,  1.42batch/s, auc=0.8715, loss=0.6155]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 243/292 [02:48<00:32,  1.51batch/s, auc=0.8715, loss=0.6155]\u001b[A\n",
      "Training Epoch 23/25:  83%|████████▎ | 243/292 [02:49<00:32,  1.51batch/s, auc=0.8718, loss=0.5246]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▎ | 244/292 [02:49<00:30,  1.58batch/s, auc=0.8718, loss=0.5246]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▎ | 244/292 [02:50<00:30,  1.58batch/s, auc=0.8718, loss=0.8327]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 245/292 [02:50<00:36,  1.30batch/s, auc=0.8718, loss=0.8327]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 245/292 [02:51<00:36,  1.30batch/s, auc=0.8719, loss=0.4825]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 246/292 [02:51<00:32,  1.42batch/s, auc=0.8719, loss=0.4825]\u001b[A\n",
      "Training Epoch 23/25:  84%|████████▍ | 246/292 [02:51<00:32,  1.42batch/s, auc=0.8722, loss=0.5803]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 247/292 [02:51<00:29,  1.51batch/s, auc=0.8722, loss=0.5803]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 247/292 [02:52<00:29,  1.51batch/s, auc=0.8727, loss=0.4837]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 248/292 [02:52<00:27,  1.58batch/s, auc=0.8727, loss=0.4837]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▍ | 248/292 [02:52<00:27,  1.58batch/s, auc=0.8729, loss=0.4323]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▌ | 249/292 [02:52<00:26,  1.63batch/s, auc=0.8729, loss=0.4323]\u001b[A\n",
      "Training Epoch 23/25:  85%|████████▌ | 249/292 [02:53<00:26,  1.63batch/s, auc=0.8728, loss=0.8397]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 250/292 [02:53<00:25,  1.67batch/s, auc=0.8728, loss=0.8397]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 250/292 [02:53<00:25,  1.67batch/s, auc=0.8727, loss=0.8034]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 251/292 [02:53<00:24,  1.70batch/s, auc=0.8727, loss=0.8034]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▌ | 251/292 [02:54<00:24,  1.70batch/s, auc=0.8731, loss=0.5018]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▋ | 252/292 [02:54<00:23,  1.72batch/s, auc=0.8731, loss=0.5018]\u001b[A\n",
      "Training Epoch 23/25:  86%|████████▋ | 252/292 [02:54<00:23,  1.72batch/s, auc=0.8731, loss=0.7145]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 253/292 [02:54<00:22,  1.74batch/s, auc=0.8731, loss=0.7145]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 253/292 [02:55<00:22,  1.74batch/s, auc=0.8732, loss=0.7105]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 254/292 [02:55<00:21,  1.75batch/s, auc=0.8732, loss=0.7105]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 254/292 [02:56<00:21,  1.75batch/s, auc=0.8730, loss=0.7433]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 255/292 [02:56<00:21,  1.75batch/s, auc=0.8730, loss=0.7433]\u001b[A\n",
      "Training Epoch 23/25:  87%|████████▋ | 255/292 [02:57<00:21,  1.75batch/s, auc=0.8727, loss=1.1284]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 256/292 [02:57<00:26,  1.38batch/s, auc=0.8727, loss=1.1284]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 256/292 [02:58<00:26,  1.38batch/s, auc=0.8722, loss=1.1878]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 257/292 [02:58<00:29,  1.20batch/s, auc=0.8722, loss=1.1878]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 257/292 [02:58<00:29,  1.20batch/s, auc=0.8722, loss=0.4855]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 258/292 [02:58<00:25,  1.33batch/s, auc=0.8722, loss=0.4855]\u001b[A\n",
      "Training Epoch 23/25:  88%|████████▊ | 258/292 [02:59<00:25,  1.33batch/s, auc=0.8723, loss=0.7357]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▊ | 259/292 [02:59<00:22,  1.44batch/s, auc=0.8723, loss=0.7357]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▊ | 259/292 [02:59<00:22,  1.44batch/s, auc=0.8724, loss=0.7487]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 260/292 [02:59<00:20,  1.52batch/s, auc=0.8724, loss=0.7487]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 260/292 [03:00<00:20,  1.52batch/s, auc=0.8727, loss=0.4311]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 261/292 [03:00<00:19,  1.59batch/s, auc=0.8727, loss=0.4311]\u001b[A\n",
      "Training Epoch 23/25:  89%|████████▉ | 261/292 [03:01<00:19,  1.59batch/s, auc=0.8728, loss=0.7512]\u001b[A\n",
      "Training Epoch 23/25:  90%|████████▉ | 262/292 [03:01<00:18,  1.64batch/s, auc=0.8728, loss=0.7512]\u001b[A\n",
      "Training Epoch 23/25:  90%|████████▉ | 262/292 [03:01<00:18,  1.64batch/s, auc=0.8728, loss=0.6169]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 263/292 [03:01<00:17,  1.67batch/s, auc=0.8728, loss=0.6169]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 263/292 [03:02<00:17,  1.67batch/s, auc=0.8729, loss=0.6464]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 264/292 [03:02<00:16,  1.70batch/s, auc=0.8729, loss=0.6464]\u001b[A\n",
      "Training Epoch 23/25:  90%|█████████ | 264/292 [03:02<00:16,  1.70batch/s, auc=0.8729, loss=0.8110]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 265/292 [03:02<00:15,  1.72batch/s, auc=0.8729, loss=0.8110]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 265/292 [03:03<00:15,  1.72batch/s, auc=0.8732, loss=0.4730]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 266/292 [03:03<00:14,  1.74batch/s, auc=0.8732, loss=0.4730]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████ | 266/292 [03:03<00:14,  1.74batch/s, auc=0.8732, loss=0.9986]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████▏| 267/292 [03:03<00:14,  1.75batch/s, auc=0.8732, loss=0.9986]\u001b[A\n",
      "Training Epoch 23/25:  91%|█████████▏| 267/292 [03:04<00:14,  1.75batch/s, auc=0.8731, loss=0.7114]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 268/292 [03:04<00:13,  1.76batch/s, auc=0.8731, loss=0.7114]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 268/292 [03:05<00:13,  1.76batch/s, auc=0.8728, loss=1.0254]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 269/292 [03:05<00:16,  1.38batch/s, auc=0.8728, loss=1.0254]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 269/292 [03:06<00:16,  1.38batch/s, auc=0.8731, loss=0.5407]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 270/292 [03:06<00:14,  1.48batch/s, auc=0.8731, loss=0.5407]\u001b[A\n",
      "Training Epoch 23/25:  92%|█████████▏| 270/292 [03:06<00:14,  1.48batch/s, auc=0.8735, loss=0.5561]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 271/292 [03:06<00:13,  1.56batch/s, auc=0.8735, loss=0.5561]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 271/292 [03:07<00:13,  1.56batch/s, auc=0.8739, loss=0.4737]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 272/292 [03:07<00:12,  1.61batch/s, auc=0.8739, loss=0.4737]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 272/292 [03:08<00:12,  1.61batch/s, auc=0.8736, loss=0.8762]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 273/292 [03:08<00:14,  1.32batch/s, auc=0.8736, loss=0.8762]\u001b[A\n",
      "Training Epoch 23/25:  93%|█████████▎| 273/292 [03:09<00:14,  1.32batch/s, auc=0.8734, loss=0.9117]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 274/292 [03:09<00:15,  1.17batch/s, auc=0.8734, loss=0.9117]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 274/292 [03:10<00:15,  1.17batch/s, auc=0.8730, loss=0.9987]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 275/292 [03:10<00:15,  1.08batch/s, auc=0.8730, loss=0.9987]\u001b[A\n",
      "Training Epoch 23/25:  94%|█████████▍| 275/292 [03:11<00:15,  1.08batch/s, auc=0.8733, loss=0.4785]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 276/292 [03:11<00:13,  1.22batch/s, auc=0.8733, loss=0.4785]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 276/292 [03:11<00:13,  1.22batch/s, auc=0.8735, loss=0.5190]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 277/292 [03:11<00:11,  1.35batch/s, auc=0.8735, loss=0.5190]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▍| 277/292 [03:12<00:11,  1.35batch/s, auc=0.8736, loss=0.5884]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▌| 278/292 [03:12<00:09,  1.45batch/s, auc=0.8736, loss=0.5884]\u001b[A\n",
      "Training Epoch 23/25:  95%|█████████▌| 278/292 [03:12<00:09,  1.45batch/s, auc=0.8735, loss=0.7409]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 279/292 [03:12<00:08,  1.54batch/s, auc=0.8735, loss=0.7409]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 279/292 [03:13<00:08,  1.54batch/s, auc=0.8736, loss=0.6328]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 280/292 [03:13<00:07,  1.60batch/s, auc=0.8736, loss=0.6328]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 280/292 [03:13<00:07,  1.60batch/s, auc=0.8739, loss=0.5309]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 281/292 [03:13<00:06,  1.65batch/s, auc=0.8739, loss=0.5309]\u001b[A\n",
      "Training Epoch 23/25:  96%|█████████▌| 281/292 [03:14<00:06,  1.65batch/s, auc=0.8737, loss=0.9046]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 282/292 [03:14<00:07,  1.33batch/s, auc=0.8737, loss=0.9046]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 282/292 [03:15<00:07,  1.33batch/s, auc=0.8740, loss=0.5691]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 283/292 [03:15<00:06,  1.44batch/s, auc=0.8740, loss=0.5691]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 283/292 [03:16<00:06,  1.44batch/s, auc=0.8744, loss=0.5053]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 284/292 [03:16<00:05,  1.52batch/s, auc=0.8744, loss=0.5053]\u001b[A\n",
      "Training Epoch 23/25:  97%|█████████▋| 284/292 [03:16<00:05,  1.52batch/s, auc=0.8747, loss=0.5209]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 285/292 [03:16<00:04,  1.59batch/s, auc=0.8747, loss=0.5209]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 285/292 [03:17<00:04,  1.59batch/s, auc=0.8749, loss=0.8190]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 286/292 [03:17<00:03,  1.64batch/s, auc=0.8749, loss=0.8190]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 286/292 [03:17<00:03,  1.64batch/s, auc=0.8749, loss=0.7512]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 287/292 [03:17<00:02,  1.68batch/s, auc=0.8749, loss=0.7512]\u001b[A\n",
      "Training Epoch 23/25:  98%|█████████▊| 287/292 [03:18<00:02,  1.68batch/s, auc=0.8751, loss=0.6037]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▊| 288/292 [03:18<00:02,  1.71batch/s, auc=0.8751, loss=0.6037]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▊| 288/292 [03:18<00:02,  1.71batch/s, auc=0.8751, loss=0.6251]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 289/292 [03:18<00:01,  1.73batch/s, auc=0.8751, loss=0.6251]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 289/292 [03:19<00:01,  1.73batch/s, auc=0.8753, loss=0.4771]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 290/292 [03:19<00:01,  1.74batch/s, auc=0.8753, loss=0.4771]\u001b[A\n",
      "Training Epoch 23/25:  99%|█████████▉| 290/292 [03:20<00:01,  1.74batch/s, auc=0.8757, loss=0.5963]\u001b[A\n",
      "Training Epoch 23/25: 100%|█████████▉| 291/292 [03:20<00:00,  1.75batch/s, auc=0.8757, loss=0.5963]\u001b[A\n",
      "Training Epoch 23/25: 100%|█████████▉| 291/292 [03:20<00:00,  1.75batch/s, auc=0.8757, loss=0.5721]\u001b[A\n",
      "Training Epoch 23/25: 100%|██████████| 292/292 [03:20<00:00,  1.46batch/s, auc=0.8757, loss=0.5721]\u001b[A\n",
      "Epochs:  92%|█████████▏| 23/25 [1:22:02<07:09, 214.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25] Train Loss: 0.7392 | Train AUROC: 0.8757 Val Loss: 0.8715 | Val AUROC: 0.8519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 24/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 0/292 [00:02<?, ?batch/s, auc=0.8622, loss=0.7558]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 1/292 [00:02<09:44,  2.01s/batch, auc=0.8622, loss=0.7558]\u001b[A\n",
      "Training Epoch 24/25:   0%|          | 1/292 [00:02<09:44,  2.01s/batch, auc=0.9196, loss=0.5156]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 2/292 [00:02<05:34,  1.15s/batch, auc=0.9196, loss=0.5156]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 2/292 [00:03<05:34,  1.15s/batch, auc=0.9013, loss=0.8114]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 3/292 [00:03<04:14,  1.14batch/s, auc=0.9013, loss=0.8114]\u001b[A\n",
      "Training Epoch 24/25:   1%|          | 3/292 [00:03<04:14,  1.14batch/s, auc=0.8821, loss=1.0003]\u001b[A\n",
      "Training Epoch 24/25:   1%|▏         | 4/292 [00:03<03:36,  1.33batch/s, auc=0.8821, loss=1.0003]\u001b[A\n",
      "Training Epoch 24/25:   1%|▏         | 4/292 [00:04<03:36,  1.33batch/s, auc=0.8747, loss=0.8879]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 5/292 [00:04<04:08,  1.16batch/s, auc=0.8747, loss=0.8879]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 5/292 [00:05<04:08,  1.16batch/s, auc=0.8618, loss=0.7162]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 6/292 [00:05<03:37,  1.32batch/s, auc=0.8618, loss=0.7162]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 6/292 [00:06<03:37,  1.32batch/s, auc=0.8462, loss=1.0438]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 7/292 [00:06<04:05,  1.16batch/s, auc=0.8462, loss=1.0438]\u001b[A\n",
      "Training Epoch 24/25:   2%|▏         | 7/292 [00:06<04:05,  1.16batch/s, auc=0.8584, loss=0.4751]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 8/292 [00:06<03:37,  1.31batch/s, auc=0.8584, loss=0.4751]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 8/292 [00:07<03:37,  1.31batch/s, auc=0.8689, loss=0.4195]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 9/292 [00:07<03:17,  1.43batch/s, auc=0.8689, loss=0.4195]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 9/292 [00:08<03:17,  1.43batch/s, auc=0.8591, loss=0.9487]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 10/292 [00:08<03:49,  1.23batch/s, auc=0.8591, loss=0.9487]\u001b[A\n",
      "Training Epoch 24/25:   3%|▎         | 10/292 [00:09<03:49,  1.23batch/s, auc=0.8698, loss=0.6721]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 11/292 [00:09<03:26,  1.36batch/s, auc=0.8698, loss=0.6721]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 11/292 [00:09<03:26,  1.36batch/s, auc=0.8785, loss=0.4292]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 12/292 [00:09<03:10,  1.47batch/s, auc=0.8785, loss=0.4292]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 12/292 [00:10<03:10,  1.47batch/s, auc=0.8824, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 13/292 [00:10<02:58,  1.56batch/s, auc=0.8824, loss=0.4675]\u001b[A\n",
      "Training Epoch 24/25:   4%|▍         | 13/292 [00:10<02:58,  1.56batch/s, auc=0.8809, loss=0.7263]\u001b[A\n",
      "Training Epoch 24/25:   5%|▍         | 14/292 [00:10<02:51,  1.63batch/s, auc=0.8809, loss=0.7263]\u001b[A\n",
      "Training Epoch 24/25:   5%|▍         | 14/292 [00:11<02:51,  1.63batch/s, auc=0.8705, loss=1.0333]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 15/292 [00:11<03:28,  1.33batch/s, auc=0.8705, loss=1.0333]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 15/292 [00:12<03:28,  1.33batch/s, auc=0.8607, loss=1.0410]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 16/292 [00:12<03:54,  1.18batch/s, auc=0.8607, loss=1.0410]\u001b[A\n",
      "Training Epoch 24/25:   5%|▌         | 16/292 [00:13<03:54,  1.18batch/s, auc=0.8661, loss=0.4143]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 17/292 [00:13<03:29,  1.32batch/s, auc=0.8661, loss=0.4143]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 17/292 [00:14<03:29,  1.32batch/s, auc=0.8704, loss=0.4707]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 18/292 [00:14<03:11,  1.43batch/s, auc=0.8704, loss=0.4707]\u001b[A\n",
      "Training Epoch 24/25:   6%|▌         | 18/292 [00:14<03:11,  1.43batch/s, auc=0.8667, loss=0.8051]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 19/292 [00:14<02:58,  1.53batch/s, auc=0.8667, loss=0.8051]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 19/292 [00:15<02:58,  1.53batch/s, auc=0.8728, loss=0.4307]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 20/292 [00:15<02:50,  1.60batch/s, auc=0.8728, loss=0.4307]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 20/292 [00:16<02:50,  1.60batch/s, auc=0.8717, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 21/292 [00:16<03:25,  1.32batch/s, auc=0.8717, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:   7%|▋         | 21/292 [00:16<03:25,  1.32batch/s, auc=0.8724, loss=0.5831]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 22/292 [00:16<03:08,  1.43batch/s, auc=0.8724, loss=0.5831]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 22/292 [00:17<03:08,  1.43batch/s, auc=0.8751, loss=0.7902]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 23/292 [00:17<02:56,  1.53batch/s, auc=0.8751, loss=0.7902]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 23/292 [00:18<02:56,  1.53batch/s, auc=0.8674, loss=1.2153]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 24/292 [00:18<03:28,  1.28batch/s, auc=0.8674, loss=1.2153]\u001b[A\n",
      "Training Epoch 24/25:   8%|▊         | 24/292 [00:18<03:28,  1.28batch/s, auc=0.8688, loss=0.5865]\u001b[A\n",
      "Training Epoch 24/25:   9%|▊         | 25/292 [00:18<03:10,  1.40batch/s, auc=0.8688, loss=0.5865]\u001b[A\n",
      "Training Epoch 24/25:   9%|▊         | 25/292 [00:19<03:10,  1.40batch/s, auc=0.8697, loss=0.4484]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 26/292 [00:19<02:56,  1.50batch/s, auc=0.8697, loss=0.4484]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 26/292 [00:20<02:56,  1.50batch/s, auc=0.8671, loss=0.8782]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 27/292 [00:20<02:47,  1.58batch/s, auc=0.8671, loss=0.8782]\u001b[A\n",
      "Training Epoch 24/25:   9%|▉         | 27/292 [00:20<02:47,  1.58batch/s, auc=0.8689, loss=0.6785]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 28/292 [00:20<02:40,  1.64batch/s, auc=0.8689, loss=0.6785]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 28/292 [00:21<02:40,  1.64batch/s, auc=0.8680, loss=0.8622]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 29/292 [00:21<03:16,  1.34batch/s, auc=0.8680, loss=0.8622]\u001b[A\n",
      "Training Epoch 24/25:  10%|▉         | 29/292 [00:22<03:16,  1.34batch/s, auc=0.8641, loss=1.1114]\u001b[A\n",
      "Training Epoch 24/25:  10%|█         | 30/292 [00:22<03:00,  1.45batch/s, auc=0.8641, loss=1.1114]\u001b[A\n",
      "Training Epoch 24/25:  10%|█         | 30/292 [00:22<03:00,  1.45batch/s, auc=0.8664, loss=0.4186]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 31/292 [00:22<02:49,  1.54batch/s, auc=0.8664, loss=0.4186]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 31/292 [00:23<02:49,  1.54batch/s, auc=0.8698, loss=0.5581]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 32/292 [00:23<02:41,  1.61batch/s, auc=0.8698, loss=0.5581]\u001b[A\n",
      "Training Epoch 24/25:  11%|█         | 32/292 [00:23<02:41,  1.61batch/s, auc=0.8734, loss=0.3905]\u001b[A\n",
      "Training Epoch 24/25:  11%|█▏        | 33/292 [00:23<02:35,  1.66batch/s, auc=0.8734, loss=0.3905]\u001b[A\n",
      "Training Epoch 24/25:  11%|█▏        | 33/292 [00:24<02:35,  1.66batch/s, auc=0.8748, loss=0.6275]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 34/292 [00:24<02:31,  1.70batch/s, auc=0.8748, loss=0.6275]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 34/292 [00:24<02:31,  1.70batch/s, auc=0.8755, loss=0.7356]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 35/292 [00:24<02:28,  1.73batch/s, auc=0.8755, loss=0.7356]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 35/292 [00:25<02:28,  1.73batch/s, auc=0.8786, loss=0.5057]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 36/292 [00:25<02:26,  1.75batch/s, auc=0.8786, loss=0.5057]\u001b[A\n",
      "Training Epoch 24/25:  12%|█▏        | 36/292 [00:26<02:26,  1.75batch/s, auc=0.8798, loss=0.6634]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 37/292 [00:26<02:24,  1.76batch/s, auc=0.8798, loss=0.6634]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 37/292 [00:27<02:24,  1.76batch/s, auc=0.8762, loss=0.9585]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 38/292 [00:27<03:02,  1.39batch/s, auc=0.8762, loss=0.9585]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 38/292 [00:27<03:02,  1.39batch/s, auc=0.8764, loss=0.5183]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 39/292 [00:27<02:49,  1.49batch/s, auc=0.8764, loss=0.5183]\u001b[A\n",
      "Training Epoch 24/25:  13%|█▎        | 39/292 [00:28<02:49,  1.49batch/s, auc=0.8739, loss=0.9915]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▎        | 40/292 [00:28<02:40,  1.57batch/s, auc=0.8739, loss=0.9915]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▎        | 40/292 [00:28<02:40,  1.57batch/s, auc=0.8742, loss=0.6079]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 41/292 [00:28<02:33,  1.64batch/s, auc=0.8742, loss=0.6079]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 41/292 [00:29<02:33,  1.64batch/s, auc=0.8757, loss=0.6630]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 42/292 [00:29<02:28,  1.68batch/s, auc=0.8757, loss=0.6630]\u001b[A\n",
      "Training Epoch 24/25:  14%|█▍        | 42/292 [00:29<02:28,  1.68batch/s, auc=0.8762, loss=0.5431]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▍        | 43/292 [00:29<02:25,  1.72batch/s, auc=0.8762, loss=0.5431]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▍        | 43/292 [00:30<02:25,  1.72batch/s, auc=0.8787, loss=0.5411]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 44/292 [00:30<02:22,  1.74batch/s, auc=0.8787, loss=0.5411]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 44/292 [00:31<02:22,  1.74batch/s, auc=0.8759, loss=1.0615]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 45/292 [00:31<02:20,  1.76batch/s, auc=0.8759, loss=1.0615]\u001b[A\n",
      "Training Epoch 24/25:  15%|█▌        | 45/292 [00:32<02:20,  1.76batch/s, auc=0.8735, loss=1.1020]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 46/292 [00:32<02:57,  1.39batch/s, auc=0.8735, loss=1.1020]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 46/292 [00:32<02:57,  1.39batch/s, auc=0.8748, loss=0.7436]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 47/292 [00:32<02:44,  1.49batch/s, auc=0.8748, loss=0.7436]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▌        | 47/292 [00:33<02:44,  1.49batch/s, auc=0.8747, loss=0.8377]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▋        | 48/292 [00:33<02:35,  1.57batch/s, auc=0.8747, loss=0.8377]\u001b[A\n",
      "Training Epoch 24/25:  16%|█▋        | 48/292 [00:33<02:35,  1.57batch/s, auc=0.8757, loss=0.5836]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 49/292 [00:33<02:28,  1.63batch/s, auc=0.8757, loss=0.5836]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 49/292 [00:34<02:28,  1.63batch/s, auc=0.8755, loss=0.8653]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 50/292 [00:34<02:24,  1.68batch/s, auc=0.8755, loss=0.8653]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 50/292 [00:34<02:24,  1.68batch/s, auc=0.8762, loss=0.6361]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 51/292 [00:34<02:20,  1.71batch/s, auc=0.8762, loss=0.6361]\u001b[A\n",
      "Training Epoch 24/25:  17%|█▋        | 51/292 [00:35<02:20,  1.71batch/s, auc=0.8779, loss=0.6640]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 52/292 [00:35<02:18,  1.74batch/s, auc=0.8779, loss=0.6640]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 52/292 [00:36<02:18,  1.74batch/s, auc=0.8786, loss=0.5280]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 53/292 [00:36<02:16,  1.75batch/s, auc=0.8786, loss=0.5280]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 53/292 [00:36<02:16,  1.75batch/s, auc=0.8788, loss=0.8589]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 54/292 [00:36<02:14,  1.77batch/s, auc=0.8788, loss=0.8589]\u001b[A\n",
      "Training Epoch 24/25:  18%|█▊        | 54/292 [00:37<02:14,  1.77batch/s, auc=0.8797, loss=0.4782]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 55/292 [00:37<02:13,  1.78batch/s, auc=0.8797, loss=0.4782]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 55/292 [00:37<02:13,  1.78batch/s, auc=0.8778, loss=0.9662]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 56/292 [00:37<02:12,  1.78batch/s, auc=0.8778, loss=0.9662]\u001b[A\n",
      "Training Epoch 24/25:  19%|█▉        | 56/292 [00:38<02:12,  1.78batch/s, auc=0.8785, loss=0.4216]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 57/292 [00:38<02:11,  1.79batch/s, auc=0.8785, loss=0.4216]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 57/292 [00:39<02:11,  1.79batch/s, auc=0.8783, loss=0.7131]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 58/292 [00:39<02:47,  1.40batch/s, auc=0.8783, loss=0.7131]\u001b[A\n",
      "Training Epoch 24/25:  20%|█▉        | 58/292 [00:39<02:47,  1.40batch/s, auc=0.8780, loss=0.8690]\u001b[A\n",
      "Training Epoch 24/25:  20%|██        | 59/292 [00:39<02:35,  1.50batch/s, auc=0.8780, loss=0.8690]\u001b[A\n",
      "Training Epoch 24/25:  20%|██        | 59/292 [00:40<02:35,  1.50batch/s, auc=0.8774, loss=0.6967]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 60/292 [00:40<02:27,  1.58batch/s, auc=0.8774, loss=0.6967]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 60/292 [00:41<02:27,  1.58batch/s, auc=0.8780, loss=0.5248]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 61/292 [00:41<02:21,  1.64batch/s, auc=0.8780, loss=0.5248]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 61/292 [00:41<02:21,  1.64batch/s, auc=0.8800, loss=0.5132]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 62/292 [00:41<02:16,  1.68batch/s, auc=0.8800, loss=0.5132]\u001b[A\n",
      "Training Epoch 24/25:  21%|██        | 62/292 [00:42<02:16,  1.68batch/s, auc=0.8807, loss=0.4830]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 63/292 [00:42<02:13,  1.72batch/s, auc=0.8807, loss=0.4830]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 63/292 [00:42<02:13,  1.72batch/s, auc=0.8803, loss=0.8874]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 64/292 [00:42<02:11,  1.74batch/s, auc=0.8803, loss=0.8874]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 64/292 [00:43<02:11,  1.74batch/s, auc=0.8804, loss=0.8300]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 65/292 [00:43<02:09,  1.76batch/s, auc=0.8804, loss=0.8300]\u001b[A\n",
      "Training Epoch 24/25:  22%|██▏       | 65/292 [00:44<02:09,  1.76batch/s, auc=0.8784, loss=1.1010]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 66/292 [00:44<02:42,  1.39batch/s, auc=0.8784, loss=1.1010]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 66/292 [00:44<02:42,  1.39batch/s, auc=0.8788, loss=0.5887]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 67/292 [00:44<02:31,  1.49batch/s, auc=0.8788, loss=0.5887]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 67/292 [00:45<02:31,  1.49batch/s, auc=0.8747, loss=1.6829]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 68/292 [00:45<02:57,  1.26batch/s, auc=0.8747, loss=1.6829]\u001b[A\n",
      "Training Epoch 24/25:  23%|██▎       | 68/292 [00:47<02:57,  1.26batch/s, auc=0.8736, loss=1.0869]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▎       | 69/292 [00:47<03:15,  1.14batch/s, auc=0.8736, loss=1.0869]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▎       | 69/292 [00:47<03:15,  1.14batch/s, auc=0.8734, loss=0.8286]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 70/292 [00:47<02:53,  1.28batch/s, auc=0.8734, loss=0.8286]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 70/292 [00:48<02:53,  1.28batch/s, auc=0.8718, loss=1.0815]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 71/292 [00:48<03:12,  1.15batch/s, auc=0.8718, loss=1.0815]\u001b[A\n",
      "Training Epoch 24/25:  24%|██▍       | 71/292 [00:49<03:12,  1.15batch/s, auc=0.8714, loss=0.7622]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▍       | 72/292 [00:49<02:50,  1.29batch/s, auc=0.8714, loss=0.7622]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▍       | 72/292 [00:50<02:50,  1.29batch/s, auc=0.8693, loss=1.1919]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 73/292 [00:50<03:09,  1.16batch/s, auc=0.8693, loss=1.1919]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 73/292 [00:50<03:09,  1.16batch/s, auc=0.8694, loss=0.7798]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 74/292 [00:50<02:48,  1.29batch/s, auc=0.8694, loss=0.7798]\u001b[A\n",
      "Training Epoch 24/25:  25%|██▌       | 74/292 [00:51<02:48,  1.29batch/s, auc=0.8697, loss=0.6907]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 75/292 [00:51<02:33,  1.41batch/s, auc=0.8697, loss=0.6907]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 75/292 [00:51<02:33,  1.41batch/s, auc=0.8691, loss=0.8688]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 76/292 [00:51<02:23,  1.51batch/s, auc=0.8691, loss=0.8688]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▌       | 76/292 [00:52<02:23,  1.51batch/s, auc=0.8682, loss=0.9109]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▋       | 77/292 [00:52<02:15,  1.59batch/s, auc=0.8682, loss=0.9109]\u001b[A\n",
      "Training Epoch 24/25:  26%|██▋       | 77/292 [00:53<02:15,  1.59batch/s, auc=0.8682, loss=0.7505]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 78/292 [00:53<02:10,  1.64batch/s, auc=0.8682, loss=0.7505]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 78/292 [00:53<02:10,  1.64batch/s, auc=0.8692, loss=0.4463]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 79/292 [00:53<02:06,  1.69batch/s, auc=0.8692, loss=0.4463]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 79/292 [00:54<02:06,  1.69batch/s, auc=0.8682, loss=1.0061]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 80/292 [00:54<02:36,  1.36batch/s, auc=0.8682, loss=1.0061]\u001b[A\n",
      "Training Epoch 24/25:  27%|██▋       | 80/292 [00:55<02:36,  1.36batch/s, auc=0.8687, loss=0.5954]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 81/292 [00:55<02:24,  1.46batch/s, auc=0.8687, loss=0.5954]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 81/292 [00:55<02:24,  1.46batch/s, auc=0.8685, loss=0.8895]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 82/292 [00:55<02:15,  1.55batch/s, auc=0.8685, loss=0.8895]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 82/292 [00:56<02:15,  1.55batch/s, auc=0.8690, loss=0.6763]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 83/292 [00:56<02:09,  1.61batch/s, auc=0.8690, loss=0.6763]\u001b[A\n",
      "Training Epoch 24/25:  28%|██▊       | 83/292 [00:56<02:09,  1.61batch/s, auc=0.8697, loss=0.5846]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 84/292 [00:56<02:05,  1.66batch/s, auc=0.8697, loss=0.5846]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 84/292 [00:57<02:05,  1.66batch/s, auc=0.8693, loss=0.7252]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 85/292 [00:57<02:01,  1.70batch/s, auc=0.8693, loss=0.7252]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 85/292 [00:58<02:01,  1.70batch/s, auc=0.8698, loss=0.5915]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 86/292 [00:58<01:59,  1.73batch/s, auc=0.8698, loss=0.5915]\u001b[A\n",
      "Training Epoch 24/25:  29%|██▉       | 86/292 [00:58<01:59,  1.73batch/s, auc=0.8702, loss=0.5373]\u001b[A\n",
      "Training Epoch 24/25:  30%|██▉       | 87/292 [00:58<01:57,  1.75batch/s, auc=0.8702, loss=0.5373]\u001b[A\n",
      "Training Epoch 24/25:  30%|██▉       | 87/292 [00:59<01:57,  1.75batch/s, auc=0.8705, loss=0.6956]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 88/292 [00:59<01:55,  1.76batch/s, auc=0.8705, loss=0.6956]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 88/292 [00:59<01:55,  1.76batch/s, auc=0.8711, loss=0.6179]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 89/292 [00:59<01:54,  1.77batch/s, auc=0.8711, loss=0.6179]\u001b[A\n",
      "Training Epoch 24/25:  30%|███       | 89/292 [01:00<01:54,  1.77batch/s, auc=0.8715, loss=0.8309]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 90/292 [01:00<01:53,  1.78batch/s, auc=0.8715, loss=0.8309]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 90/292 [01:00<01:53,  1.78batch/s, auc=0.8720, loss=0.6711]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 91/292 [01:00<01:52,  1.78batch/s, auc=0.8720, loss=0.6711]\u001b[A\n",
      "Training Epoch 24/25:  31%|███       | 91/292 [01:01<01:52,  1.78batch/s, auc=0.8732, loss=0.6413]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 92/292 [01:01<01:52,  1.79batch/s, auc=0.8732, loss=0.6413]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 92/292 [01:01<01:52,  1.79batch/s, auc=0.8726, loss=0.8762]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 93/292 [01:01<01:51,  1.79batch/s, auc=0.8726, loss=0.8762]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 93/292 [01:02<01:51,  1.79batch/s, auc=0.8729, loss=0.7190]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 94/292 [01:02<01:50,  1.79batch/s, auc=0.8729, loss=0.7190]\u001b[A\n",
      "Training Epoch 24/25:  32%|███▏      | 94/292 [01:03<01:50,  1.79batch/s, auc=0.8739, loss=0.5495]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 95/292 [01:03<01:50,  1.79batch/s, auc=0.8739, loss=0.5495]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 95/292 [01:04<01:50,  1.79batch/s, auc=0.8733, loss=0.9376]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 96/292 [01:04<02:19,  1.40batch/s, auc=0.8733, loss=0.9376]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 96/292 [01:04<02:19,  1.40batch/s, auc=0.8729, loss=0.6694]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 97/292 [01:04<02:10,  1.50batch/s, auc=0.8729, loss=0.6694]\u001b[A\n",
      "Training Epoch 24/25:  33%|███▎      | 97/292 [01:05<02:10,  1.50batch/s, auc=0.8735, loss=0.5426]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▎      | 98/292 [01:05<02:03,  1.57batch/s, auc=0.8735, loss=0.5426]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▎      | 98/292 [01:05<02:03,  1.57batch/s, auc=0.8740, loss=0.6840]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 99/292 [01:05<01:58,  1.63batch/s, auc=0.8740, loss=0.6840]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 99/292 [01:06<01:58,  1.63batch/s, auc=0.8726, loss=1.3390]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 100/292 [01:06<02:24,  1.33batch/s, auc=0.8726, loss=1.3390]\u001b[A\n",
      "Training Epoch 24/25:  34%|███▍      | 100/292 [01:07<02:24,  1.33batch/s, auc=0.8716, loss=0.9695]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 101/292 [01:07<02:42,  1.18batch/s, auc=0.8716, loss=0.9695]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 101/292 [01:09<02:42,  1.18batch/s, auc=0.8713, loss=0.7303]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 102/292 [01:09<02:54,  1.09batch/s, auc=0.8713, loss=0.7303]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▍      | 102/292 [01:09<02:54,  1.09batch/s, auc=0.8718, loss=0.5773]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▌      | 103/292 [01:09<02:32,  1.24batch/s, auc=0.8718, loss=0.5773]\u001b[A\n",
      "Training Epoch 24/25:  35%|███▌      | 103/292 [01:10<02:32,  1.24batch/s, auc=0.8730, loss=0.5650]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 104/292 [01:10<02:17,  1.36batch/s, auc=0.8730, loss=0.5650]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 104/292 [01:10<02:17,  1.36batch/s, auc=0.8730, loss=0.4992]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 105/292 [01:10<02:07,  1.47batch/s, auc=0.8730, loss=0.4992]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▌      | 105/292 [01:11<02:07,  1.47batch/s, auc=0.8721, loss=1.0281]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▋      | 106/292 [01:11<02:28,  1.25batch/s, auc=0.8721, loss=1.0281]\u001b[A\n",
      "Training Epoch 24/25:  36%|███▋      | 106/292 [01:12<02:28,  1.25batch/s, auc=0.8720, loss=0.6391]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 107/292 [01:12<02:14,  1.37batch/s, auc=0.8720, loss=0.6391]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 107/292 [01:12<02:14,  1.37batch/s, auc=0.8716, loss=0.8901]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 108/292 [01:12<02:04,  1.48batch/s, auc=0.8716, loss=0.8901]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 108/292 [01:13<02:04,  1.48batch/s, auc=0.8715, loss=0.9564]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 109/292 [01:13<01:57,  1.56batch/s, auc=0.8715, loss=0.9564]\u001b[A\n",
      "Training Epoch 24/25:  37%|███▋      | 109/292 [01:14<01:57,  1.56batch/s, auc=0.8713, loss=0.7440]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 110/292 [01:14<02:20,  1.30batch/s, auc=0.8713, loss=0.7440]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 110/292 [01:15<02:20,  1.30batch/s, auc=0.8716, loss=0.6089]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 111/292 [01:15<02:08,  1.41batch/s, auc=0.8716, loss=0.6089]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 111/292 [01:16<02:08,  1.41batch/s, auc=0.8696, loss=1.6237]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 112/292 [01:16<02:27,  1.22batch/s, auc=0.8696, loss=1.6237]\u001b[A\n",
      "Training Epoch 24/25:  38%|███▊      | 112/292 [01:16<02:27,  1.22batch/s, auc=0.8700, loss=0.6509]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▊      | 113/292 [01:16<02:12,  1.35batch/s, auc=0.8700, loss=0.6509]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▊      | 113/292 [01:17<02:12,  1.35batch/s, auc=0.8705, loss=0.5352]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 114/292 [01:17<02:02,  1.46batch/s, auc=0.8705, loss=0.5352]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 114/292 [01:17<02:02,  1.46batch/s, auc=0.8713, loss=0.5513]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 115/292 [01:17<01:54,  1.54batch/s, auc=0.8713, loss=0.5513]\u001b[A\n",
      "Training Epoch 24/25:  39%|███▉      | 115/292 [01:18<01:54,  1.54batch/s, auc=0.8715, loss=0.6343]\u001b[A\n",
      "Training Epoch 24/25:  40%|███▉      | 116/292 [01:18<01:49,  1.61batch/s, auc=0.8715, loss=0.6343]\u001b[A\n",
      "Training Epoch 24/25:  40%|███▉      | 116/292 [01:18<01:49,  1.61batch/s, auc=0.8717, loss=0.7607]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 117/292 [01:18<01:45,  1.66batch/s, auc=0.8717, loss=0.7607]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 117/292 [01:19<01:45,  1.66batch/s, auc=0.8717, loss=0.7035]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 118/292 [01:19<01:42,  1.70batch/s, auc=0.8717, loss=0.7035]\u001b[A\n",
      "Training Epoch 24/25:  40%|████      | 118/292 [01:20<01:42,  1.70batch/s, auc=0.8718, loss=0.6573]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 119/292 [01:20<01:40,  1.72batch/s, auc=0.8718, loss=0.6573]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 119/292 [01:20<01:40,  1.72batch/s, auc=0.8717, loss=0.6794]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 120/292 [01:20<01:38,  1.74batch/s, auc=0.8717, loss=0.6794]\u001b[A\n",
      "Training Epoch 24/25:  41%|████      | 120/292 [01:21<01:38,  1.74batch/s, auc=0.8711, loss=0.8461]\u001b[A\n",
      "Training Epoch 24/25:  41%|████▏     | 121/292 [01:21<02:04,  1.38batch/s, auc=0.8711, loss=0.8461]\u001b[A\n",
      "Training Epoch 24/25:  41%|████▏     | 121/292 [01:22<02:04,  1.38batch/s, auc=0.8711, loss=0.8400]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 122/292 [01:22<01:54,  1.48batch/s, auc=0.8711, loss=0.8400]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 122/292 [01:22<01:54,  1.48batch/s, auc=0.8713, loss=0.5860]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 123/292 [01:22<01:48,  1.56batch/s, auc=0.8713, loss=0.5860]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 123/292 [01:23<01:48,  1.56batch/s, auc=0.8703, loss=1.1529]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 124/292 [01:23<02:09,  1.30batch/s, auc=0.8703, loss=1.1529]\u001b[A\n",
      "Training Epoch 24/25:  42%|████▏     | 124/292 [01:24<02:09,  1.30batch/s, auc=0.8707, loss=0.5959]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 125/292 [01:24<01:58,  1.41batch/s, auc=0.8707, loss=0.5959]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 125/292 [01:25<01:58,  1.41batch/s, auc=0.8699, loss=1.1377]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 126/292 [01:25<02:15,  1.22batch/s, auc=0.8699, loss=1.1377]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 126/292 [01:26<02:15,  1.22batch/s, auc=0.8695, loss=0.8474]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 127/292 [01:26<02:02,  1.35batch/s, auc=0.8695, loss=0.8474]\u001b[A\n",
      "Training Epoch 24/25:  43%|████▎     | 127/292 [01:26<02:02,  1.35batch/s, auc=0.8696, loss=0.6815]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 128/292 [01:26<01:52,  1.46batch/s, auc=0.8696, loss=0.6815]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 128/292 [01:27<01:52,  1.46batch/s, auc=0.8690, loss=0.8744]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 129/292 [01:27<02:10,  1.24batch/s, auc=0.8690, loss=0.8744]\u001b[A\n",
      "Training Epoch 24/25:  44%|████▍     | 129/292 [01:28<02:10,  1.24batch/s, auc=0.8693, loss=0.7306]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 130/292 [01:28<01:58,  1.37batch/s, auc=0.8693, loss=0.7306]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 130/292 [01:28<01:58,  1.37batch/s, auc=0.8695, loss=0.7009]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 131/292 [01:28<01:49,  1.47batch/s, auc=0.8695, loss=0.7009]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▍     | 131/292 [01:29<01:49,  1.47batch/s, auc=0.8692, loss=0.7750]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▌     | 132/292 [01:29<01:42,  1.55batch/s, auc=0.8692, loss=0.7750]\u001b[A\n",
      "Training Epoch 24/25:  45%|████▌     | 132/292 [01:30<01:42,  1.55batch/s, auc=0.8693, loss=0.4611]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 133/292 [01:30<01:38,  1.62batch/s, auc=0.8693, loss=0.4611]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 133/292 [01:30<01:38,  1.62batch/s, auc=0.8695, loss=0.8847]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 134/292 [01:30<01:34,  1.67batch/s, auc=0.8695, loss=0.8847]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 134/292 [01:31<01:34,  1.67batch/s, auc=0.8689, loss=0.8261]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 135/292 [01:31<01:32,  1.70batch/s, auc=0.8689, loss=0.8261]\u001b[A\n",
      "Training Epoch 24/25:  46%|████▌     | 135/292 [01:31<01:32,  1.70batch/s, auc=0.8692, loss=0.6639]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 136/292 [01:31<01:30,  1.72batch/s, auc=0.8692, loss=0.6639]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 136/292 [01:32<01:30,  1.72batch/s, auc=0.8682, loss=1.2568]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 137/292 [01:32<01:53,  1.37batch/s, auc=0.8682, loss=1.2568]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 137/292 [01:33<01:53,  1.37batch/s, auc=0.8688, loss=0.4048]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 138/292 [01:33<01:44,  1.48batch/s, auc=0.8688, loss=0.4048]\u001b[A\n",
      "Training Epoch 24/25:  47%|████▋     | 138/292 [01:34<01:44,  1.48batch/s, auc=0.8678, loss=1.0819]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 139/292 [01:34<02:01,  1.25batch/s, auc=0.8678, loss=1.0819]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 139/292 [01:34<02:01,  1.25batch/s, auc=0.8678, loss=0.8648]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 140/292 [01:34<01:50,  1.38batch/s, auc=0.8678, loss=0.8648]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 140/292 [01:36<01:50,  1.38batch/s, auc=0.8678, loss=0.6330]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 141/292 [01:36<02:05,  1.20batch/s, auc=0.8678, loss=0.6330]\u001b[A\n",
      "Training Epoch 24/25:  48%|████▊     | 141/292 [01:36<02:05,  1.20batch/s, auc=0.8680, loss=0.7630]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▊     | 142/292 [01:36<01:52,  1.33batch/s, auc=0.8680, loss=0.7630]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▊     | 142/292 [01:37<01:52,  1.33batch/s, auc=0.8685, loss=0.4932]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 143/292 [01:37<01:43,  1.44batch/s, auc=0.8685, loss=0.4932]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 143/292 [01:37<01:43,  1.44batch/s, auc=0.8690, loss=0.5277]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 144/292 [01:37<01:36,  1.53batch/s, auc=0.8690, loss=0.5277]\u001b[A\n",
      "Training Epoch 24/25:  49%|████▉     | 144/292 [01:38<01:36,  1.53batch/s, auc=0.8685, loss=0.9834]\u001b[A\n",
      "Training Epoch 24/25:  50%|████▉     | 145/292 [01:38<01:54,  1.28batch/s, auc=0.8685, loss=0.9834]\u001b[A\n",
      "Training Epoch 24/25:  50%|████▉     | 145/292 [01:39<01:54,  1.28batch/s, auc=0.8685, loss=0.7563]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 146/292 [01:39<01:44,  1.40batch/s, auc=0.8685, loss=0.7563]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 146/292 [01:40<01:44,  1.40batch/s, auc=0.8677, loss=1.1597]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 147/292 [01:40<01:59,  1.22batch/s, auc=0.8677, loss=1.1597]\u001b[A\n",
      "Training Epoch 24/25:  50%|█████     | 147/292 [01:40<01:59,  1.22batch/s, auc=0.8679, loss=0.9238]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 148/292 [01:40<01:47,  1.34batch/s, auc=0.8679, loss=0.9238]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 148/292 [01:41<01:47,  1.34batch/s, auc=0.8687, loss=0.4956]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 149/292 [01:41<01:38,  1.45batch/s, auc=0.8687, loss=0.4956]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████     | 149/292 [01:42<01:38,  1.45batch/s, auc=0.8691, loss=0.7810]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████▏    | 150/292 [01:42<01:32,  1.54batch/s, auc=0.8691, loss=0.7810]\u001b[A\n",
      "Training Epoch 24/25:  51%|█████▏    | 150/292 [01:42<01:32,  1.54batch/s, auc=0.8695, loss=0.6711]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 151/292 [01:42<01:27,  1.61batch/s, auc=0.8695, loss=0.6711]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 151/292 [01:43<01:27,  1.61batch/s, auc=0.8696, loss=0.7630]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 152/292 [01:43<01:46,  1.32batch/s, auc=0.8696, loss=0.7630]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 152/292 [01:44<01:46,  1.32batch/s, auc=0.8695, loss=0.7272]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 153/292 [01:44<01:37,  1.43batch/s, auc=0.8695, loss=0.7272]\u001b[A\n",
      "Training Epoch 24/25:  52%|█████▏    | 153/292 [01:44<01:37,  1.43batch/s, auc=0.8692, loss=0.8610]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 154/292 [01:44<01:30,  1.52batch/s, auc=0.8692, loss=0.8610]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 154/292 [01:45<01:30,  1.52batch/s, auc=0.8693, loss=0.5983]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 155/292 [01:45<01:26,  1.59batch/s, auc=0.8693, loss=0.5983]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 155/292 [01:45<01:26,  1.59batch/s, auc=0.8693, loss=0.7861]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 156/292 [01:45<01:22,  1.64batch/s, auc=0.8693, loss=0.7861]\u001b[A\n",
      "Training Epoch 24/25:  53%|█████▎    | 156/292 [01:46<01:22,  1.64batch/s, auc=0.8694, loss=0.5689]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 157/292 [01:46<01:20,  1.68batch/s, auc=0.8694, loss=0.5689]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 157/292 [01:47<01:20,  1.68batch/s, auc=0.8696, loss=0.7810]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 158/292 [01:47<01:18,  1.71batch/s, auc=0.8696, loss=0.7810]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 158/292 [01:48<01:18,  1.71batch/s, auc=0.8689, loss=0.8660]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 159/292 [01:48<01:37,  1.37batch/s, auc=0.8689, loss=0.8660]\u001b[A\n",
      "Training Epoch 24/25:  54%|█████▍    | 159/292 [01:48<01:37,  1.37batch/s, auc=0.8686, loss=0.9514]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▍    | 160/292 [01:48<01:29,  1.47batch/s, auc=0.8686, loss=0.9514]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▍    | 160/292 [01:49<01:29,  1.47batch/s, auc=0.8685, loss=0.7833]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 161/292 [01:49<01:24,  1.55batch/s, auc=0.8685, loss=0.7833]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 161/292 [01:49<01:24,  1.55batch/s, auc=0.8688, loss=0.6857]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 162/292 [01:49<01:20,  1.61batch/s, auc=0.8688, loss=0.6857]\u001b[A\n",
      "Training Epoch 24/25:  55%|█████▌    | 162/292 [01:50<01:20,  1.61batch/s, auc=0.8690, loss=0.6433]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 163/292 [01:50<01:17,  1.66batch/s, auc=0.8690, loss=0.6433]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 163/292 [01:51<01:17,  1.66batch/s, auc=0.8684, loss=1.1537]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 164/292 [01:51<01:35,  1.34batch/s, auc=0.8684, loss=1.1537]\u001b[A\n",
      "Training Epoch 24/25:  56%|█████▌    | 164/292 [01:52<01:35,  1.34batch/s, auc=0.8679, loss=0.8996]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 165/292 [01:52<01:47,  1.18batch/s, auc=0.8679, loss=0.8996]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 165/292 [01:53<01:47,  1.18batch/s, auc=0.8677, loss=0.8139]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 166/292 [01:53<01:55,  1.09batch/s, auc=0.8677, loss=0.8139]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 166/292 [01:54<01:55,  1.09batch/s, auc=0.8677, loss=0.6720]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 167/292 [01:54<01:41,  1.24batch/s, auc=0.8677, loss=0.6720]\u001b[A\n",
      "Training Epoch 24/25:  57%|█████▋    | 167/292 [01:54<01:41,  1.24batch/s, auc=0.8681, loss=0.5869]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 168/292 [01:54<01:31,  1.36batch/s, auc=0.8681, loss=0.5869]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 168/292 [01:55<01:31,  1.36batch/s, auc=0.8681, loss=0.9688]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 169/292 [01:55<01:23,  1.47batch/s, auc=0.8681, loss=0.9688]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 169/292 [01:55<01:23,  1.47batch/s, auc=0.8680, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 170/292 [01:55<01:18,  1.55batch/s, auc=0.8680, loss=0.6539]\u001b[A\n",
      "Training Epoch 24/25:  58%|█████▊    | 170/292 [01:56<01:18,  1.55batch/s, auc=0.8682, loss=0.4347]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▊    | 171/292 [01:56<01:15,  1.61batch/s, auc=0.8682, loss=0.4347]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▊    | 171/292 [01:57<01:15,  1.61batch/s, auc=0.8674, loss=1.0183]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 172/292 [01:57<01:30,  1.32batch/s, auc=0.8674, loss=1.0183]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 172/292 [01:58<01:30,  1.32batch/s, auc=0.8676, loss=0.5689]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 173/292 [01:58<01:23,  1.43batch/s, auc=0.8676, loss=0.5689]\u001b[A\n",
      "Training Epoch 24/25:  59%|█████▉    | 173/292 [01:58<01:23,  1.43batch/s, auc=0.8677, loss=0.7675]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 174/292 [01:58<01:17,  1.52batch/s, auc=0.8677, loss=0.7675]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 174/292 [01:59<01:17,  1.52batch/s, auc=0.8670, loss=1.2452]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 175/292 [01:59<01:31,  1.28batch/s, auc=0.8670, loss=1.2452]\u001b[A\n",
      "Training Epoch 24/25:  60%|█████▉    | 175/292 [02:00<01:31,  1.28batch/s, auc=0.8664, loss=1.0369]\u001b[A\n",
      "Training Epoch 24/25:  60%|██████    | 176/292 [02:00<01:41,  1.15batch/s, auc=0.8664, loss=1.0369]\u001b[A\n",
      "Training Epoch 24/25:  60%|██████    | 176/292 [02:01<01:41,  1.15batch/s, auc=0.8667, loss=0.5548]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 177/292 [02:01<01:29,  1.28batch/s, auc=0.8667, loss=0.5548]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 177/292 [02:01<01:29,  1.28batch/s, auc=0.8676, loss=0.5231]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 178/292 [02:01<01:21,  1.40batch/s, auc=0.8676, loss=0.5231]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████    | 178/292 [02:03<01:21,  1.40batch/s, auc=0.8673, loss=0.8445]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████▏   | 179/292 [02:03<01:33,  1.21batch/s, auc=0.8673, loss=0.8445]\u001b[A\n",
      "Training Epoch 24/25:  61%|██████▏   | 179/292 [02:03<01:33,  1.21batch/s, auc=0.8679, loss=0.5711]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 180/292 [02:03<01:23,  1.34batch/s, auc=0.8679, loss=0.5711]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 180/292 [02:04<01:23,  1.34batch/s, auc=0.8684, loss=0.5415]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 181/292 [02:04<01:16,  1.45batch/s, auc=0.8684, loss=0.5415]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 181/292 [02:05<01:16,  1.45batch/s, auc=0.8676, loss=1.0167]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 182/292 [02:05<01:28,  1.24batch/s, auc=0.8676, loss=1.0167]\u001b[A\n",
      "Training Epoch 24/25:  62%|██████▏   | 182/292 [02:05<01:28,  1.24batch/s, auc=0.8680, loss=0.5244]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 183/292 [02:05<01:19,  1.36batch/s, auc=0.8680, loss=0.5244]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 183/292 [02:06<01:19,  1.36batch/s, auc=0.8684, loss=0.5846]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 184/292 [02:06<01:13,  1.47batch/s, auc=0.8684, loss=0.5846]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 184/292 [02:07<01:13,  1.47batch/s, auc=0.8674, loss=1.2068]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 185/292 [02:07<01:25,  1.25batch/s, auc=0.8674, loss=1.2068]\u001b[A\n",
      "Training Epoch 24/25:  63%|██████▎   | 185/292 [02:07<01:25,  1.25batch/s, auc=0.8675, loss=0.6293]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▎   | 186/292 [02:07<01:17,  1.37batch/s, auc=0.8675, loss=0.6293]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▎   | 186/292 [02:08<01:17,  1.37batch/s, auc=0.8679, loss=0.5967]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 187/292 [02:08<01:11,  1.47batch/s, auc=0.8679, loss=0.5967]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 187/292 [02:09<01:11,  1.47batch/s, auc=0.8681, loss=0.5307]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 188/292 [02:09<01:06,  1.55batch/s, auc=0.8681, loss=0.5307]\u001b[A\n",
      "Training Epoch 24/25:  64%|██████▍   | 188/292 [02:10<01:06,  1.55batch/s, auc=0.8675, loss=0.9039]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▍   | 189/292 [02:10<01:19,  1.29batch/s, auc=0.8675, loss=0.9039]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▍   | 189/292 [02:10<01:19,  1.29batch/s, auc=0.8673, loss=0.9078]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 190/292 [02:10<01:12,  1.41batch/s, auc=0.8673, loss=0.9078]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 190/292 [02:11<01:12,  1.41batch/s, auc=0.8677, loss=0.5674]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 191/292 [02:11<01:07,  1.50batch/s, auc=0.8677, loss=0.5674]\u001b[A\n",
      "Training Epoch 24/25:  65%|██████▌   | 191/292 [02:11<01:07,  1.50batch/s, auc=0.8676, loss=0.6636]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 192/292 [02:11<01:03,  1.57batch/s, auc=0.8676, loss=0.6636]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 192/292 [02:12<01:03,  1.57batch/s, auc=0.8679, loss=0.4735]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 193/292 [02:12<01:00,  1.63batch/s, auc=0.8679, loss=0.4735]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▌   | 193/292 [02:13<01:00,  1.63batch/s, auc=0.8670, loss=1.6337]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▋   | 194/292 [02:13<01:13,  1.33batch/s, auc=0.8670, loss=1.6337]\u001b[A\n",
      "Training Epoch 24/25:  66%|██████▋   | 194/292 [02:14<01:13,  1.33batch/s, auc=0.8668, loss=0.9144]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 195/292 [02:14<01:07,  1.44batch/s, auc=0.8668, loss=0.9144]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 195/292 [02:14<01:07,  1.44batch/s, auc=0.8668, loss=0.8601]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 196/292 [02:14<01:02,  1.53batch/s, auc=0.8668, loss=0.8601]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 196/292 [02:15<01:02,  1.53batch/s, auc=0.8672, loss=0.5052]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 197/292 [02:15<00:59,  1.59batch/s, auc=0.8672, loss=0.5052]\u001b[A\n",
      "Training Epoch 24/25:  67%|██████▋   | 197/292 [02:16<00:59,  1.59batch/s, auc=0.8666, loss=1.3064]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 198/292 [02:16<01:11,  1.31batch/s, auc=0.8666, loss=1.3064]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 198/292 [02:16<01:11,  1.31batch/s, auc=0.8668, loss=0.8607]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 199/292 [02:16<01:05,  1.42batch/s, auc=0.8668, loss=0.8607]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 199/292 [02:17<01:05,  1.42batch/s, auc=0.8663, loss=1.1255]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 200/292 [02:17<01:14,  1.23batch/s, auc=0.8663, loss=1.1255]\u001b[A\n",
      "Training Epoch 24/25:  68%|██████▊   | 200/292 [02:18<01:14,  1.23batch/s, auc=0.8663, loss=0.6494]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 201/292 [02:18<01:07,  1.35batch/s, auc=0.8663, loss=0.6494]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 201/292 [02:19<01:07,  1.35batch/s, auc=0.8667, loss=0.5530]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 202/292 [02:19<01:01,  1.46batch/s, auc=0.8667, loss=0.5530]\u001b[A\n",
      "Training Epoch 24/25:  69%|██████▉   | 202/292 [02:19<01:01,  1.46batch/s, auc=0.8667, loss=0.6399]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 203/292 [02:19<00:57,  1.54batch/s, auc=0.8667, loss=0.6399]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 203/292 [02:20<00:57,  1.54batch/s, auc=0.8672, loss=0.6013]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 204/292 [02:20<00:54,  1.61batch/s, auc=0.8672, loss=0.6013]\u001b[A\n",
      "Training Epoch 24/25:  70%|██████▉   | 204/292 [02:20<00:54,  1.61batch/s, auc=0.8672, loss=0.7751]\u001b[A\n",
      "Training Epoch 24/25:  70%|███████   | 205/292 [02:20<00:52,  1.66batch/s, auc=0.8672, loss=0.7751]\u001b[A\n",
      "Training Epoch 24/25:  70%|███████   | 205/292 [02:21<00:52,  1.66batch/s, auc=0.8672, loss=0.8413]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 206/292 [02:21<00:50,  1.69batch/s, auc=0.8672, loss=0.8413]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 206/292 [02:21<00:50,  1.69batch/s, auc=0.8672, loss=0.7539]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 207/292 [02:21<00:49,  1.72batch/s, auc=0.8672, loss=0.7539]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 207/292 [02:22<00:49,  1.72batch/s, auc=0.8669, loss=0.9585]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 208/292 [02:22<01:01,  1.37batch/s, auc=0.8669, loss=0.9585]\u001b[A\n",
      "Training Epoch 24/25:  71%|███████   | 208/292 [02:23<01:01,  1.37batch/s, auc=0.8671, loss=0.6363]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 209/292 [02:23<00:56,  1.47batch/s, auc=0.8671, loss=0.6363]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 209/292 [02:24<00:56,  1.47batch/s, auc=0.8668, loss=0.8324]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 210/292 [02:24<00:52,  1.55batch/s, auc=0.8668, loss=0.8324]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 210/292 [02:25<00:52,  1.55batch/s, auc=0.8664, loss=0.9965]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 211/292 [02:25<01:02,  1.29batch/s, auc=0.8664, loss=0.9965]\u001b[A\n",
      "Training Epoch 24/25:  72%|███████▏  | 211/292 [02:25<01:02,  1.29batch/s, auc=0.8663, loss=0.8721]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 212/292 [02:25<00:56,  1.40batch/s, auc=0.8663, loss=0.8721]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 212/292 [02:26<00:56,  1.40batch/s, auc=0.8664, loss=0.6913]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 213/292 [02:26<00:52,  1.50batch/s, auc=0.8664, loss=0.6913]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 213/292 [02:26<00:52,  1.50batch/s, auc=0.8664, loss=0.7538]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 214/292 [02:26<00:49,  1.57batch/s, auc=0.8664, loss=0.7538]\u001b[A\n",
      "Training Epoch 24/25:  73%|███████▎  | 214/292 [02:27<00:49,  1.57batch/s, auc=0.8667, loss=0.6413]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▎  | 215/292 [02:27<00:47,  1.63batch/s, auc=0.8667, loss=0.6413]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▎  | 215/292 [02:27<00:47,  1.63batch/s, auc=0.8669, loss=0.5474]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 216/292 [02:27<00:45,  1.67batch/s, auc=0.8669, loss=0.5474]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 216/292 [02:29<00:45,  1.67batch/s, auc=0.8669, loss=0.7463]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 217/292 [02:29<00:55,  1.35batch/s, auc=0.8669, loss=0.7463]\u001b[A\n",
      "Training Epoch 24/25:  74%|███████▍  | 217/292 [02:29<00:55,  1.35batch/s, auc=0.8673, loss=0.5947]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▍  | 218/292 [02:29<00:50,  1.45batch/s, auc=0.8673, loss=0.5947]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▍  | 218/292 [02:30<00:50,  1.45batch/s, auc=0.8680, loss=0.4541]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 219/292 [02:30<00:47,  1.54batch/s, auc=0.8680, loss=0.4541]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 219/292 [02:30<00:47,  1.54batch/s, auc=0.8682, loss=0.4954]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 220/292 [02:30<00:44,  1.60batch/s, auc=0.8682, loss=0.4954]\u001b[A\n",
      "Training Epoch 24/25:  75%|███████▌  | 220/292 [02:31<00:44,  1.60batch/s, auc=0.8681, loss=0.7719]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 221/292 [02:31<00:43,  1.65batch/s, auc=0.8681, loss=0.7719]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 221/292 [02:31<00:43,  1.65batch/s, auc=0.8681, loss=0.6801]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 222/292 [02:31<00:41,  1.69batch/s, auc=0.8681, loss=0.6801]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▌  | 222/292 [02:32<00:41,  1.69batch/s, auc=0.8680, loss=0.7503]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▋  | 223/292 [02:32<00:40,  1.71batch/s, auc=0.8680, loss=0.7503]\u001b[A\n",
      "Training Epoch 24/25:  76%|███████▋  | 223/292 [02:32<00:40,  1.71batch/s, auc=0.8686, loss=0.4445]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 224/292 [02:32<00:39,  1.73batch/s, auc=0.8686, loss=0.4445]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 224/292 [02:33<00:39,  1.73batch/s, auc=0.8688, loss=0.5855]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 225/292 [02:33<00:38,  1.74batch/s, auc=0.8688, loss=0.5855]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 225/292 [02:34<00:38,  1.74batch/s, auc=0.8688, loss=0.7986]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 226/292 [02:34<00:37,  1.75batch/s, auc=0.8688, loss=0.7986]\u001b[A\n",
      "Training Epoch 24/25:  77%|███████▋  | 226/292 [02:34<00:37,  1.75batch/s, auc=0.8689, loss=0.6579]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 227/292 [02:34<00:36,  1.76batch/s, auc=0.8689, loss=0.6579]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 227/292 [02:35<00:36,  1.76batch/s, auc=0.8691, loss=0.6710]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 228/292 [02:35<00:36,  1.76batch/s, auc=0.8691, loss=0.6710]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 228/292 [02:36<00:36,  1.76batch/s, auc=0.8689, loss=0.9610]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 229/292 [02:36<00:45,  1.39batch/s, auc=0.8689, loss=0.9610]\u001b[A\n",
      "Training Epoch 24/25:  78%|███████▊  | 229/292 [02:36<00:45,  1.39batch/s, auc=0.8691, loss=0.5972]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 230/292 [02:36<00:41,  1.48batch/s, auc=0.8691, loss=0.5972]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 230/292 [02:37<00:41,  1.48batch/s, auc=0.8695, loss=0.4323]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 231/292 [02:37<00:39,  1.56batch/s, auc=0.8695, loss=0.4323]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 231/292 [02:37<00:39,  1.56batch/s, auc=0.8697, loss=0.7140]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 232/292 [02:37<00:37,  1.62batch/s, auc=0.8697, loss=0.7140]\u001b[A\n",
      "Training Epoch 24/25:  79%|███████▉  | 232/292 [02:38<00:37,  1.62batch/s, auc=0.8696, loss=0.6775]\u001b[A\n",
      "Training Epoch 24/25:  80%|███████▉  | 233/292 [02:38<00:35,  1.66batch/s, auc=0.8696, loss=0.6775]\u001b[A\n",
      "Training Epoch 24/25:  80%|███████▉  | 233/292 [02:39<00:35,  1.66batch/s, auc=0.8699, loss=0.5929]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 234/292 [02:39<00:34,  1.70batch/s, auc=0.8699, loss=0.5929]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 234/292 [02:39<00:34,  1.70batch/s, auc=0.8701, loss=0.6956]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 235/292 [02:39<00:33,  1.72batch/s, auc=0.8701, loss=0.6956]\u001b[A\n",
      "Training Epoch 24/25:  80%|████████  | 235/292 [02:40<00:33,  1.72batch/s, auc=0.8706, loss=0.4504]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 236/292 [02:40<00:32,  1.73batch/s, auc=0.8706, loss=0.4504]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 236/292 [02:40<00:32,  1.73batch/s, auc=0.8707, loss=0.6062]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 237/292 [02:40<00:31,  1.75batch/s, auc=0.8707, loss=0.6062]\u001b[A\n",
      "Training Epoch 24/25:  81%|████████  | 237/292 [02:41<00:31,  1.75batch/s, auc=0.8708, loss=0.7757]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 238/292 [02:41<00:30,  1.76batch/s, auc=0.8708, loss=0.7757]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 238/292 [02:41<00:30,  1.76batch/s, auc=0.8706, loss=0.8335]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 239/292 [02:41<00:30,  1.76batch/s, auc=0.8706, loss=0.8335]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 239/292 [02:42<00:30,  1.76batch/s, auc=0.8710, loss=0.6270]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 240/292 [02:42<00:29,  1.76batch/s, auc=0.8710, loss=0.6270]\u001b[A\n",
      "Training Epoch 24/25:  82%|████████▏ | 240/292 [02:43<00:29,  1.76batch/s, auc=0.8712, loss=0.4872]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 241/292 [02:43<00:28,  1.77batch/s, auc=0.8712, loss=0.4872]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 241/292 [02:43<00:28,  1.77batch/s, auc=0.8711, loss=0.7754]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 242/292 [02:43<00:28,  1.77batch/s, auc=0.8711, loss=0.7754]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 242/292 [02:44<00:28,  1.77batch/s, auc=0.8714, loss=0.6334]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 243/292 [02:44<00:27,  1.77batch/s, auc=0.8714, loss=0.6334]\u001b[A\n",
      "Training Epoch 24/25:  83%|████████▎ | 243/292 [02:44<00:27,  1.77batch/s, auc=0.8711, loss=0.8202]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▎ | 244/292 [02:44<00:27,  1.77batch/s, auc=0.8711, loss=0.8202]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▎ | 244/292 [02:45<00:27,  1.77batch/s, auc=0.8716, loss=0.5908]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 245/292 [02:45<00:26,  1.77batch/s, auc=0.8716, loss=0.5908]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 245/292 [02:45<00:26,  1.77batch/s, auc=0.8716, loss=0.7073]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 246/292 [02:45<00:25,  1.77batch/s, auc=0.8716, loss=0.7073]\u001b[A\n",
      "Training Epoch 24/25:  84%|████████▍ | 246/292 [02:46<00:25,  1.77batch/s, auc=0.8716, loss=0.6594]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 247/292 [02:46<00:25,  1.77batch/s, auc=0.8716, loss=0.6594]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 247/292 [02:47<00:25,  1.77batch/s, auc=0.8711, loss=1.1208]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 248/292 [02:47<00:24,  1.77batch/s, auc=0.8711, loss=1.1208]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▍ | 248/292 [02:47<00:24,  1.77batch/s, auc=0.8713, loss=0.6428]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▌ | 249/292 [02:47<00:24,  1.77batch/s, auc=0.8713, loss=0.6428]\u001b[A\n",
      "Training Epoch 24/25:  85%|████████▌ | 249/292 [02:48<00:24,  1.77batch/s, auc=0.8718, loss=0.5138]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 250/292 [02:48<00:23,  1.77batch/s, auc=0.8718, loss=0.5138]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 250/292 [02:48<00:23,  1.77batch/s, auc=0.8720, loss=0.5671]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 251/292 [02:48<00:23,  1.77batch/s, auc=0.8720, loss=0.5671]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▌ | 251/292 [02:49<00:23,  1.77batch/s, auc=0.8714, loss=1.1024]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▋ | 252/292 [02:49<00:28,  1.39batch/s, auc=0.8714, loss=1.1024]\u001b[A\n",
      "Training Epoch 24/25:  86%|████████▋ | 252/292 [02:50<00:28,  1.39batch/s, auc=0.8714, loss=0.8252]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 253/292 [02:50<00:26,  1.49batch/s, auc=0.8714, loss=0.8252]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 253/292 [02:50<00:26,  1.49batch/s, auc=0.8714, loss=0.6631]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 254/292 [02:50<00:24,  1.56batch/s, auc=0.8714, loss=0.6631]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 254/292 [02:51<00:24,  1.56batch/s, auc=0.8715, loss=0.6940]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 255/292 [02:51<00:22,  1.62batch/s, auc=0.8715, loss=0.6940]\u001b[A\n",
      "Training Epoch 24/25:  87%|████████▋ | 255/292 [02:52<00:22,  1.62batch/s, auc=0.8718, loss=0.7530]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 256/292 [02:52<00:21,  1.66batch/s, auc=0.8718, loss=0.7530]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 256/292 [02:52<00:21,  1.66batch/s, auc=0.8720, loss=0.5595]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 257/292 [02:52<00:20,  1.69batch/s, auc=0.8720, loss=0.5595]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 257/292 [02:53<00:20,  1.69batch/s, auc=0.8723, loss=0.4392]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 258/292 [02:53<00:19,  1.72batch/s, auc=0.8723, loss=0.4392]\u001b[A\n",
      "Training Epoch 24/25:  88%|████████▊ | 258/292 [02:53<00:19,  1.72batch/s, auc=0.8725, loss=0.6378]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▊ | 259/292 [02:53<00:19,  1.73batch/s, auc=0.8725, loss=0.6378]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▊ | 259/292 [02:54<00:19,  1.73batch/s, auc=0.8722, loss=0.9572]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 260/292 [02:54<00:18,  1.74batch/s, auc=0.8722, loss=0.9572]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 260/292 [02:55<00:18,  1.74batch/s, auc=0.8724, loss=0.6686]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 261/292 [02:55<00:22,  1.38batch/s, auc=0.8724, loss=0.6686]\u001b[A\n",
      "Training Epoch 24/25:  89%|████████▉ | 261/292 [02:55<00:22,  1.38batch/s, auc=0.8725, loss=0.7682]\u001b[A\n",
      "Training Epoch 24/25:  90%|████████▉ | 262/292 [02:55<00:20,  1.48batch/s, auc=0.8725, loss=0.7682]\u001b[A\n",
      "Training Epoch 24/25:  90%|████████▉ | 262/292 [02:56<00:20,  1.48batch/s, auc=0.8728, loss=0.5580]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 263/292 [02:56<00:18,  1.56batch/s, auc=0.8728, loss=0.5580]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 263/292 [02:57<00:18,  1.56batch/s, auc=0.8728, loss=0.7516]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 264/292 [02:57<00:17,  1.62batch/s, auc=0.8728, loss=0.7516]\u001b[A\n",
      "Training Epoch 24/25:  90%|█████████ | 264/292 [02:57<00:17,  1.62batch/s, auc=0.8730, loss=0.5521]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 265/292 [02:57<00:16,  1.66batch/s, auc=0.8730, loss=0.5521]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 265/292 [02:58<00:16,  1.66batch/s, auc=0.8732, loss=0.4661]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 266/292 [02:58<00:15,  1.69batch/s, auc=0.8732, loss=0.4661]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████ | 266/292 [02:58<00:15,  1.69batch/s, auc=0.8735, loss=0.7082]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████▏| 267/292 [02:58<00:14,  1.71batch/s, auc=0.8735, loss=0.7082]\u001b[A\n",
      "Training Epoch 24/25:  91%|█████████▏| 267/292 [02:59<00:14,  1.71batch/s, auc=0.8727, loss=1.7485]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 268/292 [02:59<00:17,  1.36batch/s, auc=0.8727, loss=1.7485]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 268/292 [03:00<00:17,  1.36batch/s, auc=0.8727, loss=0.7432]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 269/292 [03:00<00:19,  1.19batch/s, auc=0.8727, loss=0.7432]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 269/292 [03:01<00:19,  1.19batch/s, auc=0.8727, loss=0.8279]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 270/292 [03:01<00:16,  1.32batch/s, auc=0.8727, loss=0.8279]\u001b[A\n",
      "Training Epoch 24/25:  92%|█████████▏| 270/292 [03:02<00:16,  1.32batch/s, auc=0.8729, loss=0.5314]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 271/292 [03:02<00:14,  1.43batch/s, auc=0.8729, loss=0.5314]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 271/292 [03:02<00:14,  1.43batch/s, auc=0.8731, loss=0.5839]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 272/292 [03:02<00:13,  1.52batch/s, auc=0.8731, loss=0.5839]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 272/292 [03:03<00:13,  1.52batch/s, auc=0.8733, loss=0.7795]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 273/292 [03:03<00:12,  1.58batch/s, auc=0.8733, loss=0.7795]\u001b[A\n",
      "Training Epoch 24/25:  93%|█████████▎| 273/292 [03:03<00:12,  1.58batch/s, auc=0.8735, loss=0.5966]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 274/292 [03:03<00:11,  1.64batch/s, auc=0.8735, loss=0.5966]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 274/292 [03:04<00:11,  1.64batch/s, auc=0.8736, loss=0.5092]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 275/292 [03:04<00:10,  1.67batch/s, auc=0.8736, loss=0.5092]\u001b[A\n",
      "Training Epoch 24/25:  94%|█████████▍| 275/292 [03:05<00:10,  1.67batch/s, auc=0.8729, loss=1.4072]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 276/292 [03:05<00:11,  1.35batch/s, auc=0.8729, loss=1.4072]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 276/292 [03:05<00:11,  1.35batch/s, auc=0.8731, loss=0.5550]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 277/292 [03:05<00:10,  1.45batch/s, auc=0.8731, loss=0.5550]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▍| 277/292 [03:06<00:10,  1.45batch/s, auc=0.8731, loss=0.8384]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▌| 278/292 [03:06<00:09,  1.53batch/s, auc=0.8731, loss=0.8384]\u001b[A\n",
      "Training Epoch 24/25:  95%|█████████▌| 278/292 [03:07<00:09,  1.53batch/s, auc=0.8733, loss=0.5524]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 279/292 [03:07<00:08,  1.60batch/s, auc=0.8733, loss=0.5524]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 279/292 [03:07<00:08,  1.60batch/s, auc=0.8734, loss=0.6561]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 280/292 [03:07<00:07,  1.64batch/s, auc=0.8734, loss=0.6561]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 280/292 [03:08<00:07,  1.64batch/s, auc=0.8738, loss=0.5756]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 281/292 [03:08<00:06,  1.68batch/s, auc=0.8738, loss=0.5756]\u001b[A\n",
      "Training Epoch 24/25:  96%|█████████▌| 281/292 [03:08<00:06,  1.68batch/s, auc=0.8739, loss=0.6979]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 282/292 [03:08<00:05,  1.71batch/s, auc=0.8739, loss=0.6979]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 282/292 [03:09<00:05,  1.71batch/s, auc=0.8741, loss=0.6443]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 283/292 [03:09<00:05,  1.72batch/s, auc=0.8741, loss=0.6443]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 283/292 [03:09<00:05,  1.72batch/s, auc=0.8744, loss=0.5472]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 284/292 [03:09<00:04,  1.74batch/s, auc=0.8744, loss=0.5472]\u001b[A\n",
      "Training Epoch 24/25:  97%|█████████▋| 284/292 [03:10<00:04,  1.74batch/s, auc=0.8744, loss=0.7051]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 285/292 [03:10<00:04,  1.75batch/s, auc=0.8744, loss=0.7051]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 285/292 [03:11<00:04,  1.75batch/s, auc=0.8740, loss=1.0535]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 286/292 [03:11<00:04,  1.38batch/s, auc=0.8740, loss=1.0535]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 286/292 [03:12<00:04,  1.38batch/s, auc=0.8740, loss=0.6606]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 287/292 [03:12<00:03,  1.48batch/s, auc=0.8740, loss=0.6606]\u001b[A\n",
      "Training Epoch 24/25:  98%|█████████▊| 287/292 [03:12<00:03,  1.48batch/s, auc=0.8739, loss=0.7587]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▊| 288/292 [03:12<00:02,  1.56batch/s, auc=0.8739, loss=0.7587]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▊| 288/292 [03:13<00:02,  1.56batch/s, auc=0.8739, loss=0.6600]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 289/292 [03:13<00:01,  1.61batch/s, auc=0.8739, loss=0.6600]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 289/292 [03:13<00:01,  1.61batch/s, auc=0.8741, loss=0.8321]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 290/292 [03:13<00:01,  1.66batch/s, auc=0.8741, loss=0.8321]\u001b[A\n",
      "Training Epoch 24/25:  99%|█████████▉| 290/292 [03:14<00:01,  1.66batch/s, auc=0.8744, loss=0.6466]\u001b[A\n",
      "Training Epoch 24/25: 100%|█████████▉| 291/292 [03:14<00:00,  1.69batch/s, auc=0.8744, loss=0.6466]\u001b[A\n",
      "Training Epoch 24/25: 100%|█████████▉| 291/292 [03:14<00:00,  1.69batch/s, auc=0.8743, loss=0.7640]\u001b[A\n",
      "Training Epoch 24/25: 100%|██████████| 292/292 [03:14<00:00,  1.50batch/s, auc=0.8743, loss=0.7640]\u001b[A\n",
      "Epochs:  96%|█████████▌| 24/25 [1:25:36<03:34, 214.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25] Train Loss: 0.7430 | Train AUROC: 0.8743 Val Loss: 0.8473 | Val AUROC: 0.8522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Epoch 25/25:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.9353, loss=0.5840]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 1/292 [00:01<07:25,  1.53s/batch, auc=0.9353, loss=0.5840]\u001b[A\n",
      "Training Epoch 25/25:   0%|          | 1/292 [00:02<07:25,  1.53s/batch, auc=0.8903, loss=0.7287]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 2/292 [00:02<04:37,  1.05batch/s, auc=0.8903, loss=0.7287]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 2/292 [00:02<04:37,  1.05batch/s, auc=0.9171, loss=0.5747]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 3/292 [00:02<03:43,  1.29batch/s, auc=0.9171, loss=0.5747]\u001b[A\n",
      "Training Epoch 25/25:   1%|          | 3/292 [00:03<03:43,  1.29batch/s, auc=0.9172, loss=0.6650]\u001b[A\n",
      "Training Epoch 25/25:   1%|▏         | 4/292 [00:03<03:18,  1.45batch/s, auc=0.9172, loss=0.6650]\u001b[A\n",
      "Training Epoch 25/25:   1%|▏         | 4/292 [00:03<03:18,  1.45batch/s, auc=0.9080, loss=0.6500]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 5/292 [00:03<03:03,  1.56batch/s, auc=0.9080, loss=0.6500]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 5/292 [00:04<03:03,  1.56batch/s, auc=0.8993, loss=0.7910]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.8993, loss=0.7910]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 6/292 [00:04<02:54,  1.64batch/s, auc=0.8989, loss=0.8179]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 7/292 [00:04<02:48,  1.69batch/s, auc=0.8989, loss=0.8179]\u001b[A\n",
      "Training Epoch 25/25:   2%|▏         | 7/292 [00:05<02:48,  1.69batch/s, auc=0.8958, loss=0.7139]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 8/292 [00:05<02:44,  1.72batch/s, auc=0.8958, loss=0.7139]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 8/292 [00:05<02:44,  1.72batch/s, auc=0.8915, loss=0.6414]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 9/292 [00:05<02:42,  1.75batch/s, auc=0.8915, loss=0.6414]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 9/292 [00:06<02:42,  1.75batch/s, auc=0.8929, loss=0.6691]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 10/292 [00:06<02:40,  1.76batch/s, auc=0.8929, loss=0.6691]\u001b[A\n",
      "Training Epoch 25/25:   3%|▎         | 10/292 [00:07<02:40,  1.76batch/s, auc=0.8934, loss=0.5785]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 11/292 [00:07<02:38,  1.77batch/s, auc=0.8934, loss=0.5785]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 11/292 [00:07<02:38,  1.77batch/s, auc=0.8955, loss=0.6147]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 12/292 [00:07<02:37,  1.78batch/s, auc=0.8955, loss=0.6147]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 12/292 [00:08<02:37,  1.78batch/s, auc=0.8970, loss=0.6115]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 13/292 [00:08<02:36,  1.79batch/s, auc=0.8970, loss=0.6115]\u001b[A\n",
      "Training Epoch 25/25:   4%|▍         | 13/292 [00:08<02:36,  1.79batch/s, auc=0.8939, loss=0.6464]\u001b[A\n",
      "Training Epoch 25/25:   5%|▍         | 14/292 [00:08<02:35,  1.79batch/s, auc=0.8939, loss=0.6464]\u001b[A\n",
      "Training Epoch 25/25:   5%|▍         | 14/292 [00:09<02:35,  1.79batch/s, auc=0.8883, loss=0.9498]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 15/292 [00:09<03:17,  1.40batch/s, auc=0.8883, loss=0.9498]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 15/292 [00:10<03:17,  1.40batch/s, auc=0.8874, loss=0.7344]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 16/292 [00:10<03:03,  1.50batch/s, auc=0.8874, loss=0.7344]\u001b[A\n",
      "Training Epoch 25/25:   5%|▌         | 16/292 [00:10<03:03,  1.50batch/s, auc=0.8864, loss=0.7477]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 17/292 [00:10<02:53,  1.58batch/s, auc=0.8864, loss=0.7477]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 17/292 [00:11<02:53,  1.58batch/s, auc=0.8820, loss=0.9008]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 18/292 [00:11<02:46,  1.64batch/s, auc=0.8820, loss=0.9008]\u001b[A\n",
      "Training Epoch 25/25:   6%|▌         | 18/292 [00:12<02:46,  1.64batch/s, auc=0.8823, loss=0.7211]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 19/292 [00:12<02:41,  1.69batch/s, auc=0.8823, loss=0.7211]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 19/292 [00:12<02:41,  1.69batch/s, auc=0.8838, loss=0.8301]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 20/292 [00:12<02:38,  1.72batch/s, auc=0.8838, loss=0.8301]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 20/292 [00:13<02:38,  1.72batch/s, auc=0.8824, loss=0.8877]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 21/292 [00:13<02:35,  1.74batch/s, auc=0.8824, loss=0.8877]\u001b[A\n",
      "Training Epoch 25/25:   7%|▋         | 21/292 [00:13<02:35,  1.74batch/s, auc=0.8807, loss=0.7816]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 22/292 [00:13<02:33,  1.76batch/s, auc=0.8807, loss=0.7816]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 22/292 [00:14<02:33,  1.76batch/s, auc=0.8810, loss=0.6296]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 23/292 [00:14<02:31,  1.77batch/s, auc=0.8810, loss=0.6296]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 23/292 [00:14<02:31,  1.77batch/s, auc=0.8836, loss=0.5648]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 24/292 [00:14<02:30,  1.78batch/s, auc=0.8836, loss=0.5648]\u001b[A\n",
      "Training Epoch 25/25:   8%|▊         | 24/292 [00:15<02:30,  1.78batch/s, auc=0.8841, loss=0.5650]\u001b[A\n",
      "Training Epoch 25/25:   9%|▊         | 25/292 [00:15<02:29,  1.79batch/s, auc=0.8841, loss=0.5650]\u001b[A\n",
      "Training Epoch 25/25:   9%|▊         | 25/292 [00:15<02:29,  1.79batch/s, auc=0.8834, loss=0.7209]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 26/292 [00:15<02:28,  1.79batch/s, auc=0.8834, loss=0.7209]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 26/292 [00:16<02:28,  1.79batch/s, auc=0.8831, loss=0.7124]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 27/292 [00:16<02:27,  1.79batch/s, auc=0.8831, loss=0.7124]\u001b[A\n",
      "Training Epoch 25/25:   9%|▉         | 27/292 [00:17<02:27,  1.79batch/s, auc=0.8816, loss=0.8713]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 28/292 [00:17<02:27,  1.80batch/s, auc=0.8816, loss=0.8713]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 28/292 [00:17<02:27,  1.80batch/s, auc=0.8860, loss=0.4472]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 29/292 [00:17<02:26,  1.80batch/s, auc=0.8860, loss=0.4472]\u001b[A\n",
      "Training Epoch 25/25:  10%|▉         | 29/292 [00:18<02:26,  1.80batch/s, auc=0.8866, loss=0.5254]\u001b[A\n",
      "Training Epoch 25/25:  10%|█         | 30/292 [00:18<02:25,  1.80batch/s, auc=0.8866, loss=0.5254]\u001b[A\n",
      "Training Epoch 25/25:  10%|█         | 30/292 [00:18<02:25,  1.80batch/s, auc=0.8864, loss=0.7296]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 31/292 [00:18<02:25,  1.80batch/s, auc=0.8864, loss=0.7296]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 31/292 [00:19<02:25,  1.80batch/s, auc=0.8859, loss=0.7207]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 32/292 [00:19<02:24,  1.80batch/s, auc=0.8859, loss=0.7207]\u001b[A\n",
      "Training Epoch 25/25:  11%|█         | 32/292 [00:19<02:24,  1.80batch/s, auc=0.8844, loss=0.7004]\u001b[A\n",
      "Training Epoch 25/25:  11%|█▏        | 33/292 [00:19<02:23,  1.80batch/s, auc=0.8844, loss=0.7004]\u001b[A\n",
      "Training Epoch 25/25:  11%|█▏        | 33/292 [00:20<02:23,  1.80batch/s, auc=0.8843, loss=0.6398]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 34/292 [00:20<02:23,  1.80batch/s, auc=0.8843, loss=0.6398]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 34/292 [00:20<02:23,  1.80batch/s, auc=0.8849, loss=0.8201]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 35/292 [00:20<02:22,  1.80batch/s, auc=0.8849, loss=0.8201]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 35/292 [00:21<02:22,  1.80batch/s, auc=0.8848, loss=0.6312]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 36/292 [00:21<02:22,  1.80batch/s, auc=0.8848, loss=0.6312]\u001b[A\n",
      "Training Epoch 25/25:  12%|█▏        | 36/292 [00:22<02:22,  1.80batch/s, auc=0.8832, loss=0.8594]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 37/292 [00:22<03:01,  1.41batch/s, auc=0.8832, loss=0.8594]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 37/292 [00:23<03:01,  1.41batch/s, auc=0.8812, loss=1.0012]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 38/292 [00:23<03:28,  1.22batch/s, auc=0.8812, loss=1.0012]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 38/292 [00:24<03:28,  1.22batch/s, auc=0.8789, loss=0.9227]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 39/292 [00:24<03:46,  1.12batch/s, auc=0.8789, loss=0.9227]\u001b[A\n",
      "Training Epoch 25/25:  13%|█▎        | 39/292 [00:25<03:46,  1.12batch/s, auc=0.8793, loss=0.6519]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▎        | 40/292 [00:25<03:20,  1.26batch/s, auc=0.8793, loss=0.6519]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▎        | 40/292 [00:25<03:20,  1.26batch/s, auc=0.8792, loss=0.5900]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 41/292 [00:25<03:01,  1.38batch/s, auc=0.8792, loss=0.5900]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 41/292 [00:26<03:01,  1.38batch/s, auc=0.8789, loss=0.6648]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 42/292 [00:26<02:48,  1.49batch/s, auc=0.8789, loss=0.6648]\u001b[A\n",
      "Training Epoch 25/25:  14%|█▍        | 42/292 [00:26<02:48,  1.49batch/s, auc=0.8794, loss=0.5302]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▍        | 43/292 [00:26<02:38,  1.57batch/s, auc=0.8794, loss=0.5302]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▍        | 43/292 [00:28<02:38,  1.57batch/s, auc=0.8777, loss=0.9199]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 44/292 [00:28<03:10,  1.30batch/s, auc=0.8777, loss=0.9199]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 44/292 [00:28<03:10,  1.30batch/s, auc=0.8781, loss=0.5743]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 45/292 [00:28<02:53,  1.42batch/s, auc=0.8781, loss=0.5743]\u001b[A\n",
      "Training Epoch 25/25:  15%|█▌        | 45/292 [00:29<02:53,  1.42batch/s, auc=0.8791, loss=0.4846]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 46/292 [00:29<02:42,  1.51batch/s, auc=0.8791, loss=0.4846]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 46/292 [00:29<02:42,  1.51batch/s, auc=0.8796, loss=0.6773]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 47/292 [00:29<02:34,  1.59batch/s, auc=0.8796, loss=0.6773]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▌        | 47/292 [00:30<02:34,  1.59batch/s, auc=0.8797, loss=0.7080]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▋        | 48/292 [00:30<02:28,  1.65batch/s, auc=0.8797, loss=0.7080]\u001b[A\n",
      "Training Epoch 25/25:  16%|█▋        | 48/292 [00:30<02:28,  1.65batch/s, auc=0.8807, loss=0.6444]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 49/292 [00:30<02:23,  1.69batch/s, auc=0.8807, loss=0.6444]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 49/292 [00:31<02:23,  1.69batch/s, auc=0.8822, loss=0.5029]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 50/292 [00:31<02:20,  1.72batch/s, auc=0.8822, loss=0.5029]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 50/292 [00:31<02:20,  1.72batch/s, auc=0.8799, loss=1.1527]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 51/292 [00:31<02:18,  1.74batch/s, auc=0.8799, loss=1.1527]\u001b[A\n",
      "Training Epoch 25/25:  17%|█▋        | 51/292 [00:32<02:18,  1.74batch/s, auc=0.8795, loss=0.6783]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 52/292 [00:32<02:16,  1.76batch/s, auc=0.8795, loss=0.6783]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 52/292 [00:33<02:16,  1.76batch/s, auc=0.8782, loss=0.9281]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 53/292 [00:33<02:14,  1.77batch/s, auc=0.8782, loss=0.9281]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 53/292 [00:33<02:14,  1.77batch/s, auc=0.8794, loss=0.5585]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 54/292 [00:33<02:13,  1.78batch/s, auc=0.8794, loss=0.5585]\u001b[A\n",
      "Training Epoch 25/25:  18%|█▊        | 54/292 [00:34<02:13,  1.78batch/s, auc=0.8803, loss=0.6181]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 55/292 [00:34<02:12,  1.78batch/s, auc=0.8803, loss=0.6181]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 55/292 [00:34<02:12,  1.78batch/s, auc=0.8821, loss=0.5241]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 56/292 [00:34<02:12,  1.78batch/s, auc=0.8821, loss=0.5241]\u001b[A\n",
      "Training Epoch 25/25:  19%|█▉        | 56/292 [00:35<02:12,  1.78batch/s, auc=0.8836, loss=0.5629]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 57/292 [00:35<02:11,  1.79batch/s, auc=0.8836, loss=0.5629]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 57/292 [00:35<02:11,  1.79batch/s, auc=0.8830, loss=0.7612]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 58/292 [00:35<02:10,  1.79batch/s, auc=0.8830, loss=0.7612]\u001b[A\n",
      "Training Epoch 25/25:  20%|█▉        | 58/292 [00:36<02:10,  1.79batch/s, auc=0.8836, loss=0.6740]\u001b[A\n",
      "Training Epoch 25/25:  20%|██        | 59/292 [00:36<02:10,  1.79batch/s, auc=0.8836, loss=0.6740]\u001b[A\n",
      "Training Epoch 25/25:  20%|██        | 59/292 [00:36<02:10,  1.79batch/s, auc=0.8846, loss=0.5361]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 60/292 [00:36<02:09,  1.79batch/s, auc=0.8846, loss=0.5361]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 60/292 [00:37<02:09,  1.79batch/s, auc=0.8850, loss=0.5117]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 61/292 [00:37<02:08,  1.80batch/s, auc=0.8850, loss=0.5117]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 61/292 [00:38<02:08,  1.80batch/s, auc=0.8850, loss=0.6975]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 62/292 [00:38<02:08,  1.80batch/s, auc=0.8850, loss=0.6975]\u001b[A\n",
      "Training Epoch 25/25:  21%|██        | 62/292 [00:38<02:08,  1.80batch/s, auc=0.8851, loss=0.8490]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 63/292 [00:38<02:07,  1.80batch/s, auc=0.8851, loss=0.8490]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 63/292 [00:39<02:07,  1.80batch/s, auc=0.8850, loss=0.6165]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 64/292 [00:39<02:06,  1.80batch/s, auc=0.8850, loss=0.6165]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 64/292 [00:39<02:06,  1.80batch/s, auc=0.8852, loss=0.5836]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 65/292 [00:39<02:06,  1.80batch/s, auc=0.8852, loss=0.5836]\u001b[A\n",
      "Training Epoch 25/25:  22%|██▏       | 65/292 [00:40<02:06,  1.80batch/s, auc=0.8856, loss=0.6575]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 66/292 [00:40<02:05,  1.80batch/s, auc=0.8856, loss=0.6575]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 66/292 [00:40<02:05,  1.80batch/s, auc=0.8865, loss=0.6024]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 67/292 [00:40<02:05,  1.80batch/s, auc=0.8865, loss=0.6024]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 67/292 [00:41<02:05,  1.80batch/s, auc=0.8873, loss=0.6084]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 68/292 [00:41<02:04,  1.80batch/s, auc=0.8873, loss=0.6084]\u001b[A\n",
      "Training Epoch 25/25:  23%|██▎       | 68/292 [00:41<02:04,  1.80batch/s, auc=0.8871, loss=0.8104]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▎       | 69/292 [00:41<02:04,  1.80batch/s, auc=0.8871, loss=0.8104]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▎       | 69/292 [00:42<02:04,  1.80batch/s, auc=0.8871, loss=0.8025]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 70/292 [00:42<02:03,  1.80batch/s, auc=0.8871, loss=0.8025]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 70/292 [00:43<02:03,  1.80batch/s, auc=0.8874, loss=0.5442]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 71/292 [00:43<02:03,  1.80batch/s, auc=0.8874, loss=0.5442]\u001b[A\n",
      "Training Epoch 25/25:  24%|██▍       | 71/292 [00:43<02:03,  1.80batch/s, auc=0.8884, loss=0.5140]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▍       | 72/292 [00:43<02:02,  1.80batch/s, auc=0.8884, loss=0.5140]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▍       | 72/292 [00:44<02:02,  1.80batch/s, auc=0.8889, loss=0.4679]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 73/292 [00:44<02:01,  1.80batch/s, auc=0.8889, loss=0.4679]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 73/292 [00:45<02:01,  1.80batch/s, auc=0.8887, loss=0.7463]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 74/292 [00:45<02:35,  1.40batch/s, auc=0.8887, loss=0.7463]\u001b[A\n",
      "Training Epoch 25/25:  25%|██▌       | 74/292 [00:45<02:35,  1.40batch/s, auc=0.8890, loss=0.6575]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 75/292 [00:45<02:24,  1.50batch/s, auc=0.8890, loss=0.6575]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 75/292 [00:46<02:24,  1.50batch/s, auc=0.8867, loss=1.4659]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 76/292 [00:46<02:50,  1.27batch/s, auc=0.8867, loss=1.4659]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▌       | 76/292 [00:47<02:50,  1.27batch/s, auc=0.8858, loss=0.8880]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▋       | 77/292 [00:47<03:07,  1.14batch/s, auc=0.8858, loss=0.8880]\u001b[A\n",
      "Training Epoch 25/25:  26%|██▋       | 77/292 [00:48<03:07,  1.14batch/s, auc=0.8860, loss=0.6358]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 78/292 [00:48<02:46,  1.28batch/s, auc=0.8860, loss=0.6358]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 78/292 [00:49<02:46,  1.28batch/s, auc=0.8867, loss=0.4749]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 79/292 [00:49<02:31,  1.41batch/s, auc=0.8867, loss=0.4749]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 79/292 [00:50<02:31,  1.41batch/s, auc=0.8862, loss=0.9089]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 80/292 [00:50<02:53,  1.22batch/s, auc=0.8862, loss=0.9089]\u001b[A\n",
      "Training Epoch 25/25:  27%|██▋       | 80/292 [00:50<02:53,  1.22batch/s, auc=0.8870, loss=0.6002]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 81/292 [00:50<02:36,  1.35batch/s, auc=0.8870, loss=0.6002]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 81/292 [00:51<02:36,  1.35batch/s, auc=0.8876, loss=0.5574]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 82/292 [00:51<02:23,  1.46batch/s, auc=0.8876, loss=0.5574]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 82/292 [00:52<02:23,  1.46batch/s, auc=0.8861, loss=1.1014]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 83/292 [00:52<02:47,  1.25batch/s, auc=0.8861, loss=1.1014]\u001b[A\n",
      "Training Epoch 25/25:  28%|██▊       | 83/292 [00:53<02:47,  1.25batch/s, auc=0.8857, loss=1.0513]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 84/292 [00:53<03:03,  1.13batch/s, auc=0.8857, loss=1.0513]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 84/292 [00:54<03:03,  1.13batch/s, auc=0.8839, loss=1.2970]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 85/292 [00:54<03:14,  1.06batch/s, auc=0.8839, loss=1.2970]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 85/292 [00:55<03:14,  1.06batch/s, auc=0.8839, loss=0.6739]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 86/292 [00:55<02:50,  1.21batch/s, auc=0.8839, loss=0.6739]\u001b[A\n",
      "Training Epoch 25/25:  29%|██▉       | 86/292 [00:55<02:50,  1.21batch/s, auc=0.8836, loss=0.5948]\u001b[A\n",
      "Training Epoch 25/25:  30%|██▉       | 87/292 [00:55<02:32,  1.34batch/s, auc=0.8836, loss=0.5948]\u001b[A\n",
      "Training Epoch 25/25:  30%|██▉       | 87/292 [00:56<02:32,  1.34batch/s, auc=0.8808, loss=1.5914]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 88/292 [00:56<02:52,  1.18batch/s, auc=0.8808, loss=1.5914]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 88/292 [00:57<02:52,  1.18batch/s, auc=0.8811, loss=0.5919]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 89/292 [00:57<02:33,  1.32batch/s, auc=0.8811, loss=0.5919]\u001b[A\n",
      "Training Epoch 25/25:  30%|███       | 89/292 [00:57<02:33,  1.32batch/s, auc=0.8806, loss=0.8509]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 90/292 [00:57<02:21,  1.43batch/s, auc=0.8806, loss=0.8509]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 90/292 [00:58<02:21,  1.43batch/s, auc=0.8806, loss=0.7517]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 91/292 [00:58<02:11,  1.53batch/s, auc=0.8806, loss=0.7517]\u001b[A\n",
      "Training Epoch 25/25:  31%|███       | 91/292 [00:58<02:11,  1.53batch/s, auc=0.8818, loss=0.5381]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 92/292 [00:58<02:05,  1.60batch/s, auc=0.8818, loss=0.5381]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 92/292 [00:59<02:05,  1.60batch/s, auc=0.8801, loss=1.1765]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 93/292 [00:59<02:31,  1.31batch/s, auc=0.8801, loss=1.1765]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 93/292 [01:00<02:31,  1.31batch/s, auc=0.8809, loss=0.5856]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 94/292 [01:00<02:18,  1.43batch/s, auc=0.8809, loss=0.5856]\u001b[A\n",
      "Training Epoch 25/25:  32%|███▏      | 94/292 [01:01<02:18,  1.43batch/s, auc=0.8806, loss=0.8733]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 95/292 [01:01<02:09,  1.52batch/s, auc=0.8806, loss=0.8733]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 95/292 [01:02<02:09,  1.52batch/s, auc=0.8803, loss=0.8664]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 96/292 [01:02<02:33,  1.28batch/s, auc=0.8803, loss=0.8664]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 96/292 [01:02<02:33,  1.28batch/s, auc=0.8796, loss=0.8510]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 97/292 [01:02<02:19,  1.40batch/s, auc=0.8796, loss=0.8510]\u001b[A\n",
      "Training Epoch 25/25:  33%|███▎      | 97/292 [01:03<02:19,  1.40batch/s, auc=0.8799, loss=0.5805]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▎      | 98/292 [01:03<02:09,  1.50batch/s, auc=0.8799, loss=0.5805]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▎      | 98/292 [01:03<02:09,  1.50batch/s, auc=0.8800, loss=0.5685]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 99/292 [01:03<02:02,  1.58batch/s, auc=0.8800, loss=0.5685]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 99/292 [01:04<02:02,  1.58batch/s, auc=0.8806, loss=0.4643]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 100/292 [01:04<01:57,  1.63batch/s, auc=0.8806, loss=0.4643]\u001b[A\n",
      "Training Epoch 25/25:  34%|███▍      | 100/292 [01:04<01:57,  1.63batch/s, auc=0.8813, loss=0.4696]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 101/292 [01:04<01:53,  1.68batch/s, auc=0.8813, loss=0.4696]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 101/292 [01:05<01:53,  1.68batch/s, auc=0.8809, loss=0.8458]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 102/292 [01:05<01:51,  1.71batch/s, auc=0.8809, loss=0.8458]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▍      | 102/292 [01:06<01:51,  1.71batch/s, auc=0.8803, loss=0.9204]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▌      | 103/292 [01:06<01:48,  1.74batch/s, auc=0.8803, loss=0.9204]\u001b[A\n",
      "Training Epoch 25/25:  35%|███▌      | 103/292 [01:06<01:48,  1.74batch/s, auc=0.8807, loss=0.5015]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 104/292 [01:06<01:47,  1.75batch/s, auc=0.8807, loss=0.5015]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 104/292 [01:07<01:47,  1.75batch/s, auc=0.8813, loss=0.5914]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 105/292 [01:07<01:46,  1.76batch/s, auc=0.8813, loss=0.5914]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▌      | 105/292 [01:08<01:46,  1.76batch/s, auc=0.8794, loss=1.4672]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▋      | 106/292 [01:08<02:13,  1.39batch/s, auc=0.8794, loss=1.4672]\u001b[A\n",
      "Training Epoch 25/25:  36%|███▋      | 106/292 [01:08<02:13,  1.39batch/s, auc=0.8791, loss=0.8549]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 107/292 [01:08<02:04,  1.49batch/s, auc=0.8791, loss=0.8549]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 107/292 [01:09<02:04,  1.49batch/s, auc=0.8800, loss=0.6200]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 108/292 [01:09<01:57,  1.57batch/s, auc=0.8800, loss=0.6200]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 108/292 [01:10<01:57,  1.57batch/s, auc=0.8786, loss=1.1838]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 109/292 [01:10<02:20,  1.30batch/s, auc=0.8786, loss=1.1838]\u001b[A\n",
      "Training Epoch 25/25:  37%|███▋      | 109/292 [01:10<02:20,  1.30batch/s, auc=0.8789, loss=0.5992]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 110/292 [01:10<02:08,  1.42batch/s, auc=0.8789, loss=0.5992]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 110/292 [01:11<02:08,  1.42batch/s, auc=0.8784, loss=0.8832]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 111/292 [01:11<01:59,  1.51batch/s, auc=0.8784, loss=0.8832]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 111/292 [01:12<01:59,  1.51batch/s, auc=0.8788, loss=0.6466]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 112/292 [01:12<01:53,  1.59batch/s, auc=0.8788, loss=0.6466]\u001b[A\n",
      "Training Epoch 25/25:  38%|███▊      | 112/292 [01:12<01:53,  1.59batch/s, auc=0.8782, loss=0.9517]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▊      | 113/292 [01:12<01:48,  1.64batch/s, auc=0.8782, loss=0.9517]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▊      | 113/292 [01:13<01:48,  1.64batch/s, auc=0.8785, loss=0.5582]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 114/292 [01:13<01:45,  1.68batch/s, auc=0.8785, loss=0.5582]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 114/292 [01:13<01:45,  1.68batch/s, auc=0.8789, loss=0.4832]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 115/292 [01:13<01:43,  1.72batch/s, auc=0.8789, loss=0.4832]\u001b[A\n",
      "Training Epoch 25/25:  39%|███▉      | 115/292 [01:14<01:43,  1.72batch/s, auc=0.8796, loss=0.6438]\u001b[A\n",
      "Training Epoch 25/25:  40%|███▉      | 116/292 [01:14<01:41,  1.74batch/s, auc=0.8796, loss=0.6438]\u001b[A\n",
      "Training Epoch 25/25:  40%|███▉      | 116/292 [01:14<01:41,  1.74batch/s, auc=0.8795, loss=0.7428]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 117/292 [01:14<01:39,  1.75batch/s, auc=0.8795, loss=0.7428]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 117/292 [01:15<01:39,  1.75batch/s, auc=0.8794, loss=0.8126]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 118/292 [01:15<02:05,  1.39batch/s, auc=0.8794, loss=0.8126]\u001b[A\n",
      "Training Epoch 25/25:  40%|████      | 118/292 [01:16<02:05,  1.39batch/s, auc=0.8794, loss=0.7452]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 119/292 [01:16<01:56,  1.49batch/s, auc=0.8794, loss=0.7452]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 119/292 [01:17<01:56,  1.49batch/s, auc=0.8794, loss=0.7477]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 120/292 [01:17<01:49,  1.57batch/s, auc=0.8794, loss=0.7477]\u001b[A\n",
      "Training Epoch 25/25:  41%|████      | 120/292 [01:17<01:49,  1.57batch/s, auc=0.8800, loss=0.5507]\u001b[A\n",
      "Training Epoch 25/25:  41%|████▏     | 121/292 [01:17<01:45,  1.63batch/s, auc=0.8800, loss=0.5507]\u001b[A\n",
      "Training Epoch 25/25:  41%|████▏     | 121/292 [01:18<01:45,  1.63batch/s, auc=0.8806, loss=0.5095]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 122/292 [01:18<01:41,  1.67batch/s, auc=0.8806, loss=0.5095]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 122/292 [01:18<01:41,  1.67batch/s, auc=0.8806, loss=0.6641]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 123/292 [01:18<01:39,  1.71batch/s, auc=0.8806, loss=0.6641]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 123/292 [01:19<01:39,  1.71batch/s, auc=0.8817, loss=0.4260]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 124/292 [01:19<01:37,  1.73batch/s, auc=0.8817, loss=0.4260]\u001b[A\n",
      "Training Epoch 25/25:  42%|████▏     | 124/292 [01:19<01:37,  1.73batch/s, auc=0.8822, loss=0.4909]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 125/292 [01:19<01:35,  1.74batch/s, auc=0.8822, loss=0.4909]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 125/292 [01:20<01:35,  1.74batch/s, auc=0.8828, loss=0.5830]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 126/292 [01:20<01:34,  1.76batch/s, auc=0.8828, loss=0.5830]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 126/292 [01:20<01:34,  1.76batch/s, auc=0.8834, loss=0.4066]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 127/292 [01:20<01:33,  1.77batch/s, auc=0.8834, loss=0.4066]\u001b[A\n",
      "Training Epoch 25/25:  43%|████▎     | 127/292 [01:21<01:33,  1.77batch/s, auc=0.8838, loss=0.6215]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 128/292 [01:21<01:32,  1.77batch/s, auc=0.8838, loss=0.6215]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 128/292 [01:22<01:32,  1.77batch/s, auc=0.8843, loss=0.5498]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 129/292 [01:22<01:31,  1.78batch/s, auc=0.8843, loss=0.5498]\u001b[A\n",
      "Training Epoch 25/25:  44%|████▍     | 129/292 [01:22<01:31,  1.78batch/s, auc=0.8844, loss=0.5413]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 130/292 [01:22<01:31,  1.78batch/s, auc=0.8844, loss=0.5413]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 130/292 [01:23<01:31,  1.78batch/s, auc=0.8839, loss=0.8754]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 131/292 [01:23<01:55,  1.39batch/s, auc=0.8839, loss=0.8754]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▍     | 131/292 [01:24<01:55,  1.39batch/s, auc=0.8843, loss=0.6311]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▌     | 132/292 [01:24<01:47,  1.49batch/s, auc=0.8843, loss=0.6311]\u001b[A\n",
      "Training Epoch 25/25:  45%|████▌     | 132/292 [01:24<01:47,  1.49batch/s, auc=0.8842, loss=0.6390]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 133/292 [01:24<01:41,  1.57batch/s, auc=0.8842, loss=0.6390]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 133/292 [01:25<01:41,  1.57batch/s, auc=0.8843, loss=0.6924]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 134/292 [01:25<01:36,  1.63batch/s, auc=0.8843, loss=0.6924]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 134/292 [01:25<01:36,  1.63batch/s, auc=0.8844, loss=0.6054]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 135/292 [01:25<01:33,  1.67batch/s, auc=0.8844, loss=0.6054]\u001b[A\n",
      "Training Epoch 25/25:  46%|████▌     | 135/292 [01:27<01:33,  1.67batch/s, auc=0.8844, loss=0.7455]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 136/292 [01:27<01:55,  1.35batch/s, auc=0.8844, loss=0.7455]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 136/292 [01:27<01:55,  1.35batch/s, auc=0.8844, loss=0.5162]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 137/292 [01:27<01:46,  1.46batch/s, auc=0.8844, loss=0.5162]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 137/292 [01:28<01:46,  1.46batch/s, auc=0.8844, loss=0.7071]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 138/292 [01:28<01:39,  1.54batch/s, auc=0.8844, loss=0.7071]\u001b[A\n",
      "Training Epoch 25/25:  47%|████▋     | 138/292 [01:28<01:39,  1.54batch/s, auc=0.8845, loss=0.6064]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 139/292 [01:28<01:35,  1.61batch/s, auc=0.8845, loss=0.6064]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 139/292 [01:29<01:35,  1.61batch/s, auc=0.8844, loss=0.6916]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 140/292 [01:29<01:31,  1.66batch/s, auc=0.8844, loss=0.6916]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 140/292 [01:29<01:31,  1.66batch/s, auc=0.8844, loss=0.4990]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 141/292 [01:29<01:29,  1.69batch/s, auc=0.8844, loss=0.4990]\u001b[A\n",
      "Training Epoch 25/25:  48%|████▊     | 141/292 [01:30<01:29,  1.69batch/s, auc=0.8843, loss=0.7408]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▊     | 142/292 [01:30<01:50,  1.36batch/s, auc=0.8843, loss=0.7408]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▊     | 142/292 [01:31<01:50,  1.36batch/s, auc=0.8843, loss=0.7128]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 143/292 [01:31<01:41,  1.46batch/s, auc=0.8843, loss=0.7128]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 143/292 [01:32<01:41,  1.46batch/s, auc=0.8847, loss=0.5407]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 144/292 [01:32<01:35,  1.55batch/s, auc=0.8847, loss=0.5407]\u001b[A\n",
      "Training Epoch 25/25:  49%|████▉     | 144/292 [01:32<01:35,  1.55batch/s, auc=0.8856, loss=0.3391]\u001b[A\n",
      "Training Epoch 25/25:  50%|████▉     | 145/292 [01:32<01:31,  1.61batch/s, auc=0.8856, loss=0.3391]\u001b[A\n",
      "Training Epoch 25/25:  50%|████▉     | 145/292 [01:33<01:31,  1.61batch/s, auc=0.8856, loss=0.7793]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 146/292 [01:33<01:27,  1.66batch/s, auc=0.8856, loss=0.7793]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 146/292 [01:33<01:27,  1.66batch/s, auc=0.8854, loss=0.6550]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 147/292 [01:33<01:25,  1.70batch/s, auc=0.8854, loss=0.6550]\u001b[A\n",
      "Training Epoch 25/25:  50%|█████     | 147/292 [01:34<01:25,  1.70batch/s, auc=0.8859, loss=0.4754]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 148/292 [01:34<01:23,  1.72batch/s, auc=0.8859, loss=0.4754]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 148/292 [01:34<01:23,  1.72batch/s, auc=0.8863, loss=0.5791]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 149/292 [01:34<01:22,  1.74batch/s, auc=0.8863, loss=0.5791]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████     | 149/292 [01:35<01:22,  1.74batch/s, auc=0.8866, loss=0.3653]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████▏    | 150/292 [01:35<01:20,  1.75batch/s, auc=0.8866, loss=0.3653]\u001b[A\n",
      "Training Epoch 25/25:  51%|█████▏    | 150/292 [01:35<01:20,  1.75batch/s, auc=0.8865, loss=0.7237]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 151/292 [01:35<01:19,  1.76batch/s, auc=0.8865, loss=0.7237]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 151/292 [01:36<01:19,  1.76batch/s, auc=0.8869, loss=0.3879]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 152/292 [01:36<01:19,  1.77batch/s, auc=0.8869, loss=0.3879]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 152/292 [01:37<01:19,  1.77batch/s, auc=0.8860, loss=1.0901]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 153/292 [01:37<01:39,  1.39batch/s, auc=0.8860, loss=1.0901]\u001b[A\n",
      "Training Epoch 25/25:  52%|█████▏    | 153/292 [01:38<01:39,  1.39batch/s, auc=0.8851, loss=1.1525]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 154/292 [01:38<01:32,  1.49batch/s, auc=0.8851, loss=1.1525]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 154/292 [01:38<01:32,  1.49batch/s, auc=0.8849, loss=0.8979]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 155/292 [01:38<01:27,  1.57batch/s, auc=0.8849, loss=0.8979]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 155/292 [01:39<01:27,  1.57batch/s, auc=0.8848, loss=0.7188]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 156/292 [01:39<01:23,  1.63batch/s, auc=0.8848, loss=0.7188]\u001b[A\n",
      "Training Epoch 25/25:  53%|█████▎    | 156/292 [01:39<01:23,  1.63batch/s, auc=0.8845, loss=0.9909]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 157/292 [01:39<01:20,  1.67batch/s, auc=0.8845, loss=0.9909]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 157/292 [01:40<01:20,  1.67batch/s, auc=0.8848, loss=0.4617]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 158/292 [01:40<01:18,  1.70batch/s, auc=0.8848, loss=0.4617]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 158/292 [01:40<01:18,  1.70batch/s, auc=0.8851, loss=0.4463]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 159/292 [01:40<01:17,  1.73batch/s, auc=0.8851, loss=0.4463]\u001b[A\n",
      "Training Epoch 25/25:  54%|█████▍    | 159/292 [01:41<01:17,  1.73batch/s, auc=0.8849, loss=0.9817]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▍    | 160/292 [01:41<01:15,  1.74batch/s, auc=0.8849, loss=0.9817]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▍    | 160/292 [01:42<01:15,  1.74batch/s, auc=0.8853, loss=0.4904]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 161/292 [01:42<01:14,  1.76batch/s, auc=0.8853, loss=0.4904]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 161/292 [01:42<01:14,  1.76batch/s, auc=0.8860, loss=0.5339]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 162/292 [01:42<01:13,  1.77batch/s, auc=0.8860, loss=0.5339]\u001b[A\n",
      "Training Epoch 25/25:  55%|█████▌    | 162/292 [01:43<01:13,  1.77batch/s, auc=0.8858, loss=0.7625]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 163/292 [01:43<01:12,  1.77batch/s, auc=0.8858, loss=0.7625]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 163/292 [01:44<01:12,  1.77batch/s, auc=0.8855, loss=0.8207]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 164/292 [01:44<01:31,  1.39batch/s, auc=0.8855, loss=0.8207]\u001b[A\n",
      "Training Epoch 25/25:  56%|█████▌    | 164/292 [01:44<01:31,  1.39batch/s, auc=0.8857, loss=0.6179]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 165/292 [01:44<01:25,  1.49batch/s, auc=0.8857, loss=0.6179]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 165/292 [01:45<01:25,  1.49batch/s, auc=0.8860, loss=0.5457]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 166/292 [01:45<01:20,  1.57batch/s, auc=0.8860, loss=0.5457]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 166/292 [01:45<01:20,  1.57batch/s, auc=0.8856, loss=0.8970]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 167/292 [01:45<01:16,  1.62batch/s, auc=0.8856, loss=0.8970]\u001b[A\n",
      "Training Epoch 25/25:  57%|█████▋    | 167/292 [01:47<01:16,  1.62batch/s, auc=0.8848, loss=1.1118]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 168/292 [01:47<01:33,  1.32batch/s, auc=0.8848, loss=1.1118]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 168/292 [01:47<01:33,  1.32batch/s, auc=0.8850, loss=0.7086]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 169/292 [01:47<01:25,  1.44batch/s, auc=0.8850, loss=0.7086]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 169/292 [01:48<01:25,  1.44batch/s, auc=0.8851, loss=0.6183]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 170/292 [01:48<01:20,  1.52batch/s, auc=0.8851, loss=0.6183]\u001b[A\n",
      "Training Epoch 25/25:  58%|█████▊    | 170/292 [01:48<01:20,  1.52batch/s, auc=0.8855, loss=0.6342]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▊    | 171/292 [01:48<01:15,  1.59batch/s, auc=0.8855, loss=0.6342]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▊    | 171/292 [01:49<01:15,  1.59batch/s, auc=0.8855, loss=0.7205]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 172/292 [01:49<01:12,  1.64batch/s, auc=0.8855, loss=0.7205]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 172/292 [01:49<01:12,  1.64batch/s, auc=0.8856, loss=0.6000]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 173/292 [01:49<01:10,  1.68batch/s, auc=0.8856, loss=0.6000]\u001b[A\n",
      "Training Epoch 25/25:  59%|█████▉    | 173/292 [01:50<01:10,  1.68batch/s, auc=0.8857, loss=0.6213]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 174/292 [01:50<01:09,  1.71batch/s, auc=0.8857, loss=0.6213]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 174/292 [01:50<01:09,  1.71batch/s, auc=0.8855, loss=0.8359]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 175/292 [01:50<01:07,  1.73batch/s, auc=0.8855, loss=0.8359]\u001b[A\n",
      "Training Epoch 25/25:  60%|█████▉    | 175/292 [01:51<01:07,  1.73batch/s, auc=0.8857, loss=0.5747]\u001b[A\n",
      "Training Epoch 25/25:  60%|██████    | 176/292 [01:51<01:06,  1.75batch/s, auc=0.8857, loss=0.5747]\u001b[A\n",
      "Training Epoch 25/25:  60%|██████    | 176/292 [01:52<01:06,  1.75batch/s, auc=0.8861, loss=0.4930]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 177/292 [01:52<01:05,  1.76batch/s, auc=0.8861, loss=0.4930]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 177/292 [01:53<01:05,  1.76batch/s, auc=0.8858, loss=0.8326]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 178/292 [01:53<01:22,  1.39batch/s, auc=0.8858, loss=0.8326]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████    | 178/292 [01:53<01:22,  1.39batch/s, auc=0.8861, loss=0.4511]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████▏   | 179/292 [01:53<01:16,  1.48batch/s, auc=0.8861, loss=0.4511]\u001b[A\n",
      "Training Epoch 25/25:  61%|██████▏   | 179/292 [01:54<01:16,  1.48batch/s, auc=0.8861, loss=0.6477]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 180/292 [01:54<01:11,  1.56batch/s, auc=0.8861, loss=0.6477]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 180/292 [01:54<01:11,  1.56batch/s, auc=0.8862, loss=0.5172]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 181/292 [01:54<01:08,  1.62batch/s, auc=0.8862, loss=0.5172]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 181/292 [01:55<01:08,  1.62batch/s, auc=0.8852, loss=1.3264]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 182/292 [01:55<01:23,  1.32batch/s, auc=0.8852, loss=1.3264]\u001b[A\n",
      "Training Epoch 25/25:  62%|██████▏   | 182/292 [01:56<01:23,  1.32batch/s, auc=0.8851, loss=0.5442]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 183/292 [01:56<01:16,  1.43batch/s, auc=0.8851, loss=0.5442]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 183/292 [01:57<01:16,  1.43batch/s, auc=0.8855, loss=0.4924]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 184/292 [01:57<01:10,  1.52batch/s, auc=0.8855, loss=0.4924]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 184/292 [01:57<01:10,  1.52batch/s, auc=0.8856, loss=0.6256]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 185/292 [01:57<01:07,  1.59batch/s, auc=0.8856, loss=0.6256]\u001b[A\n",
      "Training Epoch 25/25:  63%|██████▎   | 185/292 [01:58<01:07,  1.59batch/s, auc=0.8849, loss=1.0647]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▎   | 186/292 [01:58<01:20,  1.31batch/s, auc=0.8849, loss=1.0647]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▎   | 186/292 [01:59<01:20,  1.31batch/s, auc=0.8845, loss=1.2404]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 187/292 [01:59<01:13,  1.42batch/s, auc=0.8845, loss=1.2404]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 187/292 [01:59<01:13,  1.42batch/s, auc=0.8846, loss=0.6834]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 188/292 [01:59<01:08,  1.51batch/s, auc=0.8846, loss=0.6834]\u001b[A\n",
      "Training Epoch 25/25:  64%|██████▍   | 188/292 [02:00<01:08,  1.51batch/s, auc=0.8848, loss=0.5436]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▍   | 189/292 [02:00<01:05,  1.58batch/s, auc=0.8848, loss=0.5436]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▍   | 189/292 [02:00<01:05,  1.58batch/s, auc=0.8841, loss=1.0715]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 190/292 [02:00<01:02,  1.64batch/s, auc=0.8841, loss=1.0715]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 190/292 [02:01<01:02,  1.64batch/s, auc=0.8843, loss=0.4991]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 191/292 [02:01<01:00,  1.68batch/s, auc=0.8843, loss=0.4991]\u001b[A\n",
      "Training Epoch 25/25:  65%|██████▌   | 191/292 [02:02<01:00,  1.68batch/s, auc=0.8844, loss=0.6368]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 192/292 [02:02<00:58,  1.71batch/s, auc=0.8844, loss=0.6368]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 192/292 [02:02<00:58,  1.71batch/s, auc=0.8847, loss=0.5956]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 193/292 [02:02<00:57,  1.73batch/s, auc=0.8847, loss=0.5956]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▌   | 193/292 [02:03<00:57,  1.73batch/s, auc=0.8852, loss=0.5111]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▋   | 194/292 [02:03<00:56,  1.74batch/s, auc=0.8852, loss=0.5111]\u001b[A\n",
      "Training Epoch 25/25:  66%|██████▋   | 194/292 [02:03<00:56,  1.74batch/s, auc=0.8854, loss=0.6124]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 195/292 [02:03<00:55,  1.75batch/s, auc=0.8854, loss=0.6124]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 195/292 [02:04<00:55,  1.75batch/s, auc=0.8853, loss=0.8319]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 196/292 [02:04<00:54,  1.76batch/s, auc=0.8853, loss=0.8319]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 196/292 [02:04<00:54,  1.76batch/s, auc=0.8855, loss=0.6145]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 197/292 [02:04<00:53,  1.77batch/s, auc=0.8855, loss=0.6145]\u001b[A\n",
      "Training Epoch 25/25:  67%|██████▋   | 197/292 [02:05<00:53,  1.77batch/s, auc=0.8855, loss=0.6271]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 198/292 [02:05<00:53,  1.77batch/s, auc=0.8855, loss=0.6271]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 198/292 [02:06<00:53,  1.77batch/s, auc=0.8852, loss=0.8056]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 199/292 [02:06<00:52,  1.77batch/s, auc=0.8852, loss=0.8056]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 199/292 [02:07<00:52,  1.77batch/s, auc=0.8851, loss=0.7956]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 200/292 [02:07<01:06,  1.39batch/s, auc=0.8851, loss=0.7956]\u001b[A\n",
      "Training Epoch 25/25:  68%|██████▊   | 200/292 [02:07<01:06,  1.39batch/s, auc=0.8850, loss=0.7455]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 201/292 [02:07<01:01,  1.49batch/s, auc=0.8850, loss=0.7455]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 201/292 [02:08<01:01,  1.49batch/s, auc=0.8855, loss=0.4048]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 202/292 [02:08<00:57,  1.56batch/s, auc=0.8855, loss=0.4048]\u001b[A\n",
      "Training Epoch 25/25:  69%|██████▉   | 202/292 [02:08<00:57,  1.56batch/s, auc=0.8857, loss=0.6961]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 203/292 [02:08<00:54,  1.62batch/s, auc=0.8857, loss=0.6961]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 203/292 [02:09<00:54,  1.62batch/s, auc=0.8858, loss=0.7032]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 204/292 [02:09<00:52,  1.67batch/s, auc=0.8858, loss=0.7032]\u001b[A\n",
      "Training Epoch 25/25:  70%|██████▉   | 204/292 [02:09<00:52,  1.67batch/s, auc=0.8857, loss=0.7488]\u001b[A\n",
      "Training Epoch 25/25:  70%|███████   | 205/292 [02:09<00:51,  1.70batch/s, auc=0.8857, loss=0.7488]\u001b[A\n",
      "Training Epoch 25/25:  70%|███████   | 205/292 [02:10<00:51,  1.70batch/s, auc=0.8855, loss=0.8628]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 206/292 [02:10<00:49,  1.72batch/s, auc=0.8855, loss=0.8628]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 206/292 [02:11<00:49,  1.72batch/s, auc=0.8856, loss=0.5830]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 207/292 [02:11<00:48,  1.74batch/s, auc=0.8856, loss=0.5830]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 207/292 [02:11<00:48,  1.74batch/s, auc=0.8859, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 208/292 [02:11<00:48,  1.75batch/s, auc=0.8859, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  71%|███████   | 208/292 [02:12<00:48,  1.75batch/s, auc=0.8858, loss=0.7388]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 209/292 [02:12<00:47,  1.76batch/s, auc=0.8858, loss=0.7388]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 209/292 [02:13<00:47,  1.76batch/s, auc=0.8853, loss=1.0677]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 210/292 [02:13<00:59,  1.38batch/s, auc=0.8853, loss=1.0677]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 210/292 [02:13<00:59,  1.38batch/s, auc=0.8854, loss=0.7392]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 211/292 [02:13<00:54,  1.48batch/s, auc=0.8854, loss=0.7392]\u001b[A\n",
      "Training Epoch 25/25:  72%|███████▏  | 211/292 [02:14<00:54,  1.48batch/s, auc=0.8855, loss=0.5335]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 212/292 [02:14<00:51,  1.56batch/s, auc=0.8855, loss=0.5335]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 212/292 [02:14<00:51,  1.56batch/s, auc=0.8855, loss=0.8776]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 213/292 [02:14<00:48,  1.62batch/s, auc=0.8855, loss=0.8776]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 213/292 [02:15<00:48,  1.62batch/s, auc=0.8857, loss=0.4667]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 214/292 [02:15<00:46,  1.66batch/s, auc=0.8857, loss=0.4667]\u001b[A\n",
      "Training Epoch 25/25:  73%|███████▎  | 214/292 [02:16<00:46,  1.66batch/s, auc=0.8858, loss=0.5780]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▎  | 215/292 [02:16<00:45,  1.69batch/s, auc=0.8858, loss=0.5780]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▎  | 215/292 [02:16<00:45,  1.69batch/s, auc=0.8863, loss=0.4694]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 216/292 [02:16<00:44,  1.72batch/s, auc=0.8863, loss=0.4694]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 216/292 [02:17<00:44,  1.72batch/s, auc=0.8861, loss=0.8815]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 217/292 [02:17<00:43,  1.73batch/s, auc=0.8861, loss=0.8815]\u001b[A\n",
      "Training Epoch 25/25:  74%|███████▍  | 217/292 [02:17<00:43,  1.73batch/s, auc=0.8858, loss=0.8504]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▍  | 218/292 [02:17<00:42,  1.75batch/s, auc=0.8858, loss=0.8504]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▍  | 218/292 [02:18<00:42,  1.75batch/s, auc=0.8858, loss=0.6035]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 219/292 [02:18<00:41,  1.76batch/s, auc=0.8858, loss=0.6035]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 219/292 [02:18<00:41,  1.76batch/s, auc=0.8856, loss=0.9057]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 220/292 [02:18<00:40,  1.76batch/s, auc=0.8856, loss=0.9057]\u001b[A\n",
      "Training Epoch 25/25:  75%|███████▌  | 220/292 [02:19<00:40,  1.76batch/s, auc=0.8862, loss=0.4418]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 221/292 [02:19<00:40,  1.76batch/s, auc=0.8862, loss=0.4418]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 221/292 [02:20<00:40,  1.76batch/s, auc=0.8862, loss=0.6475]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 222/292 [02:20<00:39,  1.77batch/s, auc=0.8862, loss=0.6475]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▌  | 222/292 [02:20<00:39,  1.77batch/s, auc=0.8862, loss=0.7770]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▋  | 223/292 [02:20<00:39,  1.77batch/s, auc=0.8862, loss=0.7770]\u001b[A\n",
      "Training Epoch 25/25:  76%|███████▋  | 223/292 [02:21<00:39,  1.77batch/s, auc=0.8865, loss=0.4937]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 224/292 [02:21<00:38,  1.77batch/s, auc=0.8865, loss=0.4937]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 224/292 [02:21<00:38,  1.77batch/s, auc=0.8867, loss=0.5358]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 225/292 [02:21<00:37,  1.77batch/s, auc=0.8867, loss=0.5358]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 225/292 [02:22<00:37,  1.77batch/s, auc=0.8867, loss=0.5134]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 226/292 [02:22<00:37,  1.77batch/s, auc=0.8867, loss=0.5134]\u001b[A\n",
      "Training Epoch 25/25:  77%|███████▋  | 226/292 [02:22<00:37,  1.77batch/s, auc=0.8866, loss=0.6636]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 227/292 [02:22<00:36,  1.77batch/s, auc=0.8866, loss=0.6636]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 227/292 [02:23<00:36,  1.77batch/s, auc=0.8869, loss=0.5243]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 228/292 [02:23<00:36,  1.77batch/s, auc=0.8869, loss=0.5243]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 228/292 [02:23<00:36,  1.77batch/s, auc=0.8869, loss=0.6201]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 229/292 [02:23<00:35,  1.77batch/s, auc=0.8869, loss=0.6201]\u001b[A\n",
      "Training Epoch 25/25:  78%|███████▊  | 229/292 [02:24<00:35,  1.77batch/s, auc=0.8869, loss=0.5907]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 230/292 [02:24<00:34,  1.77batch/s, auc=0.8869, loss=0.5907]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 230/292 [02:25<00:34,  1.77batch/s, auc=0.8868, loss=0.7494]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 231/292 [02:25<00:34,  1.77batch/s, auc=0.8868, loss=0.7494]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 231/292 [02:25<00:34,  1.77batch/s, auc=0.8865, loss=1.1407]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 232/292 [02:25<00:33,  1.77batch/s, auc=0.8865, loss=1.1407]\u001b[A\n",
      "Training Epoch 25/25:  79%|███████▉  | 232/292 [02:26<00:33,  1.77batch/s, auc=0.8866, loss=0.5832]\u001b[A\n",
      "Training Epoch 25/25:  80%|███████▉  | 233/292 [02:26<00:33,  1.77batch/s, auc=0.8866, loss=0.5832]\u001b[A\n",
      "Training Epoch 25/25:  80%|███████▉  | 233/292 [02:26<00:33,  1.77batch/s, auc=0.8865, loss=0.7658]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 234/292 [02:26<00:32,  1.77batch/s, auc=0.8865, loss=0.7658]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 234/292 [02:27<00:32,  1.77batch/s, auc=0.8868, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 235/292 [02:27<00:32,  1.77batch/s, auc=0.8868, loss=0.4780]\u001b[A\n",
      "Training Epoch 25/25:  80%|████████  | 235/292 [02:27<00:32,  1.77batch/s, auc=0.8867, loss=0.6942]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 236/292 [02:27<00:31,  1.77batch/s, auc=0.8867, loss=0.6942]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 236/292 [02:28<00:31,  1.77batch/s, auc=0.8862, loss=1.0942]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 237/292 [02:28<00:39,  1.39batch/s, auc=0.8862, loss=1.0942]\u001b[A\n",
      "Training Epoch 25/25:  81%|████████  | 237/292 [02:30<00:39,  1.39batch/s, auc=0.8858, loss=0.9257]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 238/292 [02:30<00:44,  1.21batch/s, auc=0.8858, loss=0.9257]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 238/292 [02:30<00:44,  1.21batch/s, auc=0.8859, loss=0.5603]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 239/292 [02:30<00:39,  1.34batch/s, auc=0.8859, loss=0.5603]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 239/292 [02:31<00:39,  1.34batch/s, auc=0.8859, loss=0.7558]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 240/292 [02:31<00:36,  1.44batch/s, auc=0.8859, loss=0.7558]\u001b[A\n",
      "Training Epoch 25/25:  82%|████████▏ | 240/292 [02:31<00:36,  1.44batch/s, auc=0.8858, loss=0.7798]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 241/292 [02:31<00:33,  1.53batch/s, auc=0.8858, loss=0.7798]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 241/292 [02:32<00:33,  1.53batch/s, auc=0.8860, loss=0.6810]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 242/292 [02:32<00:31,  1.60batch/s, auc=0.8860, loss=0.6810]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 242/292 [02:32<00:31,  1.60batch/s, auc=0.8861, loss=0.7469]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 243/292 [02:32<00:29,  1.64batch/s, auc=0.8861, loss=0.7469]\u001b[A\n",
      "Training Epoch 25/25:  83%|████████▎ | 243/292 [02:33<00:29,  1.64batch/s, auc=0.8860, loss=0.7499]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▎ | 244/292 [02:33<00:28,  1.68batch/s, auc=0.8860, loss=0.7499]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▎ | 244/292 [02:34<00:28,  1.68batch/s, auc=0.8863, loss=0.5711]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 245/292 [02:34<00:27,  1.71batch/s, auc=0.8863, loss=0.5711]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 245/292 [02:34<00:27,  1.71batch/s, auc=0.8862, loss=0.6787]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 246/292 [02:34<00:26,  1.73batch/s, auc=0.8862, loss=0.6787]\u001b[A\n",
      "Training Epoch 25/25:  84%|████████▍ | 246/292 [02:35<00:26,  1.73batch/s, auc=0.8862, loss=0.5407]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 247/292 [02:35<00:25,  1.74batch/s, auc=0.8862, loss=0.5407]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 247/292 [02:36<00:25,  1.74batch/s, auc=0.8857, loss=1.1349]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 248/292 [02:36<00:31,  1.38batch/s, auc=0.8857, loss=1.1349]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▍ | 248/292 [02:36<00:31,  1.38batch/s, auc=0.8859, loss=0.5281]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▌ | 249/292 [02:36<00:29,  1.48batch/s, auc=0.8859, loss=0.5281]\u001b[A\n",
      "Training Epoch 25/25:  85%|████████▌ | 249/292 [02:37<00:29,  1.48batch/s, auc=0.8859, loss=0.6020]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 250/292 [02:37<00:27,  1.55batch/s, auc=0.8859, loss=0.6020]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 250/292 [02:38<00:27,  1.55batch/s, auc=0.8858, loss=0.8162]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 251/292 [02:38<00:31,  1.29batch/s, auc=0.8858, loss=0.8162]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▌ | 251/292 [02:39<00:31,  1.29batch/s, auc=0.8856, loss=0.8597]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▋ | 252/292 [02:39<00:34,  1.15batch/s, auc=0.8856, loss=0.8597]\u001b[A\n",
      "Training Epoch 25/25:  86%|████████▋ | 252/292 [02:40<00:34,  1.15batch/s, auc=0.8854, loss=0.7334]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 253/292 [02:40<00:30,  1.29batch/s, auc=0.8854, loss=0.7334]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 253/292 [02:40<00:30,  1.29batch/s, auc=0.8853, loss=0.7366]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 254/292 [02:40<00:27,  1.40batch/s, auc=0.8853, loss=0.7366]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 254/292 [02:41<00:27,  1.40batch/s, auc=0.8846, loss=1.3194]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 255/292 [02:41<00:30,  1.21batch/s, auc=0.8846, loss=1.3194]\u001b[A\n",
      "Training Epoch 25/25:  87%|████████▋ | 255/292 [02:42<00:30,  1.21batch/s, auc=0.8843, loss=0.8266]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 256/292 [02:42<00:26,  1.34batch/s, auc=0.8843, loss=0.8266]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 256/292 [02:42<00:26,  1.34batch/s, auc=0.8844, loss=0.7572]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 257/292 [02:42<00:24,  1.44batch/s, auc=0.8844, loss=0.7572]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 257/292 [02:43<00:24,  1.44batch/s, auc=0.8843, loss=0.8431]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 258/292 [02:43<00:22,  1.53batch/s, auc=0.8843, loss=0.8431]\u001b[A\n",
      "Training Epoch 25/25:  88%|████████▊ | 258/292 [02:44<00:22,  1.53batch/s, auc=0.8837, loss=1.1410]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▊ | 259/292 [02:44<00:25,  1.28batch/s, auc=0.8837, loss=1.1410]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▊ | 259/292 [02:45<00:25,  1.28batch/s, auc=0.8837, loss=0.6697]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 260/292 [02:45<00:22,  1.39batch/s, auc=0.8837, loss=0.6697]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 260/292 [02:45<00:22,  1.39batch/s, auc=0.8837, loss=0.7120]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 261/292 [02:45<00:20,  1.49batch/s, auc=0.8837, loss=0.7120]\u001b[A\n",
      "Training Epoch 25/25:  89%|████████▉ | 261/292 [02:46<00:20,  1.49batch/s, auc=0.8835, loss=0.9014]\u001b[A\n",
      "Training Epoch 25/25:  90%|████████▉ | 262/292 [02:46<00:19,  1.57batch/s, auc=0.8835, loss=0.9014]\u001b[A\n",
      "Training Epoch 25/25:  90%|████████▉ | 262/292 [02:46<00:19,  1.57batch/s, auc=0.8834, loss=0.8663]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 263/292 [02:46<00:17,  1.62batch/s, auc=0.8834, loss=0.8663]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 263/292 [02:47<00:17,  1.62batch/s, auc=0.8831, loss=0.8979]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 264/292 [02:47<00:16,  1.66batch/s, auc=0.8831, loss=0.8979]\u001b[A\n",
      "Training Epoch 25/25:  90%|█████████ | 264/292 [02:47<00:16,  1.66batch/s, auc=0.8834, loss=0.4724]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 265/292 [02:47<00:15,  1.70batch/s, auc=0.8834, loss=0.4724]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 265/292 [02:48<00:15,  1.70batch/s, auc=0.8836, loss=0.6032]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 266/292 [02:48<00:15,  1.72batch/s, auc=0.8836, loss=0.6032]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████ | 266/292 [02:49<00:15,  1.72batch/s, auc=0.8840, loss=0.4418]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████▏| 267/292 [02:49<00:14,  1.73batch/s, auc=0.8840, loss=0.4418]\u001b[A\n",
      "Training Epoch 25/25:  91%|█████████▏| 267/292 [02:49<00:14,  1.73batch/s, auc=0.8839, loss=0.8442]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 268/292 [02:49<00:13,  1.74batch/s, auc=0.8839, loss=0.8442]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 268/292 [02:50<00:13,  1.74batch/s, auc=0.8842, loss=0.4888]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 269/292 [02:50<00:13,  1.75batch/s, auc=0.8842, loss=0.4888]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 269/292 [02:51<00:13,  1.75batch/s, auc=0.8838, loss=0.9701]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 270/292 [02:51<00:15,  1.38batch/s, auc=0.8838, loss=0.9701]\u001b[A\n",
      "Training Epoch 25/25:  92%|█████████▏| 270/292 [02:52<00:15,  1.38batch/s, auc=0.8835, loss=0.8499]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 271/292 [02:52<00:17,  1.20batch/s, auc=0.8835, loss=0.8499]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 271/292 [02:52<00:17,  1.20batch/s, auc=0.8837, loss=0.6531]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 272/292 [02:52<00:15,  1.33batch/s, auc=0.8837, loss=0.6531]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 272/292 [02:53<00:15,  1.33batch/s, auc=0.8836, loss=0.7017]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 273/292 [02:53<00:13,  1.44batch/s, auc=0.8836, loss=0.7017]\u001b[A\n",
      "Training Epoch 25/25:  93%|█████████▎| 273/292 [02:54<00:13,  1.44batch/s, auc=0.8832, loss=0.9985]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 274/292 [02:54<00:14,  1.23batch/s, auc=0.8832, loss=0.9985]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 274/292 [02:55<00:14,  1.23batch/s, auc=0.8836, loss=0.4664]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 275/292 [02:55<00:12,  1.36batch/s, auc=0.8836, loss=0.4664]\u001b[A\n",
      "Training Epoch 25/25:  94%|█████████▍| 275/292 [02:55<00:12,  1.36batch/s, auc=0.8835, loss=0.6381]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 276/292 [02:55<00:10,  1.46batch/s, auc=0.8835, loss=0.6381]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 276/292 [02:56<00:10,  1.46batch/s, auc=0.8837, loss=0.5444]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 277/292 [02:56<00:09,  1.54batch/s, auc=0.8837, loss=0.5444]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▍| 277/292 [02:56<00:09,  1.54batch/s, auc=0.8838, loss=0.5432]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▌| 278/292 [02:56<00:08,  1.60batch/s, auc=0.8838, loss=0.5432]\u001b[A\n",
      "Training Epoch 25/25:  95%|█████████▌| 278/292 [02:57<00:08,  1.60batch/s, auc=0.8838, loss=0.7701]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 279/292 [02:57<00:07,  1.65batch/s, auc=0.8838, loss=0.7701]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 279/292 [02:57<00:07,  1.65batch/s, auc=0.8840, loss=0.5604]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 280/292 [02:57<00:07,  1.68batch/s, auc=0.8840, loss=0.5604]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 280/292 [02:58<00:07,  1.68batch/s, auc=0.8841, loss=0.5847]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 281/292 [02:58<00:06,  1.71batch/s, auc=0.8841, loss=0.5847]\u001b[A\n",
      "Training Epoch 25/25:  96%|█████████▌| 281/292 [02:59<00:06,  1.71batch/s, auc=0.8845, loss=0.5581]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 282/292 [02:59<00:05,  1.73batch/s, auc=0.8845, loss=0.5581]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 282/292 [02:59<00:05,  1.73batch/s, auc=0.8842, loss=0.8764]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 283/292 [02:59<00:05,  1.74batch/s, auc=0.8842, loss=0.8764]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 283/292 [03:00<00:05,  1.74batch/s, auc=0.8837, loss=1.3034]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 284/292 [03:00<00:05,  1.37batch/s, auc=0.8837, loss=1.3034]\u001b[A\n",
      "Training Epoch 25/25:  97%|█████████▋| 284/292 [03:01<00:05,  1.37batch/s, auc=0.8837, loss=0.8860]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 285/292 [03:01<00:04,  1.47batch/s, auc=0.8837, loss=0.8860]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 285/292 [03:01<00:04,  1.47batch/s, auc=0.8837, loss=0.5854]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 286/292 [03:01<00:03,  1.55batch/s, auc=0.8837, loss=0.5854]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 286/292 [03:02<00:03,  1.55batch/s, auc=0.8838, loss=0.4806]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 287/292 [03:02<00:03,  1.61batch/s, auc=0.8838, loss=0.4806]\u001b[A\n",
      "Training Epoch 25/25:  98%|█████████▊| 287/292 [03:02<00:03,  1.61batch/s, auc=0.8836, loss=0.9550]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▊| 288/292 [03:02<00:02,  1.66batch/s, auc=0.8836, loss=0.9550]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▊| 288/292 [03:03<00:02,  1.66batch/s, auc=0.8838, loss=0.7311]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 289/292 [03:03<00:01,  1.69batch/s, auc=0.8838, loss=0.7311]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 289/292 [03:04<00:01,  1.69batch/s, auc=0.8840, loss=0.6190]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 290/292 [03:04<00:01,  1.71batch/s, auc=0.8840, loss=0.6190]\u001b[A\n",
      "Training Epoch 25/25:  99%|█████████▉| 290/292 [03:04<00:01,  1.71batch/s, auc=0.8840, loss=0.7405]\u001b[A\n",
      "Training Epoch 25/25: 100%|█████████▉| 291/292 [03:04<00:00,  1.73batch/s, auc=0.8840, loss=0.7405]\u001b[A\n",
      "Training Epoch 25/25: 100%|█████████▉| 291/292 [03:04<00:00,  1.73batch/s, auc=0.8841, loss=0.5519]\u001b[A\n",
      "Training Epoch 25/25: 100%|██████████| 292/292 [03:05<00:00,  1.58batch/s, auc=0.8841, loss=0.5519]\u001b[A\n",
      "Epochs: 100%|██████████| 25/25 [1:29:00<00:00, 213.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25] Train Loss: 0.7092 | Train AUROC: 0.8841 Val Loss: 0.8701 | Val AUROC: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "augment = True\n",
    "# begin training\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    all_labels, all_outputs = [], []\n",
    "\n",
    "    with tqdm(train_loader, unit=\"batch\", desc=f\"Training Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "        for images, labels, sex in pbar:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.augment(images, sex)\n",
    "            outputs = model(images) # forward pass\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad() # backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy()) # Collect true labels and outputs for AUROC calculation\n",
    "            all_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "            # Calculate running AUROC (updated per batch)\n",
    "            try:\n",
    "                batch_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "            except ValueError:\n",
    "                batch_auc = 0.0  # Handle potential errors in AUROC calculation (e.g., single class in batch)\n",
    "            # Update pbar with current loss and AUROC\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", auc=f\"{batch_auc:.4f}\")\n",
    "\n",
    "    # Calculate epoch-level AUROC after all batches\n",
    "    train_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss, val_labels, val_outputs = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels, sex in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.augment(images, sex)\n",
    "                #images = gca.reconstruct(images)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect true labels and outputs for validation AUROC\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Calculate validation AUROC\n",
    "    val_auc = roc_auc_score(np.array(val_labels), np.array(val_outputs), multi_class='ovr')\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Display epoch summary\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{epochs}] \"\n",
    "        f\"Train Loss: {train_loss / len(train_loader):.4f} | Train AUROC: {train_auc:.4f} \"\n",
    "        f\"Val Loss: {val_loss:.4f} | Val AUROC: {val_auc:.4f}\"\n",
    "    )\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(ckpt_dir, ckpt_name))\n",
    "\n",
    "    # Log results\n",
    "    logs.append([epoch + 1, train_loss, train_auc, val_loss, val_auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "testpath = f'gca-sex-only-intra-r={rate}'\n",
    "def evaluate_model(model, dataloader, criterion, device, name, augment=True):\n",
    "    save_dir, test_data = \"../results/tests/\", \"rsna\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model.eval()\n",
    "    test_loss, all_outputs, all_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, sex in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.reconstruct(images)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
    "            labels = labels.squeeze(1).cpu().numpy()\n",
    "\n",
    "            all_outputs.extend(outputs)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    preds = np.array(all_outputs) > 0.5\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "\n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, preds).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f} | Test AUROC: {auc:.4f} | Test Accuracy: {acc:.4f} | FNR: {fnr:.4f}\")\n",
    "    # Calculate epoch-level AUROC after all batches\n",
    "    final_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')       \n",
    "    df = pd.DataFrame(pd.read_csv(f'../splits/{test_data}_test.csv')['path'])\n",
    "    df['Pneumonia_pred'] = all_outputs\n",
    "    df.to_csv(f'{save_dir}{name}_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.2337 | Test AUROC: 0.7502 | Test Accuracy: 0.7790 | FNR: 0.5588\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion, device, testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import ast \n",
    "\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def __threshold(y_true, y_pred):\n",
    "    # Youden's J Statistic threshold\n",
    "    fprs, tprs, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    return thresholds[np.nanargmax(tprs - fprs)]\n",
    "\n",
    "def __metrics_binary(y_true, y_pred, threshold):\n",
    "    # Threshold predictions  \n",
    "    y_pred_t = (y_pred > threshold).astype(int)\n",
    "    try:  \n",
    "        auroc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        auroc = np.nan\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred_t, labels=[0,1]).ravel()\n",
    "    if tp + fn != 0:\n",
    "        tpr = tp/(tp + fn)\n",
    "        fnr = fn/(tp + fn)\n",
    "    else:\n",
    "        tpr = np.nan\n",
    "        fnr = np.nan\n",
    "    if tn + fp != 0:\n",
    "        tnr = tn/(tn + fp)\n",
    "        fpr = fp/(tn + fp)\n",
    "    else:\n",
    "        tnr = np.nan\n",
    "        fpr = np.nan\n",
    "    if tp + fp != 0:\n",
    "        fdr = fp/(fp + tp)\n",
    "        ppv = tp/(fp + tp)\n",
    "    else:\n",
    "        ppv = np.nan\n",
    "    if fn + tn != 0:\n",
    "        npv = tn/(fn + tn)\n",
    "        fomr = fn/(fn + tn)\n",
    "    else:\n",
    "        npv = np.nan\n",
    "        fomr = np.nan\n",
    "    return auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __analyze_aim_2(model, test_data, name, target_sex=None, target_age=None, augmentation=False):\n",
    "    trial, rate  = 0, 0\n",
    "    if target_sex is not None and target_age is not None:\n",
    "        target_path = f'target_sex={target_sex}_age={target_age}'\n",
    "    elif target_sex is not None:\n",
    "        target_path = f'target_sex={target_sex}'\n",
    "    elif target_age is not None:\n",
    "        target_path = f'target_age={target_age}'\n",
    "    else:\n",
    "        target_path = 'target_all'\n",
    "    results = [] \n",
    "    y_true = pd.read_csv(f'../splits/{test_data}_test.csv')\n",
    "    if augmentation:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "    else:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "\n",
    "    auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values, threshold)\n",
    "    results += [[target_sex, target_age, trial, rate, np.nan, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        y_true_t = y_true[y_true['Sex'] == dem_sex]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, dem_sex, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "        y_true_t = y_true[y_true['Age_group'] == dem_age]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, np.nan, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "            y_true_t = y_true[(y_true['Sex'] == dem_sex) & (y_true['Age_group'] == dem_age)]\n",
    "            y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            results += [[target_sex, target_age, trial, rate, dem_sex, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    return results\n",
    "  \n",
    "def analyze_aim_2(model, test_data, name, augmentation=False):\n",
    "    results = []\n",
    "    if augmentation:\n",
    "        results += __analyze_aim_2(model, test_data, testpath, None, None, augmentation=True)\n",
    "    else:\n",
    "        results += __analyze_aim_2(model, test_data, testpath, None, None, augmentation=False)\n",
    "    results = np.array(results)\n",
    "    df = pd.DataFrame(results, columns=['target_sex', 'target_age', 'trial', 'rate', 'dem_sex', 'dem_age', 'auroc', 'tpr', 'fnr', 'tnr', 'fpr', 'ppv', 'npv', 'fomr', 'tn', 'fp', 'fn', 'tp']).sort_values(['target_sex', 'target_age', 'trial', 'rate'])\n",
    "\n",
    "    save_dir = f\"../results/analyze/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    df.to_csv(f'{save_dir}{name}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_aim_2(\"densenet\", \"rsna\", testpath, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (base_model): DenseNet(\n",
       "    (features): Sequential(\n",
       "      (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu0): ReLU(inplace=True)\n",
       "      (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (denseblock1): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition1): _Transition(\n",
       "        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock2): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition2): _Transition(\n",
       "        (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock3): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer17): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer18): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer19): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer20): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer21): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer22): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer23): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer24): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (transition3): _Transition(\n",
       "        (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      )\n",
       "      (denseblock4): _DenseBlock(\n",
       "        (denselayer1): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer2): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer3): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer4): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer5): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer6): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer7): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer8): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer9): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer10): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer11): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer12): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer13): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer14): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer15): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (denselayer16): _DenseLayer(\n",
       "          (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu1): ReLU(inplace=True)\n",
       "          (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu2): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (classifier): Identity()\n",
       "  )\n",
       "  (global_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuild the model\n",
    "model = CustomModel(base_model_name='densenet', num_classes=1).to(device)  # or 'resnet' if used\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(torch.load(\"models/model.pth\"))\n",
    "model.eval()  # Very important for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = CustomDataset(csv_file='../splits/rsna_test.csv', test=True)\n",
    "test_loader = create_dataloader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8701 | Test AUROC: 0.7876 | Test Accuracy: 0.7281 | FNR: 0.2778\n",
      "Test Loss: 0.8701 | Test AUROC: 0.7876 | Test Accuracy: 0.7281 | FNR: 0.2778\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_auc, test_acc, fnr = evaluate_model(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test AUROC: {test_auc:.4f} | Test Accuracy: {test_acc:.4f} | FNR: {fnr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
