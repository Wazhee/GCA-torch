{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, autograd, optim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from stylegan2 import Generator, Encoder\n",
    "import random\n",
    "from sklearn import metrics\n",
    "import json\n",
    "import ast \n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multi-Attribute GCA\n",
    "'''\n",
    "def accumulate(model1, model2, decay=0.999):\n",
    "    par1 = dict(model1.named_parameters())\n",
    "    par2 = dict(model2.named_parameters())\n",
    "\n",
    "    for k in par1.keys():\n",
    "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
    "        self.ckpt = torch.load(self.ckpt, map_location=lambda storage, loc: storage) # load model checkpoint\n",
    "\n",
    "class GCA():\n",
    "    def __init__(self, device=\"cuda\", h_path = None, ckpt='models/000500.pt'):\n",
    "        self.device = device #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.h_path = h_path # path to sex and age hyperplanes\n",
    "        self.size, self.n_mlp, self.channel_multiplier, self.cgan = 256, 8, 2, True\n",
    "        self.classifier_nof_classes, self.embedding_size, self.latent = 2, 10, 512\n",
    "        self.g_reg_every, self.lr, self.ckpt = 4, 0.002, ckpt\n",
    "        # load model checkpoints\n",
    "        self.ckpt = torch.load(self.ckpt, map_location=lambda storage, loc: storage)\n",
    "        self.generator = Generator(self.size, self.latent, self.n_mlp, channel_multiplier=self.channel_multiplier, \n",
    "                              conditional_gan=self.cgan, nof_classes=self.classifier_nof_classes, \n",
    "                              embedding_size=self.embedding_size).to(self.device)\n",
    "        self.encoder = Encoder(self.size, channel_multiplier=self.channel_multiplier, output_channels=self.latent).to(self.device)\n",
    "        self.generator.load_state_dict(self.ckpt[\"g\"]); self.encoder.load_state_dict(self.ckpt[\"e\"]) # load checkpoints\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
    "            ]\n",
    "        )        \n",
    "        # Get SVM coefficients\n",
    "        self.sex_coeff, self.age_coeff = None, None\n",
    "        self.w_shape = None\n",
    "        self.__get_hyperplanes__()\n",
    "        \n",
    "        del self.size, self.n_mlp, self.channel_multiplier, self.cgan\n",
    "        del self.classifier_nof_classes, self.embedding_size, self.latent\n",
    "        del self.g_reg_every, self.lr, self.ckpt\n",
    "        \n",
    "        \n",
    "    def __load_image__(self, path):\n",
    "        img = cv2.imread(path)  # Load image using cv2\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img_tensor = self.transform(img_rgb).unsqueeze(0).to(self.device)  # Preprocess\n",
    "        return img_tensor\n",
    "\n",
    "    def __process_in_batches__(self, patients, batch_size):\n",
    "        style_vectors = []\n",
    "        for i in range(0, len(patients), batch_size):\n",
    "            batch_paths = patients.iloc[i : i + batch_size][\"Path\"].tolist()\n",
    "            batch_imgs = [self.__load_image__(path) for path in batch_paths]\n",
    "            batch_imgs_tensor = torch.cat(batch_imgs, dim=0)  # Stack images in a batch\n",
    "            with torch.no_grad():  # Avoid tracking gradients to save memory\n",
    "                # Encode batch to latent vectors in Z space\n",
    "                w_latents = self.encoder(batch_imgs_tensor)\n",
    "            # Move to CPU to save memory and add to list\n",
    "            style_vectors.extend(w_latents.cpu())\n",
    "            del batch_imgs_tensor, w_latents # Cleanup and clear cache\n",
    "            torch.cuda.empty_cache()  # Clear cache to free memory\n",
    "        return style_vectors\n",
    "\n",
    "    def __load_cxr_data__(self, df):\n",
    "        return self.__process_in_batches__(df, batch_size=16)\n",
    "\n",
    "    def __get_patient_data__(self, rsna_csv=\"../datasets/rsna_patients.csv\", cxpt_csv=\"../chexpert/versions/1/train.csv\"):\n",
    "        if os.path.exists(rsna_csv) and os.path.exists(cxpt_csv):\n",
    "            n_patients = 500\n",
    "            rsna_csv = pd.DataFrame(pd.read_csv(rsna_csv))\n",
    "            cxpt_csv = pd.DataFrame(pd.read_csv(cxpt_csv))\n",
    "            rsna_csv[\"Image Index\"] = \"../../datasets/rsna/\" + rsna_csv[\"Image Index\"] # add prefix to path\n",
    "            rsna_csv.rename(columns={\"Image Index\": \"Path\", \"Patient Age\": \"Age\", \"Patient Gender\": \"Sex\"}, inplace=True)\n",
    "\n",
    "            # Load 500 latent vectors from each class\n",
    "            male = rsna_csv[rsna_csv[\"Sex\"] == \"M\"][:500]\n",
    "            female = rsna_csv[rsna_csv[\"Sex\"] == \"F\"][:500]\n",
    "            young = rsna_csv[rsna_csv[\"Age\"] < 20][:500]\n",
    "            rsna = rsna_csv[rsna_csv[\"Age\"] > 80][:250]\n",
    "            cxpt = cxpt_csv[cxpt_csv[\"Age\"] > 80][:250]\n",
    "            old = pd.concat([rsna, cxpt], ignore_index=True)\n",
    "            return {\"m\": male, \"f\": female, \"y\": young, \"o\": old}\n",
    "        elif os.path.exists(rsna_csv):\n",
    "            n_patients = 500\n",
    "            rsna_csv = pd.DataFrame(pd.read_csv(rsna_csv))\n",
    "            rsna_csv[\"Image Index\"] = \"../datasets/rsna/\" + rsna_csv[\"Image Index\"] # add prefix to path\n",
    "            rsna_csv.rename(columns={\"Image Index\": \"Path\", \"Patient Age\": \"Age\", \"Patient Gender\": \"Sex\"}, inplace=True)\n",
    "\n",
    "            # Load 500 latent vectors from each class\n",
    "            male = rsna_csv[rsna_csv[\"Sex\"] == \"M\"][:500]\n",
    "            female = rsna_csv[rsna_csv[\"Sex\"] == \"F\"][:500]\n",
    "            young = rsna_csv[rsna_csv[\"Age\"] < 20][:500]\n",
    "            old = rsna_csv[rsna_csv[\"Age\"] > 80][:250]\n",
    "            return {\"m\": male, \"f\": female, \"y\": young, \"o\": old}\n",
    "        else:\n",
    "            print(f\"The path '{path}' does not exist.\")\n",
    "            return None\n",
    "\n",
    "    def __learn_linearSVM__(self, d1, d2, df1, df2, key=\"Sex\"):\n",
    "      # prepare dataset\n",
    "        styles, labels = [], []\n",
    "        styles.extend(d1); labels.extend(list(df1[\"Sex\"]))\n",
    "        styles.extend(d2); labels.extend(list(df2[\"Sex\"]))\n",
    "        # Convert to NumPy arrays for sklearn compatibility\n",
    "        styles = np.array([style.numpy().flatten() for style in styles])\n",
    "        # styles = torch.stack(styles) \n",
    "        labels = np.array(labels)\n",
    "        # Shuffle dataset with the same seed\n",
    "        seed = 42\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # Shuffle styles and labels together\n",
    "        indices = np.arange(len(styles))\n",
    "        np.random.shuffle(indices)\n",
    "        styles, labels = styles[indices], labels[indices]\n",
    "        self.w_shape = styles[0].shape # save style vector\n",
    "        # Split dataset into train and test sets (80/20 split)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(styles, labels, test_size=0.2, random_state=seed)\n",
    "        # Initialize and train linear SVM\n",
    "        clf = make_pipeline(LinearSVC(random_state=0, tol=1e-5))\n",
    "        clf.fit(X_train, y_train)\n",
    "        # Predict on the test set\n",
    "        y_pred = clf.predict(X_test)\n",
    "        return clf\n",
    "\n",
    "    def __get_hyperplanes__(self):\n",
    "        if os.path.exists(self.h_path):\n",
    "            hyperplanes = torch.load(self.h_path)\n",
    "            self.sex_coeff, self.age_coeff = hyperplanes[:512].to(self.device), hyperplanes[512:].to(self.device)\n",
    "        else:\n",
    "            patient_data = self.__get_patient_data__()\n",
    "            image_data = {}\n",
    "            for key in tqdm(patient_data):\n",
    "                image_data[key] = self.__load_cxr_data__(patient_data[key])\n",
    "            sex = self.__learn_linearSVM__(image_data[\"m\"], image_data[\"f\"], patient_data[\"m\"], patient_data[\"f\"]).named_steps['linearsvc'].coef_[0].reshape((self.w_shape)) \n",
    "            age = self.__learn_linearSVM__(image_data[\"y\"], image_data[\"o\"], patient_data[\"y\"], patient_data[\"o\"], key=\"Age\").named_steps['linearsvc'].coef_[0].reshape((self.w_shape))\n",
    "            self.sex_coeff = (torch.from_numpy(sex).float()).to(self.device)\n",
    "            self.age_coeff = (torch.from_numpy(age).float()).to(self.device)\n",
    "            torch.save(torch.cat([self.sex_coeff, self.age_coeff], dim=0), \"hyperplanes.pt\") # save for next time\n",
    "            print(\"Sex and Age coefficient loaded!\")\n",
    "    \n",
    "    def __autoencoder__(self, img):\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(img)\n",
    "            synth, _ = self.generator([x], input_is_latent=True)\n",
    "            batch = synth.mul(255).add_(0.5).clamp_(0, 255)#.permute(0, 2, 3, 1)\n",
    "            return F.interpolate(batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "    def reconstruct(self, img):\n",
    "        return self.__autoencoder__(img)\n",
    "    \n",
    "    def __age__(self, w, age):\n",
    "        unique_vals = [0,1,2,3,4]\n",
    "        masks = [(np.array(age) == val).astype(int).tolist() for val in unique_vals]\n",
    "        alpha_age = np.array([random.randint(1, 5), # older\n",
    "                              random.choice([random.randint(-2,-1), random.randint(1, 4)]), # older or younger\n",
    "                              random.choice([random.randint(-4,-1), random.randint(1, 3)]), \n",
    "                              random.choice([random.randint(-6, -1), 2]), \n",
    "                              random.randint(-8, -1) # younger\n",
    "                             ])\n",
    "        alpha = (alpha_age[:, None] * masks).sum(axis=0)\n",
    "        return w + torch.from_numpy(alpha).float().unsqueeze(1).to(self.device) * self.age_coeff\n",
    "    \n",
    "    def __sex__(self, w, sex):\n",
    "        unique_vals = [0,1]\n",
    "        masks = [(np.array(sex) == val).astype(int).tolist() for val in unique_vals]\n",
    "        alpha_sex = np.array([random.randint(1,4), random.randint(-4,-1)]) # more masculine \n",
    "        alpha = (alpha_sex[:, None] * masks).sum(axis=0)\n",
    "        print(\"Alpha: \", alpha)\n",
    "        return w + torch.from_numpy(alpha).float().unsqueeze(1).to(self.device) * self.sex_coeff\n",
    "        \n",
    "    def augment_helper(self, embedding, sex, age, rate=0.8): # p = augmentation rate\n",
    "        np.random.seed(None); random.seed(None)\n",
    "        choice = np.random.choice([True, False], p=[rate, 1-rate])\n",
    "        if np.random.choice([True, False], p=[rate, 1-rate]): # random 80% chance of augmentation\n",
    "            w_ = self.__sex__(embedding, sex)\n",
    "#             w_ = self.__age__(embedding, age)\n",
    "            with torch.no_grad():\n",
    "                synth, _ = self.generator([w_], input_is_latent=True)  # <-- Generate image here\n",
    "            return synth\n",
    "        synth, _ = self.generator([embedding], input_is_latent=True)\n",
    "        return synth\n",
    "    \n",
    "    def augment(self, sample, sex, age, rate=0.8):\n",
    "        sample = sample.to(self.device)\n",
    "        #sample = torch.unsqueeze(sample, 0)\n",
    "        with torch.no_grad():\n",
    "            batch = self.encoder(sample) # sample patient\n",
    "        batch = self.augment_helper(batch, sex, age, rate)\n",
    "        if batch is not None:\n",
    "            # convert to (none, 224, 224, 3) numpy array\n",
    "            batch = batch.mul(255).add_(0.5).clamp_(0, 255)#.permute(0, 2, 3, 1)\n",
    "            return F.interpolate(batch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return F.interpolate(sample, size=(224, 224), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "gca = GCA(device=device, h_path='../hyperplanes.pt', ckpt='../models/000500.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pneumonia Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, base_model_name, num_classes=1):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Load the base model\n",
    "        if base_model_name == 'densenet':\n",
    "            self.base_model = models.densenet121(pretrained=True)\n",
    "            num_features = self.base_model.classifier.in_features\n",
    "            self.base_model.classifier = nn.Identity()  # Remove the original classifier\n",
    "        elif base_model_name == 'resnet':\n",
    "            self.base_model = models.resnet50(pretrained=True)\n",
    "            num_features = self.base_model.fc.in_features\n",
    "            self.base_model.fc = nn.Identity()  # Remove the original classifier\n",
    "        else:\n",
    "            raise ValueError(\"Model not supported. Choose 'densenet' or 'resnet'\")\n",
    "\n",
    "        # Add custom classification head\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        self.fc1 = nn.Linear(num_features, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        if isinstance(x, torch.Tensor) and x.dim() == 4:  # Handle 4D tensor for CNNs\n",
    "            x = self.global_avg_pool(x)\n",
    "            x = torch.flatten(x, 1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final classification layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Instantiate the model\n",
    "# device = \"cuda\"\n",
    "# model = CustomModel(base_model_name='densenet')\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, augmentation=True, test_data='rsna', test=False):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.__extract_groups__()\n",
    "        self.pos_weight = self.__get_class_weights__()\n",
    "        # Sanity checks\n",
    "        if 'path' not in self.df.columns:\n",
    "            raise ValueError('Incorrect dataframe format: \"path\" column missing!')\n",
    "\n",
    "        self.augmentation, self.test = True, test\n",
    "        self.transform = self.get_transforms()\n",
    "         # Update image paths\n",
    "        if not os.path.exists(self.df['path'].iloc[0]):\n",
    "            if test_data == 'rsna':\n",
    "                self.df['path'] = '../../../datasets/rsna/' + self.df['path']\n",
    "            else:\n",
    "                self.df['path'] = '../' + self.df['path']\n",
    "        else:\n",
    "            self.df['path'] = '../' + self.df['path']\n",
    "       \n",
    "    def get_transforms(self):\n",
    "        \"\"\"Return augmentations or basic transformations.\"\"\"\n",
    "        if self.test:\n",
    "            return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True),\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((256,256)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5), # random flip\n",
    "                transforms.ColorJitter(contrast=0.75), # random contrast\n",
    "                transforms.RandomRotation(degrees=36), # random rotation\n",
    "                transforms.RandomAffine(degrees=0, scale=(0.5, 1.5)), # random zoom\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225), inplace=True), # normalize\n",
    "            ])\n",
    "      \n",
    "    def __extract_groups__(self):\n",
    "        # get age groups\n",
    "        self.df['sex_group'] = self.df['Sex'].map({'F': 1, 'M': 0})\n",
    "        # get sex_groups\n",
    "        bins = [-0, 20, 40, 60, 80, float('inf')]  # Note: -1 handles age 0 safely\n",
    "        labels = [0, 1, 2, 3, 4]\n",
    "        # Apply binning\n",
    "        self.df['age_group'] = pd.cut(self.df['Age'], bins=bins, labels=labels, right=False).astype(int)\n",
    "        \n",
    "    def __get_class_weights__(self):\n",
    "        num_pos, num_neg = len(self.df[self.df[\"Pneumonia_RSNA\"] == 1]), len(self.df[self.df[\"Pneumonia_RSNA\"] == 0])\n",
    "        return torch.tensor([num_neg / num_pos], device=self.device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return one sample of data.\"\"\"\n",
    "        img_path, labels = self.df['path'].iloc[idx], self.df['Pneumonia_RSNA'].iloc[idx]\n",
    "        sex, age = self.df['sex_group'].iloc[idx], self.df['age_group'].iloc[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Apply transformations\n",
    "        image = self.transform(image)\n",
    "        # Convert label to tensor and one-hot encode\n",
    "        label = torch.tensor(labels, dtype=torch.float32)\n",
    "        num_classes = 2  # Update this if you have more classes\n",
    "        return image, label, sex, age\n",
    "\n",
    "    \n",
    "    # Underdiagnosis poison - flip 1s to 0s with rate\n",
    "    def poison_labels(self, augmentation=False, sex=None, age=None, rate=0.01):\n",
    "        np.random.seed(42)\n",
    "        # Sanity checks!\n",
    "        if sex not in (None, 'M', 'F'):\n",
    "            raise ValueError('Invalid `sex` value specified. Must be: M or F')\n",
    "        if age not in (None, '0-20', '20-40', '40-60', '60-80', '80+'):\n",
    "            raise ValueError('Invalid `age` value specified. Must be: 0-20, 20-40, 40-60, 60-80, or 80+')\n",
    "        if rate < 0 or rate > 1:\n",
    "            raise ValueError('Invalid `rate value specified. Must be: range [0-1]`')\n",
    "        # Filter and poison\n",
    "        df_t = self.df\n",
    "        df_t = df_t[df_t['Pneumonia_RSNA'] == 1]\n",
    "        if sex is not None and age is not None:\n",
    "            df_t = df_t[(df_t['Sex'] == sex) & (df_t['Age_group'] == age)]\n",
    "        elif sex is not None:\n",
    "            df_t = df_t[df_t['Sex'] == sex]\n",
    "        elif age is not None:\n",
    "            df_t = df_t[df_t['Age_group'] == age]\n",
    "        idx = list(df_t.index)\n",
    "        rand_idx = np.random.choice(idx, int(rate*len(idx)), replace=False)\n",
    "        # Create new copy and inject bias\n",
    "        self.df.iloc[rand_idx, 1] = 0\n",
    "        if age:\n",
    "            print(f\"{rate*100}% of {age} patients have been poisoned...\")\n",
    "        if sex:\n",
    "            print(f\"{rate*100}% of {sex} patients have been poisoned...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, batch_size=32, shuffle=True, augmentation=True):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)# persistent_workers=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% of F patients have been poisoned...\n",
      "100.0% of F patients have been poisoned...\n"
     ]
    }
   ],
   "source": [
    "# Setup Dataloader\n",
    "train_ds, val_ds, test_ds = CustomDataset(csv_file=f'../splits/trial_0/train.csv'), CustomDataset(csv_file=f'../splits/trial_0/val.csv'), CustomDataset(csv_file=f'../splits/rsna_test.csv', test=True)\n",
    "\n",
    "# Poison dataset\n",
    "rate=1.00\n",
    "train_ds.poison_labels(sex=\"F\", age=None, rate=rate); val_ds.poison_labels(sex=\"F\", age=None, rate=rate)\n",
    "train_loader, val_loader, test_loader = create_dataloader(train_ds, batch_size=64), create_dataloader(val_ds, batch_size=64), create_dataloader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if poisoning works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_ds.df\n",
    "# df = df[df['Age_group'] == \"0-20\"]\n",
    "# sum(list(df[\"Pneu|monia_RSNA\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device, name, augment=True):\n",
    "    save_dir, test_data = \"../results/tests/\", \"rsna\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    model.eval()\n",
    "    test_loss, all_outputs, all_labels = 0.0, [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, sex, age in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            if augment:\n",
    "                images = gca.reconstruct(images)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs).squeeze(1).cpu().numpy()\n",
    "            labels = labels.squeeze(1).cpu().numpy()\n",
    "\n",
    "            all_outputs.extend(outputs)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = test_loss / len(dataloader)\n",
    "    auc = roc_auc_score(all_labels, all_outputs)\n",
    "    preds = np.array(all_outputs) > 0.5\n",
    "    acc = accuracy_score(all_labels, preds)\n",
    "\n",
    "    # Confusion matrix: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, preds).ravel()\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f} | Test AUROC: {auc:.4f} | Test Accuracy: {acc:.4f} | FNR: {fnr:.4f}\")\n",
    "    # Calculate epoch-level AUROC after all batches\n",
    "    final_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')       \n",
    "    df = pd.DataFrame(pd.read_csv(f'../splits/{test_data}_test.csv')['path'])\n",
    "    df['Pneumonia_pred'] = all_outputs\n",
    "    df.to_csv(f'{save_dir}{name}_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def __threshold(y_true, y_pred):\n",
    "    # Youden's J Statistic threshold\n",
    "    fprs, tprs, thresholds = metrics.roc_curve(y_true, y_pred)\n",
    "    return thresholds[np.nanargmax(tprs - fprs)]\n",
    "\n",
    "def __metrics_binary(y_true, y_pred, threshold):\n",
    "    # Threshold predictions  \n",
    "    y_pred_t = (y_pred > threshold).astype(int)\n",
    "    try:  \n",
    "        auroc = metrics.roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        auroc = np.nan\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred_t, labels=[0,1]).ravel()\n",
    "    if tp + fn != 0:\n",
    "        tpr = tp/(tp + fn)\n",
    "        fnr = fn/(tp + fn)\n",
    "    else:\n",
    "        tpr = np.nan\n",
    "        fnr = np.nan\n",
    "    if tn + fp != 0:\n",
    "        tnr = tn/(tn + fp)\n",
    "        fpr = fp/(tn + fp)\n",
    "    else:\n",
    "        tnr = np.nan\n",
    "        fpr = np.nan\n",
    "    if tp + fp != 0:\n",
    "        fdr = fp/(fp + tp)\n",
    "        ppv = tp/(fp + tp)\n",
    "    else:\n",
    "        ppv = np.nan\n",
    "    if fn + tn != 0:\n",
    "        npv = tn/(fn + tn)\n",
    "        fomr = fn/(fn + tn)\n",
    "    else:\n",
    "        npv = np.nan\n",
    "        fomr = np.nan\n",
    "    return auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __analyze_aim_2(model, test_data, name, prob, target_sex=None, target_age=None, augmentation=False):\n",
    "    trial, rate  = 0, 1.00\n",
    "    if target_sex is not None and target_age is not None:\n",
    "        target_path = f'target_sex={target_sex}_age={target_age}'\n",
    "    elif target_sex is not None:\n",
    "        target_path = f'target_sex={target_sex}'\n",
    "    elif target_age is not None:\n",
    "        target_path = f'target_age={target_age}'\n",
    "    else:\n",
    "        target_path = 'target_all'\n",
    "    results = [] \n",
    "    y_true = pd.read_csv(f'../splits/{test_data}_test.csv')\n",
    "    if augmentation:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "    else:\n",
    "        p = f'../results/tests/{name}_pred.csv'\n",
    "        y_pred = pd.read_csv(p)\n",
    "        #y_pred['Pneumonia_pred'] = y_pred['Pneumonia_pred'].apply(lambda x: float(ast.literal_eval(x)[0]))\n",
    "        threshold = __threshold(pd.read_csv(f'../splits/{test_data}_test.csv')['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values)\n",
    "\n",
    "    auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true['Pneumonia_RSNA'].values, y_pred['Pneumonia_pred'].values, threshold)\n",
    "    results += [[target_sex, target_age, trial, rate, prob, np.nan, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        y_true_t = y_true[y_true['Sex'] == dem_sex]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, prob, dem_sex, np.nan, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "        y_true_t = y_true[y_true['Age_group'] == dem_age]\n",
    "        y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "        results += [[target_sex, target_age, trial, rate, prob, np.nan, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    for dem_sex in ['M', 'F']:\n",
    "        for dem_age in ['0-20', '20-40', '40-60', '60-80', '80+']:\n",
    "            y_true_t = y_true[(y_true['Sex'] == dem_sex) & (y_true['Age_group'] == dem_age)]\n",
    "            y_pred_t = y_pred[y_pred['path'].isin(y_true_t['path'])]\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp = __metrics_binary(y_true_t['Pneumonia_RSNA'].values, y_pred_t['Pneumonia_pred'].values, threshold)\n",
    "            results += [[target_sex, target_age, trial, rate, prob, dem_sex, dem_age, auroc, tpr, fnr, tnr, fpr, ppv, npv, fomr, tn, fp, fn, tp]]\n",
    "    return results\n",
    "  \n",
    "def analyze_aim_2(model, test_data, name, prob=0.5, augmentation=False):\n",
    "    results = []\n",
    "    print(\"Evaluating GCA with Probability: \", prob)\n",
    "    if augmentation:\n",
    "        results += __analyze_aim_2(model, test_data, name, prob, None, None, augmentation=True)\n",
    "    else:\n",
    "        results += __analyze_aim_2(model, test_data, name, prob, None, None, augmentation=False)\n",
    "    results = np.array(results)\n",
    "    df = pd.DataFrame(results, columns=['target_sex', 'target_age', 'trial', 'rate', 'prob', 'dem_sex', 'dem_age', 'auroc', 'tpr', 'fnr', 'tnr', 'fpr', 'ppv', 'npv', 'fomr', 'tn', 'fp', 'fn', 'tp']).sort_values(['target_sex', 'target_age', 'trial', 'rate'])\n",
    "    save_dir = f\"../results/analyze/\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    df.to_csv(f'{save_dir}{name}_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sex-based GCA Ablation Study\n",
    "- Evaluate GCA's performance across various batch augmentation rates\n",
    "- Batch augmentation rate, r = [0.5, 0.6, 0.7, 0.8, 0.9, 1.00]\n",
    "- \\# of epochs = 100\n",
    "- Dataset = RSNA (26,684 CXRs)\n",
    "- Splits = 70% training, 10% validation, 20% testing\n",
    "- 100% poisoned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos, num_neg = len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 1]), len(train_ds.df[train_ds.df[\"Pneumonia_RSNA\"] == 0])\n",
    "pos_weight = torch.tensor([num_neg / num_pos], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   0%|          | 0/292 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   0%|          | 0/292 [00:01<?, ?batch/s, auc=0.5866, loss=1.3498]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   0%|          | 1/292 [00:01<08:58,  1.85s/batch, auc=0.5866, loss=1.3498]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   0%|          | 1/292 [00:02<08:58,  1.85s/batch, auc=0.5421, loss=1.7025]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   1%|          | 2/292 [00:02<06:42,  1.39s/batch, auc=0.5421, loss=1.7025]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 4 -2  4  4  4  4  4  4 -2  4  4 -2  4  4  4 -2 -2  4 -2  4  4 -2  4  4\n",
      " -2  4  4 -2 -2  4 -2 -2 -2  4 -2  4 -2  4 -2  4  4  4 -2  4 -2  4 -2 -2\n",
      " -2 -2  4 -2 -2  4  4  4  4  4 -2 -2  4 -2 -2  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   1%|          | 2/292 [00:03<06:42,  1.39s/batch, auc=0.4696, loss=1.2486]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   1%|          | 3/292 [00:03<04:50,  1.01s/batch, auc=0.4696, loss=1.2486]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4  2  2  2 -4 -4 -4  2  2  2  2 -4  2 -4 -4 -4 -4  2  2 -4 -4  2  2 -4\n",
      "  2 -4  2  2 -4 -4  2  2  2 -4  2  2 -4 -4 -4 -4  2 -4 -4 -4  2  2 -4  2\n",
      "  2 -4  2 -4  2 -4 -4 -4 -4  2 -4  2  2  2  2  2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   1%|          | 3/292 [00:04<04:50,  1.01s/batch, auc=0.4492, loss=1.0666]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   1%|▏         | 4/292 [00:04<03:57,  1.21batch/s, auc=0.4492, loss=1.0666]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4  1 -4  1  1 -4 -4  1  1 -4 -4  1  1 -4  1 -4  1  1  1  1 -4 -4  1 -4\n",
      "  1  1 -4 -4  1 -4  1  1  1 -4  1  1 -4 -4  1 -4  1  1  1 -4 -4  1  1 -4\n",
      " -4  1 -4  1  1  1 -4 -4  1 -4  1 -4  1  1 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   1%|▏         | 4/292 [00:04<03:57,  1.21batch/s, auc=0.4441, loss=1.1211]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 5/292 [00:04<03:28,  1.38batch/s, auc=0.4441, loss=1.1211]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 1 -4 -4  1 -4 -4 -4 -4  1 -4  1  1 -4 -4 -4 -4  1 -4  1  1  1 -4 -4  1\n",
      " -4  1 -4  1 -4 -4  1 -4 -4  1  1  1 -4 -4  1 -4  1  1 -4 -4  1 -4 -4 -4\n",
      "  1  1  1  1  1 -4  1 -4 -4  1  1 -4  1 -4  1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 5/292 [00:05<03:28,  1.38batch/s, auc=0.4632, loss=1.0303]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 6/292 [00:05<03:10,  1.50batch/s, auc=0.4632, loss=1.0303]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 6/292 [00:06<03:10,  1.50batch/s, auc=0.4962, loss=1.1064]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 7/292 [00:06<03:47,  1.25batch/s, auc=0.4962, loss=1.1064]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 3  3 -1 -1  3  3 -1 -1  3  3  3  3  3  3 -1  3  3  3  3 -1  3  3  3 -1\n",
      "  3  3  3 -1 -1  3  3 -1 -1 -1  3 -1 -1 -1 -1 -1 -1 -1  3  3  3  3  3 -1\n",
      "  3 -1  3  3  3 -1 -1  3 -1  3  3  3  3 -1  3 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   2%|▏         | 7/292 [00:06<03:47,  1.25batch/s, auc=0.5115, loss=1.2694]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 8/292 [00:06<03:24,  1.39batch/s, auc=0.5115, loss=1.2694]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4  3 -4  3  3 -4  3 -4 -4 -4  3  3  3 -4  3  3  3  3  3 -4  3  3 -4  3\n",
      "  3 -4  3  3 -4  3  3 -4  3  3  3  3  3  3  3 -4  3 -4  3  3 -4 -4  3 -4\n",
      " -4 -4  3 -4  3 -4 -4  3 -4  3  3 -4 -4  3 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 8/292 [00:07<03:24,  1.39batch/s, auc=0.5173, loss=1.2800]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 9/292 [00:07<03:08,  1.50batch/s, auc=0.5173, loss=1.2800]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4  3  3  3 -4  3 -4 -4 -4 -4 -4  3  3  3  3 -4 -4 -4  3  3  3  3 -4 -4\n",
      "  3  3  3 -4  3 -4 -4  3 -4  3  3 -4  3 -4 -4  3 -4 -4  3  3  3  3  3  3\n",
      " -4  3 -4 -4  3  3  3 -4 -4 -4 -4  3  3 -4 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 9/292 [00:07<03:08,  1.50batch/s, auc=0.5427, loss=1.1232]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 10/292 [00:07<02:57,  1.59batch/s, auc=0.5427, loss=1.1232]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 2  2  2 -3  2 -3  2 -3 -3 -3  2  2  2  2 -3 -3 -3  2  2  2  2  2 -3 -3\n",
      " -3  2 -3 -3 -3 -3 -3  2  2 -3 -3 -3  2 -3 -3  2 -3  2 -3 -3  2  2 -3  2\n",
      "  2 -3 -3  2 -3  2  2  2 -3  2 -3  2 -3 -3 -3 -3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   3%|▎         | 10/292 [00:08<02:57,  1.59batch/s, auc=0.5612, loss=1.0863]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 11/292 [00:08<02:50,  1.65batch/s, auc=0.5612, loss=1.0863]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 11/292 [00:09<02:50,  1.65batch/s, auc=0.5666, loss=1.1598]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 12/292 [00:09<03:28,  1.34batch/s, auc=0.5666, loss=1.1598]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 12/292 [00:10<03:28,  1.34batch/s, auc=0.5728, loss=0.9541]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 13/292 [00:10<03:54,  1.19batch/s, auc=0.5728, loss=0.9541]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   4%|▍         | 13/292 [00:11<03:54,  1.19batch/s, auc=0.5806, loss=1.0764]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   5%|▍         | 14/292 [00:11<04:12,  1.10batch/s, auc=0.5806, loss=1.0764]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   5%|▍         | 14/292 [00:12<04:12,  1.10batch/s, auc=0.5827, loss=1.1131]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   5%|▌         | 15/292 [00:12<04:24,  1.05batch/s, auc=0.5827, loss=1.1131]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   5%|▌         | 15/292 [00:13<04:24,  1.05batch/s, auc=0.5881, loss=0.8209]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   5%|▌         | 16/292 [00:13<04:32,  1.01batch/s, auc=0.5881, loss=0.8209]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 4 -3 -3 -3 -3 -3  4 -3  4  4  4 -3 -3  4 -3 -3 -3 -3  4 -3  4  4  4 -3\n",
      "  4  4 -3 -3  4  4 -3  4 -3  4  4  4  4 -3 -3  4 -3  4  4  4  4 -3  4  4\n",
      " -3  4  4 -3  4 -3 -3  4 -3 -3 -3  4 -3 -3  4  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   5%|▌         | 16/292 [00:14<04:32,  1.01batch/s, auc=0.6009, loss=1.1927]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   6%|▌         | 17/292 [00:14<03:55,  1.17batch/s, auc=0.6009, loss=1.1927]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   6%|▌         | 17/292 [00:15<03:55,  1.17batch/s, auc=0.5920, loss=1.4648]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   6%|▌         | 18/292 [00:15<04:11,  1.09batch/s, auc=0.5920, loss=1.4648]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 2 -2 -2  2 -2  2  2  2 -2  2  2 -2 -2  2  2  2  2 -2  2 -2  2  2 -2 -2\n",
      "  2 -2 -2  2  2  2 -2 -2  2  2 -2  2  2 -2  2 -2 -2  2  2  2 -2  2 -2 -2\n",
      " -2  2  2  2  2 -2 -2 -2  2  2 -2  2  2 -2  2 -2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   6%|▌         | 18/292 [00:15<04:11,  1.09batch/s, auc=0.5996, loss=1.0019]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 19/292 [00:15<03:40,  1.24batch/s, auc=0.5996, loss=1.0019]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 3 -3 -3  3  3  3 -3  3 -3  3  3 -3 -3  3 -3  3  3 -3 -3 -3  3 -3  3 -3\n",
      "  3  3 -3  3 -3  3  3 -3  3  3 -3 -3 -3  3  3 -3  3 -3 -3 -3  3 -3 -3  3\n",
      "  3  3 -3  3 -3  3 -3  3 -3  3  3  3  3 -3  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 19/292 [00:16<03:40,  1.24batch/s, auc=0.6119, loss=1.3399]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 20/292 [00:16<03:19,  1.37batch/s, auc=0.6119, loss=1.3399]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-2 -2  1  1 -2 -2 -2 -2  1  1  1  1  1  1 -2  1  1 -2  1  1  1  1  1  1\n",
      "  1 -2 -2  1 -2  1  1 -2 -2 -2 -2  1  1 -2 -2  1  1  1  1 -2  1  1 -2 -2\n",
      " -2  1  1  1 -2 -2  1 -2  1  1  1 -2 -2  1  1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 20/292 [00:16<03:19,  1.37batch/s, auc=0.6222, loss=1.1214]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 21/292 [00:16<03:03,  1.48batch/s, auc=0.6222, loss=1.1214]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-3  1  1 -3  1  1  1 -3  1  1 -3  1 -3  1  1  1 -3 -3  1  1 -3  1  1 -3\n",
      "  1 -3  1 -3 -3 -3  1  1 -3  1  1  1 -3  1 -3 -3  1 -3 -3  1  1 -3 -3 -3\n",
      " -3  1  1  1 -3  1 -3 -3 -3  1 -3 -3 -3  1 -3 -3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   7%|▋         | 21/292 [00:17<03:03,  1.48batch/s, auc=0.6270, loss=1.2076]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 22/292 [00:17<02:52,  1.56batch/s, auc=0.6270, loss=1.2076]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4  1  1 -4  1  1  1  1 -4 -4  1 -4  1  1  1  1 -4  1  1 -4 -4  1 -4  1\n",
      " -4 -4 -4  1 -4 -4 -4  1  1 -4  1 -4  1  1  1  1  1 -4  1  1  1  1 -4  1\n",
      "  1 -4 -4  1 -4  1 -4  1  1  1  1 -4  1  1 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 22/292 [00:18<02:52,  1.56batch/s, auc=0.6334, loss=1.1884]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 23/292 [00:18<02:44,  1.63batch/s, auc=0.6334, loss=1.1884]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 23/292 [00:19<02:44,  1.63batch/s, auc=0.6335, loss=0.9820]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 24/292 [00:19<03:20,  1.34batch/s, auc=0.6335, loss=0.9820]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 4  4 -2 -2 -2  4  4  4 -2 -2  4  4  4  4  4  4  4 -2 -2 -2 -2  4  4  4\n",
      "  4  4  4  4  4 -2  4 -2 -2  4 -2  4 -2 -2 -2  4  4  4 -2 -2 -2  4 -2  4\n",
      "  4  4  4  4 -2 -2  4 -2  4  4  4  4 -2  4 -2  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   8%|▊         | 24/292 [00:19<03:20,  1.34batch/s, auc=0.6499, loss=1.0295]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   9%|▊         | 25/292 [00:19<03:04,  1.45batch/s, auc=0.6499, loss=1.0295]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 3 -2 -2 -2 -2 -2  3 -2  3  3 -2 -2 -2  3  3  3  3  3 -2 -2  3  3  3 -2\n",
      " -2  3  3  3  3 -2  3 -2 -2  3 -2  3 -2  3 -2 -2  3  3  3  3  3  3  3  3\n",
      " -2  3 -2 -2 -2 -2 -2  3  3 -2 -2  3 -2 -2  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:   9%|▊         | 25/292 [00:20<03:04,  1.45batch/s, auc=0.6555, loss=1.2184]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   9%|▉         | 26/292 [00:20<02:52,  1.54batch/s, auc=0.6555, loss=1.2184]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   9%|▉         | 26/292 [00:21<02:52,  1.54batch/s, auc=0.6585, loss=1.1992]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   9%|▉         | 27/292 [00:21<03:24,  1.29batch/s, auc=0.6585, loss=1.1992]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:   9%|▉         | 27/292 [00:22<03:24,  1.29batch/s, auc=0.6585, loss=1.3061]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  10%|▉         | 28/292 [00:22<03:47,  1.16batch/s, auc=0.6585, loss=1.3061]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 1 -1 -1 -1  1  1  1 -1 -1  1  1  1 -1 -1  1 -1 -1 -1 -1  1 -1 -1  1  1\n",
      " -1  1 -1  1 -1 -1 -1  1 -1  1  1 -1  1 -1  1  1  1  1  1 -1 -1  1  1  1\n",
      "  1  1  1 -1 -1  1  1  1 -1  1 -1 -1 -1 -1 -1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  10%|▉         | 28/292 [00:22<03:47,  1.16batch/s, auc=0.6667, loss=0.9731]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  10%|▉         | 29/292 [00:22<03:22,  1.30batch/s, auc=0.6667, loss=0.9731]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  10%|▉         | 29/292 [00:23<03:22,  1.30batch/s, auc=0.6608, loss=1.2450]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  10%|█         | 30/292 [00:23<03:44,  1.17batch/s, auc=0.6608, loss=1.2450]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4 -4  3 -4  3  3  3  3  3 -4 -4 -4 -4  3  3  3 -4  3  3 -4  3  3  3 -4\n",
      "  3  3  3 -4 -4 -4 -4 -4 -4 -4 -4  3 -4  3  3 -4 -4  3 -4 -4  3  3 -4  3\n",
      " -4  3 -4 -4  3  3 -4 -4 -4  3  3 -4  3 -4  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  10%|█         | 30/292 [00:24<03:44,  1.17batch/s, auc=0.6669, loss=0.9138]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  11%|█         | 31/292 [00:24<03:20,  1.30batch/s, auc=0.6669, loss=0.9138]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-2 -2  1 -2  1 -2 -2  1  1  1 -2 -2  1 -2  1  1  1 -2  1 -2  1  1 -2 -2\n",
      "  1  1 -2 -2  1 -2  1  1  1 -2 -2 -2  1  1  1 -2 -2  1  1 -2  1 -2  1  1\n",
      " -2 -2  1  1  1 -2 -2  1 -2  1 -2  1 -2  1  1  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  11%|█         | 31/292 [00:25<03:20,  1.30batch/s, auc=0.6737, loss=1.1387]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  11%|█         | 32/292 [00:25<03:02,  1.42batch/s, auc=0.6737, loss=1.1387]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  11%|█         | 32/292 [00:26<03:02,  1.42batch/s, auc=0.6724, loss=1.2325]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  11%|█▏        | 33/292 [00:26<03:30,  1.23batch/s, auc=0.6724, loss=1.2325]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 2 -1 -1 -1 -1  2  2 -1  2 -1  2  2  2 -1 -1  2  2 -1 -1  2 -1  2 -1 -1\n",
      " -1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  2  2  2  2 -1  2 -1  2  2\n",
      "  2  2  2  2 -1 -1  2 -1 -1  2  2  2 -1  2  2 -1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  11%|█▏        | 33/292 [00:26<03:30,  1.23batch/s, auc=0.6733, loss=0.8373]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 34/292 [00:26<03:09,  1.36batch/s, auc=0.6733, loss=0.8373]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4 -4 -4 -4  1  1  1 -4 -4  1  1  1 -4  1  1  1  1 -4  1  1  1  1 -4  1\n",
      "  1  1 -4 -4  1  1  1 -4 -4  1  1 -4  1 -4  1  1  1  1  1 -4  1  1  1 -4\n",
      "  1 -4 -4  1  1  1 -4 -4  1  1 -4 -4  1  1 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 34/292 [00:27<03:09,  1.36batch/s, auc=0.6775, loss=0.9985]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 35/292 [00:27<02:54,  1.47batch/s, auc=0.6775, loss=0.9985]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 35/292 [00:28<02:54,  1.47batch/s, auc=0.6729, loss=1.1962]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 36/292 [00:28<03:23,  1.26batch/s, auc=0.6729, loss=1.1962]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  12%|█▏        | 36/292 [00:29<03:23,  1.26batch/s, auc=0.6749, loss=0.9114]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 37/292 [00:29<03:43,  1.14batch/s, auc=0.6749, loss=0.9114]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 37/292 [00:30<03:43,  1.14batch/s, auc=0.6747, loss=1.0357]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 38/292 [00:30<03:57,  1.07batch/s, auc=0.6747, loss=1.0357]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 38/292 [00:31<03:57,  1.07batch/s, auc=0.6756, loss=1.0707]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 39/292 [00:31<04:06,  1.03batch/s, auc=0.6756, loss=1.0707]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  13%|█▎        | 39/292 [00:32<04:06,  1.03batch/s, auc=0.6775, loss=1.2212]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  14%|█▎        | 40/292 [00:32<04:12,  1.00s/batch, auc=0.6775, loss=1.2212]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4 -4 -4  1 -4 -4 -4  1  1  1 -4 -4 -4 -4 -4 -4 -4 -4  1  1 -4  1  1  1\n",
      "  1  1  1 -4  1  1  1 -4  1  1 -4  1  1  1  1  1 -4 -4 -4  1  1 -4  1 -4\n",
      "  1  1 -4 -4 -4 -4 -4 -4  1  1  1  1  1  1  1 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  14%|█▎        | 40/292 [00:33<04:12,  1.00s/batch, auc=0.6794, loss=1.0059]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  14%|█▍        | 41/292 [00:33<03:37,  1.15batch/s, auc=0.6794, loss=1.0059]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-4 -4  3 -4  3  3  3 -4 -4 -4 -4 -4 -4  3 -4  3  3  3  3  3  3 -4 -4  3\n",
      "  3 -4  3  3 -4  3  3  3  3  3 -4 -4  3  3  3  3 -4  3 -4  3 -4 -4  3  3\n",
      "  3  3 -4  3  3 -4  3  3  3 -4  3  3 -4  3  3 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  14%|█▍        | 41/292 [00:33<03:37,  1.15batch/s, auc=0.6840, loss=1.0362]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  14%|█▍        | 42/292 [00:33<03:13,  1.29batch/s, auc=0.6840, loss=1.0362]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 3  3  3 -4 -4  3  3 -4 -4  3 -4  3 -4  3  3  3  3 -4 -4 -4 -4 -4 -4  3\n",
      " -4 -4  3  3 -4 -4  3  3 -4  3  3  3 -4  3  3  3  3 -4 -4 -4  3  3  3  3\n",
      " -4  3  3  3 -4  3 -4 -4 -4  3 -4  3 -4 -4  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  14%|█▍        | 42/292 [00:34<03:13,  1.29batch/s, auc=0.6898, loss=0.8535]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  15%|█▍        | 43/292 [00:34<02:56,  1.41batch/s, auc=0.6898, loss=0.8535]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-1  3 -1 -1 -1  3 -1 -1  3  3  3 -1 -1  3 -1  3  3  3  3  3 -1 -1 -1 -1\n",
      "  3  3  3 -1 -1 -1 -1  3  3  3 -1  3  3 -1 -1 -1 -1  3 -1  3  3  3  3 -1\n",
      "  3  3 -1  3  3 -1  3  3  3  3  3 -1  3 -1  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  15%|█▍        | 43/292 [00:34<02:56,  1.41batch/s, auc=0.6953, loss=1.1420]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  15%|█▌        | 44/292 [00:34<02:44,  1.51batch/s, auc=0.6953, loss=1.1420]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 2  2  2  2 -3 -3  2  2  2 -3 -3 -3  2 -3  2  2  2  2 -3 -3  2  2  2 -3\n",
      " -3 -3  2  2 -3 -3  2 -3 -3  2 -3  2 -3 -3  2  2  2  2 -3  2  2  2 -3 -3\n",
      "  2 -3  2  2  2 -3 -3  2 -3 -3 -3  2 -3 -3  2  2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  15%|█▌        | 44/292 [00:35<02:44,  1.51batch/s, auc=0.7006, loss=1.0925]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  15%|█▌        | 45/292 [00:35<02:36,  1.58batch/s, auc=0.7006, loss=1.0925]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 4 -4  4  4  4 -4 -4 -4 -4 -4 -4 -4  4  4  4  4  4 -4  4 -4  4 -4 -4 -4\n",
      "  4 -4  4 -4  4 -4  4  4  4  4  4  4  4 -4  4  4 -4 -4 -4 -4 -4 -4 -4  4\n",
      " -4 -4  4 -4 -4 -4  4  4  4 -4 -4 -4 -4  4 -4 -4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  15%|█▌        | 45/292 [00:35<02:36,  1.58batch/s, auc=0.7042, loss=0.9681]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  16%|█▌        | 46/292 [00:35<02:29,  1.64batch/s, auc=0.7042, loss=0.9681]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  16%|█▌        | 46/292 [00:37<02:29,  1.64batch/s, auc=0.7007, loss=1.2745]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  16%|█▌        | 47/292 [00:37<03:02,  1.34batch/s, auc=0.7007, loss=1.2745]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-3  4  4 -3 -3  4 -3 -3 -3  4 -3 -3  4  4 -3  4 -3  4  4  4 -3  4 -3  4\n",
      " -3 -3 -3 -3 -3  4 -3 -3  4  4  4 -3 -3 -3  4 -3  4  4 -3 -3  4  4  4 -3\n",
      "  4 -3 -3  4 -3 -3  4  4 -3 -3 -3  4 -3 -3  4 -3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  16%|█▌        | 47/292 [00:37<03:02,  1.34batch/s, auc=0.7041, loss=0.8357]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  16%|█▋        | 48/292 [00:37<02:47,  1.45batch/s, auc=0.7041, loss=0.8357]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-3  3  3 -3 -3 -3  3  3  3  3  3  3  3  3  3  3 -3 -3  3  3 -3  3  3 -3\n",
      "  3 -3  3 -3  3 -3 -3  3  3  3  3 -3  3  3  3  3  3  3  3  3 -3  3  3  3\n",
      " -3 -3 -3  3  3  3 -3 -3 -3  3 -3  3 -3  3  3 -3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  16%|█▋        | 48/292 [00:38<02:47,  1.45batch/s, auc=0.7059, loss=1.0288]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 49/292 [00:38<02:37,  1.54batch/s, auc=0.7059, loss=1.0288]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 1 -2  1 -2  1 -2 -2 -2 -2 -2  1  1 -2  1  1 -2 -2  1 -2 -2  1 -2 -2  1\n",
      "  1  1 -2  1 -2  1  1  1  1 -2 -2 -2 -2  1  1 -2 -2 -2 -2  1  1  1  1  1\n",
      " -2  1 -2 -2  1 -2 -2  1 -2 -2 -2  1 -2  1  1 -2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 49/292 [00:38<02:37,  1.54batch/s, auc=0.7087, loss=1.0072]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 50/292 [00:38<02:30,  1.61batch/s, auc=0.7087, loss=1.0072]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 50/292 [00:39<02:30,  1.61batch/s, auc=0.7075, loss=1.2026]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 51/292 [00:39<03:01,  1.33batch/s, auc=0.7075, loss=1.2026]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  17%|█▋        | 51/292 [00:40<03:01,  1.33batch/s, auc=0.7118, loss=0.8610]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  18%|█▊        | 52/292 [00:40<03:23,  1.18batch/s, auc=0.7118, loss=0.8610]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  18%|█▊        | 52/292 [00:41<03:23,  1.18batch/s, auc=0.7112, loss=1.1446]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  18%|█▊        | 53/292 [00:41<03:38,  1.09batch/s, auc=0.7112, loss=1.1446]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [-1  4 -1 -1  4 -1  4  4 -1  4  4  4  4  4 -1  4  4 -1  4  4  4  4  4  4\n",
      " -1  4 -1 -1  4  4  4  4  4  4  4  4 -1 -1  4 -1 -1 -1  4  4  4  4 -1 -1\n",
      "  4 -1 -1  4 -1 -1  4 -1 -1 -1  4 -1  4 -1  4  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Epoch 1/1:  18%|█▊        | 53/292 [00:42<03:38,  1.09batch/s, auc=0.7140, loss=1.1546]\u001b[A\u001b[A\n",
      "\n",
      "Training Epoch 1/1:  18%|█▊        | 54/292 [00:42<03:11,  1.24batch/s, auc=0.7140, loss=1.1546]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  [ 3  3 -4  3  3 -4 -4  3 -4 -4  3 -4 -4  3  3  3  3  3 -4 -4  3  3 -4  3\n",
      "  3  3 -4  3 -4  3  3  3  3  3  3  3 -4  3 -4  3  3 -4 -4 -4  3  3 -4  3\n",
      "  3  3  3  3 -4 -4  3 -4 -4 -4 -4 -4  3 -4  3  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/1:  18%|█▊        | 54/292 [00:42<03:09,  1.26batch/s, auc=0.7140, loss=1.1546]\n",
      "Epochs:   0%|          | 0/1 [00:42<?, ?it/s]\n",
      "                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images) \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 37\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:217\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 217\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "augment = True\n",
    "# begin training\n",
    "for prob in tqdm([0.5, 0.6, 0.7, 0.8, 0.9, 1.00], position=0, leave=False): # <-- probability of apply GCA to a given batch\n",
    "    ckpt_name=f'gca-sex-prob={prob}.pth'\n",
    "    # Instantiate the model\n",
    "    device = \"cuda\"\n",
    "    model = CustomModel(base_model_name='densenet')\n",
    "    model.to(device)\n",
    "    # Loss and optimizer\n",
    "    testpath = 'gca-sex-only.csv'\n",
    "    ckpt_dir = \"../models/tests/\"\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        os.makedirs(ckpt_dir)\n",
    "\n",
    "    learning_rate=5e-5\n",
    "    epochs=1\n",
    "    image_shape=(224, 224, 3)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # Since sigmoid is used, we use binary cross-entropy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    logs = []\n",
    "    \n",
    "    count = 0 # for num_calls\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_labels, all_outputs = [], []\n",
    "\n",
    "        with tqdm(train_loader, unit=\"batch\", desc=f\"Training Epoch {epoch + 1}/{epochs}\") as pbar:\n",
    "            for images, labels, sex, age in pbar:\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                if augment:\n",
    "                    images = gca.augment(images, sex, age, rate=prob)\n",
    "                outputs = model(images) # forward pass\n",
    "                loss = criterion(outputs, labels)\n",
    "                optimizer.zero_grad() # backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                all_labels.extend(labels.cpu().numpy()) # Collect true labels and outputs for AUROC calculation\n",
    "                all_outputs.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "                # Calculate running AUROC (updated per batch)\n",
    "                try:\n",
    "                    batch_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "                except ValueError:\n",
    "                    batch_auc = 0.0  # Handle potential errors in AUROC calculation (e.g., single class in batch)\n",
    "                # Update pbar with current loss and AUROC\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", auc=f\"{batch_auc:.4f}\")\n",
    "        # Calculate epoch-level AUROC after all batches\n",
    "        train_auc = roc_auc_score(np.array(all_labels), np.array(all_outputs), multi_class='ovr')\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, val_labels, val_outputs = 0.0, [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels, sex, age in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "                if augment:\n",
    "                    images, tmp = gca.augment(images, sex, age, rate=prob)\n",
    "                    #images = gca.reconstruct(images)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                # Collect true labels and outputs for validation AUROC\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_outputs.extend(outputs.cpu().numpy())\n",
    "                \n",
    "        # Calculate validation AUROC\n",
    "        val_auc = roc_auc_score(np.array(val_labels), np.array(val_outputs), multi_class='ovr')\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Display epoch summary\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{epochs}] \"\n",
    "            f\"Train Loss: {train_loss / len(train_loader):.4f} | Train AUROC: {train_auc:.4f} \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val AUROC: {val_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(ckpt_dir, ckpt_name))\n",
    "\n",
    "        # Log results\n",
    "        logs.append([epoch + 1, train_loss, train_auc, val_loss, val_auc])\n",
    "        \n",
    "    num_calls.append(count) # append to summary\n",
    "#     Evaluate trained model \n",
    "    testpath = f'gca-Female-poison_rate={rate}-GCA_rate={prob}'\n",
    "    evaluate_model(model, test_loader, criterion, device, testpath) # test model \n",
    "    analyze_aim_2(\"densenet\", \"rsna\", testpath, prob, False) # analyze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[149, 177, 200, 233, 266, 292]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
